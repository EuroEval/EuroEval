{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#evaluation-of-pretrained-language-models-on-mono-or-multilingual-language-tasks","title":"Evaluation of pretrained language models on mono- or multilingual language tasks.","text":""},{"location":"#maintainers","title":"Maintainers","text":"<ul> <li>Dan Saattrup Nielsen (@saattrupdan, dan.nielsen@alexandra.dk)</li> <li>Kenneth Enevoldsen (@KennethEnevoldsen, kenneth.enevoldsen@cas.au.dk)</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To install the package simply write the following command in your favorite terminal: <pre><code>$ pip install scandeval[all]\n</code></pre></p> <p>This will install the ScandEval package with all extras. You can also install the minimal version by leaving out the <code>[all]</code>, in which case the package will let you know when an evaluation requires a certain extra dependency, and how you install it.</p>"},{"location":"#quickstart","title":"Quickstart","text":""},{"location":"#benchmarking-from-the-command-line","title":"Benchmarking from the Command Line","text":"<p>The easiest way to benchmark pretrained models is via the command line interface. After having installed the package, you can benchmark your favorite model like so: <pre><code>$ scandeval --model &lt;model-id&gt;\n</code></pre></p> <p>Here <code>model</code> is the HuggingFace model ID, which can be found on the HuggingFace Hub. By default this will benchmark the model on all the tasks available. If you want to benchmark on a particular task, then use the <code>--task</code> argument: <pre><code>$ scandeval --model &lt;model-id&gt; --task sentiment-classification\n</code></pre></p> <p>We can also narrow down which languages we would like to benchmark on. This can be done by setting the <code>--language</code> argument. Here we thus benchmark the model on the Danish sentiment classification task: <pre><code>$ scandeval --model &lt;model-id&gt; --task sentiment-classification --language da\n</code></pre></p> <p>Multiple models, datasets and/or languages can be specified by just attaching multiple arguments. Here is an example with two models: <pre><code>$ scandeval --model &lt;model-id1&gt; --model &lt;model-id2&gt;\n</code></pre></p> <p>The specific model version/revision to use can also be added after the suffix '@': <pre><code>$ scandeval --model &lt;model-id&gt;@&lt;commit&gt;\n</code></pre></p> <p>This can be a branch name, a tag name, or a commit id. It defaults to 'main' for latest.</p> <p>See all the arguments and options available for the <code>scandeval</code> command by typing <pre><code>$ scandeval --help\n</code></pre></p>"},{"location":"#benchmarking-from-a-script","title":"Benchmarking from a Script","text":"<p>In a script, the syntax is similar to the command line interface. You simply initialise an object of the <code>Benchmarker</code> class, and call this benchmark object with your favorite model: <pre><code>&gt;&gt;&gt; from scandeval import Benchmarker\n&gt;&gt;&gt; benchmark = Benchmarker()\n&gt;&gt;&gt; benchmark(model=\"&lt;model&gt;\")\n</code></pre></p> <p>To benchmark on a specific task and/or language, you simply specify the <code>task</code> or <code>language</code> arguments, shown here with same example as above: <pre><code>&gt;&gt;&gt; benchmark(model=\"&lt;model&gt;\", task=\"sentiment-classification\", language=\"da\")\n</code></pre></p> <p>If you want to benchmark a subset of all the models on the Hugging Face Hub, you can simply leave out the <code>model</code> argument. In this example, we're benchmarking all Danish models on the Danish sentiment classification task: <pre><code>&gt;&gt;&gt; benchmark(task=\"sentiment-classification\", language=\"da\")\n</code></pre></p>"},{"location":"#benchmarking-from-docker","title":"Benchmarking from Docker","text":"<p>A Dockerfile is provided in the repo, which can be downloaded and run, without needing to clone the repo and installing from source. This can be fetched programmatically by running the following: <pre><code>$ wget https://raw.githubusercontent.com/ScandEval/ScandEval/main/Dockerfile.cuda\n</code></pre></p> <p>Next, to be able to build the Docker image, first ensure that the NVIDIA Container Toolkit is installed and configured. Ensure that the the CUDA version stated at the top of the Dockerfile matches the CUDA version installed (which you can check using <code>nvidia-smi</code>). After that, we build the image as follows: <pre><code>$ docker build --pull -t scandeval -f Dockerfile.cuda .\n</code></pre></p> <p>With the Docker image built, we can now evaluate any model as follows: <pre><code>$ docker run -e args=\"&lt;scandeval-arguments&gt;\" --gpus 1 --name scandeval --rm scandeval\n</code></pre></p> <p>Here <code>&lt;scandeval-arguments&gt;</code> consists of the arguments added to the <code>scandeval</code> CLI argument. This could for instance be <code>--model &lt;model-id&gt; --task sentiment-classification</code>.</p>"},{"location":"#special-thanks-pray","title":"Special Thanks :pray:","text":"<ul> <li>Thanks to OpenAI for sponsoring OpenAI credits as part of their   Researcher Access Program.</li> <li>Thanks to UWV and KU   Leuven for sponsoring the Azure OpenAI   credits used to evaluate GPT-4-turbo in Dutch.</li> <li>Thanks to Mi\u00f0eind for sponsoring the OpenAI   credits used to evaluate GPT-4-turbo in Icelandic and Faroese.</li> <li>Thanks to CHC for sponsoring the OpenAI credits used to   evaluate GPT-4-turbo in German.</li> </ul>"},{"location":"#citing-scandeval","title":"Citing ScandEval","text":"<p>If you want to cite the framework then feel free to use this:</p> <pre><code>@article{nielsen2024encoder,\n  title={Encoder vs Decoder: Comparative Analysis of Encoder and Decoder Language Models on Multilingual NLU Tasks},\n  author={Nielsen, Dan Saattrup and Enevoldsen, Kenneth and Schneider-Kamp, Peter},\n  journal={arXiv preprint arXiv:2406.13469},\n  year={2024}\n}\n@inproceedings{nielsen2023scandeval,\n  author = {Nielsen, Dan Saattrup},\n  booktitle = {Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  month = may,\n  pages = {185--201},\n  title = {{ScandEval: A Benchmark for Scandinavian Natural Language Processing}},\n  year = {2023}\n}\n</code></pre>"},{"location":"#remarks","title":"Remarks","text":"<p>The image used in the logo has been created by the amazing Scandinavia and the World team. Go check them out!</p>"},{"location":"api/scandeval/","title":"scandeval","text":"scandeval<p> source package scandeval </p> <p>ScandEval - A benchmarking framework for language models.</p> <p> Modules </p> <ul> <li> <p>scandeval.benchmark_config_factory \u2014 Factory class for creating dataset configurations.</p> </li> <li> <p>scandeval.config \u2014 Configuration classes used throughout the project.</p> </li> <li> <p>scandeval.dataset_configs \u2014 All dataset configurations used in ScandEval.</p> </li> <li> <p>scandeval.enums \u2014 Enums used in the project.</p> </li> <li> <p>scandeval.exceptions \u2014 Exceptions to used by other functions.</p> </li> <li> <p>scandeval.languages \u2014 List of languages and their ISO 639-1 codes.</p> </li> <li> <p>scandeval.tasks \u2014 All benchmarks tasks used in ScandEval.</p> </li> </ul>"},{"location":"src/scandeval/","title":"scandeval","text":"scandeval<p> docs package scandeval </p> <pre><code>\"\"\"ScandEval - A benchmarking framework for language models.\"\"\"\n\nimport importlib.metadata\nimport logging\nimport os\nimport sys\n\nfrom dotenv import load_dotenv\nfrom termcolor import colored\n\nfrom .benchmarker import Benchmarker\nfrom .utils import block_terminal_output\n\n# Fetches the version of the package as defined in pyproject.toml\n__version__ = importlib.metadata.version(__package__)\n\n\n# Block unwanted terminal outputs\nblock_terminal_output()\n\n\n# Loads environment variables\nload_dotenv()\n\n\n# Set up logging\nfmt = colored(\"%(asctime)s\", \"light_blue\") + \" \u22c5 \" + colored(\"%(message)s\", \"green\")\nlogging.basicConfig(\n    level=logging.CRITICAL if hasattr(sys, \"_called_from_test\") else logging.INFO,\n    format=fmt,\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n)\n\n\n# Disable parallelisation when tokenizing, as that can lead to errors\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n\n# Enable MPS fallback\nos.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n\n\n# Set amount of threads per GPU - this is the default and is only set to prevent a\n# warning from showing\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\n</code></pre>"},{"location":"api/scandeval/benchmarker/","title":"Benchmarker","text":"<p>Failure</p> <p>module 'scandeval.benchmarker' not found.</p>"},{"location":"src/scandeval/benchmarker/","title":"Benchmarker","text":"<p>Failure</p> <p>module 'scandeval.benchmarker' not found.</p>"},{"location":"api/scandeval/benchmark_config_factory/","title":"scandeval.benchmark_config_factory","text":"scandeval.benchmark_config_factory<p> source module scandeval.benchmark_config_factory </p> <p>Factory class for creating dataset configurations.</p> <p> Functions </p> <ul> <li> <p>build_benchmark_config \u2014 Create a benchmark configuration.</p> </li> <li> <p>get_correct_language_codes \u2014 Get correct language code(s).</p> </li> <li> <p>prepare_languages \u2014 Prepare language(s) for benchmarking.</p> </li> <li> <p>prepare_tasks_and_datasets \u2014 Prepare task(s) and dataset(s) for benchmarking.</p> </li> <li> <p>prepare_device \u2014 Prepare device for benchmarking.</p> </li> </ul> <p> source build_benchmark_config(progress_bar: bool, save_results: bool, task: str | list[str] | None, dataset: str | list[str] | None, language: str | list[str], model_language: str | list[str] | None, dataset_language: str | list[str] | None, framework: Framework | str | None, device: Device | None, batch_size: int, evaluate_train: bool, raise_errors: bool, cache_dir: str, token: bool | str | None, openai_api_key: str | None, prefer_azure: bool, azure_openai_api_key: str | None, azure_openai_endpoint: str | None, azure_openai_api_version: str | None, force: bool, verbose: bool, trust_remote_code: bool, load_in_4bit: bool | None, use_flash_attention: bool | None, clear_model_cache: bool, only_validation_split: bool, few_shot: bool, num_iterations: int, debug: bool, run_with_cli: bool, first_time: bool = False) \u2192 BenchmarkConfig </p> <p>Create a benchmark configuration.</p> <p> Parameters </p> <ul> <li> <p>progress_bar :  bool \u2014</p> <p>Whether to show a progress bar when running the benchmark.</p> </li> <li> <p>save_results :  bool \u2014</p> <p>Whether to save the benchmark results to a file.</p> </li> <li> <p>task :  str | list[str] | None \u2014</p> <p>The tasks to include for dataset. If None then datasets will not be filtered based on their task.</p> </li> <li> <p>dataset :  str | list[str] | None \u2014</p> <p>The datasets to include for task. If None then all datasets will be included, limited by the <code>task</code> parameter.</p> </li> <li> <p>language :  str | list[str] \u2014</p> <p>The language codes of the languages to include, both for models and datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this to 'all' if all languages (also non-Scandinavian) should be considered.</p> </li> <li> <p>model_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for models. If None then the <code>language</code> parameter will be used.</p> </li> <li> <p>dataset_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for datasets. If None then the <code>language</code> parameter will be used.</p> </li> <li> <p>framework :  Framework | str | None \u2014</p> <p>The framework to use for running the models. If None then the framework will be set automatically.</p> </li> <li> <p>device :  Device | None \u2014</p> <p>The device to use for running the models. If None then the device will be set automatically.</p> </li> <li> <p>batch_size :  int \u2014</p> <p>The batch size to use for running the models.</p> </li> <li> <p>evaluate_train :  bool \u2014</p> <p>Whether to evaluate the models on the training set.</p> </li> <li> <p>raise_errors :  bool \u2014</p> <p>Whether to raise errors when running the benchmark.</p> </li> <li> <p>cache_dir :  str \u2014</p> <p>The directory to use for caching the models.</p> </li> <li> <p>token :  bool | str | None \u2014</p> <p>The token to use for running the models.</p> </li> <li> <p>openai_api_key :  str | None \u2014</p> <p>The OpenAI API key to use for running the models.</p> </li> <li> <p>prefer_azure :  bool \u2014</p> <p>Whether to prefer the Azure OpenAI API for running the models, over the OpenAI API.</p> </li> <li> <p>azure_openai_api_key :  str | None \u2014</p> <p>The Azure OpenAI API key to use for running the models.</p> </li> <li> <p>azure_openai_endpoint :  str | None \u2014</p> <p>The Azure OpenAI endpoint to use for running the models.</p> </li> <li> <p>azure_openai_api_version :  str | None \u2014</p> <p>The Azure OpenAI api version to use for running the models.</p> </li> <li> <p>force :  bool \u2014</p> <p>Whether to force the benchmark to run even if the results are already cached.</p> </li> <li> <p>verbose :  bool \u2014</p> <p>Whether to print verbose output when running the benchmark.</p> </li> <li> <p>trust_remote_code :  bool \u2014</p> <p>Whether to trust remote code when running the benchmark.</p> </li> <li> <p>load_in_4bit :  bool | None \u2014</p> <p>Whether to load the models in 4-bit precision.</p> </li> <li> <p>use_flash_attention :  bool | None \u2014</p> <p>Whether to use Flash Attention for the models. If None then it will be used if it is available.</p> </li> <li> <p>clear_model_cache :  bool \u2014</p> <p>Whether to clear the model cache before running the benchmark.</p> </li> <li> <p>only_validation_split :  bool \u2014</p> <p>Whether to only use the validation split for the datasets.</p> </li> <li> <p>few_shot :  bool \u2014</p> <p>Whether to use few-shot learning for the models.</p> </li> <li> <p>num_iterations :  int \u2014</p> <p>The number of iterations each model should be evaluated for.</p> </li> <li> <p>debug :  bool \u2014</p> <p>Whether to run the benchmark in debug mode.</p> </li> <li> <p>run_with_cli :  bool \u2014</p> <p>Whether the benchmark is being run with the CLI.</p> </li> <li> <p>first_time :  bool \u2014</p> <p>Whether this is the first time the benchmark configuration is being created. Defaults to False.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> source get_correct_language_codes(language_codes: str | list[str]) \u2192 list[str] </p> <p>Get correct language code(s).</p> <p> Parameters </p> <ul> <li> <p>language_codes :  str | list[str] \u2014</p> <p>The language codes of the languages to include, both for models and datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this to 'all' if all languages (also non-Scandinavian) should be considered.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 The correct language codes.</p> </li> </ul> <p> source prepare_languages(language_codes: str | list[str] | None, default_language_codes: list[str]) \u2192 list[Language] </p> <p>Prepare language(s) for benchmarking.</p> <p> Parameters </p> <ul> <li> <p>language_codes :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for models or datasets. If specified then this overrides the <code>language</code> parameter for model or dataset languages.</p> </li> <li> <p>default_language_codes :  list[str] \u2014</p> <p>The default language codes of the languages to include.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[Language] \u2014 The prepared model or dataset languages.</p> </li> </ul> <p> source prepare_tasks_and_datasets(task: str | list[str] | None, dataset_languages: list[Language], dataset: str | list[str] | None) \u2192 tuple[list[Task], list[str]] </p> <p>Prepare task(s) and dataset(s) for benchmarking.</p> <p> Parameters </p> <ul> <li> <p>task :  str | list[str] | None \u2014</p> <p>The tasks to include for dataset. If None then datasets will not be filtered based on their task.</p> </li> <li> <p>dataset_languages :  list[Language] \u2014</p> <p>The languages of the datasets in the benchmark.</p> </li> <li> <p>dataset :  str | list[str] | None \u2014</p> <p>The datasets to include for task. If None then all datasets will be included, limited by the <code>task</code> and <code>dataset_languages</code> parameters.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[list[Task], list[str]] \u2014 The prepared tasks and datasets.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark \u2014</p> <p>If the task or dataset is not found in the benchmark tasks or datasets.</p> </li> </ul> <p> source prepare_device(device: Device | None) \u2192 torch.device </p> <p>Prepare device for benchmarking.</p> <p> Parameters </p> <ul> <li> <p>device :  Device | None \u2014</p> <p>The device to use for running the models. If None then the device will be set automatically.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>torch.device \u2014 The prepared device.</p> </li> </ul>"},{"location":"src/scandeval/benchmark_config_factory/","title":"scandeval.benchmark_config_factory","text":"scandeval.benchmark_config_factory<p> docs module scandeval.benchmark_config_factory </p> <pre><code>\"\"\"Factory class for creating dataset configurations.\"\"\"\n\nimport importlib.util\nimport logging\nimport os\nimport sys\nfrom typing import TYPE_CHECKING\n\nimport torch\n\nfrom .config import BenchmarkConfig\nfrom .dataset_configs import get_all_dataset_configs\nfrom .enums import Device, Framework\nfrom .exceptions import InvalidBenchmark\nfrom .languages import get_all_languages\nfrom .tasks import get_all_tasks\n\nif TYPE_CHECKING:\n    from .config import Language, Task\n\n\nlogger = logging.getLogger(__package__)\n\n\ndef build_benchmark_config(docs\n    progress_bar: bool,\n    save_results: bool,\n    task: str | list[str] | None,\n    dataset: str | list[str] | None,\n    language: str | list[str],\n    model_language: str | list[str] | None,\n    dataset_language: str | list[str] | None,\n    framework: Framework | str | None,\n    device: Device | None,\n    batch_size: int,\n    evaluate_train: bool,\n    raise_errors: bool,\n    cache_dir: str,\n    token: bool | str | None,\n    openai_api_key: str | None,\n    prefer_azure: bool,\n    azure_openai_api_key: str | None,\n    azure_openai_endpoint: str | None,\n    azure_openai_api_version: str | None,\n    force: bool,\n    verbose: bool,\n    trust_remote_code: bool,\n    load_in_4bit: bool | None,\n    use_flash_attention: bool | None,\n    clear_model_cache: bool,\n    only_validation_split: bool,\n    few_shot: bool,\n    num_iterations: int,\n    debug: bool,\n    run_with_cli: bool,\n    first_time: bool = False,\n) -&gt; BenchmarkConfig:\n    \"\"\"Create a benchmark configuration.\n\n    Args:\n        progress_bar:\n            Whether to show a progress bar when running the benchmark.\n        save_results:\n            Whether to save the benchmark results to a file.\n        task:\n            The tasks to include for dataset. If None then datasets will not be\n            filtered based on their task.\n        dataset:\n            The datasets to include for task. If None then all datasets will be\n            included, limited by the `task` parameter.\n        language:\n            The language codes of the languages to include, both for models and\n            datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this\n            to 'all' if all languages (also non-Scandinavian) should be considered.\n        model_language:\n            The language codes of the languages to include for models. If None then\n            the `language` parameter will be used.\n        dataset_language:\n            The language codes of the languages to include for datasets. If None then\n            the `language` parameter will be used.\n        framework:\n            The framework to use for running the models. If None then the framework\n            will be set automatically.\n        device:\n            The device to use for running the models. If None then the device will be\n            set automatically.\n        batch_size:\n            The batch size to use for running the models.\n        evaluate_train:\n            Whether to evaluate the models on the training set.\n        raise_errors:\n            Whether to raise errors when running the benchmark.\n        cache_dir:\n            The directory to use for caching the models.\n        token:\n            The token to use for running the models.\n        openai_api_key:\n            The OpenAI API key to use for running the models.\n        prefer_azure:\n            Whether to prefer the Azure OpenAI API for running the models, over the\n            OpenAI API.\n        azure_openai_api_key:\n            The Azure OpenAI API key to use for running the models.\n        azure_openai_endpoint:\n            The Azure OpenAI endpoint to use for running the models.\n        azure_openai_api_version:\n            The Azure OpenAI api version to use for running the models.\n        force:\n            Whether to force the benchmark to run even if the results are already\n            cached.\n        verbose:\n            Whether to print verbose output when running the benchmark.\n        trust_remote_code:\n            Whether to trust remote code when running the benchmark.\n        load_in_4bit:\n            Whether to load the models in 4-bit precision.\n        use_flash_attention:\n            Whether to use Flash Attention for the models. If None then it will be used\n            if it is available.\n        clear_model_cache:\n            Whether to clear the model cache before running the benchmark.\n        only_validation_split:\n            Whether to only use the validation split for the datasets.\n        few_shot:\n            Whether to use few-shot learning for the models.\n        num_iterations:\n            The number of iterations each model should be evaluated for.\n        debug:\n            Whether to run the benchmark in debug mode.\n        run_with_cli:\n            Whether the benchmark is being run with the CLI.\n        first_time:\n            Whether this is the first time the benchmark configuration is being created.\n            Defaults to False.\n\n    Returns:\n        The benchmark configuration.\n    \"\"\"\n    language_codes = get_correct_language_codes(language_codes=language)\n    model_languages = prepare_languages(\n        language_codes=model_language, default_language_codes=language_codes\n    )\n    dataset_languages = prepare_languages(\n        language_codes=dataset_language, default_language_codes=language_codes\n    )\n\n    tasks, datasets = prepare_tasks_and_datasets(\n        task=task, dataset=dataset, dataset_languages=dataset_languages\n    )\n\n    torch_device = prepare_device(device=device)\n\n    if openai_api_key is None:\n        openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n    if azure_openai_api_key is None:\n        azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n    if azure_openai_endpoint is None:\n        azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n    if azure_openai_api_version is None:\n        azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n\n    # Ensure that we are not using both OpenAI and Azure OpenAI API keys\n    if all(\n        value is not None\n        for value in (\n            openai_api_key,\n            azure_openai_api_key,\n            azure_openai_endpoint,\n            azure_openai_api_version,\n        )\n    ):\n        if prefer_azure:\n            logger.info(\n                \"Both OpenAI and Azure OpenAI API keys are set. Using Azure OpenAI.\"\n            )\n            openai_api_key = None\n        else:\n            if run_with_cli:\n                logger.info(\n                    \"Both OpenAI and Azure OpenAI API keys are set. Using OpenAI since \"\n                    \"the `--prefer-azure` flag is not set.\"\n                )\n            else:\n                logger.info(\n                    \"Both OpenAI and Azure OpenAI API keys are set. Using OpenAI since \"\n                    \"the `prefer_azure` argument is not set to True.\"\n                )\n            azure_openai_api_key = None\n            azure_openai_endpoint = None\n\n    # Sanity check\n    assert not (openai_api_key is not None and azure_openai_api_key is not None)\n\n    framework_obj = Framework(framework) if framework is not None else None\n\n    if token is True:\n        token = None\n\n    if use_flash_attention is None:\n        if torch_device.type != \"cuda\":\n            use_flash_attention = False\n        elif (\n            importlib.util.find_spec(\"flash_attn\") is None\n            and importlib.util.find_spec(\"vllm_flash_attn\") is None\n        ):\n            use_flash_attention = False\n            if first_time and torch_device.type == \"cuda\":\n                message = (\n                    \"Flash attention has not been installed, so this will not be used. \"\n                    \"To install it, run `pip install -U wheel &amp;&amp; \"\n                    \"FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE pip install flash-attn \"\n                    \"--no-build-isolation`. Alternatively, you can disable this \"\n                    \"message by setting \"\n                )\n                if run_with_cli:\n                    message += \"the flag `--no-use-flash-attention`.\"\n                else:\n                    message += (\n                        \"the argument `use_flash_attention=False` in the `Benchmarker`.\"\n                    )\n                logger.info(message)\n\n    # Set variable with number of iterations\n    if hasattr(sys, \"_called_from_test\"):\n        num_iterations = 1\n\n    return BenchmarkConfig(\n        model_languages=model_languages,\n        dataset_languages=dataset_languages,\n        tasks=tasks,\n        datasets=datasets,\n        batch_size=batch_size,\n        raise_errors=raise_errors,\n        cache_dir=cache_dir,\n        evaluate_train=evaluate_train,\n        token=token,\n        openai_api_key=openai_api_key,\n        azure_openai_api_key=azure_openai_api_key,\n        azure_openai_endpoint=azure_openai_endpoint,\n        azure_openai_api_version=azure_openai_api_version,\n        force=force,\n        progress_bar=progress_bar,\n        save_results=save_results,\n        verbose=verbose,\n        framework=framework_obj,\n        device=torch_device,\n        trust_remote_code=trust_remote_code,\n        load_in_4bit=load_in_4bit,\n        use_flash_attention=use_flash_attention,\n        clear_model_cache=clear_model_cache,\n        only_validation_split=only_validation_split,\n        few_shot=few_shot,\n        num_iterations=num_iterations,\n        debug=debug,\n        run_with_cli=run_with_cli,\n    )\n\ndocs\ndef get_correct_language_codes(language_codes: str | list[str]) -&gt; list[str]:\n    \"\"\"Get correct language code(s).\n\n    Args:\n        language_codes:\n            The language codes of the languages to include, both for models and\n            datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this\n            to 'all' if all languages (also non-Scandinavian) should be considered.\n\n    Returns:\n        The correct language codes.\n    \"\"\"\n    # Create a dictionary that maps languages to their associated language objects\n    language_mapping = get_all_languages()\n\n    # Create the list `languages`\n    if \"all\" in language_codes:\n        languages = list(language_mapping.keys())\n    elif isinstance(language_codes, str):\n        languages = [language_codes]\n    else:\n        languages = language_codes\n\n    # If `languages` contains 'no' then also include 'nb' and 'nn'. Conversely, if\n    # either 'nb' or 'nn' are specified then also include 'no'.\n    if \"no\" in languages:\n        languages = list(set(languages) | {\"nb\", \"nn\"})\n    elif \"nb\" in languages or \"nn\" in languages:\n        languages = list(set(languages) | {\"no\"})\n\n    return languages\n\n\ndef prepare_languages(docs\n    language_codes: str | list[str] | None, default_language_codes: list[str]\n) -&gt; list[\"Language\"]:\n    \"\"\"Prepare language(s) for benchmarking.\n\n    Args:\n        language_codes:\n            The language codes of the languages to include for models or datasets.\n            If specified then this overrides the `language` parameter for model or\n            dataset languages.\n        default_language_codes:\n            The default language codes of the languages to include.\n\n    Returns:\n        The prepared model or dataset languages.\n    \"\"\"\n    # Create a dictionary that maps languages to their associated language objects\n    language_mapping = get_all_languages()\n\n    # Create the list `languages_str` of language codes to use for models or datasets\n    languages_str: list[str]\n    if language_codes is None:\n        languages_str = default_language_codes\n    elif isinstance(language_codes, str):\n        languages_str = [language_codes]\n    else:\n        languages_str = language_codes\n\n    # Convert the model languages to language objects\n    if \"all\" in languages_str:\n        prepared_languages = list(language_mapping.values())\n    else:\n        prepared_languages = [language_mapping[language] for language in languages_str]\n\n    return prepared_languages\n\n\ndef prepare_tasks_and_datasets(docs\n    task: str | list[str] | None,\n    dataset_languages: list[\"Language\"],\n    dataset: str | list[str] | None,\n) -&gt; tuple[list[\"Task\"], list[str]]:\n    \"\"\"Prepare task(s) and dataset(s) for benchmarking.\n\n    Args:\n        task:\n            The tasks to include for dataset. If None then datasets will not be\n            filtered based on their task.\n        dataset_languages:\n            The languages of the datasets in the benchmark.\n        dataset:\n            The datasets to include for task. If None then all datasets will be\n            included, limited by the `task` and `dataset_languages` parameters.\n\n    Returns:\n        The prepared tasks and datasets.\n\n    Raises:\n        InvalidBenchmark:\n            If the task or dataset is not found in the benchmark tasks or datasets.\n    \"\"\"\n    # Create a dictionary that maps benchmark tasks to their associated benchmark\n    # task objects, and a dictionary that maps dataset names to their associated\n    # dataset configuration objects\n    task_mapping = get_all_tasks()\n    all_dataset_configs = get_all_dataset_configs()\n\n    # Create the list of dataset tasks\n    try:\n        if task is None:\n            tasks = list(task_mapping.values())\n        elif isinstance(task, str):\n            tasks = [task_mapping[task]]\n        else:\n            tasks = [task_mapping[t] for t in task]\n    except KeyError as e:\n        raise InvalidBenchmark(f\"Task {e} not found in the benchmark tasks.\") from e\n\n    all_official_datasets = [\n        dataset_name\n        for dataset_name, dataset_config in all_dataset_configs.items()\n        if not dataset_config.unofficial\n    ]\n    if dataset is None:\n        dataset = all_official_datasets\n    elif isinstance(dataset, str):\n        dataset = [dataset]\n\n    all_datasets = list(all_dataset_configs.keys())\n    invalid_datasets = set(dataset) - set(all_datasets)\n    if invalid_datasets:\n        raise InvalidBenchmark(\n            f\"Dataset(s) {', '.join(invalid_datasets)} not found in the benchmark \"\n            \"datasets.\"\n        )\n\n    datasets = [\n        dataset_name\n        for dataset_name, dataset_config in all_dataset_configs.items()\n        if dataset_name in dataset\n        and dataset_config.task in tasks\n        and set(dataset_config.languages).intersection(dataset_languages)\n    ]\n\n    return tasks, datasets\n\n\ndef prepare_device(device: Device | None) -&gt; torch.device:docs\n    \"\"\"Prepare device for benchmarking.\n\n    Args:\n        device:\n            The device to use for running the models. If None then the device will be\n            set automatically.\n\n    Returns:\n        The prepared device.\n    \"\"\"\n    device_mapping = {\n        Device.CPU: torch.device(\"cpu\"),\n        Device.CUDA: torch.device(\"cuda\"),\n        Device.MPS: torch.device(\"mps\"),\n    }\n    if isinstance(device, Device):\n        return device_mapping[device]\n\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    elif torch.backends.mps.is_available():\n        return torch.device(\"mps\")\n    else:\n        return torch.device(\"cpu\")\n</code></pre>"},{"location":"api/scandeval/benchmark_dataset/","title":"Benchmark dataset","text":"<p>Failure</p> <p>module 'scandeval.benchmark_dataset' not found.</p>"},{"location":"src/scandeval/benchmark_dataset/","title":"Benchmark dataset","text":"<p>Failure</p> <p>module 'scandeval.benchmark_dataset' not found.</p>"},{"location":"api/scandeval/callbacks/","title":"Callbacks","text":"<p>Failure</p> <p>module 'scandeval.callbacks' not found.</p>"},{"location":"src/scandeval/callbacks/","title":"Callbacks","text":"<p>Failure</p> <p>module 'scandeval.callbacks' not found.</p>"},{"location":"api/scandeval/cli/","title":"Cli","text":"<p>Failure</p> <p>module 'scandeval.cli' not found.</p>"},{"location":"src/scandeval/cli/","title":"Cli","text":"<p>Failure</p> <p>module 'scandeval.cli' not found.</p>"},{"location":"api/scandeval/config/","title":"scandeval.config","text":"scandeval.config<p> source module scandeval.config </p> <p>Configuration classes used throughout the project.</p> <p> Classes </p> <ul> <li> <p>MetricConfig \u2014 Configuration for a metric.</p> </li> <li> <p>Task \u2014 A dataset task.</p> </li> <li> <p>Language \u2014 A benchmarkable language.</p> </li> <li> <p>BenchmarkConfig \u2014 General benchmarking configuration, across datasets and models.</p> </li> <li> <p>DatasetConfig \u2014 Configuration for a dataset.</p> </li> <li> <p>ModelConfig \u2014 Configuration for a model.</p> </li> </ul> <p> source dataclass MetricConfig(name: str, pretty_name: str, huggingface_id: str, results_key: str, compute_kwargs: dict[str, Any] = field(default_factory=dict), postprocessing_fn: Callable[[float], tuple[float, str]] = field(default_factory=lambda: lambda raw_score: (100 * raw_score, f'{raw_score:.2%}'))) </p> <p>Configuration for a metric.</p> <p> Attributes </p> <ul> <li> <p>name :  str \u2014</p> <p>The name of the metric.</p> </li> <li> <p>pretty_name :  str \u2014</p> <p>A longer prettier name for the metric, which allows cases and spaces. Used for logging.</p> </li> <li> <p>huggingface_id :  str \u2014</p> <p>The Hugging Face ID of the metric.</p> </li> <li> <p>results_key :  str \u2014</p> <p>The name of the key used to extract the metric scores from the results dictionary.</p> </li> <li> <p>compute_kwargs :  dict[str, Any] \u2014</p> <p>Keyword arguments to pass to the metric's compute function. Defaults to an empty dictionary.</p> </li> <li> <p>postprocessing_fn :  Callable[[float], tuple[float, str]] \u2014</p> <p>A function to apply to the metric scores after they are computed, taking the score to the postprocessed score along with its string representation. Defaults to x -&gt; (100 * x, f\"{x:.2%}\").</p> </li> </ul> <p> source dataclass Task(name: str, supertask: str, metrics: list[MetricConfig], labels: list[str]) </p> <p>A dataset task.</p> <p> Attributes </p> <ul> <li> <p>name :  str \u2014</p> <p>The name of the task.</p> </li> <li> <p>supertask :  str \u2014</p> <p>The supertask of the task, describing the overall type of task.</p> </li> <li> <p>metrics :  list[MetricConfig] \u2014</p> <p>The metrics used to evaluate the task.</p> </li> <li> <p>labels :  list[str] \u2014</p> <p>The labels used in the task.</p> </li> </ul> <p> source dataclass Language(code: str, name: str) </p> <p>A benchmarkable language.</p> <p> Attributes </p> <ul> <li> <p>code :  str \u2014</p> <p>The ISO 639-1 language code of the language.</p> </li> <li> <p>name :  str \u2014</p> <p>The name of the language.</p> </li> </ul> <p> source dataclass BenchmarkConfig(model_languages: list[Language], dataset_languages: list[Language], tasks: list[Task], datasets: list[str], framework: Framework | None, batch_size: int, raise_errors: bool, cache_dir: str, evaluate_train: bool, token: bool | str | None, openai_api_key: str | None, azure_openai_api_key: str | None, azure_openai_endpoint: str | None, azure_openai_api_version: str | None, force: bool, progress_bar: bool, save_results: bool, device: torch.device, verbose: bool, trust_remote_code: bool, load_in_4bit: bool | None, use_flash_attention: bool | None, clear_model_cache: bool, only_validation_split: bool, few_shot: bool, num_iterations: int, debug: bool, run_with_cli: bool) </p> <p>General benchmarking configuration, across datasets and models.</p> <p> Attributes </p> <ul> <li> <p>model_languages :  list[Language] \u2014</p> <p>The languages of the models to benchmark.</p> </li> <li> <p>dataset_languages :  list[Language] \u2014</p> <p>The languages of the datasets in the benchmark.</p> </li> <li> <p>tasks :  list[Task] \u2014</p> <p>The tasks benchmark the model(s) on.</p> </li> <li> <p>datasets :  list[str] \u2014</p> <p>The datasets to benchmark on.</p> </li> <li> <p>framework :  Framework | None \u2014</p> <p>The framework of the models to benchmark. If None then the framework will be inferred.</p> </li> <li> <p>batch_size :  int \u2014</p> <p>The batch size to use.</p> </li> <li> <p>raise_errors :  bool \u2014</p> <p>Whether to raise errors instead of skipping them.</p> </li> <li> <p>cache_dir :  str \u2014</p> <p>Directory to store cached models and datasets.</p> </li> <li> <p>evaluate_train :  bool \u2014</p> <p>Whether to evaluate on the training set.</p> </li> <li> <p>token :  bool | str | None \u2014</p> <p>The authentication token for the Hugging Face Hub. If a boolean value is specified then the token will be fetched from the Hugging Face CLI, where the user has logged in through <code>huggingface-cli login</code>. If a string is specified then it will be used as the token.</p> </li> <li> <p>openai_api_key :  str | None \u2014</p> <p>The API key for the OpenAI API. If None then OpenAI models will not be benchmarked.</p> </li> <li> <p>azure_openai_api_key :  str | None \u2014</p> <p>The API key for the Azure OpenAI API. If None then Azure OpenAI models will not be benchmarked.</p> </li> <li> <p>azure_openai_endpoint :  str | None \u2014</p> <p>The endpoint for the Azure OpenAI API. If None then Azure OpenAI models will not be benchmarked.</p> </li> <li> <p>azure_openai_api_version :  str | None \u2014</p> <p>The api version for the Azure OpenAI API, e.g. \"2023-12-01-preview\". If None then Azure OpenAI models will not be benchmarked.</p> </li> <li> <p>force :  bool \u2014</p> <p>Whether to force the benchmark to run even if the results are already cached.</p> </li> <li> <p>progress_bar :  bool \u2014</p> <p>Whether to show a progress bar.</p> </li> <li> <p>save_results :  bool \u2014</p> <p>Whether to save the benchmark results to 'scandeval_benchmark_results.json'.</p> </li> <li> <p>device :  torch.device \u2014</p> <p>The device to use for benchmarking.</p> </li> <li> <p>verbose :  bool \u2014</p> <p>Whether to print verbose output.</p> </li> <li> <p>trust_remote_code :  bool \u2014</p> <p>Whether to trust remote code when loading models from the Hugging Face Hub.</p> </li> <li> <p>load_in_4bit :  bool | None \u2014</p> <p>Whether to load models in 4-bit precision. If None then this will be done if CUDA is available and the model is a decoder model.</p> </li> <li> <p>use_flash_attention :  bool | None \u2014</p> <p>Whether to use Flash Attention. If None then this will be used for generative models.</p> </li> <li> <p>clear_model_cache :  bool \u2014</p> <p>Whether to clear the model cache after benchmarking each model.</p> </li> <li> <p>only_validation_split :  bool \u2014</p> <p>Whether to only evaluate on the validation split.</p> </li> <li> <p>few_shot :  bool \u2014</p> <p>Whether to only evaluate the model using few-shot evaluation. Only relevant if the model is generative.</p> </li> <li> <p>num_iterations :  int \u2014</p> <p>The number of iterations each model should be evaluated for.</p> </li> <li> <p>debug :  bool \u2014</p> <p>Whether to run the benchmark in debug mode.</p> </li> <li> <p>run_with_cli :  bool \u2014</p> <p>Whether the benchmark is being run with the CLI.</p> </li> </ul> <p> source dataclass DatasetConfig(name: str, pretty_name: str, huggingface_id: str, task: Task, languages: list[Language], prompt_template: str, max_generated_tokens: int, prompt_prefix: str, num_few_shot_examples: int, instruction_prompt: str, prompt_label_mapping: dict[str, str] = field(default_factory=dict), unofficial: bool = False) </p> <p>Configuration for a dataset.</p> <p> Attributes </p> <ul> <li> <p>name :  str \u2014</p> <p>The name of the dataset. Must be lower case with no spaces.</p> </li> <li> <p>pretty_name :  str \u2014</p> <p>A longer prettier name for the dataset, which allows cases and spaces. Used for logging.</p> </li> <li> <p>huggingface_id :  str \u2014</p> <p>The Hugging Face ID of the dataset.</p> </li> <li> <p>task :  Task \u2014</p> <p>The task of the dataset.</p> </li> <li> <p>languages :  list[Language] \u2014</p> <p>The ISO 639-1 language codes of the entries in the dataset.</p> </li> <li> <p>id2label :  dict[int, str] \u2014</p> <p>The mapping from ID to label.</p> </li> <li> <p>label2id :  dict[str, int] \u2014</p> <p>The mapping from label to ID.</p> </li> <li> <p>num_labels :  int \u2014</p> <p>The number of labels in the dataset.</p> </li> <li> <p>prompt_template :  str \u2014</p> <p>The template for the prompt to use when benchmarking the dataset using few-shot evaluation.</p> </li> <li> <p>max_generated_tokens :  int \u2014</p> <p>The maximum number of tokens to generate when benchmarking the dataset using few-shot evaluation.</p> </li> <li> <p>prompt_prefix :  str \u2014</p> <p>The prefix to use in the few-shot prompt.</p> </li> <li> <p>num_few_shot_examples :  int \u2014</p> <p>The number of examples to use when benchmarking the dataset using few-shot evaluation. For a classification task, these will be drawn evenly from each label.</p> </li> <li> <p>instruction_prompt :  str \u2014</p> <p>The prompt to use when benchmarking the dataset using instruction-based evaluation.</p> </li> <li> <p>prompt_label_mapping :  optional \u2014</p> <p>A mapping from the labels to another phrase which is used as a substitute for the label in few-shot evaluation. Defaults to an empty dictionary.</p> </li> <li> <p>unofficial :  optional \u2014</p> <p>Whether the dataset is unofficial. Defaults to False.</p> </li> </ul> <p> source property DatasetConfig.id2label: dict[int, str] </p> <p>The mapping from ID to label.</p> <p> source property DatasetConfig.label2id: dict[str, int] </p> <p>The mapping from label to ID.</p> <p> source property DatasetConfig.num_labels: int </p> <p>The number of labels in the dataset.</p> <p> source dataclass ModelConfig(model_id: str, revision: str, framework: Framework, task: str, languages: list[Language], model_type: ModelType | str, model_cache_dir: str, adapter_base_model_id: str | None) </p> <p>Configuration for a model.</p> <p> Attributes </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The ID of the model.</p> </li> <li> <p>revision :  str \u2014</p> <p>The revision of the model.</p> </li> <li> <p>framework :  Framework \u2014</p> <p>The framework of the model.</p> </li> <li> <p>task :  str \u2014</p> <p>The task that the model was trained on.</p> </li> <li> <p>languages :  list[Language] \u2014</p> <p>The languages of the model.</p> </li> <li> <p>model_type :  ModelType | str \u2014</p> <p>The type of the model.</p> </li> <li> <p>model_cache_dir :  str \u2014</p> <p>The directory to cache the model in.</p> </li> <li> <p>adapter_base_model_id :  str | None \u2014</p> <p>The model ID of the base model if the model is an adapter model. Can be None if the model is not an adapter model.</p> </li> </ul>"},{"location":"src/scandeval/config/","title":"scandeval.config","text":"scandeval.config<p> docs module scandeval.config </p> <pre><code>\"\"\"Configuration classes used throughout the project.\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import TYPE_CHECKING, Any, Callable\n\nimport torch\n\nif TYPE_CHECKING:\n    from .enums import Framework, ModelType\n\n\n@dataclass\nclass MetricConfig:docs\n    \"\"\"Configuration for a metric.\n\n    Attributes:\n        name:\n            The name of the metric.\n        pretty_name:\n            A longer prettier name for the metric, which allows cases and spaces. Used\n            for logging.\n        huggingface_id:\n            The Hugging Face ID of the metric.\n        results_key:\n            The name of the key used to extract the metric scores from the results\n            dictionary.\n        compute_kwargs:\n            Keyword arguments to pass to the metric's compute function. Defaults to\n            an empty dictionary.\n        postprocessing_fn:\n            A function to apply to the metric scores after they are computed, taking\n            the score to the postprocessed score along with its string representation.\n            Defaults to x -&gt; (100 * x, f\"{x:.2%}\").\n    \"\"\"\n\n    name: str\n    pretty_name: str\n    huggingface_id: str\n    results_key: str\n    compute_kwargs: dict[str, Any] = field(default_factory=dict)\n    postprocessing_fn: Callable[[float], tuple[float, str]] = field(\n        default_factory=lambda: lambda raw_score: (100 * raw_score, f\"{raw_score:.2%}\")\n    )\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hash of the metric configuration.\"\"\"\n        return hash(self.name)\n\n\n@dataclass\nclass Task:docs\n    \"\"\"A dataset task.\n\n    Attributes:\n        name:\n            The name of the task.\n        supertask:\n            The supertask of the task, describing the overall type of task.\n        metrics:\n            The metrics used to evaluate the task.\n        labels:\n            The labels used in the task.\n    \"\"\"\n\n    name: str\n    supertask: str\n    metrics: list[MetricConfig]\n    labels: list[str]\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hash of the task.\"\"\"\n        return hash(self.name)\n\n\n@dataclass\nclass Language:docs\n    \"\"\"A benchmarkable language.\n\n    Attributes:\n        code:\n            The ISO 639-1 language code of the language.\n        name:\n            The name of the language.\n    \"\"\"\n\n    code: str\n    name: str\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hash of the language.\"\"\"\n        return hash(self.code)\n\n\n@dataclass\nclass BenchmarkConfig:docs\n    \"\"\"General benchmarking configuration, across datasets and models.\n\n    Attributes:\n        model_languages:\n            The languages of the models to benchmark.\n        dataset_languages:\n            The languages of the datasets in the benchmark.\n        tasks:\n            The tasks benchmark the model(s) on.\n        datasets:\n            The datasets to benchmark on.\n        framework:\n            The framework of the models to benchmark. If None then the framework will be\n            inferred.\n        batch_size:\n            The batch size to use.\n        raise_errors:\n            Whether to raise errors instead of skipping them.\n        cache_dir:\n            Directory to store cached models and datasets.\n        evaluate_train:\n            Whether to evaluate on the training set.\n        token:\n            The authentication token for the Hugging Face Hub. If a boolean value is\n            specified then the token will be fetched from the Hugging Face CLI, where\n            the user has logged in through `huggingface-cli login`. If a string is\n            specified then it will be used as the token.\n        openai_api_key:\n            The API key for the OpenAI API. If None then OpenAI models will not be\n            benchmarked.\n        azure_openai_api_key:\n            The API key for the Azure OpenAI API. If None then Azure OpenAI models will\n            not be benchmarked.\n        azure_openai_endpoint:\n            The endpoint for the Azure OpenAI API. If None then Azure OpenAI models will\n            not be benchmarked.\n        azure_openai_api_version:\n            The api version for the Azure OpenAI API, e.g. \"2023-12-01-preview\". If\n            None then Azure OpenAI models will not be benchmarked.\n        force:\n            Whether to force the benchmark to run even if the results are already\n            cached.\n        progress_bar:\n            Whether to show a progress bar.\n        save_results:\n            Whether to save the benchmark results to 'scandeval_benchmark_results.json'.\n        device:\n            The device to use for benchmarking.\n        verbose:\n            Whether to print verbose output.\n        trust_remote_code:\n            Whether to trust remote code when loading models from the Hugging Face Hub.\n        load_in_4bit:\n            Whether to load models in 4-bit precision. If None then this will be done\n            if CUDA is available and the model is a decoder model.\n        use_flash_attention:\n            Whether to use Flash Attention. If None then this will be used for\n            generative models.\n        clear_model_cache:\n            Whether to clear the model cache after benchmarking each model.\n        only_validation_split:\n            Whether to only evaluate on the validation split.\n        few_shot:\n            Whether to only evaluate the model using few-shot evaluation. Only relevant\n            if the model is generative.\n        num_iterations:\n            The number of iterations each model should be evaluated for.\n        debug:\n            Whether to run the benchmark in debug mode.\n        run_with_cli:\n            Whether the benchmark is being run with the CLI.\n    \"\"\"\n\n    model_languages: list[Language]\n    dataset_languages: list[Language]\n    tasks: list[Task]\n    datasets: list[str]\n    framework: \"Framework | None\"\n    batch_size: int\n    raise_errors: bool\n    cache_dir: str\n    evaluate_train: bool\n    token: bool | str | None\n    openai_api_key: str | None\n    azure_openai_api_key: str | None\n    azure_openai_endpoint: str | None\n    azure_openai_api_version: str | None\n    force: bool\n    progress_bar: bool\n    save_results: bool\n    device: torch.device\n    verbose: bool\n    trust_remote_code: bool\n    load_in_4bit: bool | None\n    use_flash_attention: bool | None\n    clear_model_cache: bool\n    only_validation_split: bool\n    few_shot: bool\n    num_iterations: int\n    debug: bool\n    run_with_cli: bool\n\n\n@dataclass\nclass DatasetConfig:docs\n    \"\"\"Configuration for a dataset.\n\n    Attributes:\n        name:\n            The name of the dataset. Must be lower case with no spaces.\n        pretty_name:\n            A longer prettier name for the dataset, which allows cases and spaces. Used\n            for logging.\n        huggingface_id:\n            The Hugging Face ID of the dataset.\n        task:\n            The task of the dataset.\n        languages:\n            The ISO 639-1 language codes of the entries in the dataset.\n        id2label:\n            The mapping from ID to label.\n        label2id:\n            The mapping from label to ID.\n        num_labels:\n            The number of labels in the dataset.\n        prompt_template:\n            The template for the prompt to use when benchmarking the dataset using\n            few-shot evaluation.\n        max_generated_tokens:\n            The maximum number of tokens to generate when benchmarking the dataset\n            using few-shot evaluation.\n        prompt_prefix:\n            The prefix to use in the few-shot prompt.\n        num_few_shot_examples:\n            The number of examples to use when benchmarking the dataset using few-shot\n            evaluation. For a classification task, these will be drawn evenly from\n            each label.\n        instruction_prompt:\n            The prompt to use when benchmarking the dataset using instruction-based\n            evaluation.\n        prompt_label_mapping (optional):\n            A mapping from the labels to another phrase which is used as a substitute\n            for the label in few-shot evaluation. Defaults to an empty dictionary.\n        unofficial (optional):\n            Whether the dataset is unofficial. Defaults to False.\n    \"\"\"\n\n    name: str\n    pretty_name: str\n    huggingface_id: str\n    task: Task\n    languages: list[Language]\n    prompt_template: str\n    max_generated_tokens: int\n    prompt_prefix: str\n    num_few_shot_examples: int\n    instruction_prompt: str\n    prompt_label_mapping: dict[str, str] = field(default_factory=dict)\n    unofficial: bool = False\n\n    @property\n    def id2label(self) -&gt; dict[int, str]:docs\n        \"\"\"The mapping from ID to label.\"\"\"\n        return {idx: label for idx, label in enumerate(self.task.labels)}\n\n    @property\n    def label2id(self) -&gt; dict[str, int]:docs\n        \"\"\"The mapping from label to ID.\"\"\"\n        return {label: i for i, label in enumerate(self.task.labels)}\n\n    @property\n    def num_labels(self) -&gt; int:docs\n        \"\"\"The number of labels in the dataset.\"\"\"\n        return len(self.task.labels)\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hash of the dataset configuration.\"\"\"\n        return hash(self.name)\n\n\n@dataclass\nclass ModelConfig:docs\n    \"\"\"Configuration for a model.\n\n    Attributes:\n        model_id:\n            The ID of the model.\n        revision:\n            The revision of the model.\n        framework:\n            The framework of the model.\n        task:\n            The task that the model was trained on.\n        languages:\n            The languages of the model.\n        model_type:\n            The type of the model.\n        model_cache_dir:\n            The directory to cache the model in.\n        adapter_base_model_id:\n            The model ID of the base model if the model is an adapter model. Can be None\n            if the model is not an adapter model.\n    \"\"\"\n\n    model_id: str\n    revision: str\n    framework: \"Framework\"\n    task: str\n    languages: list[Language]\n    model_type: \"ModelType | str\"\n    model_cache_dir: str\n    adapter_base_model_id: str | None\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hash of the model configuration.\"\"\"\n        return hash(self.model_id)\n</code></pre>"},{"location":"api/scandeval/dataset_configs/","title":"scandeval.dataset_configs","text":"scandeval.dataset_configs<p> source module scandeval.dataset_configs </p> <p>All dataset configurations used in ScandEval.</p> <p> Functions </p> <ul> <li> <p>get_all_dataset_configs \u2014 Get a mapping of all the dataset configurations.</p> </li> <li> <p>get_dataset_config \u2014 Get the dataset configuration for a dataset.</p> </li> </ul> <p> source get_all_dataset_configs() \u2192 dict[str, DatasetConfig] </p> <p>Get a mapping of all the dataset configurations.</p> <p> Returns </p> <ul> <li> <p>dict[str, DatasetConfig] \u2014 A mapping between names of datasets and their configurations.</p> </li> </ul> <p> source get_dataset_config(dataset_name: str) \u2192 DatasetConfig </p> <p>Get the dataset configuration for a dataset.</p> <p> Parameters </p> <ul> <li> <p>dataset_name :  str \u2014</p> <p>The name of the dataset.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetConfig \u2014 The dataset configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014</p> <p>If the dataset is not found.</p> </li> </ul>"},{"location":"src/scandeval/dataset_configs/","title":"scandeval.dataset_configs","text":"scandeval.dataset_configs<p> docs module scandeval.dataset_configs </p> <pre><code>\"\"\"All dataset configurations used in ScandEval.\"\"\"\n\nfrom .config import DatasetConfig\nfrom .languages import DA, DE, EN, FO, IS, NB, NL, NN, NO, SV, get_all_languages\nfrom .tasks import COMMON_SENSE, KNOW, LA, MCRC, NER, RC, SENT, SPEED, SUMM\n\n\ndef get_all_dataset_configs() -&gt; dict[str, DatasetConfig]:docs\n    \"\"\"Get a mapping of all the dataset configurations.\n\n    Returns:\n        A mapping between names of datasets and their configurations.\n    \"\"\"\n    dataset_configs = [\n        cfg for cfg in globals().values() if isinstance(cfg, DatasetConfig)\n    ]\n    assert len(dataset_configs) == len({cfg.name for cfg in dataset_configs}), (\n        \"There are duplicate dataset configurations. Please ensure that each dataset \"\n        \"has a unique name.\"\n    )\n    return {cfg.name: cfg for cfg in dataset_configs}\n\n\ndef get_dataset_config(dataset_name: str) -&gt; DatasetConfig:docs\n    \"\"\"Get the dataset configuration for a dataset.\n\n    Args:\n        dataset_name:\n            The name of the dataset.\n\n    Returns:\n        The dataset configuration.\n\n    Raises:\n        ValueError:\n            If the dataset is not found.\n    \"\"\"\n    # Get mapping of all dataset configs\n    dataset_configs = get_all_dataset_configs()\n\n    # If there are no matches for the dataset name, raise an error\n    if dataset_name not in dataset_configs:\n        raise ValueError(f\"No dataset config found for dataset {dataset_name}.\")\n\n    # Otherwise, return the dataset configuration\n    return dataset_configs[dataset_name]\n\n\n### SENTIMENT DATASETS ###\n\nSWEREC_CONFIG = DatasetConfig(\n    name=\"swerec\",\n    pretty_name=\"the truncated version of the Swedish sentiment classification \"\n    \"dataset SweReC\",\n    huggingface_id=\"ScandEval/swerec-mini\",\n    task=SENT,\n    languages=[SV],\n    prompt_prefix=\"F\u00f6ljande \u00e4r recensioner och deras sentiment, som kan vara \"\n    \"'positiv', 'neutral' eller 'negativ'.\",\n    prompt_template=\"Recension: {text}\\nSentiment: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positiv\", neutral=\"neutral\", negative=\"negativ\"\n    ),\n    instruction_prompt=\"Recension: {text}\\n\\nKlassificera sentimentet i recensionen. \"\n    \"Svara med 'positiv', 'neutral' eller 'negativ'.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nANGRY_TWEETS_CONFIG = DatasetConfig(\n    name=\"angry-tweets\",\n    pretty_name=\"the truncated version of the Danish sentiment classification \"\n    \"dataset AngryTweets\",\n    huggingface_id=\"ScandEval/angry-tweets-mini\",\n    task=SENT,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er tweets og deres sentiment, som kan v\u00e6re 'positiv', \"\n    \"'neutral' eller 'negativ'.\",\n    prompt_template=\"Tweet: {text}\\nSentiment: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positiv\", neutral=\"neutral\", negative=\"negativ\"\n    ),\n    instruction_prompt=\"Tweet: {text}\\n\\nKlassificer sentimentet i tweetet. Svar kun \"\n    \"med 'positiv', 'neutral' eller 'negativ'.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nNOREC_CONFIG = DatasetConfig(\n    name=\"norec\",\n    pretty_name=\"the truncated version of the Norwegian sentiment classification \"\n    \"dataset NoReC\",\n    huggingface_id=\"ScandEval/norec-mini\",\n    task=SENT,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"F\u00f8lgende er anmeldelser og deres sentiment, som kan v\u00e6re 'positiv', \"\n    \"'n\u00f8ytral' eller 'negativ'.\",\n    prompt_template=\"Anmeldelse: {text}\\nSentiment: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positiv\", neutral=\"n\u00f8ytral\", negative=\"negativ\"\n    ),\n    instruction_prompt=\"Anmeldelse: {text}\\n\\nKlassifiser sentimentet i anmeldelsen. \"\n    \"Svar med 'positiv', 'n\u00f8ytral' eller 'negativ'.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nHOTTER_AND_COLDER_SENTIMENT_CONFIG = DatasetConfig(\n    name=\"hotter-and-colder-sentiment\",\n    pretty_name=\"the sentiment classification part of the Icelandic dataset Hotter \"\n    \"and Colder\",\n    huggingface_id=\"ScandEval/hotter-and-colder-sentiment\",\n    task=SENT,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru yfirfer\u00f0ir \u00e1samt lyndisgildi \u00feeirra, sem getur \"\n    \"veri\u00f0 'j\u00e1kv\u00e6tt', 'hlutlaust' e\u00f0a 'neikv\u00e6tt'.\",\n    prompt_template=\"Yfirfer\u00f0: {text}\\nLyndi: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"j\u00e1kv\u00e6tt\", neutral=\"hlutlaust\", negative=\"neikv\u00e6tt\"\n    ),\n    instruction_prompt=\"\",  # TODO: Add instruction prompt\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nSB10K_CONFIG = DatasetConfig(\n    name=\"sb10k\",\n    pretty_name=\"the truncated version of the German sentiment classification \"\n    \"dataset SB10k\",\n    huggingface_id=\"ScandEval/sb10k-mini\",\n    task=SENT,\n    languages=[DE],\n    prompt_prefix=\"Im Folgenden sind Tweets und ihre Stimmung aufgef\u00fchrt, die \"\n    \"'positiv', 'neutral' oder 'negativ' sein kann.\",\n    prompt_template=\"Tweet: {text}\\nStimmungslage: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positiv\", neutral=\"neutral\", negative=\"negativ\"\n    ),\n    instruction_prompt=\"Tweet: {text}\\n\\nKlassifizieren Sie die Stimmung im Tweet. \"\n    \"Antworten Sie mit 'positiv', 'neutral' oder 'negativ'.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nDUTCH_SOCIAL_CONFIG = DatasetConfig(\n    name=\"dutch-social\",\n    pretty_name=\"the truncated version of the Dutch sentiment classification \"\n    \"dataset Dutch Social\",\n    huggingface_id=\"ScandEval/dutch-social-mini\",\n    task=SENT,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan tweets en hun sentiment, dat 'positief', \"\n    \"'neutraal' of 'negatief' kan zijn.\",\n    prompt_template=\"Tweet: {text}\\nSentiment: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positief\", neutral=\"neutraal\", negative=\"negatief\"\n    ),\n    instruction_prompt=\"Tweet: {text}\\n\\nClassificeer het sentiment in de tweet. \"\n    \"Antwoord met 'positief', 'neutraal' of 'negatief'.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nSST5_CONFIG = DatasetConfig(\n    name=\"sst5\",\n    pretty_name=\"the truncated version of the English sentiment classification \"\n    \"dataset SST5\",\n    huggingface_id=\"ScandEval/sst5-mini\",\n    task=SENT,\n    languages=[EN],\n    prompt_prefix=\"The following are texts and their sentiment, which can be \"\n    \"'positive', 'neutral' or 'negative'.\",\n    prompt_template=\"Text: {text}\\nSentiment: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positive\", neutral=\"neutral\", negative=\"negative\"\n    ),\n    instruction_prompt=\"Text: {text}\\n\\nClassify the sentiment in the text. Answer \"\n    \"with 'positive', 'neutral' or 'negative'.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\n# TODO: Faroese Sentiment Classification\n# FOREC_CONFIG = DatasetConfig(\n#     name=\"forec\",\n#     pretty_name=\"the truncated version of FoReC\",\n#     huggingface_id=\"ScandEval/forec-mini\",\n#     task=SENT,\n#     languages=[FO],\n#     prompt_prefix=\"Her koma n\u00f8kur umm\u00e6li og teirra kensluliga sj\u00f3narmi\u00f0, sum kunnu \"\n#     \"vera 'positivur', 'neutralur' ella 'negativur'.\",\n#     prompt_template=\"Umm\u00e6li: {text}\\nKensluligt sj\u00f3narmi\u00f0: {label}\",\n#     prompt_label_mapping=dict(\n#         positive=\"positivur\", neutral=\"neutralur\", negative=\"negativur\"\n#     ),\n#     num_few_shot_examples=12,\n#     max_generated_tokens=1,\n# )\n\n\n### NAMED ENTITY RECOGNITION DATASETS ###\n\nSUC3_CONFIG = DatasetConfig(\n    name=\"suc3\",\n    pretty_name=\"the truncated version of the Swedish named entity recognition \"\n    \"dataset SUC 3.0\",\n    huggingface_id=\"ScandEval/suc3-mini\",\n    task=NER,\n    languages=[SV],\n    prompt_prefix=\"F\u00f6ljande \u00e4r meningar och JSON-ordb\u00f6cker med de namngivna enheter \"\n    \"som f\u00f6rekommer i den givna meningen.\",\n    prompt_template=\"Mening: {text}\\nNamngivna entiteter: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"plats\",\n        \"i-loc\": \"plats\",\n        \"b-org\": \"organisation\",\n        \"i-org\": \"organisation\",\n        \"b-misc\": \"diverse\",\n        \"i-misc\": \"diverse\",\n    },\n    instruction_prompt=\"Mening: {text}\\n\\nIdentifiera de namngivna enheterna i \"\n    \"meningen. Du ska outputta detta som en JSON-ordbok med nycklarna 'person', \"\n    \"'plats', 'organisation' och 'diverse'. V\u00e4rdena ska vara listor \u00f6ver de namngivna \"\n    \"enheter av den typen, precis som de f\u00f6rekommer i meningen.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nDANSK_CONFIG = DatasetConfig(\n    name=\"dansk\",\n    pretty_name=\"the truncated version of the Danish named entity recognition \"\n    \"dataset DANSK\",\n    huggingface_id=\"ScandEval/dansk-mini\",\n    task=NER,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er s\u00e6tninger og JSON-ordb\u00f8ger med de navngivne enheder, \"\n    \"som forekommer i den givne s\u00e6tning.\",\n    prompt_template=\"S\u00e6tning: {text}\\nNavngivne enheder: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"sted\",\n        \"i-loc\": \"sted\",\n        \"b-org\": \"organisation\",\n        \"i-org\": \"organisation\",\n        \"b-misc\": \"diverse\",\n        \"i-misc\": \"diverse\",\n    },\n    instruction_prompt=\"S\u00e6tning: {text}\\n\\nIdentific\u00e9r de navngivne enheder i \"\n    \"s\u00e6tningen. Du skal outputte dette som en JSON-ordbog med n\u00f8glerne 'person', \"\n    \"'sted', 'organisation' og 'diverse'. V\u00e6rdierne skal v\u00e6re lister over de navngivne \"\n    \"enheder af den type, pr\u00e6cis som de forekommer i s\u00e6tningen.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nNORNE_NB_CONFIG = DatasetConfig(\n    name=\"norne-nb\",\n    pretty_name=\"the truncated version of the Bokm\u00e5l part of the Norwegian named \"\n    \"entity recognition dataset NorNE\",\n    huggingface_id=\"ScandEval/norne-nb-mini\",\n    task=NER,\n    languages=[NB, NO],\n    prompt_prefix=\"F\u00f8lgende er fraser og JSON-ordb\u00f8ker med de navngitte enhetene \"\n    \"som forekommer i den gitte frasen.\",\n    prompt_template=\"Frase: {text}\\nNavngitte enheter: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"sted\",\n        \"i-loc\": \"sted\",\n        \"b-org\": \"organisasjon\",\n        \"i-org\": \"organisasjon\",\n        \"b-misc\": \"diverse\",\n        \"i-misc\": \"diverse\",\n    },\n    instruction_prompt=\"Frase: {text}\\n\\nIdentifiser de navngitte enhetene i frasen. \"\n    \"Du b\u00f8r outputte dette som en JSON-ordbok med n\u00f8klene 'person', 'sted', \"\n    \"'organisasjon' og 'diverse'. Verdiene skal v\u00e6re lister over de navngitte enhetene \"\n    \"av den typen, akkurat som de vises i frasen.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nNORNE_NN_CONFIG = DatasetConfig(\n    name=\"norne-nn\",\n    pretty_name=\"the truncated version of the Nynorsk part of the Norwegian named \"\n    \"entity recognition dataset NorNE\",\n    huggingface_id=\"ScandEval/norne-nn-mini\",\n    task=NER,\n    languages=[NN],\n    prompt_prefix=\"F\u00f8lgende er fraser og JSON-ordb\u00f8ker med de navngitte enhetene \"\n    \"som forekommer i den gitte frasen.\",\n    prompt_template=\"Frase: {text}\\nNavngitte enheter: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"sted\",\n        \"i-loc\": \"sted\",\n        \"b-org\": \"organisasjon\",\n        \"i-org\": \"organisasjon\",\n        \"b-misc\": \"diverse\",\n        \"i-misc\": \"diverse\",\n    },\n    instruction_prompt=\"Frase: {text}\\n\\nIdentifiser de navngitte enhetene i frasen. \"\n    \"Du b\u00f8r outputte dette som en JSON-ordbok med n\u00f8klene 'person', 'sted', \"\n    \"'organisasjon' og 'diverse'. Verdiene skal v\u00e6re lister over de navngitte enhetene \"\n    \"av den typen, akkurat som de vises i frasen.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nMIM_GOLD_NER_CONFIG = DatasetConfig(\n    name=\"mim-gold-ner\",\n    pretty_name=\"the truncated version of the Icelandic named entity recognition \"\n    \"dataset MIM-GOLD-NER\",\n    huggingface_id=\"ScandEval/mim-gold-ner-mini\",\n    task=NER,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru setningar \u00e1samt JSON lyklum me\u00f0 nefndum einingum \"\n    \"sem koma fyrir \u00ed setningunum.\",\n    prompt_template=\"Setning: {text}\\nNefndar einingar: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"einstaklingur\",\n        \"i-per\": \"einstaklingur\",\n        \"b-loc\": \"sta\u00f0setning\",\n        \"i-loc\": \"sta\u00f0setning\",\n        \"b-org\": \"stofnun\",\n        \"i-org\": \"stofnun\",\n        \"b-misc\": \"\u00fdmislegt\",\n        \"i-misc\": \"\u00fdmislegt\",\n    },\n    instruction_prompt=\"Setning: {text}\\n\\nGreini\u00f0 nefndu einingarnar \u00ed setningunni. \"\n    \"\u00de\u00fa \u00e6ttir a\u00f0 skila \u00feessu sem JSON or\u00f0ab\u00f3k me\u00f0 lyklunum 'einstaklingur', \"\n    \"'sta\u00f0setning', 'stofnun' og '\u00fdmislegt'. Gildin \u00e6ttu a\u00f0 vera listi yfir nefndu \"\n    \"einingarnar af \u00feeirri ger\u00f0, n\u00e1kv\u00e6mlega eins og \u00fe\u00e6r koma fram \u00ed setningunni.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nFONE_CONFIG = DatasetConfig(\n    name=\"fone\",\n    pretty_name=\"the truncated version of the Faroese named entity recognition \"\n    \"dataset FoNE\",\n    huggingface_id=\"ScandEval/fone-mini\",\n    task=NER,\n    languages=[FO],\n    prompt_prefix=\"Her eru nakrir setningar og nakrar JSON or\u00f0ab\u00f8kur vi\u00f0 nevndar \"\n    \"eindir, sum eru \u00ed setningunum.\",\n    prompt_template=\"Setningur: {text}\\nNevndar eindir: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"pers\u00f3nur\",\n        \"i-per\": \"pers\u00f3nur\",\n        \"b-loc\": \"sta\u00f0ur\",\n        \"i-loc\": \"sta\u00f0ur\",\n        \"b-org\": \"felagsskapur\",\n        \"i-org\": \"felagsskapur\",\n        \"b-misc\": \"ymiskt\",\n        \"i-misc\": \"ymiskt\",\n    },\n    instruction_prompt=\"Setningur: {text}\\n\\nGreini\u00f0 nevndu einingarnar \u00ed setningunni. \"\n    \"\u00de\u00fa \u00e6ttir a\u00f0 skila \u00feessu sem JSON or\u00f0ab\u00f3k me\u00f0 lyklunum 'pers\u00f3nur', 'sta\u00f0ur', \"\n    \"'felagsskapur' og 'ymiskt'. Gildin \u00e6ttu a\u00f0 vera listi yfir nevndu einingarnar af \"\n    \"\u00feeirri ger\u00f0, n\u00e1kv\u00e6mlega eins og \u00fe\u00e6r koma fram \u00ed setningunni.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nGERMEVAL_CONFIG = DatasetConfig(\n    name=\"germeval\",\n    pretty_name=\"the truncated version of the German named entity recognition \"\n    \"dataset GermEval\",\n    huggingface_id=\"ScandEval/germeval-mini\",\n    task=NER,\n    languages=[DE],\n    prompt_prefix=\"Es folgen S\u00e4tze und JSON-W\u00f6rterb\u00fccher mit den benannten \"\n    \"Entit\u00e4ten, die in der angegebenen Phrase vorkommen.\",\n    prompt_template=\"Satz: {text}\\nBenannte Entit\u00e4ten: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"ort\",\n        \"i-loc\": \"ort\",\n        \"b-org\": \"organisation\",\n        \"i-org\": \"organisation\",\n        \"b-misc\": \"verschiedenes\",\n        \"i-misc\": \"verschiedenes\",\n    },\n    instruction_prompt=\"Satz: {text}\\n\\nIdentifizieren Sie die benannten Entit\u00e4ten im \"\n    \"Satz. Sie sollten dies als JSON-W\u00f6rterbuch mit den Schl\u00fcsseln 'person', 'ort', \"\n    \"'organisation' und 'verschiedenes' ausgeben. Die Werte sollten Listen der \"\n    \"benannten Entit\u00e4ten dieses Typs sein, genau wie sie im Satz erscheinen.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nCONLL_NL_CONFIG = DatasetConfig(\n    name=\"conll-nl\",\n    pretty_name=\"the Dutch part of the truncated version of the named entity \"\n    \"recognition dataset CoNLL 2002\",\n    huggingface_id=\"ScandEval/conll-nl-mini\",\n    task=NER,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan zinnen en JSON woordenboeken met de genoemde \"\n    \"entiteiten die voorkomen in de gegeven zin.\",\n    prompt_template=\"Zin: {text}\\nGenoemde entiteiten: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"persoon\",\n        \"i-per\": \"persoon\",\n        \"b-loc\": \"locatie\",\n        \"i-loc\": \"locatie\",\n        \"b-org\": \"organisatie\",\n        \"i-org\": \"organisatie\",\n        \"b-misc\": \"diversen\",\n        \"i-misc\": \"diversen\",\n    },\n    instruction_prompt=\"Zin: {text}\\n\\nIdentificeer de genoemde entiteiten in de zin. \"\n    \"Je moet dit uitvoeren als een JSON-woordenboek met de sleutels 'persoon', \"\n    \"'locatie', 'organisatie' en 'diversen'. De waarden moeten lijsten zijn van de \"\n    \"genoemde entiteiten van dat type, precies zoals ze voorkomen in de zin.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nCONLL_EN_CONFIG = DatasetConfig(\n    name=\"conll-en\",\n    pretty_name=\"the truncated version of the English named entity recognition \"\n    \"dataset CoNLL 2003\",\n    huggingface_id=\"ScandEval/conll-en-mini\",\n    task=NER,\n    languages=[EN],\n    prompt_prefix=\"Below are sentences and JSON dictionaries with the named \"\n    \"entities that occur in the given sentence.\",\n    prompt_template=\"Sentence: {text}\\nNamed entities: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"location\",\n        \"i-loc\": \"location\",\n        \"b-org\": \"organization\",\n        \"i-org\": \"organization\",\n        \"b-misc\": \"miscellaneous\",\n        \"i-misc\": \"miscellaneous\",\n    },\n    instruction_prompt=\"Sentence: {text}\\n\\nIdentify the named entities in the \"\n    \"sentence. You should output this as a JSON dictionary with the keys being \"\n    \"'person', 'location', 'organization' and 'miscellaneous'. The values should be \"\n    \"lists of the named entities of that type, exactly as they appear in the sentence.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nDANE_CONFIG = DatasetConfig(\n    name=\"dane\",\n    pretty_name=\"the truncated version of the Danish named entity recognition \"\n    \"dataset DaNE\",\n    huggingface_id=\"ScandEval/dane-mini\",\n    task=NER,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er s\u00e6tninger og JSON-ordb\u00f8ger med de navngivne enheder, \"\n    \"som forekommer i den givne s\u00e6tning.\",\n    prompt_template=\"S\u00e6tning: {text}\\nNavngivne enheder: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"sted\",\n        \"i-loc\": \"sted\",\n        \"b-org\": \"organisation\",\n        \"i-org\": \"organisation\",\n        \"b-misc\": \"diverse\",\n        \"i-misc\": \"diverse\",\n    },\n    instruction_prompt=\"S\u00e6tning: {text}\\n\\nIdentific\u00e9r de navngivne enheder i \"\n    \"s\u00e6tningen. Du skal outputte dette som en JSON-ordbog med n\u00f8glerne 'person', \"\n    \"'sted', 'organisation' og 'diverse'. V\u00e6rdierne skal v\u00e6re lister over de navngivne \"\n    \"enheder af den type, pr\u00e6cis som de forekommer i s\u00e6tningen.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n    unofficial=True,\n)\n\nWIKIANN_FO_CONFIG = DatasetConfig(\n    name=\"wikiann-fo\",\n    pretty_name=\"the truncated version of the Faroese part of the named entity \"\n    \"recognition dataset WikiANN\",\n    huggingface_id=\"ScandEval/wikiann-fo-mini\",\n    task=NER,\n    languages=[FO],\n    prompt_prefix=\"Her eru nakrir setningar og nakrar JSON or\u00f0ab\u00f8kur vi\u00f0 nevndar \"\n    \"eindir, sum eru \u00ed setningunum.\",\n    prompt_template=\"Setningur: {text}\\nNevndar eindir: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"pers\u00f3nur\",\n        \"i-per\": \"pers\u00f3nur\",\n        \"b-loc\": \"sta\u00f0ur\",\n        \"i-loc\": \"sta\u00f0ur\",\n        \"b-org\": \"felagsskapur\",\n        \"i-org\": \"felagsskapur\",\n        \"b-misc\": \"ymiskt\",\n        \"i-misc\": \"ymiskt\",\n    },\n    instruction_prompt=\"Setningur: {text}\\n\\nGreini\u00f0 nevndu einingarnar \u00ed setningunni. \"\n    \"\u00de\u00fa \u00e6ttir a\u00f0 skila \u00feessu sem JSON or\u00f0ab\u00f3k me\u00f0 lyklunum 'pers\u00f3nur', 'sta\u00f0ur', \"\n    \"'felagsskapur' og 'ymiskt'. Gildin \u00e6ttu a\u00f0 vera listi yfir nevndu einingarnar af \"\n    \"\u00feeirri ger\u00f0, n\u00e1kv\u00e6mlega eins og \u00fe\u00e6r koma fram \u00ed setningunni.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n    unofficial=True,\n)\n\n\n### LINGUISTIC ACCEPTABILITY DATASETS ###\n\nSCALA_SV_CONFIG = DatasetConfig(\n    name=\"scala-sv\",\n    pretty_name=\"The Swedish part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-sv\",\n    task=LA,\n    languages=[SV],\n    prompt_prefix=\"F\u00f6ljande \u00e4r meningar och huruvida de \u00e4r grammatiskt korrekta.\",\n    prompt_template=\"Mening: {text}\\nGrammatisk korrekt: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nej\"),\n    instruction_prompt=\"Mening: {text}\\n\\nBest\u00e4m om meningen \u00e4r grammatiskt korrekt \"\n    \"eller inte. Svara med 'ja' om meningen \u00e4r korrekt och 'nej' om den inte \u00e4r.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nSCALA_DA_CONFIG = DatasetConfig(\n    name=\"scala-da\",\n    pretty_name=\"the Danish part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-da\",\n    task=LA,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er s\u00e6tninger og om de er grammatisk korrekte.\",\n    prompt_template=\"S\u00e6tning: {text}\\nGrammatisk korrekt: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nej\"),\n    instruction_prompt=\"S\u00e6tning: {text}\\n\\nBestem om s\u00e6tningen er grammatisk korrekt \"\n    \"eller ej. Svar med 'ja', hvis s\u00e6tningen er korrekt, og 'nej', hvis den ikke er.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nSCALA_NB_CONFIG = DatasetConfig(\n    name=\"scala-nb\",\n    pretty_name=\"the Bokm\u00e5l part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-nb\",\n    task=LA,\n    languages=[NB, NO],\n    prompt_prefix=\"F\u00f8lgende er setninger og hvorvidt de er grammatisk korrekte.\",\n    prompt_template=\"Setning: {text}\\nGrammatisk korrekt: {label}\",\n    instruction_prompt=\"Setning: {text}\\n\\nBestem om setningen er grammatisk korrekt \"\n    \"eller ikke. Svar med 'ja' hvis setningen er korrekt og 'nei' hvis den ikke er.\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nei\"),\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nSCALA_NN_CONFIG = DatasetConfig(\n    name=\"scala-nn\",\n    pretty_name=\"the Nynorsk part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-nn\",\n    task=LA,\n    languages=[NN],\n    prompt_prefix=\"F\u00f8lgende er setninger og hvorvidt de er grammatisk korrekte.\",\n    prompt_template=\"Setning: {text}\\nGrammatisk korrekt: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nei\"),\n    instruction_prompt=\"Setning: {text}\\n\\nBestem om setningen er grammatisk korrekt \"\n    \"eller ikke. Svar med 'ja' hvis setningen er korrekt og 'nei' hvis den ikke er.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nSCALA_IS_CONFIG = DatasetConfig(\n    name=\"scala-is\",\n    pretty_name=\"the Icelandic part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-is\",\n    task=LA,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru setningar og hvort \u00fe\u00e6r eru m\u00e1lfr\u00e6\u00f0ilega r\u00e9ttar.\",\n    prompt_template=\"Setning: {text}\\nM\u00e1lfr\u00e6\u00f0ilega r\u00e9tt: {label}\",\n    prompt_label_mapping=dict(correct=\"j\u00e1\", incorrect=\"nei\"),\n    instruction_prompt=\"Setning: {text}\\n\\nGreini\u00f0 hvort setningin er m\u00e1lfr\u00e6\u00f0ilega \"\n    \"r\u00e9tt e\u00f0a ekki. Svari\u00f0 skal vera 'j\u00e1' ef setningin er r\u00e9tt og 'nei' ef h\u00fan er \"\n    \"ekki.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nSCALA_FO_CONFIG = DatasetConfig(\n    name=\"scala-fo\",\n    pretty_name=\"the Faroese part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-fo\",\n    task=LA,\n    languages=[FO],\n    prompt_prefix=\"Hetta eru nakrir setningar og um teir eru m\u00e1ll\u00e6ruliga r\u00e6ttir.\",\n    prompt_template=\"Setningur: {text}\\nM\u00e1ll\u00e6ruliga r\u00e6ttur: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nei\"),\n    instruction_prompt=\"Setningur: {text}\\n\\nGreini\u00f0 hvort setningurin er m\u00e1ll\u00e6ruliga \"\n    \"r\u00e6ttur ella ikki. Svari\u00f0 skal vera 'ja' um setningurin er r\u00e6ttur og 'nei' um \"\n    \"hann ikki er.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nSCALA_DE_CONFIG = DatasetConfig(\n    name=\"scala-de\",\n    pretty_name=\"the German part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-de\",\n    task=LA,\n    languages=[DE],\n    prompt_prefix=\"Die folgenden S\u00e4tze und ob sie grammatikalisch korrekt sind.\",\n    prompt_template=\"Satz: {text}\\nGrammatikalisch richtig: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nein\"),\n    instruction_prompt=\"Satz: {text}\\n\\nBestimmen Sie, ob der Satz grammatikalisch \"\n    \"korrekt ist oder nicht. Antworten Sie mit 'ja', wenn der Satz korrekt ist und \"\n    \"'nein', wenn er es nicht ist.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nSCALA_NL_CONFIG = DatasetConfig(\n    name=\"scala-nl\",\n    pretty_name=\"the Dutch part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-nl\",\n    task=LA,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan zinnen en of ze grammaticaal correct zijn.\",\n    prompt_template=\"Zin: {text}\\nGrammaticaal correct: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nee\"),\n    instruction_prompt=\"Zin: {text}\\n\\nBepaal of de zin grammaticaal correct is of \"\n    \"niet. Antwoord met 'ja' als de zin correct is en 'nee' als dat niet het geval is.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nSCALA_EN_CONFIG = DatasetConfig(\n    name=\"scala-en\",\n    pretty_name=\"the English part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-en\",\n    task=LA,\n    languages=[EN],\n    prompt_prefix=\"The following are sentences and whether they are grammatically \"\n    \"correct.\",\n    prompt_template=\"Sentence: {text}\\nGrammatically correct: {label}\",\n    prompt_label_mapping=dict(correct=\"yes\", incorrect=\"no\"),\n    instruction_prompt=\"Sentence: {text}\\n\\nDetermine whether the sentence is \"\n    \"grammatically correct or not. Reply with 'yes' if the sentence is correct and \"\n    \"'no' if it is not.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nDUTCH_COLA_CONFIG = DatasetConfig(\n    name=\"dutch-cola\",\n    pretty_name=\"the truncated version of the Dutch linguistic acceptability dataset \"\n    \"Dutch CoLA\",\n    huggingface_id=\"ScandEval/dutch-cola\",\n    task=LA,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan zinnen en of ze grammaticaal correct ('ja') of \"\n    \"incorrect ('nee') zijn.\",\n    prompt_template=\"Zin: {text}\\nGrammaticaal correct: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nee\"),\n    instruction_prompt=\"Zin: {text}\\n\\nBepaal of de zin grammaticaal correct is of \"\n    \"niet. Antwoord met 'ja' als de zin correct is en 'nee' als dat niet het geval is.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=3,\n    unofficial=True,\n)\n\nDUTCH_COLA_FULL_CONFIG = DatasetConfig(\n    name=\"dutch-cola-full\",\n    pretty_name=\"the Dutch linguistic acceptability dataset Dutch CoLA\",\n    huggingface_id=\"ScandEval/dutch-cola-full\",\n    task=LA,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan zinnen en of ze grammaticaal correct ('ja') of \"\n    \"incorrect ('nee') zijn.\",\n    prompt_template=\"Zin: {text}\\nGrammaticaal correct: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nee\"),\n    instruction_prompt=\"Zin: {text}\\n\\nBepaal of de zin grammaticaal correct is of \"\n    \"niet. Antwoord met 'ja' als de zin correct is en 'nee' als dat niet het geval is.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=3,\n    unofficial=True,\n)\n\nICE_EC_CONFIG = DatasetConfig(\n    name=\"ice-ec\",\n    pretty_name=\"the truncated version of the Icelandic Error Corpus\",\n    huggingface_id=\"ScandEval/ice-ec\",\n    task=LA,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru setningar og hvort \u00fe\u00e6r eru m\u00e1lfr\u00e6\u00f0ilega r\u00e9ttar.\",\n    prompt_template=\"Setning: {text}\\nM\u00e1lfr\u00e6\u00f0ilega r\u00e9tt: {label}\",\n    prompt_label_mapping=dict(correct=\"j\u00e1\", incorrect=\"nei\"),\n    instruction_prompt=\"Setning: {text}\\n\\nGreini\u00f0 hvort setningin er m\u00e1lfr\u00e6\u00f0ilega \"\n    \"r\u00e9tt e\u00f0a ekki. Svari\u00f0 skal vera 'j\u00e1' ef setningin er r\u00e9tt og 'nei' ef h\u00fan er \"\n    \"ekki.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nICE_EC_FULL_CONFIG = DatasetConfig(\n    name=\"ice-ec-full\",\n    pretty_name=\"the Icelandic Error Corpus\",\n    huggingface_id=\"ScandEval/ice-ec-full\",\n    task=LA,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru setningar og hvort \u00fe\u00e6r eru m\u00e1lfr\u00e6\u00f0ilega r\u00e9ttar.\",\n    prompt_template=\"Setning: {text}\\nM\u00e1lfr\u00e6\u00f0ilega r\u00e9tt: {label}\",\n    prompt_label_mapping=dict(correct=\"j\u00e1\", incorrect=\"nei\"),\n    instruction_prompt=\"Setning: {text}\\n\\nGreini\u00f0 hvort setningin er m\u00e1lfr\u00e6\u00f0ilega \"\n    \"r\u00e9tt e\u00f0a ekki. Svari\u00f0 skal vera 'j\u00e1' ef setningin er r\u00e9tt og 'nei' ef h\u00fan er ekki.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nICE_LINGUISTIC_CONFIG = DatasetConfig(\n    name=\"ice-linguistic\",\n    pretty_name=\"the Icelandic linguistic acceptability dataset IceLinguistic\",\n    huggingface_id=\"ScandEval/ice-linguistic\",\n    task=LA,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru setningar og hvort \u00fe\u00e6r eru m\u00e1lfr\u00e6\u00f0ilega r\u00e9ttar.\",\n    prompt_template=\"Setning: {text}\\nM\u00e1lfr\u00e6\u00f0ilega r\u00e9tt: {label}\",\n    prompt_label_mapping=dict(correct=\"j\u00e1\", incorrect=\"nei\"),\n    instruction_prompt=\"Setning: {text}\\n\\nGreini\u00f0 hvort setningin er m\u00e1lfr\u00e6\u00f0ilega \"\n    \"r\u00e9tt e\u00f0a ekki. Svari\u00f0 skal vera 'j\u00e1' ef setningin er r\u00e9tt og 'nei' ef h\u00fan er ekki.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\n\n### READING COMPREHENSION DATASETS ###\n\nSCANDIQA_DA_CONFIG = DatasetConfig(\n    name=\"scandiqa-da\",\n    pretty_name=\"the Danish part of the truncated version of the question answering \"\n    \"dataset ScandiQA\",\n    huggingface_id=\"ScandEval/scandiqa-da-mini\",\n    task=RC,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er tekster med tilh\u00f8rende sp\u00f8rgsm\u00e5l og svar.\",\n    prompt_template=\"Tekst: {text}\\nSp\u00f8rgsm\u00e5l: {question}\\nSvar med maks. 3 ord: \"\n    \"{label}\",\n    instruction_prompt=\"Tekst: {text}\\n\\nBesvar f\u00f8lgende sp\u00f8rgsm\u00e5l om teksten ovenfor \"\n    \"med maks. 3 ord.\\n\\nSp\u00f8rgsm\u00e5l: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nNORQUAD_CONFIG = DatasetConfig(\n    name=\"norquad\",\n    pretty_name=\"the truncated version of the Norwegian question answering \"\n    \"dataset NorQuAD\",\n    huggingface_id=\"ScandEval/norquad-mini\",\n    task=RC,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"Her f\u00f8lger tekster med tilh\u00f8rende sp\u00f8rsm\u00e5l og svar.\",\n    prompt_template=\"Tekst: {text}\\nSp\u00f8rsm\u00e5l: {question}\\nSvar p\u00e5 maks 3 ord: {label}\",\n    instruction_prompt=\"Tekst: {text}\\n\\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l om teksten ovenfor \"\n    \"med maks 3 ord.\\n\\nSp\u00f8rsm\u00e5l: {question}\",\n    num_few_shot_examples=2,\n    max_generated_tokens=32,\n)\n\nSCANDIQA_SV_CONFIG = DatasetConfig(\n    name=\"scandiqa-sv\",\n    pretty_name=\"the Swedish part of the truncated version of the question answering \"\n    \"dataset ScandiQA\",\n    huggingface_id=\"ScandEval/scandiqa-sv-mini\",\n    task=RC,\n    languages=[SV],\n    prompt_prefix=\"Nedan f\u00f6ljer texter med tillh\u00f6rande fr\u00e5gor och svar.\",\n    prompt_template=\"Text: {text}\\nFr\u00e5ga: {question}\\nSvar p\u00e5 max 3 ord: {label}\",\n    instruction_prompt=\"Text: {text}\\n\\nBesvara f\u00f6ljande fr\u00e5ga om texten ovan med \"\n    \"h\u00f6gst 3 ord.\\n\\nFr\u00e5ga: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nNQII_CONFIG = DatasetConfig(\n    name=\"nqii\",\n    pretty_name=\"the truncated version of the Icelandic question answering dataset \"\n    \"Natural Questions in Icelandic\",\n    huggingface_id=\"ScandEval/nqii-mini\",\n    task=RC,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru textar me\u00f0 tilheyrandi spurningum og sv\u00f6rum.\",\n    prompt_template=\"Texti: {text}\\nSpurning: {question}\\nSvara\u00f0u me\u00f0 a\u00f0 h\u00e1marki 3 \"\n    \"or\u00f0um: {label}\",\n    instruction_prompt=\"Texti: {text}\\n\\nSvara\u00f0u eftirfarandi spurningu um textann a\u00f0 \"\n    \"h\u00e1marki \u00ed 3 or\u00f0um.\\n\\nSpurning: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nFOQA_CONFIG = DatasetConfig(\n    name=\"foqa\",\n    pretty_name=\"the Faroese question answering dataset FoQA\",\n    huggingface_id=\"ScandEval/foqa\",\n    task=RC,\n    languages=[FO],\n    prompt_prefix=\"Hetta eru tekstir saman vi\u00f0 spurningum og svar.\",\n    prompt_template=\"{text}\\nSpurningur: {question}\\nSvara vi\u00f0 \u00ed mesta lagi trimum \"\n    \"or\u00f0um: {label}\",\n    instruction_prompt=\"Tekstur: {text}\\n\\nSvara hesum spurninginum um tekstin \"\n    \"uppiyvir vi\u00f0 \u00ed mesta lagi trimum or\u00f0um.\\n\\nSpurningur: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nGERMANQUAD_CONFIG = DatasetConfig(\n    name=\"germanquad\",\n    pretty_name=\"the truncated version of the German question answering dataset \"\n    \"GermanQuAD\",\n    huggingface_id=\"ScandEval/germanquad-mini\",\n    task=RC,\n    languages=[DE],\n    prompt_prefix=\"Im Folgenden finden Sie Texte mit den dazugeh\u00f6rigen Fragen und \"\n    \"Antworten.\",\n    prompt_template=\"Text: {text}\\nFragen: {question}\\nFragen Antwort in maximal 3 \"\n    \"W\u00f6rtern: {label}\",\n    instruction_prompt=\"Text: {text}\\n\\nBeantworten Sie die folgende Frage zum obigen \"\n    \"Text in h\u00f6chstens 3 W\u00f6rtern.\\n\\nFrage: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nSQUAD_CONFIG = DatasetConfig(\n    name=\"squad\",\n    pretty_name=\"the truncated version of the English question answering \"\n    \"dataset SQuAD\",\n    huggingface_id=\"ScandEval/squad-mini\",\n    task=RC,\n    languages=[EN],\n    prompt_prefix=\"The following are texts with accompanying questions and answers.\",\n    prompt_template=\"Text: {text}\\nQuestion: {question}\\nAnswer in max 3 words: \"\n    \"{label}\",\n    instruction_prompt=\"Text: {text}\\n\\nAnswer the following question about the \"\n    \"above text in at most 3 words.\\n\\nQuestion: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nSQUAD_NL_CONFIG = DatasetConfig(\n    name=\"squad-nl\",\n    pretty_name=\"the truncated version of the Dutch question answering dataset \"\n    \"SQuAD-nl, translated from the English SQuAD dataset\",\n    huggingface_id=\"ScandEval/squad-nl-mini\",\n    task=RC,\n    languages=[NL],\n    prompt_prefix=\"Hieronder volgen teksten met bijbehorende vragen en antwoorden.\",\n    prompt_template=\"Tekst: {text}\\nVraag: {question}\\nAntwoord in max 3 woorden: \"\n    \"{label}\",\n    instruction_prompt=\"Tekst: {text}\\n\\nBeantwoord de volgende vraag over de \"\n    \"bovenstaande tekst in maximaal 3 woorden.\\n\\nVraag: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nICELANDIC_QA_CONFIG = DatasetConfig(\n    name=\"icelandic-qa\",\n    pretty_name=\"the Icelandic question answering dataset about Icelandic culture and \"\n    \"history\",\n    huggingface_id=\"ScandEval/icelandic-qa\",\n    task=RC,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru textar me\u00f0 tilheyrandi spurningum og sv\u00f6rum.\",\n    prompt_template=\"Texti: {text}\\nSpurning: {question}\\nSvara\u00f0u me\u00f0 a\u00f0 h\u00e1marki 3 \"\n    \"or\u00f0um: {label}\",\n    instruction_prompt=\"Texti: {text}\\n\\nSvara\u00f0u eftirfarandi spurningu um textann a\u00f0 \"\n    \"h\u00e1marki \u00ed 3 or\u00f0um.\\n\\nSpurning: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n    unofficial=True,\n)\n\n### SUMMARIZATION DATASETS ###\n\nNORDJYLLAND_NEWS_CONFIG = DatasetConfig(\n    name=\"nordjylland-news\",\n    pretty_name=\"the truncated version of the Danish summarisation dataset \"\n    \"Nordjylland News\",\n    huggingface_id=\"ScandEval/nordjylland-news-mini\",\n    task=SUMM,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er nyhedsartikler med tilh\u00f8rende resum\u00e9er.\",\n    prompt_template=\"Nyhedsartikel: {text}\\nResum\u00e9: {target_text}\",\n    instruction_prompt=\"Nyhedsartikel: {text}\\n\\nSkriv et resum\u00e9 af ovenst\u00e5ende \"\n    \"artikel.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nMLSUM_CONFIG = DatasetConfig(\n    name=\"mlsum\",\n    pretty_name=\"the truncated version of the German summarisation dataset MLSum\",\n    huggingface_id=\"ScandEval/mlsum-mini\",\n    task=SUMM,\n    languages=[DE],\n    prompt_prefix=\"Im Folgenden finden Sie Nachrichtenartikel mit den dazugeh\u00f6rigen \"\n    \"Zusammenfassungen.\",\n    prompt_template=\"Nachrichtenartikel: {text}\\nZusammenfassung: {target_text}\",\n    instruction_prompt=\"Nachrichtenartikel: {text}\\n\\nSchreiben Sie eine \"\n    \"Zusammenfassung des obigen Artikels.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nRRN_CONFIG = DatasetConfig(\n    name=\"rrn\",\n    pretty_name=\"the truncated version of the Icelandic summarisation dataset \"\n    \"R\u00daV Radio News\",\n    huggingface_id=\"ScandEval/rrn-mini\",\n    task=SUMM,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru fr\u00e9ttagreinar me\u00f0 tilheyrandi samantektum.\",\n    prompt_template=\"Fr\u00e9ttagrein: {text}\\nSamantekt: {target_text}\",\n    instruction_prompt=\"Fr\u00e9ttagrein: {text}\\n\\nSkrifa\u00f0u samantekt um ofangreindu \"\n    \"grein.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nICESUM_CONFIG = DatasetConfig(\n    name=\"icesum\",\n    pretty_name=\"the Icelandic summarisation dataset IceSum\",\n    huggingface_id=\"ScandEval/icesum\",\n    task=SUMM,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru fr\u00e9ttagreinar me\u00f0 tilheyrandi samantektum.\",\n    prompt_template=\"Fr\u00e9ttagrein: {text}\\nSamantekt: {target_text}\",\n    instruction_prompt=\"Fr\u00e9ttagrein: {text}\\n\\nSkrifa\u00f0u samantekt um ofangreindu \"\n    \"grein.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n    unofficial=True,\n)\n\nNO_SAMMENDRAG_CONFIG = DatasetConfig(\n    name=\"no-sammendrag\",\n    pretty_name=\"the truncated version of the Norwegian summarisation dataset \"\n    \"Norske Sammendrag\",\n    huggingface_id=\"ScandEval/no-sammendrag-mini\",\n    task=SUMM,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"Her f\u00f8lger nyhetsartikler med tilh\u00f8rende sammendrag.\",\n    prompt_template=\"Nyhetsartikkel: {text}\\nSammendrag: {target_text}\",\n    instruction_prompt=\"Nyhetsartikkel: {text}\\n\\nSkriv et sammendrag av den \"\n    \"ovennevnte artikkelen.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nNORGLM_MULTI_SUM = DatasetConfig(\n    name=\"norglm-multi-sum\",\n    pretty_name=\"the summarisation part of the Norwegian NorGLM multi-task human \"\n    \"annotated dataset NO-Multi-QA-Sum\",\n    huggingface_id=\"ScandEval/norglm-multi-sum\",\n    task=SUMM,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"Her f\u00f8lger nyhetsartikler med tilh\u00f8rende sammendrag.\",\n    prompt_template=\"Nyhetsartikkel: {text}\\nSammendrag: {target_text}\",\n    instruction_prompt=\"Nyhetsartikkel: {text}\\n\\nSkriv et sammendrag av den \"\n    \"ovennevnte artikkelen.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n    unofficial=True,\n)\n\nWIKI_LINGUA_NL_CONFIG = DatasetConfig(\n    name=\"wiki-lingua-nl\",\n    pretty_name=\"the Dutch part of the truncated version of the summarisation dataset \"\n    \"WikiLingua\",\n    huggingface_id=\"ScandEval/wiki-lingua-nl-mini\",\n    task=SUMM,\n    languages=[NL],\n    prompt_prefix=\"Hieronder volgen artikelen met bijbehorende samenvattingen.\",\n    prompt_template=\"Artikel: {text}\\nSamenvatting: {target_text}\",\n    instruction_prompt=\"Artikel: {text}\\n\\nSchrijf een samenvatting van het \"\n    \"bovenstaande artikel.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nSWEDN_CONFIG = DatasetConfig(\n    name=\"swedn\",\n    pretty_name=\"the truncated version of the Swedish summarisation dataset SweDN\",\n    huggingface_id=\"ScandEval/swedn-mini\",\n    task=SUMM,\n    languages=[SV],\n    prompt_prefix=\"Nedan f\u00f6ljer artiklar med tillh\u00f6rande sammanfattningar.\",\n    prompt_template=\"Artikel: {text}\\nSammanfattning: {target_text}\",\n    instruction_prompt=\"Artikel: {text}\\n\\nSkriv en sammanfattning av artikeln ovan.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nCNN_DAILYMAIL_CONFIG = DatasetConfig(\n    name=\"cnn-dailymail\",\n    pretty_name=\"the truncated version of the English summarisation dataset \"\n    \"CNN-DailyMail\",\n    huggingface_id=\"ScandEval/cnn-dailymail-mini\",\n    task=SUMM,\n    languages=[EN],\n    prompt_prefix=\"The following are articles with accompanying summaries.\",\n    prompt_template=\"News article: {text}\\nSummary: {target_text}\",\n    instruction_prompt=\"News article: {text}\\n\\nWrite a summary of the above \"\n    \"article.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nSCHIBSTED_SV = DatasetConfig(\n    name=\"schibsted-sv\",\n    pretty_name=\"article summaries from Schibsted Media Swedish newsrooms.\",\n    huggingface_id=\"ScandEval/schibsted-article-summaries-sv\",\n    task=SUMM,\n    languages=[SV],\n    prompt_prefix=\"Nedan f\u00f6ljer artiklar med tillh\u00f6rande sammanfattningar.\",\n    prompt_template=\"Artikel: {text}\\nSammanfattning: {target_text}\",\n    instruction_prompt=\"Artikel: {text}\\n\\nSkriv en sammanfattning av artikeln ovan.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n    unofficial=True,\n)\n\nSCHIBSTED_NO = DatasetConfig(\n    name=\"schibsted-no\",\n    pretty_name=\"article summaries from Schibsted Medias Norwegian newsrooms.\",\n    huggingface_id=\"ScandEval/schibsted-article-summaries-no\",\n    task=SUMM,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"Her f\u00f8lger nyhetsartikler med tilh\u00f8rende sammendrag.\",\n    prompt_template=\"Nyhetsartikkel: {text}\\nSammendrag: {target_text}\",\n    instruction_prompt=\"Nyhetsartikkel: {text}\\n\\nSkriv et sammendrag av den \"\n    \"ovennevnte artikkelen.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n    unofficial=True,\n)\n\n# TODO: Faroese summarization\n\n\n### KNOWLEDGE DATASETS ###\n\nDANSKE_TALEMAADER_CONFIG = DatasetConfig(\n    name=\"danske-talemaader\",\n    pretty_name=\"the truncated version of the Danish knowledge dataset Danske \"\n    \"Talem\u00e5der\",\n    huggingface_id=\"ScandEval/danske-talemaader-mini\",\n    task=KNOW,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\",\n    prompt_template=\"Hvad er betydningen af f\u00f8lgende talem\u00e5de: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rgsm\u00e5l: {text}\\n\\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at \"\n    \"svare med 'a', 'b', 'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nDANISH_CITIZEN_TESTS_CONFIG = DatasetConfig(\n    name=\"danish-citizen-tests\",\n    pretty_name=\"the Danish knowledge dataset Danish Citizen Tests\",\n    huggingface_id=\"ScandEval/danish-citizen-tests\",\n    task=KNOW,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rgsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rgsm\u00e5l: {text}\\n\\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at \"\n    \"svare med 'a', 'b', 'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nMMLU_NO_CONFIG = DatasetConfig(\n    name=\"mmlu-no\",\n    pretty_name=\"the truncated version of the Norwegian knowledge dataset MMLU-no, \"\n    \"translated from the English MMLU dataset\",\n    huggingface_id=\"ScandEval/mmlu-no-mini\",\n    task=KNOW,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"F\u00f8lgende er flervalgssp\u00f8rsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rsm\u00e5l: {text}\\n\\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', \"\n    \"'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nMMLU_SV_CONFIG = DatasetConfig(\n    name=\"mmlu-sv\",\n    pretty_name=\"the truncated version of the Swedish knowledge dataset MMLU-sv, \"\n    \"translated from the English MMLU dataset\",\n    huggingface_id=\"ScandEval/mmlu-sv-mini\",\n    task=KNOW,\n    languages=[SV],\n    prompt_prefix=\"F\u00f6ljande \u00e4r flervalsfr\u00e5gor (med svar).\",\n    prompt_template=\"Fr\u00e5ga: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Fr\u00e5ga: {text}\\n\\nBesvara f\u00f6ljande fr\u00e5ga med 'a', 'b', 'c' \"\n    \"eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nMMLU_IS_CONFIG = DatasetConfig(\n    name=\"mmlu-is\",\n    pretty_name=\"the truncated version of the Icelandic knowledge dataset MMLU-is, \"\n    \"translated from the English MMLU dataset\",\n    huggingface_id=\"ScandEval/mmlu-is-mini\",\n    task=KNOW,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\",\n    prompt_template=\"Spurningar: {text}\\nSvara: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Spurningar: {text}\\n\\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', \"\n    \"'b', 'c' e\u00f0a 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nMMLU_DE_CONFIG = DatasetConfig(\n    name=\"mmlu-de\",\n    pretty_name=\"the truncated version of the German knowledge dataset MMLU-de, \"\n    \"translated from the English MMLU dataset\",\n    huggingface_id=\"ScandEval/mmlu-de-mini\",\n    task=KNOW,\n    languages=[DE],\n    prompt_prefix=\"Die folgenden Fragen sind Multiple-Choice-Fragen (mit Antworten).\",\n    prompt_template=\"Frage: {text}\\nAntwort: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Frage: {text}\\n\\nBeantworten Sie die obige Frage mit 'a', 'b', \"\n    \"'c' oder 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nMMLU_NL_CONFIG = DatasetConfig(\n    name=\"mmlu-nl\",\n    pretty_name=\"the truncated version of the Dutch knowledge dataset MMLU-nl, \"\n    \"translated from the English MMLU dataset\",\n    huggingface_id=\"ScandEval/mmlu-nl-mini\",\n    task=KNOW,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan meerkeuzevragen (met antwoorden).\",\n    prompt_template=\"Vraag: {text}\\nAntwoord: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Vraag: {text}\\n\\nBeantwoord de bovenstaande vraag met 'a', 'b', \"\n    \"'c' of 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nMMLU_CONFIG = DatasetConfig(\n    name=\"mmlu\",\n    pretty_name=\"the truncated version of the English knowledge dataset MMLU\",\n    huggingface_id=\"ScandEval/mmlu-mini\",\n    task=KNOW,\n    languages=[EN],\n    prompt_prefix=\"The following are multiple choice questions (with answers).\",\n    prompt_template=\"Question: {text}\\nAnswer: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Question: {text}\\n\\nAnswer the above question by \"\n    \"replying with 'a', 'b', 'c' or 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nMMLU_DA_CONFIG = DatasetConfig(\n    name=\"mmlu-da\",\n    pretty_name=\"the truncated version of the Danish knowledge dataset MMLU-da, \"\n    \"translated from the English MMLU dataset\",\n    huggingface_id=\"ScandEval/mmlu-da-mini\",\n    task=KNOW,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rgsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rgsm\u00e5l: {text}\\n\\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at \"\n    \"svare med 'a', 'b', 'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nARC_DA_CONFIG = DatasetConfig(\n    name=\"arc-da\",\n    pretty_name=\"the truncated version of the Danish knowledge dataset ARC-da, \"\n    \"translated from the English ARC dataset\",\n    huggingface_id=\"ScandEval/arc-da-mini\",\n    task=KNOW,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rgsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rgsm\u00e5l: {text}\\n\\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at \"\n    \"svare med 'a', 'b', 'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nARC_NO_CONFIG = DatasetConfig(\n    name=\"arc-no\",\n    pretty_name=\"the truncated version of the Norwegian knowledge dataset ARC-no, \"\n    \"translated from the English ARC dataset\",\n    huggingface_id=\"ScandEval/arc-no-mini\",\n    task=KNOW,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"F\u00f8lgende er flervalgssp\u00f8rsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rsm\u00e5l: {text}\\n\\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', \"\n    \"'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nARC_SV_CONFIG = DatasetConfig(\n    name=\"arc-sv\",\n    pretty_name=\"the truncated version of the Swedish knowledge dataset ARC-sv, \"\n    \"translated from the English ARC dataset\",\n    huggingface_id=\"ScandEval/arc-sv-mini\",\n    task=KNOW,\n    languages=[SV],\n    prompt_prefix=\"F\u00f6ljande \u00e4r flervalsfr\u00e5gor (med svar).\",\n    prompt_template=\"Fr\u00e5ga: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Fr\u00e5ga: {text}\\n\\nBesvara f\u00f6ljande fr\u00e5ga med 'a', 'b', 'c' \"\n    \"eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nARC_IS_CONFIG = DatasetConfig(\n    name=\"arc-is\",\n    pretty_name=\"the truncated version of the Icelandic knowledge dataset ARC-is, \"\n    \"translated from the English ARC dataset\",\n    huggingface_id=\"ScandEval/arc-is-mini\",\n    task=KNOW,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\",\n    prompt_template=\"Spurningar: {text}\\nSvara: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Spurningar: {text}\\n\\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', \"\n    \"'b', 'c' e\u00f0a 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nARC_DE_CONFIG = DatasetConfig(\n    name=\"arc-de\",\n    pretty_name=\"the truncated version of the German knowledge dataset ARC-de, \"\n    \"translated from the English ARC dataset\",\n    huggingface_id=\"ScandEval/arc-de-mini\",\n    task=KNOW,\n    languages=[DE],\n    prompt_prefix=\"Die folgenden Fragen sind Multiple-Choice-Fragen (mit Antworten).\",\n    prompt_template=\"Frage: {text}\\nAntwort: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Frage: {text}\\n\\nBeantworten Sie die obige Frage mit 'a', 'b', \"\n    \"'c' oder 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nARC_NL_CONFIG = DatasetConfig(\n    name=\"arc-nl\",\n    pretty_name=\"the truncated version of the Dutch knowledge dataset ARC-nl, \"\n    \"translated from the English ARC dataset\",\n    huggingface_id=\"ScandEval/arc-nl-mini\",\n    task=KNOW,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan meerkeuzevragen (met antwoorden).\",\n    prompt_template=\"Vraag: {text}\\nAntwoord: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Vraag: {text}\\n\\nBeantwoord de bovenstaande vraag met 'a', \"\n    \"'b', 'c' of 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nARC_CONFIG = DatasetConfig(\n    name=\"arc\",\n    pretty_name=\"the truncated version of the English knowledge dataset ARC\",\n    huggingface_id=\"ScandEval/arc-mini\",\n    task=KNOW,\n    languages=[EN],\n    prompt_prefix=\"The following are multiple choice questions (with answers).\",\n    prompt_template=\"Question: {text}\\nAnswer: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Question: {text}\\n\\nAnswer the above question by \"\n    \"replying with 'a', 'b', 'c' or 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\n# TODO: Faroese knowledge\n\n\n### COMMON SENSE REASONING DATASETS ###\n\nHELLASWAG_DA_CONFIG = DatasetConfig(\n    name=\"hellaswag-da\",\n    pretty_name=\"the truncated version of the Danish common-sense reasoning dataset \"\n    \"HellaSwag-da, translated from the English HellaSwag dataset\",\n    huggingface_id=\"ScandEval/hellaswag-da-mini\",\n    task=COMMON_SENSE,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rgsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rgsm\u00e5l: {text}\\n\\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at \"\n    \"svare med 'a', 'b', 'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nHELLASWAG_NO_CONFIG = DatasetConfig(\n    name=\"hellaswag-no\",\n    pretty_name=\"the truncated version of the Norwegian common-sense reasoning dataset \"\n    \"HellaSwag-no, translated from the English HellaSwag dataset\",\n    huggingface_id=\"ScandEval/hellaswag-no-mini\",\n    task=COMMON_SENSE,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"F\u00f8lgende er flervalgssp\u00f8rsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rsm\u00e5l: {text}\\n\\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', \"\n    \"'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nHELLASWAG_SV_CONFIG = DatasetConfig(\n    name=\"hellaswag-sv\",\n    pretty_name=\"the truncated version of the Swedish common-sense reasoning dataset \"\n    \"HellaSwag-sv, translated from the English HellaSwag dataset\",\n    huggingface_id=\"ScandEval/hellaswag-sv-mini\",\n    task=COMMON_SENSE,\n    languages=[SV],\n    prompt_prefix=\"F\u00f6ljande \u00e4r flervalsfr\u00e5gor (med svar).\",\n    prompt_template=\"Fr\u00e5ga: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Fr\u00e5ga: {text}\\n\\nBesvara f\u00f6ljande fr\u00e5ga med 'a', 'b', 'c' \"\n    \"eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nHELLASWAG_IS_CONFIG = DatasetConfig(\n    name=\"hellaswag-is\",\n    pretty_name=\"the truncated version of the Icelandic common-sense reasoning dataset \"\n    \"HellaSwag-is, translated from the English HellaSwag dataset\",\n    huggingface_id=\"ScandEval/hellaswag-is-mini\",\n    task=COMMON_SENSE,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\",\n    prompt_template=\"Spurningar: {text}\\nSvara: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Spurningar: {text}\\n\\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', \"\n    \"'b', 'c' e\u00f0a 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nWINOGRANDE_IS = DatasetConfig(\n    name=\"winogrande-is\",\n    pretty_name=\"the Icelandic common-sense reasoning dataset \"\n    \"Winogrande-is, manually translated from the English Winogrande dataset\",\n    huggingface_id=\"ScandEval/winogrande-is\",\n    task=COMMON_SENSE,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\",\n    prompt_template=\"Spurningar: {text}\\nSvara: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Spurningar: {text}\\n\\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', \"\n    \"'b', 'c' e\u00f0a 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nHELLASWAG_DE_CONFIG = DatasetConfig(\n    name=\"hellaswag-de\",\n    pretty_name=\"the truncated version of the German common-sense reasoning dataset \"\n    \"HellaSwag-de, translated from the English HellaSwag dataset\",\n    huggingface_id=\"ScandEval/hellaswag-de-mini\",\n    task=COMMON_SENSE,\n    languages=[DE],\n    prompt_prefix=\"Die folgenden Fragen sind Multiple-Choice-Fragen (mit Antworten).\",\n    prompt_template=\"Frage: {text}\\nAntwort: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Frage: {text}\\n\\nBeantworten Sie die obige Frage mit 'a', 'b', \"\n    \"'c' oder 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nHELLASWAG_NL_CONFIG = DatasetConfig(\n    name=\"hellaswag-nl\",\n    pretty_name=\"the truncated version of the Dutch common-sense reasoning dataset \"\n    \"HellaSwag-nl, translated from the English HellaSwag dataset\",\n    huggingface_id=\"ScandEval/hellaswag-nl-mini\",\n    task=COMMON_SENSE,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan meerkeuzevragen (met antwoorden).\",\n    prompt_template=\"Vraag: {text}\\nAntwoord: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Vraag: {text}\\n\\nBeantwoord de bovenstaande vraag met 'a', 'b', \"\n    \"'c' of 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nHELLASWAG_CONFIG = DatasetConfig(\n    name=\"hellaswag\",\n    pretty_name=\"the truncated version of the English common-sense reasoning \"\n    \"dataset HellaSwag\",\n    huggingface_id=\"ScandEval/hellaswag-mini\",\n    task=COMMON_SENSE,\n    languages=[EN],\n    prompt_prefix=\"The following are multiple choice questions (with answers).\",\n    prompt_template=\"Question: {text}\\nAnswer: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Question: {text}\\n\\nAnswer the above question by \"\n    \"replying with 'a', 'b', 'c' or 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\n# TODO: Faroese common sense reasoning\n\n\n### MULTIPLE CHOICE READING COMPREHENSION DATASETS ###\n\nBELEBELE_DA_CONFIG = DatasetConfig(\n    name=\"belebele-da\",\n    pretty_name=\"the Danish multiple choice reading comprehension dataset BeleBele-da, \"\n    \"translated from the English BeleBele dataset\",\n    huggingface_id=\"ScandEval/belebele-da-mini\",\n    task=MCRC,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er tekster med tilh\u00f8rende multiple choice sp\u00f8rgsm\u00e5l og \"\n    \"svar.\",\n    prompt_template=\"{text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rgsm\u00e5l: {text}\\n\\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at \"\n    \"svare med 'a', 'b', 'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nBELEBELE_SV_CONFIG = DatasetConfig(\n    name=\"belebele-sv\",\n    pretty_name=\"the Swedish multiple choice reading comprehension dataset \"\n    \"BeleBele-sv, translated from the English BeleBele dataset\",\n    huggingface_id=\"ScandEval/belebele-sv-mini\",\n    task=MCRC,\n    languages=[SV],\n    prompt_prefix=\"Nedan f\u00f6ljer texter med tillh\u00f6rande multiple choice fr\u00e5gor och \"\n    \"svar.\",\n    prompt_template=\"{text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Fr\u00e5ga: {text}\\n\\nBesvara f\u00f6ljande fr\u00e5ga med 'a', 'b', 'c' \"\n    \"eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nBELEBELE_NO_CONFIG = DatasetConfig(\n    name=\"belebele-no\",\n    pretty_name=\"the Norwegian multiple choice reading comprehension dataset \"\n    \"BeleBele-no, translated from the English BeleBele dataset\",\n    huggingface_id=\"ScandEval/belebele-no-mini\",\n    task=MCRC,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"Her f\u00f8lger tekster med tilh\u00f8rende multiple choice sp\u00f8rsm\u00e5l og svar.\",\n    prompt_template=\"{text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rsm\u00e5l: {text}\\n\\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', \"\n    \"'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nBELEBELE_IS_CONFIG = DatasetConfig(\n    name=\"belebele-is\",\n    pretty_name=\"the Icelandic multiple choice reading comprehension dataset \"\n    \"BeleBele-is, translated from the English BeleBele dataset\",\n    huggingface_id=\"ScandEval/belebele-is-mini\",\n    task=MCRC,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru textar me\u00f0 tilheyrandi fj\u00f6lvalsspurningum og \"\n    \"sv\u00f6rum.\",\n    prompt_template=\"{text}\\nSvara: {label}\",\n    instruction_prompt=\"Spurningar: {text}\\n\\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', \"\n    \"'b', 'c' e\u00f0a 'd'.\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nBELEBELE_DE_CONFIG = DatasetConfig(\n    name=\"belebele-de\",\n    pretty_name=\"the German multiple choice reading comprehension dataset BeleBele-de, \"\n    \"translated from the English BeleBele dataset\",\n    huggingface_id=\"ScandEval/belebele-de-mini\",\n    task=MCRC,\n    languages=[DE],\n    prompt_prefix=\"Die folgenden Texte sind mit dazugeh\u00f6rigen Multiple-Choice-Fragen \"\n    \"und Antworten.\",\n    prompt_template=\"{text}\\nAntwort: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Frage: {text}\\n\\nBeantworten Sie die obige Frage mit 'a', 'b', \"\n    \"'c' oder 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nBELEBELE_NL_CONFIG = DatasetConfig(\n    name=\"belebele-nl\",\n    pretty_name=\"the Dutch multiple choice reading comprehension dataset BeleBele-nl, \"\n    \"translated from the English BeleBele dataset\",\n    huggingface_id=\"ScandEval/belebele-nl-mini\",\n    task=MCRC,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan teksten met bijbehorende multiple choice vragen en \"\n    \"antwoorden.\",\n    prompt_template=\"{text}\\nAntwoord: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Vraag: {text}\\n\\nBeantwoord de bovenstaande vraag met 'a', \"\n    \"'b', 'c' of 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nBELEBELE_CONFIG = DatasetConfig(\n    name=\"belebele\",\n    pretty_name=\"the English multiple choice reading comprehension dataset BeleBele\",\n    huggingface_id=\"ScandEval/belebele-mini\",\n    task=MCRC,\n    languages=[EN],\n    prompt_prefix=\"The following are texts with accompanying multiple choice questions \"\n    \"and answers.\",\n    prompt_template=\"{text}\\nAnswer: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Question: {text}\\n\\nAnswer the above question by \"\n    \"replying with 'a', 'b', 'c' or 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\n\n### SPEED ESTIMATION DATASETS ###\n\nSPEED_CONFIG = DatasetConfig(\n    name=\"speed\",\n    pretty_name=\"the speed estimation benchmark\",\n    huggingface_id=\"\",\n    task=SPEED,\n    languages=list(get_all_languages().values()),\n    prompt_prefix=\"\",\n    prompt_template=\"\",\n    instruction_prompt=\"\",\n    num_few_shot_examples=0,\n    max_generated_tokens=1,\n)\n</code></pre>"},{"location":"api/scandeval/dataset_factory/","title":"Dataset factory","text":"<p>Failure</p> <p>module 'scandeval.dataset_factory' not found.</p>"},{"location":"src/scandeval/dataset_factory/","title":"Dataset factory","text":"<p>Failure</p> <p>module 'scandeval.dataset_factory' not found.</p>"},{"location":"api/scandeval/enums/","title":"scandeval.enums","text":"scandeval.enums<p> source module scandeval.enums </p> <p>Enums used in the project.</p> <p> Classes </p> <ul> <li> <p>AutoStrEnum \u2014 StrEnum where auto() returns the field name in lower case.</p> </li> <li> <p>Device \u2014 The compute device to use for the evaluation.</p> </li> <li> <p>Framework \u2014 The framework of a model.</p> </li> <li> <p>ModelType \u2014 The type of a model.</p> </li> <li> <p>DataType \u2014 The data type of the model weights.</p> </li> </ul> <p> source enum AutoStrEnum() </p> <p><p>Bases : str, Enum</p></p> <p>StrEnum where auto() returns the field name in lower case.</p> <p> source enum Device() </p> <p><p>Bases : AutoStrEnum</p></p> <p>The compute device to use for the evaluation.</p> <p> Attributes </p> <ul> <li> <p>CPU \u2014</p> <p>CPU device.</p> </li> <li> <p>MPS \u2014</p> <p>MPS GPU, used in M-series MacBooks.</p> </li> <li> <p>CUDA \u2014</p> <p>CUDA GPU, used with NVIDIA GPUs.</p> </li> </ul> <p> source enum Framework() </p> <p><p>Bases : AutoStrEnum</p></p> <p>The framework of a model.</p> <p> Attributes </p> <ul> <li> <p>PYTORCH \u2014</p> <p>PyTorch framework.</p> </li> <li> <p>JAX \u2014</p> <p>JAX framework.</p> </li> <li> <p>API \u2014</p> <p>Accessible via an API.</p> </li> <li> <p>HUMAN \u2014</p> <p>Human evaluator.</p> </li> </ul> <p> source enum ModelType() </p> <p><p>Bases : AutoStrEnum</p></p> <p>The type of a model.</p> <p> Attributes </p> <ul> <li> <p>FRESH \u2014</p> <p>Randomly initialised Hugging Face model.</p> </li> <li> <p>HF \u2014</p> <p>Model from the Hugging Face Hub.</p> </li> <li> <p>LOCAL \u2014</p> <p>Locally stored Hugging Face model.</p> </li> <li> <p>OPENAI \u2014</p> <p>Model from OpenAI.</p> </li> <li> <p>HUMAN \u2014</p> <p>Human evaluator.</p> </li> </ul> <p> source enum DataType() </p> <p><p>Bases : AutoStrEnum</p></p> <p>The data type of the model weights.</p> <p> Attributes </p> <ul> <li> <p>FP32 \u2014</p> <p>32-bit floating point.</p> </li> <li> <p>FP16 \u2014</p> <p>16-bit floating point.</p> </li> <li> <p>BF16 \u2014</p> <p>16-bit bfloat.</p> </li> </ul>"},{"location":"src/scandeval/enums/","title":"scandeval.enums","text":"scandeval.enums<p> docs module scandeval.enums </p> <pre><code>\"\"\"Enums used in the project.\"\"\"\n\nfrom enum import Enum, auto\n\n\nclass AutoStrEnum(str, Enum):docs\n    \"\"\"StrEnum where auto() returns the field name in lower case.\"\"\"\n\n    @staticmethod\n    def _generate_next_value_(\n        name: str, start: int, count: int, last_values: list\n    ) -&gt; str:\n        return name.lower()\n\n\nclass Device(AutoStrEnum):docs\n    \"\"\"The compute device to use for the evaluation.\n\n    Attributes:\n        CPU:\n            CPU device.\n        MPS:\n            MPS GPU, used in M-series MacBooks.\n        CUDA:\n            CUDA GPU, used with NVIDIA GPUs.\n    \"\"\"\n\n    CPU = auto()\n    MPS = auto()\n    CUDA = auto()\n\n\nclass Framework(AutoStrEnum):docs\n    \"\"\"The framework of a model.\n\n    Attributes:\n        PYTORCH:\n            PyTorch framework.\n        JAX:\n            JAX framework.\n        API:\n            Accessible via an API.\n        HUMAN:\n            Human evaluator.\n    \"\"\"\n\n    PYTORCH = auto()\n    JAX = auto()\n    API = auto()\n    HUMAN = auto()\n\n\nclass ModelType(AutoStrEnum):docs\n    \"\"\"The type of a model.\n\n    Attributes:\n        FRESH:\n            Randomly initialised Hugging Face model.\n        HF:\n            Model from the Hugging Face Hub.\n        LOCAL:\n            Locally stored Hugging Face model.\n        OPENAI:\n            Model from OpenAI.\n        HUMAN:\n            Human evaluator.\n    \"\"\"\n\n    FRESH = auto()\n    HF = auto()\n    LOCAL = auto()\n    OPENAI = auto()\n    HUMAN = auto()\n\n\nclass DataType(AutoStrEnum):docs\n    \"\"\"The data type of the model weights.\n\n    Attributes:\n        FP32:\n            32-bit floating point.\n        FP16:\n            16-bit floating point.\n        BF16:\n            16-bit bfloat.\n    \"\"\"\n\n    FP32 = auto()\n    FP16 = auto()\n    BF16 = auto()\n</code></pre>"},{"location":"api/scandeval/exceptions/","title":"scandeval.exceptions","text":"scandeval.exceptions<p> source module scandeval.exceptions </p> <p>Exceptions to used by other functions.</p> <p> Classes </p> <ul> <li> <p>InvalidBenchmark \u2014 The (model, dataset) combination cannot be benchmarked.</p> </li> <li> <p>InvalidModel \u2014 The model cannot be benchmarked on any datasets.</p> </li> <li> <p>HuggingFaceHubDown \u2014 The Hugging Face Hub seems to be down.</p> </li> <li> <p>NoInternetConnection \u2014 There seems to be no internet connection.</p> </li> <li> <p>NaNValueInModelOutput \u2014 There is a NaN value in the model output.</p> </li> <li> <p>FlashAttentionNotInstalled \u2014 The <code>flash-attn</code> package has not been installed.</p> </li> <li> <p>NeedsExtraInstalled \u2014 The evaluation requires extra to be installed.</p> </li> <li> <p>NeedsManualDependency \u2014 The evaluation requires a dependency to be manually installed.</p> </li> <li> <p>NeedsAdditionalArgument \u2014 The evaluation requires additional arguments to the <code>scandeval</code> command.</p> </li> <li> <p>MissingHuggingFaceToken \u2014 The evaluation requires a Hugging Face token.</p> </li> </ul> <p> source class InvalidBenchmark() </p> <p><p>Bases : Exception</p></p> <p>The (model, dataset) combination cannot be benchmarked.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to display.</p> </li> </ul> <p> source class InvalidModel() </p> <p><p>Bases : Exception</p></p> <p>The model cannot be benchmarked on any datasets.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to display.</p> </li> </ul> <p> source class HuggingFaceHubDown() </p> <p><p>Bases : Exception</p></p> <p>The Hugging Face Hub seems to be down.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to display.</p> </li> </ul> <p> source class NoInternetConnection() </p> <p><p>Bases : Exception</p></p> <p>There seems to be no internet connection.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to display.</p> </li> </ul> <p> source class NaNValueInModelOutput() </p> <p><p>Bases : Exception</p></p> <p>There is a NaN value in the model output.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to display.</p> </li> </ul> <p> source class FlashAttentionNotInstalled() </p> <p><p>Bases : Exception</p></p> <p>The <code>flash-attn</code> package has not been installed.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to display.</p> </li> </ul> <p> source class NeedsExtraInstalled() </p> <p><p>Bases : InvalidModel</p></p> <p>The evaluation requires extra to be installed.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>extra :  str \u2014</p> <p>The extra that needs to be installed.</p> </li> </ul> <p> source class NeedsManualDependency() </p> <p><p>Bases : InvalidModel</p></p> <p>The evaluation requires a dependency to be manually installed.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>package :  str \u2014</p> <p>The package that needs to be manually installed.</p> </li> </ul> <p> source class NeedsAdditionalArgument(script_argument: str, run_with_cli: bool) </p> <p><p>Bases : InvalidModel</p></p> <p>The evaluation requires additional arguments to the <code>scandeval</code> command.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>cli_argument :  str \u2014</p> <p>The argument that needs to be passed to the <code>scandeval</code> command.</p> </li> <li> <p>script_argument :  str \u2014</p> <p>The argument that needs to be passed to the <code>Benchmarker</code> class.</p> </li> <li> <p>run_with_cli :  bool \u2014</p> <p>Whether the benchmark is being run with the CLI.</p> </li> </ul> <p> source class MissingHuggingFaceToken() </p> <p><p>Bases : InvalidModel</p></p> <p>The evaluation requires a Hugging Face token.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>run_with_cli :  bool \u2014</p> <p>Whether the benchmark is being run with the CLI.</p> </li> </ul>"},{"location":"src/scandeval/exceptions/","title":"scandeval.exceptions","text":"scandeval.exceptions<p> docs module scandeval.exceptions </p> <pre><code>\"\"\"Exceptions to used by other functions.\"\"\"\n\n\nclass InvalidBenchmark(Exception):docs\n    \"\"\"The (model, dataset) combination cannot be benchmarked.\"\"\"\n\n    def __init__(\n        self, message: str = \"This model cannot be benchmarked on the given dataset.\"\n    ):\n        \"\"\"Initialize the exception.\n\n        Args:\n            message:\n                The message to display.\n        \"\"\"\n        self.message = message\n        super().__init__(self.message)\n\n\nclass InvalidModel(Exception):docs\n    \"\"\"The model cannot be benchmarked on any datasets.\"\"\"\n\n    def __init__(\n        self, message: str = \"The model cannot be benchmarked on any datasets.\"\n    ):\n        \"\"\"Initialize the exception.\n\n        Args:\n            message:\n                The message to display.\n        \"\"\"\n        self.message = message\n        super().__init__(self.message)\n\n\nclass HuggingFaceHubDown(Exception):docs\n    \"\"\"The Hugging Face Hub seems to be down.\"\"\"\n\n    def __init__(self, message: str = \"The Hugging Face Hub is currently down.\"):\n        \"\"\"Initialize the exception.\n\n        Args:\n            message:\n                The message to display.\n        \"\"\"\n        self.message = message\n        super().__init__(self.message)\n\n\nclass NoInternetConnection(Exception):docs\n    \"\"\"There seems to be no internet connection.\"\"\"\n\n    def __init__(self, message: str = \"There is currently no internet connection.\"):\n        \"\"\"Initialize the exception.\n\n        Args:\n            message:\n                The message to display.\n        \"\"\"\n        self.message = message\n        super().__init__(self.message)\n\n\nclass NaNValueInModelOutput(Exception):docs\n    \"\"\"There is a NaN value in the model output.\"\"\"\n\n    def __init__(self, message: str = \"There is a NaN value in the model output.\"):\n        \"\"\"Initialize the exception.\n\n        Args:\n            message:\n                The message to display.\n        \"\"\"\n        self.message = message\n        super().__init__(self.message)\n\n\nclass FlashAttentionNotInstalled(Exception):docs\n    \"\"\"The `flash-attn` package has not been installed.\"\"\"\n\n    def __init__(\n        self,\n        message: str = (\n            \"The model you are trying to load requires Flash Attention. To use Flash \"\n            \"Attention, please install the `flash-attn` package, which can be done by \"\n            \"running `pip install -U wheel &amp;&amp; FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE \"\n            \"pip install flash-attn --no-build-isolation`.\"\n        ),\n    ):\n        \"\"\"Initialize the exception.\n\n        Args:\n            message:\n                The message to display.\n        \"\"\"\n        self.message = message\n        super().__init__(self.message)\n\n\nclass NeedsExtraInstalled(InvalidModel):docs\n    \"\"\"The evaluation requires extra to be installed.\"\"\"\n\n    def __init__(self, extra: str):\n        \"\"\"Initialize the exception.\n\n        Args:\n            extra:\n                The extra that needs to be installed.\n        \"\"\"\n        self.message = (\n            f\"The model you are trying to load requires the `{extra}` extra to be \"\n            f\"installed. To install the `{extra}` extra, please run `pip install \"\n            f\"scandeval[{extra}]` or `pip install scandeval[all]`.\"\n        )\n        super().__init__(self.message)\n\n\nclass NeedsManualDependency(InvalidModel):docs\n    \"\"\"The evaluation requires a dependency to be manually installed.\"\"\"\n\n    def __init__(self, package: str):\n        \"\"\"Initialize the exception.\n\n        Args:\n            package:\n                The package that needs to be manually installed.\n        \"\"\"\n        self.message = (\n            f\"The model you are trying to load requires the `{package}` package to be \"\n            f\"installed - please run `pip install {package}` and try again.\"\n        )\n        super().__init__(self.message)\n\n\nclass NeedsAdditionalArgument(InvalidModel):docs\n    \"\"\"The evaluation requires additional arguments to the `scandeval` command.\"\"\"\n\n    def __init__(self, cli_argument: str, script_argument: str, run_with_cli: bool):\n        \"\"\"Initialize the exception.\n\n        Args:\n            cli_argument:\n                The argument that needs to be passed to the `scandeval` command.\n            script_argument:\n                The argument that needs to be passed to the `Benchmarker` class.\n            run_with_cli:\n                Whether the benchmark is being run with the CLI.\n        \"\"\"\n        if run_with_cli:\n            self.message = (\n                f\"The model you are trying to load requires the `{cli_argument}` \"\n                \"argument to be passed to the `scandeval` command. Please pass the \"\n                \"argument and try again.\"\n            )\n        else:\n            self.message = (\n                f\"The model you are trying to load requires the `{script_argument}` \"\n                \"argument  to be passed to the `Benchmarker` class. Please pass the \"\n                \"argument and try again.\"\n            )\n        super().__init__(self.message)\n\n\nclass MissingHuggingFaceToken(InvalidModel):docs\n    \"\"\"The evaluation requires a Hugging Face token.\"\"\"\n\n    def __init__(self, run_with_cli: bool):\n        \"\"\"Initialize the exception.\n\n        Args:\n            run_with_cli:\n                Whether the benchmark is being run with the CLI.\n        \"\"\"\n        self.message = (\n            \"The model you are trying to load requires a Hugging Face token. \"\n        )\n        if run_with_cli:\n            self.message += (\n                \"Please run `huggingface-cli login` to login to the Hugging Face \"\n                \"Hub and try again. Alternatively, you can pass your Hugging Face Hub \"\n                \"token directly to the `scandeval` command with the `--token &lt;token&gt;` \"\n                \"argument.\"\n            )\n        else:\n            self.message += (\n                \"Please pass your Hugging Face Hub token to the `Benchmarker` class \"\n                \"with the `token=&lt;token&gt;` argument  to the `Benchmarker` class and try \"\n                \"again. Alternatively, you can also simply pass `token=True` (which is \"\n                \"the default) to the `Benchmarker` class, which assumes that you have \"\n                \"logged into the Hugging Face Hub. This can be done by running \"\n                \"`huggingface-cli login` in the terminal or `from huggingface_hub \"\n                \"import login; login()` in a Python script.\"\n            )\n        super().__init__(self.message)\n</code></pre>"},{"location":"api/scandeval/finetuning/","title":"Finetuning","text":"<p>Failure</p> <p>module 'scandeval.finetuning' not found.</p>"},{"location":"src/scandeval/finetuning/","title":"Finetuning","text":"<p>Failure</p> <p>module 'scandeval.finetuning' not found.</p>"},{"location":"api/scandeval/generation/","title":"Generation","text":"<p>Failure</p> <p>module 'scandeval.generation' not found.</p>"},{"location":"src/scandeval/generation/","title":"Generation","text":"<p>Failure</p> <p>module 'scandeval.generation' not found.</p>"},{"location":"api/scandeval/human_evaluation/","title":"Human evaluation","text":"<p>Failure</p> <p>module 'scandeval.human_evaluation' not found.</p>"},{"location":"src/scandeval/human_evaluation/","title":"Human evaluation","text":"<p>Failure</p> <p>module 'scandeval.human_evaluation' not found.</p>"},{"location":"api/scandeval/languages/","title":"scandeval.languages","text":"scandeval.languages<p> source module scandeval.languages </p> <p>List of languages and their ISO 639-1 codes.</p> <p>Taken from https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes.</p> <p>Last updated 19 June 2022.</p> <p> Functions </p> <ul> <li> <p>get_all_languages \u2014 Get a list of all the languages.</p> </li> </ul> <p> source get_all_languages() \u2192 dict[str, Language] </p> <p>Get a list of all the languages.</p> <p> Returns </p> <ul> <li> <p>dict[str, Language] \u2014 A mapping between language codes and their configurations.</p> </li> </ul>"},{"location":"src/scandeval/languages/","title":"scandeval.languages","text":"scandeval.languages<p> docs module scandeval.languages </p> <pre><code>\"\"\"List of languages and their ISO 639-1 codes.\n\nTaken from https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes.\n\nLast updated 19 June 2022.\n\"\"\"\n\nfrom .config import Language\n\n\ndef get_all_languages() -&gt; dict[str, Language]:docs\n    \"\"\"Get a list of all the languages.\n\n    Returns:\n        A mapping between language codes and their configurations.\n    \"\"\"\n    return {cfg.code: cfg for cfg in globals().values() if isinstance(cfg, Language)}\n\n\nAB = Language(code=\"ab\", name=\"Abkhazian\")\nAA = Language(code=\"aa\", name=\"Afar\")\nAF = Language(code=\"af\", name=\"Afrikaans\")\nSQ = Language(code=\"sq\", name=\"Albanian\")\nAM = Language(code=\"am\", name=\"Amharic\")\nAR = Language(code=\"ar\", name=\"Arabic\")\nAN = Language(code=\"an\", name=\"Aragonese\")\nHY = Language(code=\"hy\", name=\"Armenian\")\nAS = Language(code=\"as\", name=\"Assamese\")\nAV = Language(code=\"av\", name=\"Avaric\")\nAE = Language(code=\"ae\", name=\"Avestan\")\nAY = Language(code=\"ay\", name=\"Aymara\")\nAZ = Language(code=\"az\", name=\"Azerbaijani\")\nBM = Language(code=\"bm\", name=\"Bambara\")\nBA = Language(code=\"ba\", name=\"Bashkir\")\nEU = Language(code=\"eu\", name=\"Basque\")\nBE = Language(code=\"be\", name=\"Belarusian\")\nBN = Language(code=\"bn\", name=\"Bengali\")\nBI = Language(code=\"bi\", name=\"Bislama\")\nBS = Language(code=\"bs\", name=\"Bosnian\")\nBR = Language(code=\"br\", name=\"Breton\")\nBG = Language(code=\"bg\", name=\"Bulgarian\")\nMY = Language(code=\"my\", name=\"Burmese\")\nCA = Language(code=\"ca\", name=\"Catalan\")\nCH = Language(code=\"ch\", name=\"Chamorro\")\nCE = Language(code=\"ce\", name=\"Chechen\")\nNY = Language(code=\"ny\", name=\"Chichewa\")\nZH = Language(code=\"zh\", name=\"Chinese\")\nCU = Language(code=\"cu\", name=\"Church Slavic\")\nCV = Language(code=\"cv\", name=\"Chuvash\")\nKW = Language(code=\"kw\", name=\"Cornish\")\nCO = Language(code=\"co\", name=\"Corsican\")\nCR = Language(code=\"cr\", name=\"Cree\")\nHR = Language(code=\"hr\", name=\"Croatian\")\nCS = Language(code=\"cs\", name=\"Czech\")\nDA = Language(code=\"da\", name=\"Danish\")\nDV = Language(code=\"dv\", name=\"Divehi\")\nNL = Language(code=\"nl\", name=\"Dutch\")\nDZ = Language(code=\"dz\", name=\"Dzongkha\")\nEN = Language(code=\"en\", name=\"English\")\nEO = Language(code=\"eo\", name=\"Esperanto\")\nET = Language(code=\"et\", name=\"Estonian\")\nEE = Language(code=\"ee\", name=\"Ewe\")\nFO = Language(code=\"fo\", name=\"Faroese\")\nFJ = Language(code=\"fj\", name=\"Fijian\")\nFI = Language(code=\"fi\", name=\"Finnish\")\nFR = Language(code=\"fr\", name=\"French\")\nFY = Language(code=\"fy\", name=\"Western Frisian\")\nFF = Language(code=\"ff\", name=\"Fulah\")\nGD = Language(code=\"gd\", name=\"Gaelic\")\nGL = Language(code=\"gl\", name=\"Galician\")\nLG = Language(code=\"lg\", name=\"Ganda\")\nKA = Language(code=\"ka\", name=\"Georgian\")\nDE = Language(code=\"de\", name=\"German\")\nEL = Language(code=\"el\", name=\"Greek\")\nKL = Language(code=\"kl\", name=\"Greenlandic\")\nGN = Language(code=\"gn\", name=\"Guarani\")\nGU = Language(code=\"gu\", name=\"Gujarati\")\nHT = Language(code=\"ht\", name=\"Haitian\")\nHA = Language(code=\"ha\", name=\"Hausa\")\nHE = Language(code=\"he\", name=\"Hebrew\")\nHZ = Language(code=\"hz\", name=\"Herero\")\nHI = Language(code=\"hi\", name=\"Hindi\")\nHO = Language(code=\"ho\", name=\"Hiri Motu\")\nHU = Language(code=\"hu\", name=\"Hungarian\")\nIS = Language(code=\"is\", name=\"Icelandic\")\nIO = Language(code=\"io\", name=\"Ido\")\nIG = Language(code=\"ig\", name=\"Igbo\")\nID = Language(code=\"id\", name=\"Indonesian\")\nIA = Language(code=\"ia\", name=\"Interlingua\")\nIE = Language(code=\"ie\", name=\"Interlingue\")\nIU = Language(code=\"iu\", name=\"Inuktitut\")\nIK = Language(code=\"ik\", name=\"Inupiaq\")\nGA = Language(code=\"ga\", name=\"Irish\")\nIT = Language(code=\"it\", name=\"Italian\")\nJA = Language(code=\"ja\", name=\"Japanese\")\nKN = Language(code=\"kn\", name=\"Kannada\")\nKR = Language(code=\"kr\", name=\"Kanuri\")\nKS = Language(code=\"ks\", name=\"Kashmiri\")\nKK = Language(code=\"kk\", name=\"Kazakh\")\nKM = Language(code=\"km\", name=\"Central Khmer\")\nKI = Language(code=\"ki\", name=\"Kikuyu\")\nRW = Language(code=\"rw\", name=\"Kinyarwanda\")\nKY = Language(code=\"ky\", name=\"Kirghiz\")\nKV = Language(code=\"kv\", name=\"Komi\")\nKG = Language(code=\"kg\", name=\"Kongo\")\nKO = Language(code=\"ko\", name=\"Korean\")\nKJ = Language(code=\"kj\", name=\"Kuanyama\")\nKU = Language(code=\"ku\", name=\"Kurdish\")\nLO = Language(code=\"lo\", name=\"Lao\")\nLA = Language(code=\"la\", name=\"Latin\")\nLV = Language(code=\"lv\", name=\"Latvian\")\nLI = Language(code=\"li\", name=\"Limburgan\")\nLN = Language(code=\"ln\", name=\"Lingala\")\nLT = Language(code=\"lt\", name=\"Lithuanian\")\nLU = Language(code=\"lu\", name=\"Luba-Katanga\")\nLB = Language(code=\"lb\", name=\"Luxembourgish\")\nMK = Language(code=\"mk\", name=\"Macedonian\")\nMG = Language(code=\"mg\", name=\"Malagasy\")\nMS = Language(code=\"ms\", name=\"Malay\")\nML = Language(code=\"ml\", name=\"Malayalam\")\nMT = Language(code=\"mt\", name=\"Maltese\")\nGV = Language(code=\"gv\", name=\"Manx\")\nMI = Language(code=\"mi\", name=\"Maori\")\nMR = Language(code=\"mr\", name=\"Marathi\")\nMH = Language(code=\"mh\", name=\"Marshallese\")\nMN = Language(code=\"mn\", name=\"Mongolian\")\nNA = Language(code=\"na\", name=\"Nauru\")\nNV = Language(code=\"nv\", name=\"Navajo\")\nND = Language(code=\"nd\", name=\"Northern Ndebele\")\nNR = Language(code=\"nr\", name=\"South Ndebele\")\nNG = Language(code=\"ng\", name=\"Ndonga\")\nNE = Language(code=\"ne\", name=\"Nepali\")\nNO = Language(code=\"no\", name=\"Norwegian\")\nNB = Language(code=\"nb\", name=\"Norwegian Bokm\u00e5l\")\nNN = Language(code=\"nn\", name=\"Norwegian Nynorsk\")\nII = Language(code=\"ii\", name=\"Sichuan Yi\")\nOC = Language(code=\"oc\", name=\"Occitan\")\nOJ = Language(code=\"oj\", name=\"Ojibwa\")\nOR = Language(code=\"or\", name=\"Oriya\")\nOM = Language(code=\"om\", name=\"Oromo\")\nOS = Language(code=\"os\", name=\"Ossetian\")\nPI = Language(code=\"pi\", name=\"Pali\")\nPS = Language(code=\"ps\", name=\"Pashto\")\nFA = Language(code=\"fa\", name=\"Persian\")\nPL = Language(code=\"pl\", name=\"Polish\")\nPT = Language(code=\"pt\", name=\"Portuguese\")\nPA = Language(code=\"pa\", name=\"Punjabi\")\nQU = Language(code=\"qu\", name=\"Quechua\")\nRO = Language(code=\"ro\", name=\"Romanian\")\nRM = Language(code=\"rm\", name=\"Romansh\")\nRN = Language(code=\"rn\", name=\"Rundi\")\nRU = Language(code=\"ru\", name=\"Russian\")\nSE = Language(code=\"se\", name=\"Northern Sami\")\nSM = Language(code=\"sm\", name=\"Samoan\")\nSG = Language(code=\"sg\", name=\"Sango\")\nSA = Language(code=\"sa\", name=\"Sanskrit\")\nSC = Language(code=\"sc\", name=\"Sardinian\")\nSR = Language(code=\"sr\", name=\"Serbian\")\nSN = Language(code=\"sn\", name=\"Shona\")\nSD = Language(code=\"sd\", name=\"Sindhi\")\nSI = Language(code=\"si\", name=\"Sinhala\")\nSK = Language(code=\"sk\", name=\"Slovak\")\nSL = Language(code=\"sl\", name=\"Slovenian\")\nSO = Language(code=\"so\", name=\"Somali\")\nST = Language(code=\"st\", name=\"Sotho\")\nES = Language(code=\"es\", name=\"Spanish\")\nSU = Language(code=\"su\", name=\"Sundanese\")\nSW = Language(code=\"sw\", name=\"Swahili\")\nSS = Language(code=\"ss\", name=\"Swati\")\nSV = Language(code=\"sv\", name=\"Swedish\")\nTL = Language(code=\"tl\", name=\"Tagalog\")\nTY = Language(code=\"ty\", name=\"Tahitian\")\nTG = Language(code=\"tg\", name=\"Tajik\")\nTA = Language(code=\"ta\", name=\"Tamil\")\nTT = Language(code=\"tt\", name=\"Tatar\")\nTE = Language(code=\"te\", name=\"Telugu\")\nTH = Language(code=\"th\", name=\"Thai\")\nBO = Language(code=\"bo\", name=\"Tibetan\")\nTI = Language(code=\"ti\", name=\"Tigrinya\")\nTO = Language(code=\"to\", name=\"Tonga\")\nTS = Language(code=\"ts\", name=\"Tsonga\")\nTN = Language(code=\"tn\", name=\"Tswana\")\nTR = Language(code=\"tr\", name=\"Turkish\")\nTK = Language(code=\"tk\", name=\"Turkmen\")\nTW = Language(code=\"tw\", name=\"Twi\")\nUG = Language(code=\"ug\", name=\"Uighur\")\nUK = Language(code=\"uk\", name=\"Ukrainian\")\nUR = Language(code=\"ur\", name=\"Urdu\")\nUZ = Language(code=\"uz\", name=\"Uzbek\")\nVE = Language(code=\"ve\", name=\"Venda\")\nVI = Language(code=\"vi\", name=\"Vietnamese\")\nVO = Language(code=\"vo\", name=\"Volap\u00fck\")\nWA = Language(code=\"wa\", name=\"Walloon\")\nCY = Language(code=\"cy\", name=\"Welsh\")\nWO = Language(code=\"wo\", name=\"Wolof\")\nXH = Language(code=\"xh\", name=\"Xhosa\")\nYI = Language(code=\"yi\", name=\"Yiddish\")\nYO = Language(code=\"yo\", name=\"Yoruba\")\nZA = Language(code=\"za\", name=\"Zhuang\")\nZU = Language(code=\"zu\", name=\"Zulu\")\n</code></pre>"},{"location":"api/scandeval/model_cache/","title":"Model cache","text":"<p>Failure</p> <p>module 'scandeval.model_cache' not found.</p>"},{"location":"src/scandeval/model_cache/","title":"Model cache","text":"<p>Failure</p> <p>module 'scandeval.model_cache' not found.</p>"},{"location":"api/scandeval/model_config/","title":"Model config","text":"<p>Failure</p> <p>module 'scandeval.model_config' not found.</p>"},{"location":"src/scandeval/model_config/","title":"Model config","text":"<p>Failure</p> <p>module 'scandeval.model_config' not found.</p>"},{"location":"api/scandeval/model_loading/","title":"Model loading","text":"<p>Failure</p> <p>module 'scandeval.model_loading' not found.</p>"},{"location":"src/scandeval/model_loading/","title":"Model loading","text":"<p>Failure</p> <p>module 'scandeval.model_loading' not found.</p>"},{"location":"api/scandeval/model_setups/","title":"Model setups","text":"<p>Failure</p> <p>module 'scandeval.model_setups' not found.</p>"},{"location":"src/scandeval/model_setups/","title":"Model setups","text":"<p>Failure</p> <p>module 'scandeval.model_setups' not found.</p>"},{"location":"api/scandeval/named_entity_recognition/","title":"Named entity recognition","text":"<p>Failure</p> <p>module 'scandeval.named_entity_recognition' not found.</p>"},{"location":"src/scandeval/named_entity_recognition/","title":"Named entity recognition","text":"<p>Failure</p> <p>module 'scandeval.named_entity_recognition' not found.</p>"},{"location":"api/scandeval/openai_models/","title":"Openai models","text":"<p>Failure</p> <p>module 'scandeval.openai_models' not found.</p>"},{"location":"src/scandeval/openai_models/","title":"Openai models","text":"<p>Failure</p> <p>module 'scandeval.openai_models' not found.</p>"},{"location":"api/scandeval/protocols/","title":"Protocols","text":"<p>Failure</p> <p>module 'scandeval.protocols' not found.</p>"},{"location":"src/scandeval/protocols/","title":"Protocols","text":"<p>Failure</p> <p>module 'scandeval.protocols' not found.</p>"},{"location":"api/scandeval/question_answering/","title":"Question answering","text":"<p>Failure</p> <p>module 'scandeval.question_answering' not found.</p>"},{"location":"src/scandeval/question_answering/","title":"Question answering","text":"<p>Failure</p> <p>module 'scandeval.question_answering' not found.</p>"},{"location":"api/scandeval/question_answering_trainer/","title":"Question answering trainer","text":"<p>Failure</p> <p>module 'scandeval.question_answering_trainer' not found.</p>"},{"location":"src/scandeval/question_answering_trainer/","title":"Question answering trainer","text":"<p>Failure</p> <p>module 'scandeval.question_answering_trainer' not found.</p>"},{"location":"api/scandeval/scores/","title":"Scores","text":"<p>Failure</p> <p>module 'scandeval.scores' not found.</p>"},{"location":"src/scandeval/scores/","title":"Scores","text":"<p>Failure</p> <p>module 'scandeval.scores' not found.</p>"},{"location":"api/scandeval/sequence_classification/","title":"Sequence classification","text":"<p>Failure</p> <p>module 'scandeval.sequence_classification' not found.</p>"},{"location":"src/scandeval/sequence_classification/","title":"Sequence classification","text":"<p>Failure</p> <p>module 'scandeval.sequence_classification' not found.</p>"},{"location":"api/scandeval/speed_benchmark/","title":"Speed benchmark","text":"<p>Failure</p> <p>module 'scandeval.speed_benchmark' not found.</p>"},{"location":"src/scandeval/speed_benchmark/","title":"Speed benchmark","text":"<p>Failure</p> <p>module 'scandeval.speed_benchmark' not found.</p>"},{"location":"api/scandeval/structured_generation_utils/","title":"Structured generation utils","text":"<p>Failure</p> <p>module 'scandeval.structured_generation_utils' not found.</p>"},{"location":"src/scandeval/structured_generation_utils/","title":"Structured generation utils","text":"<p>Failure</p> <p>module 'scandeval.structured_generation_utils' not found.</p>"},{"location":"api/scandeval/tasks/","title":"scandeval.tasks","text":"scandeval.tasks<p> source module scandeval.tasks </p> <p>All benchmarks tasks used in ScandEval.</p> <p> Functions </p> <ul> <li> <p>get_all_tasks \u2014 Get a list of all the dataset tasks.</p> </li> </ul> <p> source get_all_tasks() \u2192 dict[str, Task] </p> <p>Get a list of all the dataset tasks.</p> <p> Returns </p> <ul> <li> <p>dict[str, Task] \u2014 A mapping between names of dataset tasks and their configurations.</p> </li> </ul>"},{"location":"src/scandeval/tasks/","title":"scandeval.tasks","text":"scandeval.tasks<p> docs module scandeval.tasks </p> <pre><code>\"\"\"All benchmarks tasks used in ScandEval.\"\"\"\n\nfrom .config import MetricConfig, Task\n\n\ndef get_all_tasks() -&gt; dict[str, Task]:docs\n    \"\"\"Get a list of all the dataset tasks.\n\n    Returns:\n        A mapping between names of dataset tasks and their configurations.\n    \"\"\"\n    return {cfg.name: cfg for cfg in globals().values() if isinstance(cfg, Task)}\n\n\nLA = Task(\n    name=\"linguistic-acceptability\",\n    supertask=\"sequence-classification\",\n    metrics=[\n        MetricConfig(\n            name=\"mcc\",\n            pretty_name=\"Matthew's Correlation Coefficient\",\n            huggingface_id=\"matthews_correlation\",\n            results_key=\"matthews_correlation\",\n        ),\n        MetricConfig(\n            name=\"macro_f1\",\n            pretty_name=\"Macro-average F1-score\",\n            huggingface_id=\"f1\",\n            results_key=\"f1\",\n            compute_kwargs=dict(average=\"macro\"),\n        ),\n    ],\n    labels=[\"incorrect\", \"correct\"],\n)\n\n\nNER = Task(\n    name=\"named-entity-recognition\",\n    supertask=\"token-classification\",\n    metrics=[\n        MetricConfig(\n            name=\"micro_f1_no_misc\",\n            pretty_name=\"Micro-average F1-score without MISC tags\",\n            huggingface_id=\"seqeval\",\n            results_key=\"overall_f1\",\n        ),\n        MetricConfig(\n            name=\"micro_f1\",\n            pretty_name=\"Micro-average F1-score with MISC tags\",\n            huggingface_id=\"seqeval\",\n            results_key=\"overall_f1\",\n        ),\n    ],\n    labels=[\n        \"o\",\n        \"b-loc\",\n        \"i-loc\",\n        \"b-org\",\n        \"i-org\",\n        \"b-per\",\n        \"i-per\",\n        \"b-misc\",\n        \"i-misc\",\n    ],\n)\n\n\nRC = Task(\n    name=\"reading-comprehension\",\n    supertask=\"question-answering\",\n    metrics=[\n        MetricConfig(\n            name=\"em\",\n            pretty_name=\"Exact Match\",\n            huggingface_id=\"squad_v2\",\n            results_key=\"exact\",\n            postprocessing_fn=lambda raw_score: (raw_score, f\"{raw_score:.2f}%\"),\n        ),\n        MetricConfig(\n            name=\"f1\",\n            pretty_name=\"F1-score\",\n            huggingface_id=\"squad_v2\",\n            results_key=\"f1\",\n            postprocessing_fn=lambda raw_score: (raw_score, f\"{raw_score:.2f}%\"),\n        ),\n    ],\n    labels=[\"start_positions\", \"end_positions\"],\n)\n\n\nSENT = Task(\n    name=\"sentiment-classification\",\n    supertask=\"sequence-classification\",\n    metrics=[\n        MetricConfig(\n            name=\"mcc\",\n            pretty_name=\"Matthew's Correlation Coefficient\",\n            huggingface_id=\"matthews_correlation\",\n            results_key=\"matthews_correlation\",\n        ),\n        MetricConfig(\n            name=\"macro_f1\",\n            pretty_name=\"Macro-average F1-score\",\n            huggingface_id=\"f1\",\n            results_key=\"f1\",\n            compute_kwargs=dict(average=\"macro\"),\n        ),\n    ],\n    labels=[\"negative\", \"neutral\", \"positive\"],\n)\n\n\nSUMM = Task(\n    name=\"summarization\",\n    supertask=\"text-to-text\",\n    metrics=[\n        MetricConfig(\n            name=\"bertscore\",\n            pretty_name=\"BERTScore\",\n            huggingface_id=\"bertscore\",\n            results_key=\"f1\",\n            compute_kwargs=dict(\n                model_type=\"microsoft/mdeberta-v3-base\", device=\"auto\", batch_size=32\n            ),\n        ),\n        MetricConfig(\n            name=\"rouge_l\",\n            pretty_name=\"ROUGE-L\",\n            huggingface_id=\"rouge\",\n            results_key=\"rougeL\",\n        ),\n    ],\n    labels=[],\n)\n\n\nKNOW = Task(\n    name=\"knowledge\",\n    supertask=\"sequence-classification\",\n    metrics=[\n        MetricConfig(\n            name=\"mcc\",\n            pretty_name=\"Matthew's Correlation Coefficient\",\n            huggingface_id=\"matthews_correlation\",\n            results_key=\"matthews_correlation\",\n        ),\n        MetricConfig(\n            name=\"accuracy\",\n            pretty_name=\"Accuracy\",\n            huggingface_id=\"accuracy\",\n            results_key=\"accuracy\",\n        ),\n    ],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n)\n\n\nMCRC = Task(\n    name=\"multiple-choice-reading-comprehension\",\n    supertask=\"sequence-classification\",\n    metrics=[\n        MetricConfig(\n            name=\"mcc\",\n            pretty_name=\"Matthew's Correlation Coefficient\",\n            huggingface_id=\"matthews_correlation\",\n            results_key=\"matthews_correlation\",\n        ),\n        MetricConfig(\n            name=\"accuracy\",\n            pretty_name=\"Accuracy\",\n            huggingface_id=\"accuracy\",\n            results_key=\"accuracy\",\n        ),\n    ],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n)\n\n\nCOMMON_SENSE = Task(\n    name=\"common-sense-reasoning\",\n    supertask=\"sequence-classification\",\n    metrics=[\n        MetricConfig(\n            name=\"mcc\",\n            pretty_name=\"Matthew's Correlation Coefficient\",\n            huggingface_id=\"matthews_correlation\",\n            results_key=\"matthews_correlation\",\n        ),\n        MetricConfig(\n            name=\"accuracy\",\n            pretty_name=\"Accuracy\",\n            huggingface_id=\"accuracy\",\n            results_key=\"accuracy\",\n        ),\n    ],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n)\n\n\nTEXT_MODELLING = Task(\n    name=\"text-modelling\",\n    supertask=\"text-modelling\",\n    metrics=[\n        MetricConfig(\n            name=\"perplexity\",\n            pretty_name=\"Perplexity\",\n            huggingface_id=\"perplexity\",\n            results_key=\"mean_perplexity\",\n        )\n    ],\n    labels=[],\n)\n\n\nSPEED = Task(\n    name=\"speed\",\n    supertask=\"sequence-classification\",\n    metrics=[\n        MetricConfig(\n            name=\"speed\",\n            pretty_name=\"Tokens per second\",\n            huggingface_id=\"\",\n            results_key=\"speed\",\n            postprocessing_fn=lambda raw_score: (raw_score, f\"{raw_score:,.0f}\"),\n        ),\n        MetricConfig(\n            name=\"speed_short\",\n            pretty_name=\"Tokens per second on short documents\",\n            huggingface_id=\"\",\n            results_key=\"speed\",\n            postprocessing_fn=lambda raw_score: (raw_score, f\"{raw_score:,.0f}\"),\n        ),\n    ],\n    labels=[],\n)\n</code></pre>"},{"location":"api/scandeval/text_to_text/","title":"Text to text","text":"<p>Failure</p> <p>module 'scandeval.text_to_text' not found.</p>"},{"location":"src/scandeval/text_to_text/","title":"Text to text","text":"<p>Failure</p> <p>module 'scandeval.text_to_text' not found.</p>"},{"location":"api/scandeval/types/","title":"Types","text":"<p>Failure</p> <p>module 'scandeval.types' not found.</p>"},{"location":"src/scandeval/types/","title":"Types","text":"<p>Failure</p> <p>module 'scandeval.types' not found.</p>"},{"location":"api/scandeval/utils/","title":"Utils","text":"<p>Failure</p> <p>module 'scandeval.utils' not found.</p>"},{"location":"src/scandeval/utils/","title":"Utils","text":"<p>Failure</p> <p>module 'scandeval.utils' not found.</p>"},{"location":"api/scandeval/vllm_models/","title":"Vllm models","text":"<p>Failure</p> <p>module 'scandeval.vllm_models' not found.</p>"},{"location":"src/scandeval/vllm_models/","title":"Vllm models","text":"<p>Failure</p> <p>module 'scandeval.vllm_models' not found.</p>"}]}
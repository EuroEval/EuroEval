{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"The robust European language model benchmark. <p>EuroEval is a language model benchmarking framework that supports evaluating all types of language models out there: encoders, decoders, encoder-decoders, base models, and instruction tuned models. EuroEval has been battle-tested for more than three years and are the standard evaluation benchmark for many companies, universities and organisations around Europe.</p> <p>Check out the leaderboards to see how different language models perform on a wide range of tasks in various European languages. The leaderboards are updated regularly with new models and new results. All benchmark results have been computed using the associated EuroEval Python package, which you can use to replicate all the results. It supports all models on the Hugging Face Hub, as well as models accessible through 100+ different APIs, including models you are hosting yourself via, e.g., Ollama or LM Studio.</p> <p>The idea of EuroEval grew out of the development of Danish language model R\u00f8B\u00c6RTa in 2021, when we realised that there was no standard way to evaluate Danish language models. It started as a hobby project including Danish, Swedish and Norwegian, but has since grown to include 12+ European languages.</p> <p>EuroEval is maintained by Dan Saattrup Smart from the Alexandra Institute, and is funded by the EU project TrustLLM.</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#how-do-you-determine-if-a-model-is-commercial-or-not","title":"How do you determine if a model is \"Commercial\" or not?","text":"<p>We generally determine this based on whether a model's license allows commercial use of the model. However if we are aware that a model is trained on data, that does not allow for commercial use, we will specify it as non-commercial model, despite the stated license. If you find an issue with any of models feel free to open an issue.</p>"},{"location":"faq/#not-finding-the-answer-that-you-are-looking-for","title":"Not finding the answer that you are looking for?","text":"<p>If don't find the answer that you are looking for feel free to ask your question in the forum.</p>"},{"location":"methodology/","title":"Evaluation Methodology","text":"<p>The evaluation methodology is different depending on the architecture of the model. For encoder models, we use a finetuning approach, where we finetune the model on the training data of the task, and evaluate it on the test data. For decoder models, we use either a few-shot or zero-shot approach, where we evaluate the model on the test data without any finetuning, but where the few-shot examples come from the training data of the task. It has been shown that the few-shot approach corresponds to finetuning in the sense of being equivalent to gradient updates on the training data, making the two evaluation methodologies comparable.</p>"},{"location":"methodology/#robust-evaluation","title":"Robust Evaluation","text":"<p>For each model and dataset, we evaluate the model as described above 10 times, each time on a bootstrapped (i.e., sampling with replacement) version of the training and test set. The evaluation score is then the mean of these scores, along with a 95% confidence interval, computed as the mean \u00b1 1.96 x standard error of the mean, where the standard error of the mean is the sample standard deviation divided by the square root of the number of samples.</p> <p>The bootstrap theorem means that this mean and associated confidence interval will be asymptotically correct, giving us a more reliable estimate of the true performance of the model, rather than just the performance on a single test set, which can be noisy.</p>"},{"location":"methodology/#formulating-nlu-tasks-as-generative-tasks","title":"Formulating NLU Tasks as Generative Tasks","text":"<p>In this section we describe how we rephrase the NLU tasks as text-to-text tasks, which makes it possible to evaluate generative models on the tasks. We set up the prompts differently depending on whether the model is instruction tuned or not, as the instruction tuned models require a different prompt structure to ensure that they generate the correct output.</p> <p>For the base (i.e., non-instruction tuned) models, we use the following prompt structure:</p> <pre><code>[prefix prompt]\n\n{% for each few-shot example %}\n  [document prefix]: [few-shot example document]\n\n  [label prefix]: [few-shot example label]\n{% end for %}\n\n[document prefix]: [new document]\n\n[label prefix]:\n</code></pre> <p>For the instruction tuned models, we use the following prompt structure:</p> <pre><code>{% for each few-shot example %}\n  USER: [instruction with few-shot example]\n  ASSISTANT: [label]\n{% end for %}\nUSER: [instruction with new example]\nASSISTANT:\n</code></pre> <p>Here we would use the model's chat template to set up the <code>USER</code> and <code>ASSISTANT</code> parts of the prompt. See all the specific prompts used for each dataset in the dataset configs module.</p> <p>For the sentiment classification task, we simply have the models generate translations of the three labels (positive, negative and neutral). For the linguistic acceptability task, also a text classification task, we use the translations of \"yes\" and \"no\" as the two labels, corresponding to whether the document is grammatically correct or not. For the extractive question answering task, we have the model output the answer directly. For this task we found that changing the label prefix from \"Answer\" to \"Answer in max 3 words\" resulted in a drastic improvement, due to many of the answers of instruction tuned models starting with unnecessary text akin to \"The answer is\". Lastly, for the named entity recognition task, we require the output to be a JSON dictionary, with keys being the translated named entity tags, and values being lists of named entities of that category. To ensure that we are not biasing the evaluation toward models knowing the JSON format, we employ structured generation using the outlines package, which modifies the logits outputted by the model to ensure that the output is always a valid JSON dictionary in the aforementioned format.</p>"},{"location":"methodology/#score-aggregation","title":"Score Aggregation","text":"<p>From the raw scores of the 10 evaluations per dataset, we need to aggregate the model scores into a single score. We want an aggregation method that satisfies the following criteria:</p> <ul> <li>Task Fairness: Each task should be weighted equally.</li> <li>Comparison: If we evaluate models in multiple languages, then it should be   possible to meaningfully compare the language scores of these models with each other.</li> <li>Robustness: If two models do not have a significantly different score on a   dataset, then the aggregated score should reflect this.</li> <li>Magnitude Preservation: The magnitude of the difference between the dataset score   of two models should be reflected in the aggregated score.</li> <li>Minimal Change: Adding a new model should minimally affect the aggregated scores   of the other models.</li> </ul> <p>Before we introduce our chosen aggregation method, we will briefly discuss some common aggregation methods and how they do not satisfy the criteria.</p> <p>The mean score is the most common aggregation method, which would simply be the mean of the 10 scores for each dataset, and then the mean of the dataset scores for each task. This method does not satisfy the Task Fairness criterion, as it does not take into account that metrics have different ranges and variances. The Comparison criterion is also not satisfied, as datasets vary from language to language, with some datasets being more difficult than others. It does, however, satisfy the Robustness, Magnitude Preservation and Minimal Change criteria.</p> <p>The mean rank is another common aggregation method, where we compute the rank of each model on each dataset, and then take the mean of the ranks. This method satisfies the Task Fairness criterion, as it re-casts the scores into a common comparable framework, which therefore weights each task equally. For the same reason, it also satisfies the Comparison criterion (it is important here that we evaluate all the models on all the languages for this to be satisfied). It does not satisfy the Robustness and Magnitude Preservation criteria, by definition of rank. It partially satisfies the Minimal Change criterion, since it only affects the scores of the models which are worse than the new model.</p> <p>We thus see that the mean score and mean rank methods satisfy a disjoint set of the criteria, but that they together satisfy all the criteria. Based on this observation, we introduce the mean rank score method, defined as follows. For each dataset, we start by sorting the models by their mean score on the dataset. As with a rank, we assign the best model with rank score 1. For the next best model, we conduct a one-tailed Welch's t-test to see if the next best model is significantly worse than the first model (p &lt; 0.05). If so, we compute the absolute difference between the mean score of the two models, and divide that by the standard deviation of all the mean scores of the models on the dataset.</p> <p>We then add this to the rank score of the first model. We continue this process for all the models to get the rank scores for the dataset, and to compute the overall score for the model, we take the mean of the rank scores for the datasets. We note that the mean rank score has an intuitive interpretation: it is the average number of standard deviations from the best scoring model (+1).</p> <p>This metric satisfies Task Fairness since we normalise all the scores by dividing by the standard deviation of the dataset scores. The Robustness criterion is satisfied due to our use of a one-tailed Welch's t-test. The Magnitude Preservation criterion is also satisfied, as the magnitude of the difference between the dataset score of two models is reflected in the rank score. It also satisfies Comparison, as we compare the models on a common scale (same argument as the mean rank method). Finally, the Minimal Change criterion is partially satisfied, as adding new models only minimally changes the score of existing models. Concretely, adding new scores will affect the standard deviation normalising factor (this effect tends to zero as the number of models grows, however), and if the model beats all the other models then all the scores will be affected, due to the relative nature of the metric.</p>"},{"location":"methodology/#papers","title":"Papers","text":"<p>Check out more in-depth descriptions of the methodology in the associated research papers:</p> <ul> <li>Encoder vs Decoder: Comparative Analysis of Encoder and Decoder Language Models on   Multilingual NLU Tasks</li> <li>ScandEval: A Benchmark for Scandinavian Natural Language   Processing</li> </ul>"},{"location":"python-package/","title":"The <code>euroeval</code> Python Package","text":"<p>The <code>euroeval</code> Python package is the Python package used to evaluate language models in EuroEval. This page will give you a brief overview of the package and how to use it. You can also check out the full API reference for more details.</p>"},{"location":"python-package/#installation","title":"Installation","text":"<p>To install the package simply write the following command in your favorite terminal:</p> <pre><code>$ pip install euroeval[all]\n</code></pre> <p>This will install the EuroEval package with all extras. You can also install the minimal version by leaving out the <code>[all]</code>, in which case the package will let you know when an evaluation requires a certain extra dependency, and how you install it.</p>"},{"location":"python-package/#quickstart","title":"Quickstart","text":""},{"location":"python-package/#benchmarking-from-the-command-line","title":"Benchmarking from the Command Line","text":"<p>The easiest way to benchmark pretrained models is via the command line interface. After having installed the package, you can benchmark your favorite model like so:</p> <pre><code>$ euroeval --model &lt;model-id&gt;\n</code></pre> <p>Here <code>model</code> is the HuggingFace model ID, which can be found on the HuggingFace Hub. By default this will benchmark the model on all the tasks available. If you want to benchmark on a particular task, then use the <code>--task</code> argument:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --task sentiment-classification\n</code></pre> <p>We can also narrow down which languages we would like to benchmark on. This can be done by setting the <code>--language</code> argument. Here we thus benchmark the model on the Danish sentiment classification task:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --task sentiment-classification --language da\n</code></pre> <p>Multiple models, datasets and/or languages can be specified by just attaching multiple arguments. Here is an example with two models:</p> <pre><code>$ euroeval --model &lt;model-id1&gt; --model &lt;model-id2&gt;\n</code></pre> <p>The specific model version/revision to use can also be added after the suffix '@':</p> <pre><code>$ euroeval --model &lt;model-id&gt;@&lt;commit&gt;\n</code></pre> <p>This can be a branch name, a tag name, or a commit id. It defaults to 'main' for latest.</p> <p>See all the arguments and options available for the <code>euroeval</code> command by typing</p> <pre><code>$ euroeval --help\n</code></pre>"},{"location":"python-package/#benchmarking-from-a-script","title":"Benchmarking from a Script","text":"<p>In a script, the syntax is similar to the command line interface. You simply initialise an object of the <code>Benchmarker</code> class, and call this benchmark object with your favorite model:</p> <pre><code>&gt;&gt;&gt; from euroeval import Benchmarker\n&gt;&gt;&gt; benchmark = Benchmarker()\n&gt;&gt;&gt; benchmark(model=\"&lt;model&gt;\")\n</code></pre> <p>To benchmark on a specific task and/or language, you simply specify the <code>task</code> or <code>language</code> arguments, shown here with same example as above:</p> <pre><code>&gt;&gt;&gt; benchmark(model=\"&lt;model&gt;\", task=\"sentiment-classification\", language=\"da\")\n</code></pre> <p>If you want to benchmark a subset of all the models on the Hugging Face Hub, you can simply leave out the <code>model</code> argument. In this example, we're benchmarking all Danish models on the Danish sentiment classification task:</p> <pre><code>&gt;&gt;&gt; benchmark(task=\"sentiment-classification\", language=\"da\")\n</code></pre>"},{"location":"python-package/#benchmarking-from-docker","title":"Benchmarking from Docker","text":"<p>A Dockerfile is provided in the repo, which can be downloaded and run, without needing to clone the repo and installing from source. This can be fetched programmatically by running the following:</p> <pre><code>$ wget https://raw.githubusercontent.com/EuroEval/EuroEval/main/Dockerfile.cuda\n</code></pre> <p>Next, to be able to build the Docker image, first ensure that the NVIDIA Container Toolkit is installed and configured. Ensure that the the CUDA version stated at the top of the Dockerfile matches the CUDA version installed (which you can check using <code>nvidia-smi</code>). After that, we build the image as follows:</p> <pre><code>$ docker build --pull -t euroeval -f Dockerfile.cuda .\n</code></pre> <p>With the Docker image built, we can now evaluate any model as follows:</p> <pre><code>$ docker run -e args=\"&lt;euroeval-arguments&gt;\" --gpus 1 --name euroeval --rm euroeval\n</code></pre> <p>Here <code>&lt;euroeval-arguments&gt;</code> consists of the arguments added to the <code>euroeval</code> CLI argument. This could for instance be <code>--model &lt;model-id&gt; --task sentiment-classification</code>.</p>"},{"location":"datasets/","title":"Datasets","text":"<p>\ud83d\udc48 Choose a language on the left to see all the evaluation datasets available for that language.</p>"},{"location":"datasets/danish/","title":"\ud83c\udde9\ud83c\uddf0 Danish","text":"<p>This is an overview of all the datasets used in the Danish part of EuroEval. The datasets are grouped by their task - see the task overview for more information about what these constitute.</p>"},{"location":"datasets/danish/#sentiment-classification","title":"Sentiment Classification","text":""},{"location":"datasets/danish/#angry-tweeets","title":"Angry Tweeets","text":"<p>This dataset was published in this paper and was a crowd-sourcing effort to annotate sentiment of Danish tweets.</p> <p>The original full dataset consists of 3,458 samples, and we are using a split of 1,024 / 256 / 2,048 samples for training, validation and testing, respectively (so 3,328 samples used in total). All the samples in the original test set are included in our test set, but our test set is furthermore using a subset of the original training set as test samples as well. The original dataset did not have a validation split, so we have created one by sampling from the training set.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Jeg tror, det der var kampen. Goff virker lost\",\n  \"label\": \"negative\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"@USER @USER Vi bruger ogs\u00e5 snildt 1-2 timer (nogle gange flere timer end det) p\u00e5 at putte den yngste. Det er oftest Tommi, som g\u00f8r det, for jeg g\u00e5r helt amok i processen. S\u00e5 sm\u00f8rer jeg madpakker og rydder op i stedet.\",\n  \"label\": \"neutral\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Er du nysgerrig p\u00e5, hvordan du diskvalificerer dig selv fra at blive taget seri\u00f8st i den offentlige debat? Naser har svaret. #dkpol #dkmedier [LINK]\",\n  \"label\": \"negative\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er tweets og deres sentiment, som kan v\u00e6re 'positiv', 'neutral' eller 'negativ'.\n</code></pre></li> <li>Base prompt template:   <pre><code>Tweet: {text}\nSentiment: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Tweet: {text}\n\nKlassificer sentimentet i tweetet. Svar kun med 'positiv', 'neutral' eller 'negativ'.\n</code></pre></li> <li>Label mapping:<ul> <li><code>positive</code> \u27a1\ufe0f <code>positiv</code></li> <li><code>neutral</code> \u27a1\ufe0f <code>neutral</code></li> <li><code>negative</code> \u27a1\ufe0f <code>negativ</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset angry-tweeets\n</code></pre>"},{"location":"datasets/danish/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"datasets/danish/#dansk","title":"DANSK","text":"<p>This dataset was published in this paper and is a manually annotated subset of Danish Gigaword with the 18 different named entities, following the OntoNotes 5.0 scheme. It was annotated by 10 different annotators.</p> <p>The original full dataset consists of 15,062 samples, and we are using a split of 1,024 / 256 / 1,024 samples for training, validation and testing, respectively (so 2,304 samples used in total). All samples in the validation and test sets of our version also belong to the original validation and test set, respectively.</p> <p>We have furthermore converted the OntoNotes 5.0 labelling scheme to the CoNLL-2003 labelling scheme, which is more common in the NER literature. The mapping is as follows:</p> <ul> <li><code>PERSON</code> \u27a1\ufe0f <code>PER</code></li> <li><code>LOCATION</code> \u27a1\ufe0f <code>LOC</code></li> <li><code>FACILITY</code> \u27a1\ufe0f <code>LOC</code></li> <li><code>GPE</code> \u27a1\ufe0f <code>LOC</code></li> <li><code>ORGANIZATION</code> \u27a1\ufe0f <code>PER</code></li> <li><code>EVENT</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>LANGUAGE</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>PRODUCT</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>WORK OF ART</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>NORP</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>CARDINAL</code> \u27a1\ufe0f <code>O</code></li> <li><code>DATE</code> \u27a1\ufe0f <code>O</code></li> <li><code>LAW</code> \u27a1\ufe0f <code>O</code></li> <li><code>MONEY</code> \u27a1\ufe0f <code>O</code></li> <li><code>ORDINAL</code> \u27a1\ufe0f <code>O</code></li> <li><code>PERCENT</code> \u27a1\ufe0f <code>O</code></li> <li><code>QUANTITY</code> \u27a1\ufe0f <code>O</code></li> <li><code>TIME</code> \u27a1\ufe0f <code>O</code></li> </ul> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"tokens\": array(['I', 'dette', 'efter\u00e5r', 'har', 'Gr\u00f8nland', 'taget', 'en', 'stor', 'beslutning', 'ved', 'folkeafstemningen', 'den', '25.', 'november', '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['\u00c5h', ',', 'Petra', ',', 'vis', 'mig', 'din', 'krop', '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['Fravalget', 'af', 'revision', 'registreres', 'automatisk', 'ved', 'anmeldelse', 'af', 'stiftelse', 'af', 'selskabet', 'hos', 'Erhvervs-styrelsen', '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O'], dtype=object)\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er s\u00e6tninger og JSON-ordb\u00f8ger med de navngivne enheder, som forekommer i den givne s\u00e6tning.\n</code></pre></li> <li>Base prompt template:   <pre><code>S\u00e6tning: {text}\nNavngivne enheder: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>S\u00e6tning: {text}\n\nIdentific\u00e9r de navngivne enheder i s\u00e6tningen. Du skal outputte dette som en JSON-ordbog med n\u00f8glerne 'person', 'sted', 'organisation' og 'diverse'. V\u00e6rdierne skal v\u00e6re lister over de navngivne enheder af den type, pr\u00e6cis som de forekommer i s\u00e6tningen.\n</code></pre></li> <li>Label mapping:<ul> <li><code>B-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>sted</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>sted</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>organisation</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>organisation</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>diverse</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>diverse</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset dansk\n</code></pre>"},{"location":"datasets/danish/#unofficial-dane","title":"Unofficial: DaNE","text":"<p>This dataset was published in this paper and is a manually NER annotated version of the Danish Universal Dependencies treebank. The NER labels follow the CoNLL-2003 labelling scheme.</p> <p>The original full dataset consists of 4,383 / 564 / 565 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). These splits are new and there can thus be some overlap between the original validation and test sets and our validation and test sets.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"tokens\": array(['Det', 'var', 'det', '\u00e5r', ',', 'hans', 'f\u00f8rste', 'LP', ',', '\"', 'With', 'A', 'Little', 'Help', 'From', 'My', 'Friends', '\"', ',', 'udkom', '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['Eddie', 'Carbone', ',', 'italiensk-amerikansk', 'havnearbejder', 'i', 'New', 'York', '.'], dtype=object),\n  \"labels\": array(['B-PER', 'I-PER', 'O', 'B-MISC', 'O', 'O', 'B-LOC', 'I-LOC', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['\"', 'Jeg', 'er', 'mig', '!', '\"', 'insisterer', 'han', 'under', 'det', 'flere', 'hundrede', '\u00e5r', 'gamle', 'egetr\u00e6', ',', 'liggende', ',', 'som', 'den', 'popflab', 'han', 'er', ',', 'p\u00e5', 'ryggen', 'i', 'sine', 'orange', 'jeans', ',', 't-shirt', '-', 'som', 'naturligvis', 'stiller', 'et', 'solbrunt', 'beh\u00e5ret', 'bryst', 'til', 'skue', '-', 'et', 'par', '68er', '\"', 'make', 'love', 'not', 'war', '\"', 'solbriller', 'han', 'netop', 'har', 'k\u00f8bt', 'i', 'Paris', ',', 'og', 'en', 'Kings', 'i', 'k\u00e6ften', '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre></p>"},{"location":"datasets/danish/#linguistic-acceptability","title":"Linguistic Acceptability","text":""},{"location":"datasets/danish/#scala-da","title":"ScaLA-da","text":"<p>This dataset was published in this paper and was automatically created from the Danish Universal Dependencies treebank by assuming that the documents in the treebank are correct, and corrupting the samples to create grammatically incorrect samples. The corruptions were done by either removing a word from a sentence, or by swapping two neighbouring words in a sentence. To ensure that this does indeed break the grammaticality of the sentence, a set of rules were used on the part-of-speech tags of the words in the sentence.</p> <p>The original dataset consists of 5,512 samples, from which we use 1,024 / 256 / 2,048 samples for training, validation and testing, respectively (so 3,328 samples used in total). These splits are used as-is in the framework.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Samme dame dukkede netop nu op sammen med Odd-Catla's erkl\u00e6rede yndling, v\u00e6bneren Aikin af Cantir.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Gebyrets st\u00f8rrelse afh\u00e6nger nemlig af helt, i hvilken kategori den p\u00e5g\u00e6ldende \\\"levnedsmiddelvirksomhed\\\" placeres.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Den statsansatte dyrl\u00e6ge Kronf\u00e5gels p\u00e5 slagteri i Kristiansstad, Karl Erik Bj\u00f8rkman, understreger, bel\u00e6gningen hos producenten betyder meget for dyrenes trivsel:\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er s\u00e6tninger og om de er grammatisk korrekte.\n</code></pre></li> <li>Base prompt template:   <pre><code>S\u00e6tning: {text}\nGrammatisk korrekt: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>S\u00e6tning: {text}\n\nBestem om s\u00e6tningen er grammatisk korrekt eller ej. Svar med 'ja', hvis s\u00e6tningen er korrekt, og 'nej', hvis den ikke er.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>ja</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>nej</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scala-da\n</code></pre>"},{"location":"datasets/danish/#reading-comprehension","title":"Reading Comprehension","text":""},{"location":"datasets/danish/#scandiqa-da","title":"ScandiQA-da","text":"<p>This dataset was published in this paper and was automatically created from the Danish part of the MKQA dataset. The MKQA dataset is based on the English Natural Questions dataset, based on search queries from the Google search engine. The questions and answers were manually translated to Danish (and other languages) as part of MKQA, and the contexts were in ScandiQA-da machine translated using the DeepL translation API. A rule-based approach was used to ensure that the translated contexts still contained the answer to the question, potentially by changing the answers slightly.</p> <p>The original full dataset consists of 6,810 / 500 / 500 samples for training, validation and testing, respectively (so 3,328 samples used in total). We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively, where the splits are made by randomly sampling from the full dataset without considering the original train/validation/test splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"context\": '\"(Sittin\\' On) The Dock of the Bay\" er en sang, der er skrevet af soul-sangeren Otis Redding og guitaristen Steve Cropper sammen. Den blev indspillet af Redding to gange i 1967, herunder en gang f\u00e5 dage f\u00f8r hans d\u00f8d i et flystyrt. Sangen blev udgivet p\u00e5 Stax Records\\' Volt-label i 1968 og blev den f\u00f8rste posthume single, der l\u00e5 \u00f8verst p\u00e5 hitlisterne i USA. Den n\u00e5ede op som nummer 3 p\u00e5 den britiske single-liste.',\n  \"question\": 'Hvem sang sitting on the dock of the bay?',\n  \"answers\": {\n    \"answer_start\": array([79]),\n    \"text\": array(['Otis Redding'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  \"context\": \"The Cat in the Hat Knows a Lot About That!\\nKatten i hatten ved meget om det!\\n\\n\\n\\nKatten i hatten pilot\\n\\n\\n\\nGenre\\nB\u00f8rne-tv/undervisning/komedie\\n\\n\\nInstrueret af\\nTony Collingwood\\n\\n\\nStemmer fra\\nMartin Short\\nJacob Ewaniuk\\nAlexa Torrington\\nRob Tinkler\\n\\n\\nKomponist af temamusik\\nDavid Schweitzer\\n\\n\\nKomponist(er)\\nDavid Schweitzer\\n\\n\\nOprindelsesland\\nCanada\\nDet Forenede Kongerige\\nUSA\\n\\n\\nOprindelige sprog\\nEngelsk\\n\\n\\nAntal s\u00e6soner\\n2\\n\\n\\nAntal episoder\\n60 (liste over episoder)\\n\\n\\nProduktion\\n\\n\\nL\u00f8betid\\n30 minutter\\n\\n\\nProduktionsselskab(er)\\nCollingwood O'Hare Productions\\nPortfolio Entertainment\\nRandom House Children's Entertainment\\nTreehouse TV\\n\\n\\nDistribut\u00f8r\\nTreehouse TV\\n\\n\\nUdgivelse\\n\\n\\nOprindelige netv\u00e6rk\\nTreehouse TV (Canada)\\nPBS Kids (USA)\\nCITV og Tiny Pop (UK)\\n\\n\\nBilledformat\\n480i (SDTV)\\n1080i (HDTV)\\n\\n\\nOriginaludgivelse\\n7. august 2010 (2010-08-07) - nu\\n\\n\\nEksterne links\\n\\n\\nWebsted\\npbskids.org/catinthehat/\",\n  \"question\": 'Hvem synger titelmelodien til the cat in the hat?',\n  \"answers\": {\n    \"answer_start\": array([269]),\n    \"text\": array(['David Schweitzer'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  \"context\": 'Modern Slavery Act 2015\\nLoven om moderne slaveri fra 2015 er en lov fra Det Forenede Kongeriges parlament. Den har til form\u00e5l at bek\u00e6mpe slaveri i Det Forenede Kongerige og konsoliderer tidligere lovovertr\u00e6delser vedr\u00f8rende menneskehandel og slaveri. Loven g\u00e6lder for England og Wales. Lovforslaget blev forelagt underhuset i udkast i oktober 2013 af James Brokenshire, parlamentarisk undersekret\u00e6r for kriminalitet og sikkerhed, i oktober 2013. Lovforslagets sponsorer i indenrigsministeriet var Theresa May og Lord Bates. Det fik kongelig samstemmende udtalelse og blev lov den 26. marts 2015.',\n  \"question\": 'Hvorn\u00e5r tr\u00e5dte den moderne slaveri i kraft?',\n  \"answers\": {\n    \"answer_start\": array([580]),\n    \"text\": array(['26. marts 2015'], dtype=object)\n  }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er tekster med tilh\u00f8rende sp\u00f8rgsm\u00e5l og svar.\n</code></pre></li> <li>Base prompt template:   <pre><code>Tekst: {text}\nSp\u00f8rgsm\u00e5l: {question}\nSvar med maks. 3 ord: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Tekst: {text}\n\nBesvar f\u00f8lgende sp\u00f8rgsm\u00e5l om teksten ovenfor med maks. 3 ord.\n\nSp\u00f8rgsm\u00e5l: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scandiqa-da\n</code></pre>"},{"location":"datasets/danish/#unofficial-belebele-da","title":"Unofficial: BeleBele-da","text":"<p>This dataset was published in this paper and features multiple-choice reading comprehension questions across 122 languages.</p> <p>The original dataset contains 900 unique multiple-choice reading comprehension passages and questions. From these, we use a 256 / 64 / 580 split for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Tekst: Prognoserne siger, at stormen, der er omkring 645 mil (1040 km) vest for Kap Verde-\u00f8erne, sandsynligvis vil forsvinde, f\u00f8r den truer nogen landomr\u00e5der. Fred har i \u00f8jeblikket vinde p\u00e5 165 km/t og bev\u00e6ger sig mod nordvest. Fred er den heftigste tropiske cyklon, der nogensinde er blevet registreret s\u00e5 sydligt og \u00f8stligt i Atlanterhavet, siden man begyndte at bruge satellitbilleder, og kun den tredje store orkan, der er registreret \u00f8st for 35\u00b0V.\\nSp\u00f8rgsm\u00e5l: Da Fred befandt sig n\u00e6r Kap Verde-\u00f8erne, hvilken retning bev\u00e6gede den sig s\u00e5 mod?\\nSvarmuligheder:\\na. Vest\\nb. Syd\\nc. \u00d8st\\nd. Nordvest\",\n  \"label\": \"d\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Tekst: \"Siden Pakistan i 1947 blev uafh\u00e6ngigt af det britiske styre, har den pakistanske pr\u00e6sident udpeget \"\"politiske agenter\"\", som styrer FATA, og som har n\u00e6sten fuldst\u00e6ndig kontrol over omr\u00e5derne. Disse agenter er ansvarlige for at levere regerings- og retstjenester i henhold til artikel 247 i den pakistanske forfatning.\"\\nSp\u00f8rgsm\u00e5l: Hvem leverer retslige tjenester til FATA?\\nSvarmuligheder:\\na. Den pakistanske regering\\nb. Politiske agenter\\nc. Pakistans pr\u00e6sident\\nd. Den britiske regering\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Tekst: Alle er en del af samfundet og benytter transportsystemerne. N\u00e6sten alle klager over transportsystemerne. I udviklede lande h\u00f8rer du sj\u00e6ldent liges\u00e5 mange klager over vandkvalitet eller broer, der styrter sammen. Hvorfor giver transportsystemerne anledning til s\u00e5danne klager, hvorfor svigter de p\u00e5 daglig basis? Er transportingeni\u00f8rer blot inkompetente? Eller foreg\u00e5r der noget mere fundamentalt?\\nSp\u00f8rgsm\u00e5l: Hvilken offentlig service siges at skabe st\u00f8rst utilfredshed i udviklede lande?\\nSvarmuligheder:\\na. Vandkvalitet\\nb. Brobyggelse\\nc. Offentlig transport\\nd. Uddannelse\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Sp\u00f8rgsm\u00e5l: {text}\nSvarmuligheder:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Sp\u00f8rgsm\u00e5l: {text}\nSvarmuligheder:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at svare med 'a', 'b', 'c' eller 'd', og intet andet.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset belebele-da\n</code></pre>"},{"location":"datasets/danish/#knowledge","title":"Knowledge","text":""},{"location":"datasets/danish/#danske-talemader","title":"Danske Talem\u00e5der","text":"<p>This dataset was created by The Danish Language and Literature Society, published here. The dataset features Danish idioms along with their official meaning. For each idiom, three negative samples were created: (a) a random idiom, (b) a concrete made-up idiom, and (c) an abstract made-up idiom. The dataset was created to evaluate the ability of language models to understand Danish idioms.</p> <p>The original full dataset consists of 1,000 samples. We use a 128 / 64 / 808 split for training, validation and testing, respectively (so 1,000 samples used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Hvad betyder udtrykket 'tale nogen efter munden'?\\nSvarmuligheder:\\na. v\u00e6re f\u00f8jelig og give nogen ret selvom man ikke n\u00f8dvendigvis er enig\\nb. erkl\u00e6re sig helt enig med en anden person\\nc. sige det pr\u00e6cis samme som en anden; efterabe\\nd. v\u00e6re egoistisk og sn\u00e6versynet; kun t\u00e6nke p\u00e5 sig selv\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hvad betyder udtrykket 'der falder en sten fra \u00e9ns hjerte'?\\nSvarmuligheder:\\na. en bestemt (kriminel, efters\u00f8gt) person er forsvundet\\nb. man bliver fri for en sorg eller bekymring; man bliver lettet\\nc. man mister \u00e9n man har k\u00e6r\\nd. en sten forlader et hjerte man er i besiddelse af\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hvad betyder udtrykket 'have spidse albuer'?\\nSvarmuligheder:\\na. person der har det meget d\u00e5rligt fysisk og psykisk\\nb. have ophobet vrede over l\u00e6ngere tid\\nc. h\u00e6vde sig p\u00e5 andres bekostning\\nd. have knogler der tr\u00e6der tydeligt frem p\u00e5 ens albuer\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Hvad er betydningen af f\u00f8lgende talem\u00e5de: {text}\nSvarmuligheder:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Hvad er betydningen af f\u00f8lgende talem\u00e5de: {text}\nSvarmuligheder:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at svare med 'a', 'b', 'c' eller 'd', og intet andet.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset danske-talemaader\n</code></pre>"},{"location":"datasets/danish/#danish-citizen-tests","title":"Danish Citizen Tests","text":"<p>This dataset was created by scraping the Danish citizenship tests (indf\u00f8dsretspr\u00f8ven) and permanent residency tests (medborgerskabspr\u00f8ven) from 2016 to 2024. These are available on the official website of the Danish Ministry of International Recruitment and Integration.</p> <p>The original full dataset consists of 870 samples. We use an 345 / 90 / 525 split for training, validation and testing, respectively. Here all the citizenship tests belong to the test split, as well as the newest permanent residency tests. The validation split contains the newer permanent residency tests after the ones in the test split, and the training split contains the oldest permanent residency tests.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Hvilket parti tilh\u00f8rte Lars L\u00f8kke Rasmussen, da han var statsminister i perioderne 2009-11 og 2015-19?\\nSvarmuligheder:\\na. Venstre\\nb. Socialdemokratiet\\nc. Det Konservative Folkeparti\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hvilket af f\u00f8lgende omr\u00e5der har kommunerne ansvaret for driften af?\\nSvarmuligheder:\\na. Domstole\\nb. Vuggestuer\\nc. Sygehuse\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hvilken organisation blev Danmark medlem af i 1945?\\nSvarmuligheder:\\na. Verdenshandelsorganisationen (WTO)\\nb. Den Europ\u00e6iske Union (EU)\\nc. De Forenede Nationer (FN)\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Sp\u00f8rgsm\u00e5l: {text}\nSvarmuligheder:\na. {option_a}\nb. {option_b}\nc. {option_c}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Sp\u00f8rgsm\u00e5l: {text}\nSvarmuligheder:\na. {option_a}\nb. {option_b}\nc. {option_c}\n\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at svare med 'a', 'b', 'c' eller 'd', og intet andet.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset danish-citizen-tests\n</code></pre>"},{"location":"datasets/danish/#unofficial-mmlu-da","title":"Unofficial: MMLU-da","text":"<p>This dataset is a machine translated version of the English MMLU dataset and features questions within 57 different topics, such as elementary mathematics, US history and law. The translation to Danish was done by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 269 / 1,410 / 13,200 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). These splits are new and there can thus be some overlap between the original validation and test sets and our validation and test sets.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Hvilket af f\u00f8lgende coronavirusser har for\u00e5rsaget tusindvis af d\u00f8dsfald over hele verden som en 'opst\u00e5et' virus?\\nSvarmuligheder:\\na. MERS\\nb. SARS\\nc. OC43\\nd. HKU1\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hvilken orbitale v\u00e6g er mest sandsynligt at kollapse i en 'blow out' fraktur?\\nSvarmuligheder:\\na. Taget\\nb. Gulvet\\nc. Den laterale v\u00e6g\\nd. Den mediale v\u00e6g\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hvad er navnet p\u00e5 den st\u00f8rste struktur i Teotihuac\u00e1n, og hvor mange platforme og pyramider blev bygget der?\\nSvarmuligheder:\\na. M\u00e5nepyramiden; 250\\nb. Templet for den fjerkr\u00e6kl\u00e6dte slange; 400\\nc. Solpyramiden; 600\\nd. Inskriptionstemplen; 700\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Sp\u00f8rgsm\u00e5l: {text}\nSvarmuligheder:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Sp\u00f8rgsm\u00e5l: {text}\nSvarmuligheder:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at svare med 'a', 'b', 'c' eller 'd', og intet andet.\n</code></pre></li> </ul>"},{"location":"datasets/danish/#unofficial-arc-da","title":"Unofficial: ARC-da","text":"<p>This dataset is a machine translated version of the English ARC dataset and features US grade-school science questions. The translation to Danish was done by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 1,110 / 297 / 1,170 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively (so 2,304 samples used in total). All new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Et farmaceutisk firma har offentliggjort resultaterne af et begr\u00e6nset eksperiment, der unders\u00f8ger den beskyttende virkning af en kemisk forbindelse mod h\u00f8je doser af UV-str\u00e5ler p\u00e5 hudceller. Senere blev det opdaget, at resultaterne ikke var reproducerbare. Hvilken handling kunne forskere fra firmaet have foretaget for at undg\u00e5 at offentligg\u00f8re fejlagtige resultater?\\nSvarmuligheder:\\na. Udf\u00f8r flere fors\u00f8g.\\nb. Brug kun lave niveauer af str\u00e5ling.\\nc. Brug forskellige b\u00f8lgel\u00e6ngder af str\u00e5ling.\\nd. Unders\u00f8g resultaterne af lignende eksperimenter, f\u00f8r man dannede en hypotese.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"En ingeni\u00f8r skal beregne den potentielle energi af en rutschebanekabine \u00f8verst p\u00e5 en skr\u00e5ning. Hvilken information ville bedst hj\u00e6lpe ingeni\u00f8ren med at bestemme den potentielle energi af kabine?\\nSvarmuligheder:\\na. den afstand, som rutschebanekabinen skal rejse\\nb. massen af rutschebanekabinen ved fuld kapacitet\\nc. den gennemsnitlige v\u00e6gt af en tom rutschebanekabine\\nd. retningen, som rutschebanekabinen bev\u00e6ger sig i\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"En studerende h\u00e6ldte vand i en plastbakke. Studerende satte derefter bakken i fryseren. Hvilken egenskab ved vand \u00e6ndrede sig, da vandet fryser?\\nSvarmuligheder:\\na. Vandet blev til en gas.\\nb. Massen af vandet steg.\\nc. Vandet tog en bestemt form.\\nd. Smagen af vandet \u00e6ndrede sig ikke.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Sp\u00f8rgsm\u00e5l: {text}\nSvarmuligheder:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Sp\u00f8rgsm\u00e5l: {text}\nSvarmuligheder:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at svare med 'a', 'b', 'c' eller 'd', og intet andet.\n</code></pre></li> </ul>"},{"location":"datasets/danish/#common-sense-reasoning","title":"Common-sense Reasoning","text":""},{"location":"datasets/danish/#hellaswag-da","title":"HellaSwag-da","text":"<p>This dataset is a machine translated version of the English HellaSwag dataset. The original dataset was based on both video descriptions from ActivityNet as well as how-to articles from WikiHow. The dataset was translated by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 9,310 samples. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Disse mennesker tr\u00e6der pedalerne med kun det ene ben og st\u00e5r midt p\u00e5 cyklen med det andet ben, der holder deres h\u00e6nder oppe. n\u00e6ste g\u00f8r de\\nSvarmuligheder:\\na. en anden \u00f8velse, hvor de s\u00e6tter det ene ben p\u00e5 pedalen, mens de har det andet ben ude og hopper op og ned.\\nb. tager hinandens h\u00e6nder og udf\u00f8rer en eller anden dansebev\u00e6gelse p\u00e5 b\u00f8rsterne, som de bruger til at snurre rundt med deres kroppe og hoppe med h\u00e6nderne oppe.\\nc. drejer med deres forstenede h\u00e6nder, laver en U-vending og starter derefter deres handlinger igen og igen.\\nd. skifter til at st\u00e5 ved hj\u00e6lp af to arme for at balancere sig selv.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"[header] S\u00e5dan dr\u00e6ber du frugtfluer [title] Brug r\u00e5dden frugt. [step] Dit problem med frugtfluer begyndte sandsynligvis f\u00f8rst, da du opdagede, at du havde efterladt nogle frugter, der til sidst blev r\u00e5dne. Brug den metode, der samlede fluene f\u00f8rste gang til at fange dem igen, men denne gang f\u00f8r dem til en mere morbide slutning.\\nSvarmuligheder:\\na. Dr\u00e6b fluene ved at tr\u00e6kke dem fra deres rede eller ved at bruge tunge k\u00e6der med t\u00e6nger til at fange dem og placere dem i en spand eller stuen. Du kan ogs\u00e5 bruge dyreaff\u00f8ring s\u00e5som fiske- og ande-urin.\\nb. Placer et stykke r\u00e5dden frugt i en sk\u00e5l og str\u00e6k klart plastik over toppen. Sk\u00e6r flere sm\u00e5 huller i plastikken med en tandstik og lad det st\u00e5 t\u00e6t p\u00e5 stedet med fluene.\\nc. Efter at have fors\u00f8gt at fange dobbelt s\u00e5 mange fluer, som du kan, skal du fjerne de ubehagelige frugtstykker fra pakken og bage dem i 2-3 minutter. Fluene vil flyde \u00f8verst p\u00e5 den s\u00f8de marmelade, n\u00e5r du fjerner frugten fra marmeladen.\\nd. [substeps] Tjek d\u00e5ser for knotten, melbiller og fluer. K\u00f8b blomster fra havecentret, hvis du ikke har al produktion i n\u00e6rheden.\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"En mand st\u00e5r indend\u00f8rs p\u00e5 en platform foran tre tilskuere og l\u00f8fter en tung v\u00e6gtstang. En mand n\u00e6rmer sig en v\u00e6gtstang p\u00e5 gulvet og st\u00e5r foran den og forbereder sig p\u00e5 at l\u00f8fte den. manden\\nSvarmuligheder:\\na. l\u00f8fter v\u00e6gtstangen, der h\u00e6nger i luften p\u00e5 platformen, og vender sig mod tilskuerne.\\nb. l\u00f8fter v\u00e6gtstangen og viser, hvordan han udf\u00f8rer det, idet han pauser p\u00e5 hver stang for at m\u00e5le v\u00e6gten.\\nc. b\u00f8jer sig derefter i kn\u00e6ene og l\u00e6gger h\u00e6nderne p\u00e5 v\u00e6gtens stangdel.\\nd. l\u00f8fter derefter klokken p\u00e5 sine skuldre, l\u00e6ner sig tilbage, s\u00e6tter armene bag hovedet og l\u00f8fter den let.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Sp\u00f8rgsm\u00e5l: {text}\nSvarmuligheder:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Sp\u00f8rgsm\u00e5l: {text}\nSvarmuligheder:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at svare med 'a', 'b', 'c' eller 'd', og intet andet.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset hellaswag-da\n</code></pre>"},{"location":"datasets/danish/#summarization","title":"Summarization","text":""},{"location":"datasets/danish/#nordjylland-news","title":"Nordjylland News","text":"<p>This dataset is based on news articles from the Danish news site TV2 Nord, where the summaries are taken as the introductory paragraphs of the articles.</p> <p>The original full dataset consists of 75,200 samples. We use an 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Jacob Emil Andersen viste s\u00f8ndag rundt p\u00e5 Halvorsminde Efterskole ved Hj\u00f8rring. Skolen har ligget p\u00e5 samme sted siden 1903. Han er selv elev, da en IT-linje p\u00e5 skolen fangede hans interesse. -\u00a0Det betyder meget for mig, jeg ville ikke have v\u00e6ret lige s\u00e5 interesseret\u00a0i den her skole, hvis der ikke havde v\u00e6ret IT, fort\u00e6ller Jacob Emil Andersen, der oprindeligt stammer fra Aalborg, til TV2 Nord. En af dem, han viser rundt til Efterskolernes dag, er Isabella Kristensen, der g\u00e5r i skole i Hune. Hun er p\u00e5 jagt efter noget helt specielt. -\u00a0Helt sikkert dans, springgymnastik og fitness med noget puls, forklarer Isabella Kristensen til TV2 Nord. Netop efterskolernes specialisering er en af grundene til, at rekordmange v\u00e6lger at bruge et \u00e5r v\u00e6k fra familien i 8.-, 9.- eller 10.-klasse. De s\u00e6rlige linjefag har man flere af p\u00e5 Halvorsminde Efterskole. Jern og metal, arbejde med tr\u00e6 og vinterbadning er blot nogle af de aktiviteter, eleverne kan st\u00f8de ind i p\u00e5 de forskellige linjefag, som skolen tilbyder. Men efterskolerne skal ogs\u00e5 huske at have fokus p\u00e5 den\u00a0faglighe kvalitet,\u00a0lyder det fra forstanderen. -\u00a0Vi skal v\u00e6re skarpe p\u00e5 nogle nicheprodukter og nogle linjer med noget god kvalitet. S\u00e5 skal vi ogs\u00e5 lave god skole, fort\u00e6ller\u00a0forstander p\u00e5 Halvorsminde Efterskole, Jens Beermann, til TV2 Nord. Han bliver bakket op af sin kollega fra H\u00f8rby Efterskole ved S\u00e6by omkring 30 kilometer fra Halvorsminde. - N\u00e5r man laver sit valgfagsudbud, skal det ikke v\u00e6re tilf\u00e6ldigt. Man skal ikke t\u00e6nke, at \u2019det er smart! Det m\u00e5 tr\u00e6kke elever, det her!\u2019 Der skal v\u00e6re en velovervejet refleksion i forhold til, om det passer ind i det, vi gerne vil som skole,, siger forstander p\u00e5 H\u00f8rby Efterskole, Mogens Vesterg\u00e5rd, til TV2 Nord. Alene i Nordjylland gik mere end 2.000 elever p\u00e5 efterskole i skole\u00e5ret 2018-2019. B\u00e5de Halvorsminde Efterskole og H\u00f8rby Skole har plads til 130 elever. Og noget tyder p\u00e5, at der i hvert fald er sikret en ny\u00a0elev til n\u00e6ste skole\u00e5r efter dagens \u00e5bent hus. -\u00a0Jeg synes at det ser sp\u00e6ndende ud, og jeg har endnu mere lyst til at g\u00e5 her nu, siger Isabella Kristensen.\",\n  \"target_text\": \"S\u00f8ndag inviterede efterskoler landet over potentielle nye elever inden for. Efterskolerne specialiserer sig for at tiltr\u00e6kke elever, men den gode faglighed m\u00e5 ikke blive glemt, lyder det fra nordjyske forstandere.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Efter en nat med spejl glatte veje i Nordjylland melder Nordjyllands Politi om en helt problemfri morgen.\u00a0Selvom politikredse i TV2 Nords sendeomr\u00e5de melder om en rolig nat uden st\u00f8rre uheld, s\u00e5\u00a0kan de bilister, der skal af sted l\u00f8rdag morgen godt forvente\u00a0lidt l\u00e6ngere rejsetid. Der er nemlig stadig glatte veje, og der er faldet en del sne i Nordjylland.\u00a0Saltvogne og sneplove har allerede v\u00e6ret p\u00e5 vejene, og Politiet opfordre forsat bilisterne til at k\u00f8re forsigtigt ude p\u00e5 de snefyldte veje.\",\n  \"target_text\": \"Nordjyllands Politi melder om en stille morgen trods glatte veje og stort snefald i nat.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Det var meget t\u00e6t p\u00e5 at g\u00e5 galt for en 10-\u00e5rig tysk dreng onsdag eftermiddag. Klokken 15:55 modtog alarmcentralen et opkald om en drengen, der var begravet i sand ved Vorup\u00f8r Strand. - Nogle b\u00f8rn legede p\u00e5 stranden, og her har de s\u00e5 gravet et hul ind i klitten. Det er s\u00e5 det, der er kollapset omkring drengen, fort\u00e6ller vagtchef Carsten Henriksen ved Midt- og Vestjyllands Politi. Det vides ikke pr\u00e6cist, hvor meget sand der v\u00e6ltede ned over barnet, men det var nok til, at drengen ikke selv kunne komme fri. De tilstedev\u00e6rende p\u00e5 stranden m\u00e5tte grave ham fri. Han var\u00a0helt begravet i sand i omkring fem minutter. - Der var en tysk l\u00e6ge p\u00e5 stranden, der kunne give f\u00f8rstehj\u00e6lp, indtil ambulancen kunne komme frem, fort\u00e6ller vagtchefen. Drengen kom sig hurtigt og har det godt, men blev alligevel k\u00f8rt til tjek p\u00e5 Aalborg Sygehus.\",\n  \"target_text\": \"B\u00f8rn p\u00e5 Vorup\u00f8r Strand havde gravet et hul ind i klitterne, som kollapsede omkring en 10-\u00e5rig dreng.\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er nyhedsartikler med tilh\u00f8rende resum\u00e9er.\n</code></pre></li> <li>Base prompt template:   <pre><code>Nyhedsartikel: {text}\nResum\u00e9: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Nyhedsartikel: {text}\n\nSkriv et resum\u00e9 af ovenst\u00e5ende artikel.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset nordjylland-news\n</code></pre>"},{"location":"datasets/dutch/","title":"\ud83c\uddf3\ud83c\uddf1 Dutch","text":"<p>This is an overview of all the datasets used in the Dutch part of EuroEval. The datasets are grouped by their task - see the task overview for more information about what these constitute.</p>"},{"location":"datasets/dutch/#sentiment-classification","title":"Sentiment Classification","text":""},{"location":"datasets/dutch/#dbrd","title":"DBRD","text":"<p>This dataset was published in this paper and features Dutch book reviews from Hebban.nl, annotated with sentiment labels, written by the users of the website.</p> <p>The original full dataset consists of 20,000 / 2,200 samples for training and testing, respectively. We use a 1,014 / 253 / 2,014 split for training, validation and testing, respectively (so 3,328 samples used in total). The training and testing splits are subsets of the original splits, and the validation split is a disjoint subset of the original training split.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Het boek geeft uitleg in de basis technieken en heeft handige tips, hoe je de klassieke recepten ook gewoon zelf kan maken, ze zijn geschreven in een soort leermodus, dit alles ondersteunt door stap voor stap foto\u2019s.\",\n  \"label\": \"positive\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Dit boek is het debuut van de Zuid-Afrikaanse schrijver S J Naud\u00e9 , het heeft diverse prijzen gewonnen waaronder de UJ Debutprys 2012.\\nHet is een verhalenbundel, met verhalen over personages, die metaforisch rondtrekkende vogels genoemd worden. Ze vliegen letterlijk rusteloos over de wereld. De een is een muzikante die drie continenten over reist om haar broers en zussen te ontmoeten, een man volgt zijn minnaar via Londen en Berlijn naar een kasteel , in Milaan is een futuristisch lawaaimachine te zien en een andere vrouw wil er voor zorgen dat er geen hiv meer voorkomt in Afrika. Zo zijn er nog een paar verhalen. Het ene verhaal heeft me meer geraakt dan het andere, het beste verhaal vind ik het verhaal waarin een man voor zijn doodzieke moeder zorgt, samen met een Japanse man.\\nDe thema\u2019s die in dit boek voorkomen zijn liefde, troost, acceptatie en succes. Leven en dood, reizen, gevoel en verstand komen steeds weer aan bod in de verhalen. Iedereen zoekt naar antwoorden die niet gegeven worden.\\nHet is een boek dat je niet even snel leest, het zijn allemaal op zich zelf staande verhalen, hoewel sommige personen in andere verhalen weer naar voren komen. Wat precies het verband daar tussen is, heb ik niet kunnen ontdekken.\\nHet is een boek dat niet echt vrolijk is, veel verhalen zijn somber. Doordat er veel Afrikaanse namen in voorkomen raak je af en toe de draad kwijt.\\nIk ben niet erg gecharmeerd van dit boek en geef het 2 sterren .\",\n  \"label\": \"negative\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Voor mij het zwakste boek van Coben tot nu toe.\\nHet was alsof ik naar een slechte B-film aan het kijken was. Bordkartonnen personages die me totaal onverschillig lieten. Deus ex machina's die de plot ongeloofwaardig maken.\\nVerloren is als een slecht, onevenwichtig James Bond verhaal. Veel actie zonder context, background en motivatie.\",\n  \"label\": \"negative\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Hieronder staan tweets en hun sentiment, dat 'positief', 'neutraal' of 'negatief' kan zijn.\n</code></pre></li> <li>Base prompt template:   <pre><code>Tweet: {text}\nSentiment: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Tweet: {text}\n\nClassificeer het sentiment in de tweet. Antwoord met 'positief', 'neutraal' of 'negatief'.\n</code></pre></li> <li>Label mapping:<ul> <li><code>positive</code> \u27a1\ufe0f <code>positief</code></li> <li><code>negative</code> \u27a1\ufe0f <code>negatief</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset dbrd\n</code></pre>"},{"location":"datasets/dutch/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"datasets/dutch/#conll-nl","title":"CoNLL-nl","text":"<p>This dataset was published in this paper and consists of named entity recognition annotations of the Belgian newspaper \"De Morgen\" of 2000.</p> <p>The original full dataset consists of 8,324 / 1,916 / 1,518 samples for training, validation and testing, respectively (so 11,758 samples used in total). We use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively. All the new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"tokens\": array(['Puitstraat', '6', ',', '8890', 'Moorslede', '.'], dtype=object),\n  \"labels\": array(['B-LOC', 'O', 'O', 'O', 'B-LOC', 'O'], dtype=object),\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['Monami-Van', 'Roost', 'had', 'nochtans', 'verloren', '.'], dtype=object),\n  \"labels\": array(['B-PER', 'I-PER', 'O', 'O', 'O', 'O'], dtype=object),\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['Het', 'overwicht', 'lag', 'op', 'nieuw', 'nummers', 'als', \"'\", 'Maria', 'Maria', \"'\", ',', \"'\", 'Put', 'Your', 'Lights', 'On', \"'\", 'en', \"'\", 'Smooth', \"'\", ',', 'stuk', 'voor', 'stuk', 'knappe', 'songs', 'die', 'zich', 'op', 'de', 'koop', 'toe', 'in', 'korte', ',', 'krachtige', 'versies', 'lieten', 'bewonderen', '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object),\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>Hieronder staan zinnen en JSON woordenboeken met de genoemde entiteiten die voorkomen in de gegeven zin.\n</code></pre></li> <li>Base prompt template:   <pre><code>Zin: {text}\nGenoemde entiteiten: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Zin: {text}\n\nIdentificeer de genoemde entiteiten in de zin. Je moet dit uitvoeren als een JSON-woordenboek met de sleutels 'persoon', 'locatie', 'organisatie' en 'diversen'. De waarden moeten lijsten zijn van de genoemde entiteiten van dat type, precies zoals ze voorkomen in de zin.\n</code></pre></li> <li>Label mapping:<ul> <li><code>B-PER</code> \u27a1\ufe0f <code>persoon</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>persoon</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>locatie</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>locatie</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>organisatie</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>organisatie</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>diversen</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>diversen</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset conll-nl\n</code></pre>"},{"location":"datasets/dutch/#linguistic-acceptability","title":"Linguistic Acceptability","text":""},{"location":"datasets/dutch/#scala-nl","title":"ScaLA-nl","text":"<p>This dataset was published in this paper and was automatically created from the Dutch Universal Dependencies treebank by assuming that the documents in the treebank are correct, and corrupting the samples to create grammatically incorrect samples. The corruptions were done by either removing a word from a sentence, or by swapping two neighbouring words in a sentence. To ensure that this does indeed break the grammaticality of the sentence, a set of rules were used on the part-of-speech tags of the words in the sentence.</p> <p>The original dataset consists of 13,603 samples, from which we use 1,024 / 256 / 2,048 samples for training, validation and testing, respectively (so 3,328 samples used in total). These splits are used as-is in the framework.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Met het toepassen van zelfbestuur wordt ook al op de lagere school begonnen.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Vragen, die door een leek niet zo eenvoudig te zijn.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"U ziet een soort eng nachtclubomgeving, waar een groepje schertsaristocraten glazig zit te lachen om haar zouteloze tussenteksten, waarin ze wanhopig probeert een intelligent ondertoontje te leggen.\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Hieronder staan zinnen en of ze grammaticaal correct zijn.\n</code></pre></li> <li>Base prompt template:   <pre><code>Zin: {text}\nGrammaticaal correct: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Zin: {text}\n\nBepaal of de zin grammaticaal correct is of niet. Antwoord met 'ja' als de zin correct is en 'nee' als dat niet het geval is.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>ja</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>nee</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scala-nl\n</code></pre>"},{"location":"datasets/dutch/#unofficial-dutch-cola","title":"Unofficial: Dutch CoLA","text":"<p>This dataset is published here and is a manually annotated linguistic acceptability dataset, with documents coming from descriptions of Dutch syntax.</p> <p>The original full dataset consists of 19,900 / 2,400 / 2,400 samples for training, validation and testing, respectively (so 24,700 samples used in total). We use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively. The original splits were imbalanced, so we ensure a 50/50 split of correct/incorrect samples in the new splits. All new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Tasman heeft geen Maori gezien.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Jan is vrij bang voor honden en ik ben het zeer erg voor spinnen.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Wat is het duidelijk dat Jan zal krijgen?\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p>"},{"location":"datasets/dutch/#reading-comprehension","title":"Reading Comprehension","text":""},{"location":"datasets/dutch/#squad-nl","title":"SQuAD-nl","text":"<p>This dataset is published here and is a machine translated dataset of the English SQuAD and XQuAD datasets, created for the Dutch-language DUMB benchmark. Google Translate was used to translate the original datasets to Dutch. The test data was manually corrected by eight BSc students as part of their thesis work.</p> <p>The original SQuAD and XQuAD datasets are based on English Wikipedia articles and the questions and answers are written by crowdworkers.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"context\": \"Windows 8 bevat ook verbeterde ondersteuning voor mobiel breedband; het besturingssysteem kan nu de plaatsing van een simkaart detecteren en automatisch verbindingsinstellingen configureren (inclusief APN's en carrier-branding), en het internetgebruik verminderen om bandbreedte op gemeten netwerken te besparen. Windows 8 voegt ook een ge\u00efntegreerde instelling voor vliegtuigmodus toe om ook alle draadloze connectiviteit wereldwijd uit te schakelen. Vervoerders kunnen ook accountbeheersystemen aanbieden via Windows Store-apps, die automatisch kunnen worden ge\u00efnstalleerd als onderdeel van het verbindingsproces en gebruiksstatistieken bieden op hun respectievelijke tegel.\",\n  \"question\": 'Wat registreert het plaatsen van een simkaart?',\n  \"answers\": {\n    \"answer_start\": array([68]),\n    \"text\": array(['het besturingssysteem'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  \"context\": 'Het Duitse systeem van hoger onderwijs omvat twee vormen van academische instellingen: universiteiten en hogescholen (Fachhochschule). De universiteit van Jena is de grootste van de vier universiteiten van Th\u00fcringen en biedt bijna elke discipline. Het werd opgericht in 1558 en heeft vandaag 21.000 studenten. De op een na grootste is de Technische Universit\u00e4t Ilmenau met 7.000 studenten, opgericht in 1894, die veel technische disciplines biedt, zoals techniek en wiskunde. De universiteit van Erfurt, gesticht in 1392, heeft tegenwoordig 5.000 studenten en legt de nadruk op geesteswetenschappen en lerarenopleiding. De Bauhaus-universiteit Weimar is met 4.000 studenten de kleinste universiteit van Th\u00fcringen en is gespecialiseerd in creatieve vakken zoals architectuur en kunst. Het werd opgericht in 1860 en kreeg tijdens het interbellum bekendheid als de belangrijkste kunstacademie van Duitsland, het Bauhaus.',\n  \"question\": 'Wat is de grootste school in Th\u00fcringen?',\n  \"answers\": {\n    \"answer_start\": array([135]),\n    \"text\": array(['De universiteit van Jena'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  \"context\": 'Door di\u00ebten in westerse landen te vergelijken, hebben onderzoekers ontdekt dat hoewel de Fransen meer dierlijk vet eten, de incidentie van hartaandoeningen in Frankrijk laag blijft. Dit fenomeen wordt de Franse paradox genoemd en wordt verondersteld te ontstaan door de beschermende voordelen van het regelmatig consumeren van rode wijn. Afgezien van de mogelijke voordelen van alcohol zelf, waaronder verminderde aggregatie van bloedplaatjes en vasodilatatie, bieden polyfenolen (bijv. Resveratrol), voornamelijk in de druivenschil, andere vermoedelijke gezondheidsvoordelen, zoals:',\n  \"question\": 'Wat eten mensen in Frankrijk meer van dat in de meeste westerse landen?',\n  \"answers\": {\n    \"answer_start\": array([102]),\n    \"text\": array(['dierlijk vet'], dtype=object)\n  }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>Hieronder volgen teksten met bijbehorende vragen en antwoorden.\n</code></pre></li> <li>Base prompt template:   <pre><code>Tekst: {text}\nVraag: {question}\nAntwoord in max 3 woorden: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Tekst: {text}\n\nBeantwoord de volgende vraag over de bovenstaande tekst in maximaal 3 woorden.\n\nBesvar f\u00f8lgende sp\u00f8rgsm\u00e5l om teksten ovenfor med maks. 3 ord.\n\nVraag: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset squad-nl\n</code></pre>"},{"location":"datasets/dutch/#unofficial-belebele-nl","title":"Unofficial: BeleBele-nl","text":"<p>This dataset was published in this paper and features multiple-choice reading comprehension questions across 122 languages.</p> <p>The original dataset contains 900 unique multiple-choice reading comprehension passages and questions. From these, we use a 256 / 64 / 580 split for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Tekst: Mystiek is het geloven in, identificeren met of bewustzijn van een ultieme werkelijkheid, goddelijkheid, spirituele waarheid of God. De kerkganger streeft naar een directe gewaarwording, intu\u00eftie of inzicht in de goddelijke werkelijkheid. Volgers streven een bepaalde manier van leven na of willen ervaringen opdoen die ze datzelfde gevoel geven. In tegenstelling tot andere religieuze overtuigingen en aanbidding, legt mystiek nadruk op de rechtstreekse persoonlijke beleving van een unieke staat van bewustzijn, vooral van een vredige, inzichtelijke, gelukzalige of extatische aard.\\nVraag: Wat is geen juiste omschrijving van mystiek?\\nAntwoordopties:\\na. De nadruk ligt op het ervaren van een vredige, gelukzalige staat van bewustzijn\\nb. Volgers van mystiek streven bewustwording na van een spirituele werkelijkheid\\nc. Volgers van mystiek passen gebruiken toe die hun inzicht in een goddelijke werkelijkheid vergroten\\nd. De nadruk op het streven naar een directe persoonlijke beleving is vergelijkbaar met veel andere vormen van religieuze overtuiging en aanbidding\",\n  \"label\": \"d\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Tekst: Het favoriete maaltje van ocelotten zijn kleine dieren. Ze vangen apen, slangen, knaagdieren en vogels als dat lukt. De ocelot jaagt bijna uitsluitend op dieren die veel kleiner zijn dan hij zelf is. Geleerden vermoeden dat ocelotten hun reukvermogen gebruiken om op kleine dieren (hun prooi) te jagen, door aan de grond te ruiken waar deze zijn geweest. Ze kunnen door nachtvisie heel goed in het donker zien en bewegen zich heel onopvallend voort. Ocelotten jagen op prooi door zich \u00e9\u00e9n te maken met de omgeving en vervolgens op hun prooi te springen.\\nVraag: Welke uitspraak over een ocelot is onjuist?\\nAntwoordopties:\\na. Ze kunnen goed in het donker jagen\\nb. Ze bewegen zich in stilte voort\\nc. Hun reukvermogen is zwak\\nd. Ze jagen het liefst op kleine dieren\",\n  \"label\": \"c\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Tekst: Er was 120-160 kubieke meter brandstof aan boord van de Luno toen het schip motorproblemen kreeg en door de harde wind en golven tegen de golfbreker werd geduwd. De twaalf crewleden zijn met helikopters in veiligheid gebracht, met als enige verwonding een gebroken neus. Het 100 meter lange schip was onderweg om de gebruikelijke lading kunstmest op te halen. In eerste instantie vreesden autoriteiten dat het vaartuig met de lading zou kunnen gaan lekken.\\nVraag: Waar vreesden de autoriteiten volgens de tekst in eerste instantie voor wat betreft de Luno?\\nAntwoordopties:\\na. Gebrek aan een lading kunstmest\\nb. Golven en harde wind\\nc. Lekken van brandstof\\nd. Verwondingen van bemanningsleden\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Hieronder staan meerkeuzevragen (met antwoorden).\n</code></pre></li> <li>Base prompt template:   <pre><code>Vraag: {text}\nAntwoordopties:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nAntwoord: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Vraag: {text}\nAntwoordopties:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBeantwoord de bovenstaande vraag met 'a', 'b', 'c' of 'd', en niets anders.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset belebele-nl\n</code></pre>"},{"location":"datasets/dutch/#knowledge","title":"Knowledge","text":""},{"location":"datasets/dutch/#mmlu-nl","title":"MMLU-nl","text":"<p>This dataset is a machine translated version of the English MMLU dataset and features questions within 57 different topics, such as elementary mathematics, US history and law. The translation to Dutch was done by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 269 / 1,410 / 13,200 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). These splits are new and there can thus be some overlap between the original validation and test sets and our validation and test sets.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Polarisatie is een eigenschap van\\nAntwoordopties:\\na. transversale golven.\\nb. longitudinale golven.\\nc. alle golven.\\nd. Geen van deze.\",\n  \"label\": \"a\",\n}\n</code></pre> <pre><code>{\n  \"text\": \"Welk internetbedrijf gaat onder de afkorting AOL?\\nAntwoordopties:\\na. Amerika Over Lijnen\\nb. Amerika Online\\nc. Amerikanen op Links\\nd. Amerikanen op LOR\",\n  \"label\": \"b\",\n}\n</code></pre> <pre><code>{\n  \"text\": \"Deze vraag verwijst naar de volgende informatie. Lees het volgende fragment. Nooit waren talenten van het hoogste genie van de meest verheven soort overvloediger geschonken aan een mens. Het genie van Napoleon is verbazingwekkend. Alle takken van menselijke kennis leken even vertrouwd voor zijn gigantische geest. Zijn conversaties op St. Helena, verspreid over de talloze en omvangrijke herdenkingsstukken van degenen die ze verzamelden, zijn gevuld met de grootste interesse. Tijdens de lange doodsstrijd van zijn gevangenschap en zijn dood, sprak hij met volledige vrijheid over de gebeurtenissen van zijn wonderbaarlijke carri\\u00e8re, en over al die onderwerpen van moralen, politiek en religie, die het meest diep de welvaart van ons ras betreffen. Er is geen geest die niet zal worden versterkt door bekendheid met deze diepzinnige gedachten, uitgedrukt met zoveel gloed van gevoel en energie van dictie. \\u2014 John S. C. Abbott, historicus, Napoleon op St. Helena, 1855 Napoleon hielp de Franse Revolutie tot een internationale beweging te maken in de gebieden die hij veroverde.\\nAntwoordopties:\\na. Door een universele valuta op basis van de Franse frank op te leggen\\nb. Door de brute onderdrukking van guerrilla-verzet\\nc. Door het afschaffen van feodalisme en herenboerderijen\\nd. Door het aanmoedigen van het gebruik van Frans als universele taal\",\n  \"label\": \"c\",\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Hieronder staan meerkeuzevragen (met antwoorden).\n</code></pre></li> <li>Base prompt template:   <pre><code>Vraag: {text}\nAntwoordopties:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nAntwoord: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Vraag: {text}\nAntwoordopties:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBeantwoord de bovenstaande vraag met 'a', 'b', 'c' of 'd', en niets anders.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset mmlu-nl\n</code></pre>"},{"location":"datasets/dutch/#unofficial-arc-nl","title":"Unofficial: ARC-nl","text":"<p>This dataset is a machine translated version of the English ARC dataset and features US grade-school science questions. The translation to Dutch was done by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 1,110 / 297 / 1,170 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively (so 2,304 samples used in total). All new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"In een graslandecosysteem, als de populatie van adelaars plotseling afneemt, wat zal waarschijnlijk het effect zijn op de rest van het ecosysteem?\\nAntwoordopties:\\na. Het ecosysteem zal overbevolkt worden met slangen.\\nb. Er zal een afname zijn in de populatie van slangen in het ecosysteem.\\nc. De voedingswaarde van de bodem zal afnemen in het ecosysteem.\\nd. Er zullen meer soorten planten beginnen te groeien in het ecosysteem.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Ptolemeus was een oude astronoom die dacht dat de Aarde het centrum van het universum was. Toen hij observaties deed die hiermee niet overeenkwamen, stelde hij een verschijnsel genaamd \\\"epicycli\\\" voor om de observaties te verklaren. Hoe was Ptolemeus' proces vergelijkbaar met het moderne wetenschappelijke proces?\\nAntwoordopties:\\na. Ptolemeus baseerde zijn model deels op een geloofssysteem.\\nb. Observaties inspireerden Ptolemeus om zijn verklaringen aan te passen.\\nc. Ptolemeus probeerde het universum te beschrijven in plaats van het te verklaren.\\nd. Experimenten vormden de basis van Ptolemeus' model van het universum.\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Wat onderscheidt de organismen in het rijk Fungi van andere eukaryotische organismen?\\nAntwoordopties:\\na. Fungi zijn eencellig.\\nb. Fungi reproduceren seksueel.\\nc. Fungi verkrijgen voedingsstoffen door middel van absorptie.\\nd. Fungi maken voedsel door middel van fotosynthese.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Hieronder staan meerkeuzevragen (met antwoorden).\n</code></pre></li> <li>Base prompt template:   <pre><code>Vraag: {text}\nAntwoordopties:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nAntwoord: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Vraag: {text}\nAntwoordopties:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBeantwoord de bovenstaande vraag met 'a', 'b', 'c' of 'd', en niets anders.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset arc-nl\n</code></pre>"},{"location":"datasets/dutch/#common-sense-reasoning","title":"Common-sense Reasoning","text":""},{"location":"datasets/dutch/#hellaswag-nl","title":"HellaSwag-nl","text":"<p>This dataset is a machine translated version of the English HellaSwag dataset. The original dataset was based on both video descriptions from ActivityNet as well as how-to articles from WikiHow. The dataset was translated by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 9,310 samples. We use an 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"[header] Hoe maak je organische babydoekjes? [title] Kies een rol organische papieren handdoeken. [step] Deze dienen als de eigenlijke doekjes. Experimenteer met verschillende merken en texturen totdat je degene vindt die het beste werkt voor de huid van je baby.\\nAntwoordopties:\\na. Het is belangrijk om organische papieren handdoeken te gebruiken, omdat niet-organische papieren handdoeken bleekmiddel, verf en andere chemicali\\u00ebn kunnen bevatten die vaak worden gebruikt bij de productie van papierproducten. [substeps] Over het algemeen maken bekende merken van papieren handdoeken betere doekjes dan de goedkopere, generieke versies.\\nb. Je kunt een papieren handdoek gebruiken die gebruikt wordt voor luierdoekjes, maar je kunt dezelfde ook gebruiken voor andere doekjes. [substeps] Je kunt drie- of vierzijdige doekjes gebruiken om je te helpen bij het mengen van alle melk, yoghurt en water die je in \\u00e9\\u00e9n container hebt gemengd.\\nc. Als je zelfgemaakte lotion gebruikt, gebruik dan geen papieren handdoeken; deze moeten ook van niet-papier zijn. Rol een grote rol kleine papieren handdoeken uit en houd rekening met de algehele geur van de pad.\\nd. [substeps] Spreid het droge doekje uit over het hele oppervlak van de huid van je baby en vermijd contact met het droge doekje (tondeuse, kam of puimsteen). [title] Plaats de fles boven een kom met warm water gedurende 10 minuten.\",\n  \"label\": \"a\",\n}\n</code></pre> <pre><code>{\n  \"text\": \"[header] Hoe maak je een jurk zonder patroon [title] Koop een jurkmodel. [step] Je hebt een verstelbaar jurkmodel nodig om ervoor te zorgen dat je jurkontwerpen op exact de maat worden gemaakt die je nodig hebt. Verstelbare jurkmodellen zijn verkrijgbaar voor ongeveer $ 250 nieuw.\\nAntwoordopties:\\na. [substeps] Je kunt een schoenmakersstof, bedrukte binnenbekleding of bedrukt behang gebruiken om je jurkmodel te maken. Kies het patroon en knip het patroon zelf uit.\\nb. [title] Stel je jurkmodel af op de hoogte-, taille- en torso-maten die je gaat gebruiken voor je prototypejurk. [title] Maak een schets van de jurk die je wilt maken.\\nc. Als je van plan bent om strapless jurken te dragen, wil je misschien een jurkmodel kopen met een grotere voor-achter-maat. [title] Plaats je jurkmodel op de tafel.\\nd. Je kunt ook een jurkmodel in de supermarkt kopen. [substeps] Als je een strapless jurk wilt, kies dan voor een mouwloze jurk.\",\n  \"label\": \"b\",\n}\n</code></pre> <pre><code>{\n  \"text\": \"[header] Hoe citrusvruchten te raspen [title] Was de citrusvrucht. [step] Voordat je begint, spoel de vrucht af onder stromend koel water en wrijf het vervolgens zachtjes schoon met een schone doek of papieren handdoek. Een lichte spoeling helpt bij het verwijderen van het natuurlijke wasachtige residu aan de buitenkant van de vrucht.\\nAntwoordopties:\\na. [substeps] Zorg ervoor dat de vrucht volledig is afgespoeld voordat je doorgaat naar de volgende stap. De meeste citrusvruchten hebben het beschadigde deel verwijderd, maar met het middenstuk kun je afwisselen tussen het opfrissen van de schil met water en het verwijderen van de schil.\\nb. [substeps] Het werk kan het beste ook laat in de avond worden gedaan, nadat de suiker is verdampt. [title] Maak een zure citrus door een kom met zout in het water te dompelen.\\nc. Je kunt de citrusvrucht ook kort laten weken in een ondiepe kom met water. [substeps] Het is belangrijk om citrusvruchten altijd te wassen wanneer je ze raspt, omdat de buitenkant het deel is dat daadwerkelijk in je voedsel terechtkomt.\\nd. [title] Doe het mengsel van rasp in een druppelaar. [step] Commercieel verkrijgbare rasp komt van de schil van de citrusboom.\",\n  \"label\": \"c\",\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Hieronder staan meerkeuzevragen (met antwoorden).\n</code></pre></li> <li>Base prompt template:   <pre><code>Vraag: {text}\nAntwoordopties:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nAntwoord: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Vraag: {text}\nAntwoordopties:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBeantwoord de bovenstaande vraag met 'a', 'b', 'c' of 'd', en niets anders.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset hellaswag-nl\n</code></pre>"},{"location":"datasets/dutch/#summarization","title":"Summarization","text":""},{"location":"datasets/dutch/#wikilingua-nl","title":"WikiLingua-nl","text":"<p>This dataset was published here and consists of Dutch WikiHow articles and their summaries, where a summary consists of the first sentence of each \"how-to\" step in the article (and this first sentence is not included in the article text).</p> <p>The original full dataset consists of 21,345 / 3,058 / 6,105 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively (so 2,304 samples used in total). All new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Je gaat de ham ongeveer 15 tot 20 minuten glaceren voordat hij klaar is met koken. Om het glazuur op tijd klaar te hebben, begin je met de bereiding ervan ongeveer 45 tot 60 minuten voordat je verwacht dat de ham klaar zal zijn. Snelle glazuren zijn in een paar minuten klaar, en zelfs de glazuren die op het fornuis moeten sudderen, nemen minder dan 15 minuten in beslag. Voor de eenvoudigste optie zonder te koken, klop je gewoon 270 g donkerbruine suiker met 60 ml sinaasappelsap, rode wijn of cognac. Meng de ingredi\\u00ebnten in een kleine kom totdat de suiker volledig is opgelost. Als alternatief, combineer je 270 g lichtbruine suiker, 160 ml sojasaus, en twee gehakte knoflookteentjes in een kleine steelpan -- breng dan de ingredi\\u00ebnten aan de kook op gemiddeld vuur. Zet  de temperatuur lager zodra het mengsel aan de kook is. Roer het af en toe door en laat het 3-5 minuten sudderen, of tot het iets is ingedikt. Zet dan het vuur uit en laat het glazuur minstens 10 tot 15 minuten afkoelen alvorens het over de ham te strijken. Klop 320 ml melasse, 160 ml bourbon en \\u00bd theelepel (1 g) gemalen kruidnagel in een kleine steelpan. Breng de ingredi\\u00ebnten aan de kook op middelmatig vuur, zet het vuur dan laag en laat het onder af en toe roeren, sudderen gedurende 3-5 minuten. Op het moment dat het mengsel iets verdikt is, zet je het vuur uit en laat je het 10 tot 15 minuten afkoelen. Combineer 180 ml ahornsiroop, 120 ml sinaasappelmarmelade, 2 eetlepels (30 g) ongezouten boter, 1 eetlepel (16 g) Dijon-mosterd, 1 theelepel (2 g) gemalen zwarte peper, en \\u00bc theelepel gemalen kaneel in een kleine steelpan. Laat het mengsel op matig vuur sudderen, onder af en toe roeren, gedurende 5-10 minuten, of totdat het stroperig is en is ingedikt tot 240 ml. Laat het glazuur minstens 10 tot 15 minuten afkoelen alvorens het over de ham te strijken. Er zijn talloze recepten voor glazuren te vinden, maar het bedenken van een eigen glazuur is eenvoudig. Experimenteer met ingredi\\u00ebnten tot je de zoete, zure en hartige smaken in balans hebt gebracht. Streef naar ongeveer 240 tot 500 ml glazuur, en reserveer ongeveer een derde ervan voor op de eettafel. De basisingredi\\u00ebnten van een glazuur zijn een zoetstof (zoals bruine suiker of melasse), een zuur (zoals azijn of sinaasappelsap), en kruiden of specerijen (zoals tijm of kruidnagel).\",\n  \"target_text\": \"Bereid het glazuur voor nadat je de ham in de oven hebt gezet. Klop een glazuur van bruine suiker voor een eenvoudige klassieker. Sudder een sojasausglazuur voor een hartige smaak. Combineer bourbon, melasse en kruidnagel voor een diep, warm glazuur. Maak een esdoorn-sinaasappelglazuur voor een pittige, opvallende smaakcombinatie. Bedenk je eigen aangepaste glazuur.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Je koplampen zijn je meest belangrijke levenslijn tijdens het rijden in het donker. Als ze niet in goede conditie zijn, vergroot je onnodig het risico op een ongeval. Houd je koplampen schoon door ze om de paar weken te wassen -- dit houdt de helderheid en scherpte van de lichtbundel hoog. Als een koplamp opbrandt, vervang deze dan zo snel mogelijk en rijd niet in het donker totdat de lamp hersteld is. Het is daarnaast overigens ook verboden om auto te rijden zonder goed werkende koplampen. Bovendien moet je voor de meeste zichtbaarheid je voorruit, ramen en spiegels zo helder en schoon maken als je kunt. Veeg deze belangrijke onderdelen van je auto niet schoon met je hand -- de natuurlijke olie van je huid kan vlekken op de spiegel achterlaten. Gebruik in plaats daarvan een krant of microvezeldoekje. De verstralerlichten van je auto kunnen je veiligheid significant vergroten wanneer je 's nachts rijdt, maar alleen als je ze correct gebruikt. Verstralers gebruik je bij het rijden door zeer donkere gebieden met weinig zicht, waar er niet veel verkeer is. In deze gevallen kunnen verstralers je gezichtsbereik veel breder en langer maken, dus gebruik ze waar nodig.  Zorg dat je verstralers uitschakelt wanneer je achter een andere auto rijdt of als er tegenliggers zijn. In deze gevallen kan het heldere licht van de verstralers andere automobilisten verblinden, waardoor het moeilijker voor hen wordt om veilig te rijden. Als je afslaat bij een bocht of over een heuveltop gaat en de zwakke gloed ziet van de koplampen van een andere auto, zet je verstralers dan voor alle zekerheid uit, zodat de andere bestuurder niet plotseling wordt verblind. Soms, zijn de koplampen van een auto schuiner naar de grond gericht dan nodig is, of zijn ze niet perfect symmetrisch uitgelijnd. De helderste koplampen in de wereld zijn niet nuttig als de weg voor je niet naar behoren verlichten. Dus als je merkt dat het moeilijk is om de weg voor je te zien tijdens het rijden in het donker, dan kun je overwegen om je koplampen opnieuw bij te stellen. Bij een professionele garage is deze procedure meestal heel snel en goedkoop geregeld. Het is ook mogelijk om zelf je koplampen bij te stellen. Aangezien iedere auto anders is, zal je de handleiding van je auto moeten raadplegen. Wees geduldig, want het kan even duren om koplampen perfect uitgelijnd te krijgen. In een perfecte wereld zouden andere bestuurders altijd hun verstralers dimmen als ze je zien, net zoals jij voor hen zou doen. Helaas willen automobilisten dit nog wel eens vergeten. Als een tegemoetkomende auto verstralers aan heeft staan, kijk daar dan niet naar, want het felle licht kan je tijdelijk verblinden. Kijk in plaats daarvan naar de rechterkant van je rijbaan (of in landen waar je aan de linkerkant van de weg rijdt, naar links), terwijl je vanuit je perifere zicht op gevaren let. Dit houdt je zo opmerkzaam mogelijk op de gevaren om je heen, met behoud van je zicht. Als een auto achter je verstralers aan heeft staan, probeer dan je achteruitkijkspiegel te verstellen om het licht uit je ogen te houden. Je kunt zelfs de spiegel zo instellen dat het licht weerkaatst naar de bestuurder van die auto, om hem te wijzen op zijn fout. Als je verwacht dat je veel 's nachts gaat rijden en onder mistige omstandigheden, dan kun je overwegen om te investeren in een set mistlampen. Vaak zijn deze lichten laag gemonteerd op de voorbumper om zoveel mogelijk wegdek te verlichten (mist is het dunst tot op een halve meter of zo boven het wegdek). Niet alle aftermarket lichten zijn even goed gemaakt, dus praat met je autodealer alvorens deze aanschaf te doen. Gebruik nooit je standaard verstralers in de mist. De reflecterende waterdeeltjes waaruit mist bestaat kunnen het heldere licht naar je terugkaatsen, waardoor je nog minder van de weg kunt zien dan zonder licht. De koplampen van andere auto's (en vooral verstralers) kunnen unieke uitdagingen vormen voor chauffeurs met een bril. Glazen kunnen soms tegemoetkomend licht op manieren reflecteren die een verduisterende schittering vormt voor de brildrager. Om dit te voorkomen kun je contactlenzen proberen of een brilglazen kopen met een anti-reflecterende coating, om deze effecten te minimaliseren. Als je een paar speciale brilglazen koopt, leg die dan in je auto zodat je ze altijd bij de hand hebt wanneer je de weg op gaat.\",\n  \"target_text\": \"Houd je koplampen, spiegels en voorruit in topconditie. Gebruik je verstraler voor situaties met weinig licht. Pas eventueel je koplampen aan. Ga op de juiste manier om met verstralers van andere weggebruikers door naar de kant van de weg te kijken. Overweeg om lage mistlampen te installeren. Draag je een bril, gebruik dan een anti-reflecterende coating.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Over het algemeen hebben raszuivere Cavaliers voorspelbare eigenschappen. Als je een raszuivere Cavalier koopt, kun je verwachten dat ze energieke, knuffelbare huisdieren zijn met een redelijk te onderhouden vacht. Genetisch bepaald hebben Cavaliers een neiging tot zorgeloosheid. Als je een rashond koopt, kun je een dergelijk karakter verwachten. Niet raszuivere Cavaliers kunnen sommige van de biologische eigenschappen overnemen van om het even welk ander ras waar ze mee gekruist zijn. Als ze zijn gekruist met een jachthond, dan kunnen ze een sterker jachtinstinct hebben, op dezelfde manier kunnen ze, als ze met een ras zijn gekruist met minder energie, zoals de shih tzu, dat energieke enthousiasme kwijtraken waar je in de eerste plaats op gevallen bent. Mensen hebben hun zinnen gezet op raszuivere Cavaliers. Dit betekent dat ze uit een beperkte genenpoel gefokt zijn. Om aangeduid te worden als raszuiver, wordt er op veel plaatsen inteelt gedaan met hun honden, en anderen hebben onwetend gefokt met een genenpoel die te klein is. Dit heeft heel realistische en bijzonder ongewenste consequenties. Raszuivere Cavaliers hebben een verhoogd risico op hartklachten, hernia en/of ernstige neurologische aandoeningen.   Hartziekte: in Engeland heeft 59% van de Cavaliers ouder dan 4 jaar een hartruis. Zijnde bijna tweederden van de populatie Cavaliers in Engeland is dit een uitzonderlijk statistisch gegeven.  Chiari misvorming en Syringomyelia: Kort gezegd betekent deze aandoening dat de schedel van de hond te klein is voor zijn hersenen. Dit veroorzaakt afschuwelijke zenuwpijn. Het diergeneeskundige leerboek \\\"Breed Predispositions to Disease in the Dogs and Cats\\\" bestempelt deze aandoening als \\\"veel voorkomend\\\" met tekenen die zich ontwikkelen tussen de leeftijd van 5 maanden tot 3 jaar.   Epilepsie: Honden kunnen op elk moment aanvallen ontwikkelen, maar tussen de 6 maanden en 6 jaar is de meest voorkomende periode.  Hernia:  Dit is een andere \\\"veelvoorkomende\\\" afwijking, vooral als Cavaliers ouder worden.  In de meeste gevallen zul je niet weten dat je Cavalier gevoelig is voor een hernia, tot je hem stijf ziet lopen of zijn hoofd met tegenzin naar beneden brengt naar zijn voerbak of waterbak.\",\n  \"target_text\": \"Overweeg de voordelen als je kiest voor een raszuivere Cavalier. Stel vast wat de schaduwzijden zijn van het kopen van een rashond. Houd algemene gezondheidsproblemen van de Cavalier in gedachten.\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>Hieronder volgen artikelen met bijbehorende samenvattingen.\n</code></pre></li> <li>Base prompt template:   <pre><code>Artikel: {text}\nSamenvatting: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Artikel: {text}\n\nSchrijf een samenvatting van het bovenstaande artikel.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset wiki-lingua-nl\n</code></pre>"},{"location":"datasets/english/","title":"\ud83c\uddec\ud83c\udde7 English","text":"<p>This is an overview of all the datasets used in the English part of EuroEval. The datasets are grouped by their task - see the task overview for more information about what these constitute.</p>"},{"location":"datasets/english/#sentiment-classification","title":"Sentiment Classification","text":""},{"location":"datasets/english/#sst-5","title":"SST-5","text":"<p>This dataset was published in this paper and is based on movie reviews from rottentomatoes.com, labelled by crowdsourced workers on Amazon Mechanical Turk.</p> <p>The original full dataset consists of 8,540 / 1,100 / 2,210 samples for the training, validation and test splits, respectively. We use 1,024 / 256 / 2,048 samples for our training, validation and test splits, respectively. All the new splits are subsets of the original splits.</p> <p>The original dataset consists of 5 labels instead of our usual 3, but we map them to <code>positive</code>, <code>neutral</code> and <code>negative</code> as follows:</p> <ul> <li><code>very negative</code> \u27a1\ufe0f <code>negative</code></li> <li><code>negative</code> \u27a1\ufe0f <code>negative</code></li> <li><code>neutral</code> \u27a1\ufe0f <code>neutral</code></li> <li><code>positive</code> \u27a1\ufe0f <code>positive</code></li> <li><code>very positive</code> \u27a1\ufe0f <code>positive</code></li> </ul> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"the leads are natural and lovely , the pace is serene , the humor wry and sprightly .\",\n  \"label\": \"positive\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"labute ca n't avoid a fatal mistake in the modern era : he 's changed the male academic from a lower-class brit to an american , a choice that upsets the novel 's exquisite balance and shreds the fabric of the film .\",\n  \"label\": \"neutral\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"no cliche escapes the perfervid treatment of gang warfare called ces wild .\",\n  \"label\": \"negative\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>The following are texts and their sentiment, which can be 'positive', 'neutral' or 'negative'.\n</code></pre></li> <li>Base prompt template:   <pre><code>Text: {text}\nSentiment: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Text: {text}\n\nClassify the sentiment in the text. Answer with 'positive', 'neutral' or 'negative'.\n</code></pre></li> <li>Label mapping:<ul> <li><code>positive</code> \u27a1\ufe0f <code>positive</code></li> <li><code>neutral</code> \u27a1\ufe0f <code>neutral</code></li> <li><code>negative</code> \u27a1\ufe0f <code>negative</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset sst5\n</code></pre>"},{"location":"datasets/english/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"datasets/english/#conll-en","title":"CoNLL-en","text":"<p>This dataset was published in this paper and was part of the CoNNL-2003 shared task. The data comes from the Reuters Corpus and consists of news articles between August 1996 and August 1997, labelled with named entities.</p> <p>The original full dataset consists of 14,041 / 3,250 / 3,453 samples for the training, validation and test splits, respectively. We use 1,024 / 256 / 2,048 samples for our training, validation and test splits, respectively. All the new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  'tokens': array(['SK', 'Slavia', 'Praha', '3', '1', '2', '0', '6', '3', '5'], dtype=object),\n  'labels': array(['B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['Guy', 'Whittingham', 'stole', 'three', 'points', 'for', 'the', 'Yorkshire', 'side', 'with', 'a', 'goal', '10', 'minutes', 'from', 'time', '.'], dtype=object),\n  'labels': array(['B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['Dean', 'Palmer', 'hit', 'his', '30th', 'homer', 'for', 'the', 'Rangers', '.'], dtype=object),\n  'labels': array(['B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O'], dtype=object)\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>Below are sentences and JSON dictionaries with the named entities that occur in the given sentence.\n</code></pre></li> <li>Base prompt template:   <pre><code>Sentence: {text}\nNamed entities: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Sentence: {text}\n\nIdentify the named entities in the sentence. You should output this as a JSON dictionary with the keys being 'person', 'location', 'organization' and 'miscellaneous'. The values should be lists of the named entities of that type, exactly as they appear in the sentence.\n</code></pre></li> <li>Label mapping:<ul> <li><code>B-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>location</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>location</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>organization</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>organization</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>miscellaneous</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>miscellaneous</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset conll-en\n</code></pre>"},{"location":"datasets/english/#linguistic-acceptability","title":"Linguistic Acceptability","text":""},{"location":"datasets/english/#scala-en","title":"ScaLA-En","text":"<p>This dataset was published in this paper and was automatically created from the English Universal Dependencies treebank by assuming that the documents in the treebank are correct, and corrupting the samples to create grammatically incorrect samples. The corruptions were done by either removing a word from a sentence, or by swapping two neighbouring words in a sentence. To ensure that this does indeed break the grammaticality of the sentence, a set of rules were used on the part-of-speech tags of the words in the sentence.</p> <p>The original full dataset consists of 1,024 / 256 / 2,048 samples for training, validation and testing, respectively (so 3,328 samples used in total). These splits are used as-is in the framework.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"And so we have to labour and to work, and to work hard, to give reality to our dreams.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"This couch is also quite big, it fits three people quite comfortably, and if I have or friends staying over, it opens up into a full double bed.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"While studies the psychology of art have focused on individual works and distinctions between representative / non-representative topics, no work has been completed on the aesthetic appreciation of collections or of devotional themes.\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>The following are sentences and whether they are grammatically correct.\n</code></pre></li> <li>Base prompt template:   <pre><code>Sentence: {text}\nGrammatically correct: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Sentence: {text}\n\nDetermine whether the sentence is grammatically correct or not. Reply with 'yes' if the sentence is correct and 'no' if it is not.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>yes</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>no</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scala-en\n</code></pre>"},{"location":"datasets/english/#reading-comprehension","title":"Reading Comprehension","text":""},{"location":"datasets/english/#squad","title":"SQuAD","text":"<p>This dataset was published in this paper, which is based on English Wikipedia articles and the questions and answers are written by crowdworkers.</p> <p>The original full dataset consists of 130,000 / 11,900 samples for training and validation, respectively. We use 1,024 / 256 / 2,048 samples for training, validation and testing, respectively (so 3,328 samples used in total). The new training split is a subset of the original training split, and the new validation and test splits are disjoint subsets of the original validation split.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  'context': 'The Federation of International Gymnastics (FIG) was founded in Liege in 1881. By the end of the nineteenth century, men\\'s gymnastics competition was popular enough to be included in the first \"modern\" Olympic Games in 1896. From then on until the early 1950s, both national and international competitions involved a changing variety of exercises gathered under the rubric, gymnastics, that would seem strange to today\\'s audiences and that included for example, synchronized team floor calisthenics, rope climbing, high jumping, running, and horizontal ladder. During the 1920s, women organized and participated in gymnastics events. The first women\\'s Olympic competition was primitive, only involving synchronized calisthenics and track and field. These games were held in 1928, in Amsterdam.',\n  'question': 'When was gymnastics included in the Olympics?',\n  'answers': {\n    'answer_start': array([219], dtype=int32),\n    'text': array(['1896'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': \"London's buildings are too diverse to be characterised by any particular architectural style, partly because of their varying ages. Many grand houses and public buildings, such as the National Gallery, are constructed from Portland stone. Some areas of the city, particularly those just west of the centre, are characterised by white stucco or whitewashed buildings. Few structures in central London pre-date the Great Fire of 1666, these being a few trace Roman remains, the Tower of London and a few scattered Tudor survivors in the City. Further out is, for example, the Tudor period Hampton Court Palace, England's oldest surviving Tudor palace, built by Cardinal Thomas Wolsey c.1515.\",\n  'question': \"The area west of London's city is characterized by what type of building?\",\n  'answers': {\n    'answer_start': array([328], dtype=int32),\n    'text': array(['white stucco or whitewashed'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': 'Along with the rest of South West England, Plymouth has a temperate oceanic climate (K\u00f6ppen Cfb) which is generally wetter and milder than the rest of England. This means a wide range of exotic plants can be grown. The annual mean temperature is approximately 11 \u00b0C (52 \u00b0F). Due to the modifying effect of the sea the seasonal range is less than in most other parts of the UK. As a result of this summer highs are lower than its southerly latitude should warrant, but as a contrast the coldest month of February has mean minimum temperatures as mild as between 3 and 4 \u00b0C (37 and 39 \u00b0F). Snow is rare, not usually equating to more than a few flakes, but there have been exclusions, namely the European winter storms of 2009-10 which, in early January, covered Plymouth in at least 1 inch (2.5 cm) of snow; more on higher ground. Another period of notable snow occurred from 17\u201319 December 2010 when up to 8 inches (20 cm) of snow fell through the period \u2013 though only 2 inches (5.1 cm) would lie at any one time due to melt. Over the 1961\u20131990 period, annual snowfall accumulation averaged less than 7 cm (3 in) per year. July and August are the warmest months with mean daily maxima over 19 \u00b0C (66 \u00b0F).',\n  'question': 'What month in Plymouth has the lowest temperatures?',\n  'answers': {\n    'answer_start': array([503], dtype=int32),\n    'text': array(['February'], dtype=object)\n  }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>The following are texts with accompanying questions and answers.\n</code></pre></li> <li>Base prompt template:   <pre><code>Text: {text}\nQuestion: {question}\nAnswer in max 3 words:\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Text: {text}\n\nAnswer the following question about the above text in at most 3 words.\n\nQuestion: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset squad\n</code></pre>"},{"location":"datasets/english/#unofficial-belebele-en","title":"Unofficial: BeleBele-en","text":"<p>This dataset was published in this paper and features reading comprehension questions across 122 languages. The dataset was created by professional translators who translated 900 multiple-choice questions from English into other languages, with answers carefully validated by native speakers.</p> <p>The original dataset consists of 900 samples, and we use 256 / 64 / 580 samples for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": 'Text: \"\"\"We will endeavour to cut carbon dioxide emissions per unit of GDP by a notable margin by 2020 from the 2005 level,\"\" Hu said. He did not set a figure for the cuts, saying they will be made based on China\\'s economic output. Hu encouraged developing countries \"\"to avoid the old path of polluting first and cleaning up later.\"\" He added that \"\"they should not, however, be asked to take on obligations that go beyond their development stage, responsibility and capabilities.\"\"\"\\nQuestion: What did Hu suggest that developing countries do?\\nChoices:\\na. Take on obligations that push their development stage\\nb. Focus on economic output\\nc. Go beyond their current responsibilities\\nd. Avoiding old paths of pollution',\n  \"label\": \"d\"\n}\n</code></pre> <pre><code>{\n  \"text\": 'Text: \"All of the cave entrances, which were named \"\"The Seven Sisters\"\", are at least 100 to 250 meters (328 to 820 feet) in diameter. Infrared images show that the temperature variations from night and day show that they are likely caves. \"\"They are cooler than the surrounding surface in the day and warmer at night. Their thermal behavior is not as steady as large caves on Earth that often maintain a fairly constant temperature, but it is consistent with these being deep holes in the ground,\"\" said Glen Cushing of the United States Geological Survey (USGS) Astrogeology Team and of Northern Arizona University located in Flagstaff, Arizona.\"\\nQuestion: What information suggests that The Seven Sisters are caves?\\nChoices:\\na. Temperature variations\\nb. The diameter of the cave entrances\\nc. Geological surveys\\nd. Pictures of caves on Earth',\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": 'Text: The proposed amendment already passed both houses in 2011. A change was made this legislative session when the second sentence was deleted first by the House of Representatives and then was passed in a similar form by the Senate Monday. The failure of the second sentence, which proposes to ban same-sex civil unions, could possibly open the door for civil unions in the future. Following the process, HJR-3 will be reviewed again by the next elected legislature in either 2015 or 2016 to remain in process.\\nQuestion: According to the passage, when was the second sentence deleted?\\nChoices:\\na. During the legislative session\\nb. In 2011\\nc. On Monday\\nd. In 2015',\n  \"label\": \"a\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>The following are texts with accompanying questions and answers.\n</code></pre></li> <li>Base prompt template:   <pre><code>Text: {text}\nQuestion: {question}\nAnswer in max 3 words:\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Text: {text}\n\nAnswer the following question about the above text in at most 3 words.\n\nQuestion: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset belebele-en\n</code></pre>"},{"location":"datasets/english/#knowledge","title":"Knowledge","text":""},{"location":"datasets/english/#life-in-the-uk","title":"Life in the UK","text":"<p>This dataset was published here was scraped from lifeintheuktestweb.co.uk and contains multiple choice questions about UK history, culture, and citizenship requirements. The website was created to help people pass the Life in the UK Test for UK citizenship.</p> <p>The original dataset consists of 1,450 samples. After processing (removing questions with overly short or long texts, repetitive content, and true/false questions), we have 1,206 samples remaining. From these, we use 438 / 256 / 512 samples for our training, validation and test splits, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"What is the capital of the United Kingdom?\\nChoices:\\na. London\\nb. Manchester\\nc. Birmingham\\nd. Edinburgh\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"What TWO houses were confronted during the Wars of the Roses?\\nChoices:\\na. The House of Lancaster\\nb. The House of Leicester\\nc. The House of Canterbury\\nd. The House of York\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"What is the name of the War Memorial located in Whitehall?\\nChoices:\\na. Dumfries\\nb. Cenotaph\\nc. Royal Crescent\\nd. The White Tower\",\n  \"label\": \"b\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>The following are multiple choice questions (with answers).\n</code></pre></li> <li>Base prompt template:   <pre><code>Question: {text}\nOptions:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nAnswer: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Question: {text}\nOptions:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nAnswer the above question by replying with 'a', 'b', 'c' or 'd', and nothing else.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset life-in-the-uk\n</code></pre>"},{"location":"datasets/english/#unofficial-mmlu","title":"Unofficial: MMLU","text":"<p>This dataset was published in this paper and features questions within 57 different topics, such as elementary mathematics, US history and law.</p> <p>The original full dataset consists of 269 / 1,410 / 13,200 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). These splits are new and there can thus be some overlap between the original validation and test sets and our validation and test sets.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Use the following key to translate the given formula of PL to natural, English sentences. A: Marina reads a Percy Jackson book. B: Izzy plays Minecraft. C: Emily stops working. D: Russell makes dinner. E: Ashleigh stops by. ~(A \u2283 B) \u2022 (B \u2283 ~E)\\nChoices:\\na. It's not the case that Marina's reading a Percy Jackson book entails that Izzy plays Minecraft, but Izzy's playing Minecraft does entail that Ashleigh doesn't stop by.\\nb. If Marina doesn't read a Percy Jackson book, then Izzy plays Minecraft, which entails that Ashleigh doesn't stop by.\\nc. Marina's reading a Percy Jackson book does not entail that Izzy plays Minecraft, but Izzy plays Minecraft provided that Ashleigh doesn't stop by.\\nd. It's not true that Marina reads a Percy Jackson book only when Izzy plays Minecraft, but Izzy plays Minecraft only when Ashleigh stops by.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"As of 2017, the share of GDP spent on the military by the United States is about\\nChoices:\\na. 1%\\nb. 3%\\nc. 6%\\nd. 10%\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Question 13. A buyer sent a signed letter to a seller that stated: \\\"Ship 100 boxes of nails at $3 per box, the price quoted in your circular.\\\" The seller mailed the buyer a signed form acknowledgment that agreed to the buyer's terms and stated on the reverse side: \\\"Disputes regarding quality shall be arbitrated.\\\" The buyer did not reply to the seller's acknowledgment, and the seller shipped the nails. When the buyer received the nails, it found their quality to be unsatisfactory and sued the seller for breach of warranty. The seller has asked an attorney whether the parties' contract requires arbitration of the buyer's claim. What is the best advice the attorney can provide?\\nChoices:\\na. A contract was formed pursuant to conduct when the buyer received the nails, and a court would exclude the arbitration provision from the contract.\\nb. A contract was formed when the seller mailed its acknowledgment, and the arbitration term became part of the contract. arbitration term became part of the contract.\\nc. A contract was formed when the seller mailed its acknowledgment, and the court must decide whether the arbitration term should be excluded as a material alteration of the contract.\\nd. No contract exists, because the arbitration term in the seller's acknowledgment created a counteroffer that the buyer never accepted.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>The following are multiple choice questions (with answers).\n</code></pre></li> <li>Base prompt template:   <pre><code>Question: {text}\nOptions:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nAnswer: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Question: {text}\nOptions:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nAnswer the above question by replying with 'a', 'b', 'c' or 'd', and nothing else.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset mmlu\n</code></pre>"},{"location":"datasets/english/#unofficial-arc","title":"Unofficial: ARC","text":"<p>This dataset was published in this paper and features US grade-school science questions.</p> <p>The original full dataset consists of 1,110 / 297 / 1,170 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively (so 2,304 samples used in total). All new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Several horses grazed in a fenced area across from a home. On rainy days, soil would wash down a slope and run toward the home. After the horses were moved a few years later, the soil no longer washed down when it rained. What could account for this change?\\nChoices:\\na. The grass grew and kept the soil intact.\\nb. The fence kept the soil contained.\\nc. The soil was completely gone.\\nd. The amount of rain decreased.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"How do moose use a learned behavior to protect themselves?\\nChoices:\\na. They have hollow hair to keep warm in the winter.\\nb. They roll in a pool of muddy water to avoid fly bites.\\nc. They have keen hearing to sense danger in the forest.\\nd. They use their wide hooves to prevent sinking in deep snow.\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"A plant that grows red flowers was crossed with the same kind of plant that grows white flowers. Their offspring grew pink flowers. Which best explains why the offspring grew pink flowers?\\nChoices:\\na. The offspring experienced a genetic mutation.\\nb. The offspring resulted from asexual reproduction.\\nc. The genes for flower color exhibited incomplete dominance.\\nd. A gene for pink-colored flowers was recessive in one of the parents.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>The following are multiple choice questions (with answers).\n</code></pre></li> <li>Base prompt template:   <pre><code>Question: {text}\nOptions:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nAnswer: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Question: {text}\nOptions:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nAnswer the above question by replying with 'a', 'b', 'c' or 'd', and nothing else.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset arc\n</code></pre>"},{"location":"datasets/english/#common-sense-reasoning","title":"Common-sense Reasoning","text":""},{"location":"datasets/english/#hellaswag","title":"HellaSwag","text":"<p>This dataset was published in this paper and is based on both video descriptions from ActivityNet as well as how-to articles from WikiHow.</p> <p>The original full dataset consists of 9,310 samples. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"[header] How to solo travel to chile [title] Decide how you will get to chile. [step] Start by figuring out how you will get to chile. If you live in north america, you may decide to fly into a major city in the country, such as santiago, and then take public transit to get around or fly within chile.\\nChoices:\\na. If you live in south america, it may be possible to take a bus or a train into chile, depending on your budget and your timeframe for the trip. [substeps] There is no special visa required for you to travel into chile and no fee to cross the border into chile.\\nb. If you live in australia, you will need to negotiate a road trip, such as a train or bus, to get around chile. [substeps] Plan out the route in advance of arrival so that you can do the same to chile in the future.\\nc. If you live in a rural area or you do not plan to travel for a long time, you may opt to take a bus. Using a bus or subway to get around chile is a good route to travel.\\nd. If you live in a smaller area, or if you live near a large tourist attraction, you may decide to fly in the opposite direction. [substeps] Skiing, mountain climbing, and bicycle riding are examples of solo travel.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"The video begins with a title sequence. a young man\\nChoices:\\na. prepares to black out.\\nb. is shown in a gym performing tricks with a jump rope as music plays in the background.\\nc. is seen talking continuously about slamming the mouth of a chimpanzee into the camera.\\nd. is standing outside with a basketball in his hand, alternating between shots of dribbling for the ball.\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"A herb garden appears with a woman standing next to it in a large garden next to a wheelbarrow filled with mulch. the woman\\nChoices:\\na. moves the mulch across the ground in the wheelbarrow, falling backwards on attempts.\\nb. takes some of the mulch away and starts bagging it in the wheelbarrow.\\nc. begins to talk to the camera while gesturing to the flowerbed and the mulch, before eventually picking up a handful of the mulch.\\nd. then begins to mulch close to the wheelbarrow with mulching tool in her hand and while waving her arms in the air.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>The following are multiple choice questions (with answers).\n</code></pre></li> <li>Base prompt template:   <pre><code>Question: {text}\nOptions:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nAnswer: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Question: {text}\nOptions:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nAnswer the above question by replying with 'a', 'b', 'c' or 'd', and nothing else.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset hellaswag\n</code></pre>"},{"location":"datasets/english/#summarization","title":"Summarization","text":""},{"location":"datasets/english/#cnndailymail","title":"CNN/DailyMail","text":"<p>This dataset was published in this paper and is based on news articles from CNN and DailyMail, with the summaries derived from bullet points written by the authors of the articles.</p> <p>The original full dataset consists of 287,113 / 13,368 / 11,490 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). All new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Reality TV star and conservative firebrand Sarah Palin said today she's 'interested' in running for president in 2016 but stopped short of saying she'd actually seek higher office. 'Yeah, I mean, of course, when you have a servant\u2019s heart, when you know that there is opportunity to do all you can to put yourself forward in the name of offering service, anybody would be interested,' Palin told ABC News reporter Neal Karlinsky. Later stating, 'America has had enough of seeing that...sign on the Oval Office door saying, \\\"No Girls Allowed.\\\" ' 'It doesn't necessarily have to be me though,' she said. Scroll down for video . Conservative firebrand Sarah Palin said today she's 'interested' in running for president in 2016 but stopped short of saying she'd actually seek higher office . GIRL POWER: 'America has had enough of seeing that...sign on the Oval Office door saying, \\\"No Girls Allowed,\\\" ' Palin said . NOM NOM NOM: Palin made the comments while serving wild boar chili to the Salvation Army in Las Vegas, Nevada, on Friday. She was hosting an episode of Sportsman Channel program Hunt.Fish.Feed . ABC News caught up with Palin while she was serving wild boar chili to the homeless at a Las Vegas, Nevada, Salvation Army for an episode of Sportsman Channel program Hunt.Fish.Feed. She's also in the midst of promoting her hunting show, Amazing Alaska, about to begin its second season. Palin said the GOP needs to nominate a candidate 'who can take on Hillary' and 'show the nation what it is going to take to get the country back on the right track.' 'Because we can't afford status quo,' the former Alaska governor said in a clip of the interview released by ABC this afternoon. 'Status quo lately has been Latin for, \\\"We're getting screwed,' and status quo has got to go.' The Republican nominee out to be someone can 'turn things around, someone who will, in some respects, I don\u2019t know, maybe be considered a bit avant garde, to the establishment anyway, because this next person has got to realize this is war, this is war for our hunters\u2019 future,' she said at another point in the interview, according to ABC. Asked about former Florida Gov. Jeb Bush's candidacy and 2012 Republican presidential nominee Mitt Romney, Palin snarked, 'I can\u2019t wait for new energy.' Moments later asserting that the GOP primary 'had better be a competition and not a coronation.' Palin, the 2008 vice presidential nominee, said she doesn't 'have to be' the Republican candidate for president but she's 'happy to drive that competition, because competition will make everyone better and produce more and be more candid regarding their solutions they will offer this country. 'I am very interested in that competitive process and, again, not necessarily me.' Former Alaska Palin is pictured here on Thursday at an event to promote her television show, Amazing America with Sarah Palin, at the Shooting, Hunting and Outdoor Trade Show in Las Vegas . The hard-charging, Tea Party icon appears to have a change of heart in the last week about the Oval Office needing a female touch. 'I don't give a flying flip about what gender the person will be,' Palin told Inside Edition after host Deborah Norville asker her about the importance of electing a female president. 'I want the absolute best because America deserves the best, in terms of leadership, getting this country on the right track,' she continued. She ultimately concluded 'it would be nice' to have a woman president, though, 'and it will be nice to see women jump into the ring.' Voicing her support for female candidates in December, Palin told\u00a0Extra TV, 'I would love to see a woman on both sides of the aisle shooting for that top spot.'\",\n  \"target_text\": \"'When you know that there is opportunity to do all you can to put yourself forward in the name of offering service, anybody would be interested'\\nPalin added: 'It doesn't necessarily have to be me though'\\nThe conservative firebrand appears to have a change of heart about the Oval Office needing a female touch .\\nLast week she said: 'I don't give a flying flip about what gender the person will be'\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"By . Amanda Williams . The dictionary makers have taken to Twitter to find new words for the next edition of the lexicon - asking users to choose which words should make the final edition . The latest edition of the Collins English Dictionary could include Twitter slang words such as 'adorkable' and 'fatberg'. The dictionary makers have taken to Twitter to find new words for the next edition of the lexicon - asking users to choose which words should make the final edition. The list of suggested words includes fracktivist - someone who protests against fracking - and felfie, a term used to describe a farmer who takes a selfie, or photograph of themselves. The 12th edition of the dictionary will be the first to contain a word that has been voted for by Twitter users - who have until midnight on May 28 to vote for the new word. Once selected, it will be included in the next edition of the dictionary, which is released in October. The dictionary publisher says that the rise of social media and the hashtag has seen new words and ideas - that they scout for every year - become mainstream much quicker than in the past. Andrew Freeman, associate publisher at Collins, said: 'Twitter offers us an immediate snapshot of how much a word is used. 'The tried and tested approach to compiling dictionaries has to adapt to embrace the ways in which language is developing through use on social media, and this is a fun way to get Twitter users involved in defining the English language.' Collins has been publishing the dictionary since 1819 and is the largest single volume dictionary in print, with the words it contains sourced from the Collins Corpus, which contains more than 4.5 billion words, as well as the open source site collinsdictionary.com, where users can submit words for consideration. The latest edition of the Collins English Dictionary could include Twitter slang words such as 'adorkable' The word felfie, a term used to describe a farmer who takes a selfie, or photograph of themselves could also be included . Nomakeupselfie - a selfie of a woman without make-up, posted online to raise awareness for a charity - is also in the running to be used in the dictionary . Lucy Mangan, a blogger for collinsdictionary.com and a contributor to the Collins English Dictionary, said: 'Twitter is the perfect place to find out what people are really saying and how they\u2019re saying it. 'It\u2019s a space in which you\u2019re freer than almost anywhere else to combine old words, resurrect others or invent totally new ones whenever the need arises.' According to language experts, the list, which also contains the word adorkable, referring to someone who is dorky in an adorable way, is a sign of the way language is changing in the 21st century. Ian Brookes, lexicographer and consultant editor to the Collins English Dictionary, said: 'Language has always had to develop in response to changes in society and technology. In the 20th century the development of the motor car, air travel, television, and the personal computer changed the things that people did and so brought many new words into the language. 'In the 21st century, the growth of social media has had a comparable effect. Twitter users can vote for their choice by visiting twictionary.collinsdictionary.com . Adorkable - dorky in an adorable way . Fatberg - a large mass of solid waste, grease etc, clogging a sewage system . Felfie - a farmer selfie . Gaybourhood - a gay-friendly neighbourhood, e.g. Castro in San Francisco . Nomakeupselfie - a selfie of a woman without make-up, posted online to raise awareness for a charity . Vaguebooking - posting a deliberately vague status updates on social media to prompt a response . Duckface - the traditional pouting facial expression in selfies . Fracktivist - an activist who protests against fracking . Euromaiden - the original pro-Europe protests in Ukraine, named for Maidan Square in Kiev .\",\n  \"target_text\": \"Dictionary makers have taken to Twitter to find new words for next edition .\\nThe suggested words include fracktivist - an anti-fracking protester .\\nFelfie - a term used to describe a farmer who takes a selfie - also included .\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"There were three of them, one of them probably a child, and at least one met a gruesome end at the hands of a terrifying predator. About 67 million years later, a Wyoming rancher led scientists to their remains. Now experts are digging out one of the most complete skeletons yet of a Triceratops, the three-horned, plant-eating dinosaur that was one of the last of the giant reptiles. \\\"There's only three other skeletons that will match the completeness of one of the specimens we're excavating right now,\\\" said paleontologist Peter Larson, president of the Black Hills Institute of Geological Research. Most of the remains found before now have included fewer than half of the prehistoric creatures' bones, Larson said Monday. The most complete to date, now on display at the Houston Museum of Natural Science in Texas, has about 76% of its skeleton. \\\"The largest, more mature individual appears to be the most complete,\\\" Larson said. \\\"One is just a bit smaller, and there's another one that by live weight is probably only half the size.\\\" Will mammoths be brought back to life? Liquid blood fuels cloning hopes . The dig is going on near Newcastle, Wyoming, more than 200 miles north of Cheyenne. \\\"The fact that there are three of them together is really cool,\\\" Larson said. The trio could be male and female and their young, or they could be two females looking after a juvenile dinosaur, he said. And before now, there was no indication that the Triceratops moved in groups. The Black Hills Institute is working with the Naturalis Biodiversity Center, from the Netherlands, on the dig. Larson called the discovery of a young Triceratops a \\\"very significant\\\" find as well, since it will give scientists an insight into how the great lizards grew up. Newly discovered dinosaur fossil is a primitive bird . Triceratops lived in the twilight of the Cretaceous Period, about a half a million years before the dinosaurs' extinction. Much of what is now the Great Plains and southern Canada was once part of a vast inland sea, and the region is rich in fossils. \\\"Like most of the specimens that were found, it was brought to our attention by a rancher,\\\"  Larson said. The rancher sent photos to the Black Hills Institute, located in neighboring South Dakota, in late 2012. Excavation began in May and is expected to take about a month. So far, the bones that have turned up point to a violent end, probably at the hands of the feared Tyrannosaurus rex. On the largest of the three specimens, at least two of the major limb bones were \\\"bitten through,\\\" Larson said. \\\"If you can imagine, this is a bone that is nearly four feet long,\\\" he said. But a T.rex \\\"would kind of chop the carcass up with their giant, shearing jaws,\\\" ripping through flesh and bone alike. \\\"I think we also have a feeding site for Tyrannosaurus rex, which is very exciting,\\\" he said. \\\"This is potentially a site where we can learn the behavior of two different species.\\\" More science news on CNN's Light Years blog .\",\n  \"target_text\": \"A rancher led scientists to the remains of three Triceratops .\\nOne of the three may be the most complete skeleton yet found .\\nA young dinosaur is among the trio .\\nAt least one may have been killed by a Tyrannosaurus rex .\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>The following are articles with accompanying summaries.\n</code></pre></li> <li>Base prompt template:   <pre><code>News article: {text}\nSummary: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>News article: {text}\n\nWrite a summary of the above article.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset cnn-dailymail\n</code></pre>"},{"location":"datasets/faroese/","title":"\ud83c\uddeb\ud83c\uddf4 Faroese","text":"<p>This is an overview of all the datasets used in the Faroese part of EuroEval. The datasets are grouped by their task - see the task overview for more information about what these constitute.</p>"},{"location":"datasets/faroese/#sentiment-classification","title":"Sentiment Classification","text":""},{"location":"datasets/faroese/#fosent","title":"FoSent","text":"<p>This dataset was published in this paper and is based on 170 news articles from the Faroese news sites Portalurin and Dimmal\u00e6tting. The sentiment labels were manually annotated by two native speakers.</p> <p>The original full dataset consists of 245 samples, which consisted of both a news article, a chosen sentence from the article, and the sentiment label. We use both the news article and the chosen sentence as two separate samples, to increase the size of the dataset (keeping them within the same dataset split). In total, we use a 72 / 40 / 279 split for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Eg koyri teg, t\u00fa koyrir meg Hetta er \u00e1rst\u00ed\u00f0in, har vit vanliga fara \u00ed j\u00f3labor\u00f0hald at hugna okkum saman vi\u00f0 vinum og starvsfel\u00f8gum. Og h\u00f3ast vit kanska ikki hittast og koma saman \u00e1 j\u00fast sama h\u00e1tt, sum \u00e1\u00f0renn korona rakti samfelagi\u00f0, so eru \u00f3iva\u00f0 n\u00f3gv sum kortini gle\u00f0a seg til hesa t\u00ed\u00f0ina vi\u00f0 hugna og veitslulag Eins og undanfarin \u00e1r, fara R\u00e1\u00f0i\u00f0 fyri Fer\u00f0slutrygd (\u00ed samstarvi vi\u00f0 Betri Trygging og Trygd) at fremja \u00e1tak fyri at ste\u00f0ga r\u00faskoyring. Hetta ver\u00f0ur gj\u00f8rt vi\u00f0 filminum \u00a0\u201dEg koyri teg, t\u00fa koyrir meg\u201d, i\u00f0 er \u00farsliti\u00f0 av st\u00f3ru hugskotskappingini hj\u00e1 R\u00e1\u00f0num fyri Fer\u00f0slutrygd s\u00ed\u00f0sta vetur. Filmsl\u00fdsingin ver\u00f0ur \u00ed\u00a0hesum d\u00f8gum v\u00edst \u00ed sj\u00f3nvarpi, biografi og \u00e1 sosialum mi\u00f0lum. Brynhild Nols\u00f8e \u00ed L\u00e1gab\u00f8 \u00far V\u00e1gi vann kappingina, og luttekur saman vi\u00f0 vinf\u00f3lki \u00ed l\u00fdsingini. Brynhild kennir sj\u00e1lv til avbj\u00f3\u00f0ingarnar av at vera partur av n\u00e1ttarl\u00edvinum \u00ed\u00a0a\u00f0rari bygd, enn teirri t\u00fa b\u00fdrt \u00ed. T\u00ed bygdi hennara hugskot \u00e1 egnar royndir. \u00cd vinarb\u00f3lkinum hj\u00e1 Brynhild hava tey gj\u00f8rt eina avtalu, i\u00f0 byggir \u00e1 tankan: \u201dEg koyri teg, t\u00fa koyrir meg.\u201d Hetta merkir, at tey skiftast um at koyra: - Avtalan er tann, at um eitt vinf\u00f3lk er fari\u00f0 \u00ed b\u00fdin og eg liggi heima, so ringja tey til m\u00edn, og eg fari upp at koyra tey. Um eg eri farin \u00ed b\u00fdin og okkurt vinf\u00f3lk liggur heima,\u00a0so koma tey eisini upp at koyra meg. Ta\u00f0 er l\u00edkamiki\u00f0 um ta\u00f0 er morgun, dagur ella n\u00e1tt, greiddi Brynhild fr\u00e1 \u00ed l\u00fdsingarfilminum, i\u00f0 er komin burtur \u00far hugskotinum hj\u00e1\u00a0Brynhild. Vit valdu at gera eina hugskotskapping, har ung f\u00f3lk sluppu at seta dagsskr\u00e1nna, og \u00farsliti\u00f0 gj\u00f8rdist hesin filmurin, i\u00f0 byggir \u00e1 tey hugskot, i\u00f0 tey ungu sj\u00e1lvi h\u00f8vdu, sigur Lovisa Petersen Glerfoss, stj\u00f3ri \u00ed R\u00e1\u00f0num fyri Fer\u00f0slutrygd. Eftir at vinnarin var\u00f0 funnin, hevur Brynhild arbeitt saman vi\u00f0 eini l\u00fdsingarstovu vi\u00f0 at menna hugskoti\u00f0 til eina lidna l\u00fdsing. \u00cd l\u00fdsingini s\u00edggja vit Brynhild og hennara\u00a0vinf\u00f3lk \u00ed b\u00fdnum og \u00e1 veg til h\u00fas. \u00cd samr\u00e1\u00f0 vi\u00f0 Brynhild er l\u00fdsingin blivin jalig og uppbyggjandi, heldur enn ford\u00f8mandi og neilig. Hugbur\u00f0urin til r\u00faskoyring er broyttur munandi seinastu n\u00f3gvu \u00e1rini, og heili 98% av f\u00f8royingum siga at r\u00faskoyring ver\u00f0ur ikki g\u00f3\u00f0tikin. Men kortini ver\u00f0a bilf\u00f8rarar\u00a0javnan tiknir vi\u00f0 promillu \u00ed bl\u00f3\u00f0inum. Harafturat er r\u00faskoyring ors\u00f8k til fj\u00f3r\u00f0u hv\u00f8rja dey\u00f0svanlukku \u00ed fer\u00f0sluni, v\u00edsa t\u00f8l \u00far nor\u00f0urlondum. T\u00ed er ta\u00f0 eisini \u00ed 2021\u00a0t\u00fddningarmiki\u00f0 at tosa um at ste\u00f0ga r\u00faskoyring! \u00c1taki\u00f0 heldur fram hetta til n\u00fdggj\u00e1rs og l\u00f8greglan ger r\u00faskanningar, me\u00f0an \u00e1taki\u00f0 er. Eisini fer l\u00f8greglan at lata bilf\u00f8rarum, sum hava s\u00edni vi\u00f0urskifti \u00ed ordan, sn\u00f8ggar lyklaringar vi\u00f0 bo\u00f0skapinum \\\"Eg koyri teg, t\u00fa koyrir meg\\\". \",\n  \"label\": \"positive\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Vestmanna sk\u00fali hevur hesar lei\u00f0reglur \u00ed sambandi vi\u00f0 sj\u00fakar n\u00e6mingar: Ta\u00f0 er \u00f3gvuliga umr\u00e1\u00f0andi at n\u00e6mingar, sum ikki eru koppsettir, og hava veri\u00f0 \u00ed samband vi\u00f0 f\u00f3lk, sum eru testa\u00f0 positiv fyri koronu, halda tilm\u00e6lini. \",\n  \"label\": \"neutral\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Landsverk arbei\u00f0ur \u00ed l\u00f8tuni vi\u00f0 at f\u00e1a trailaran, sum er fult lasta\u00f0ur, upp aftur, og arbei\u00f0i\u00f0 fer v\u00e6ntandi at taka nakrar t\u00edmar, t\u00ed st\u00f3rar maskinur skulu til, og t\u00e6r mugu koyra um Ei\u00f0iskar\u00f0 fyri at koma til hj\u00e1lpar. \",\n  \"label\": \"negative\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Her eru nakrir tekstir flokka\u00f0ir eftir lyndi, sum kann vera 'positivt', 'neutralt' ella 'negativt'.\n</code></pre></li> <li>Base prompt template:   <pre><code>Text: {text}\nLyndi: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Tekstur: {text}\n\nFlokka lyndi\u00f0 \u00ed tekstinum. Svara vi\u00f0 'positivt', 'neutralt' ella 'negativt'.\n</code></pre></li> <li>Label mapping:<ul> <li><code>positive</code> \u27a1\ufe0f <code>positivt</code></li> <li><code>neutral</code> \u27a1\ufe0f <code>neutralt</code></li> <li><code>negative</code> \u27a1\ufe0f <code>negativt</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset fosent\n</code></pre>"},{"location":"datasets/faroese/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"datasets/faroese/#fone","title":"FoNE","text":"<p>This dataset was published in this paper and is based on news articles from Sosialurin. The named entities were automatically tagged, but verified manually. They use a superset of the CoNNL-2003 dataset, with the following additional entity types: <code>Date</code>, <code>Money</code>, <code>Percent</code> and <code>Time</code>. We remove these additional entity types from our dataset and keep only the original CoNNL-2003 entity types (<code>PER</code>, <code>ORG</code>, <code>LOC</code>, <code>MISC</code>).</p> <p>The original full dataset consists of 6,286 samples, which we split into 1,024 / 256 / 2,048 samples for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  'tokens': array(['Millum', 'teirra', 'er', 'Tommy', 'Petersen', ',', 'sum', 'eitt', 'skifti', 'hev\u00f0i', 'ES', 'sum', 's\u00edtt', 'm\u00e1ls\u00f8ki', '\u00ed', 'Tinganesi', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'B-LOC', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['Fleiri', 'l\u00e6rarat\u00edmar', 'skulu', '\u00ed', '\u00e1r', 'br\u00fakast', '\u00e1', 'HF', '-', 'sk\u00falanum', '\u00ed', 'Klaksv\u00edk', ',', 'men', 'samb\u00e6rt', 'lei\u00f0aranum', '\u00e1', 'sk\u00falanum', 'hevur', 'ta\u00f0', 'bara', 'vi\u00f0', 's\u00e6r', ',', 'at', 'l\u00e6rarar', ',', 'sum', 'eru', 'b\u00fasitandi', '\u00ed', 'Klaksv\u00edk', ',', 'koma', 'at', 'fer\u00f0ast', 'minni', '\u00e1', 'Kambsdal', 'og', '\u00edsta\u00f0in', 'br\u00faka', 'meira', 'undirv\u00edsingart\u00ed\u00f0', '\u00ed', 'b\u00fdnum', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['Solei\u00f0is', ',', 'at', 'Starvsstovan', 'kann', 'fylgja', 'vi\u00f0', ',', 'at', 'ta\u00f0', 'ikki', 'er', 'n\u00fdliga', 'heiliv\u00e1gsvi\u00f0gj\u00f8rdur', 'fiskur', ',', 'sum', 'tikin', 'ver\u00f0ur', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>Her eru nakrir setningar og nakrar JSON or\u00f0ab\u00f8kur vi\u00f0 nevndar eindir, sum eru \u00ed setningunum.\n</code></pre></li> <li>Base prompt template:   <pre><code>Setningur: {text}\nNevndar eindir: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Setningur: {text}\n\nGreini\u00f0 nevndu einingarnar \u00ed setningunni. \u00de\u00fa \u00e6ttir a\u00f0 skila \u00feessu sem JSON or\u00f0ab\u00f3k me\u00f0 lyklunum 'pers\u00f3nur', 'sta\u00f0ur', 'felagsskapur' og 'ymiskt'. Gildin \u00e6ttu a\u00f0 vera listi yfir nevndu einingarnar af \u00feeirri ger\u00f0, n\u00e1kv\u00e6mlega eins og \u00fe\u00e6r koma fram \u00ed setningunni.\n</code></pre></li> <li>Label mapping:<ul> <li><code>B-PER</code> \u27a1\ufe0f <code>pers\u00f3nur</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>pers\u00f3nur</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>sta\u00f0ur</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>sta\u00f0ur</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>felagsskapur</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>felagsskapur</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>ymiskt</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>ymiskt</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset fone\n</code></pre>"},{"location":"datasets/faroese/#unofficial-wikiann-fo","title":"Unofficial: WikiANN-fo","text":"<p>This dataset was part of the WikiANN dataset (also known as PAN-X), published in this paper. It is based on Wikipedia articles, and the labels have been automatically annotated using knowledge base mining. There are no <code>MISC</code> entities in this dataset, so we only keep the <code>PER</code>, <code>LOC</code> and <code>ORG</code> entities.</p> <p>The original full dataset consists of an unknown amount of samples, which we split into 1,024 / 256 / 2,048 samples for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  'tokens': array([\"'\", \"''\", 'P\u00f3lland', \"''\", \"'\"], dtype=object),\n  'labels': array(['O', 'O', 'B-LOC', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['Skulu', '\u00farvalssvimjararnir', 'betra', '\u00farslit', 's\u00edni', ',', 'so', 'er', 'ney\u00f0ugt', 'hj\u00e1', 'teimum', 'at', 'fara', 'uttanlands', 'at', 'venja', '(', 'Danmark', ',', 'USA', ')', ';', 'hinvegin', 'minkar', 'hetta', 'um', 'kappingina', 'hj\u00e1', 'teimum', 'heimligu', 'svimjarunum', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['Nor\u00f0uramerika', '-', '16', '%'], dtype=object),\n  'labels': array(['B-LOC', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>Her eru nakrir setningar og nakrar JSON or\u00f0ab\u00f8kur vi\u00f0 nevndar eindir, sum eru \u00ed setningunum.\n</code></pre></li> <li>Base prompt template:   <pre><code>Setningur: {text}\nNevndar eindir: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Setningur: {text}\n\nGreini\u00f0 nevndu einingarnar \u00ed setningunni. \u00de\u00fa \u00e6ttir a\u00f0 skila \u00feessu sem JSON or\u00f0ab\u00f3k me\u00f0 lyklunum 'pers\u00f3nur', 'sta\u00f0ur', 'felagsskapur' og 'ymiskt'. Gildin \u00e6ttu a\u00f0 vera listi yfir nevndu einingarnar af \u00feeirri ger\u00f0, n\u00e1kv\u00e6mlega eins og \u00fe\u00e6r koma fram \u00ed setningunni.\n</code></pre></li> <li>Label mapping:<ul> <li><code>B-PER</code> \u27a1\ufe0f <code>pers\u00f3nur</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>pers\u00f3nur</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>sta\u00f0ur</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>sta\u00f0ur</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>felagsskapur</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>felagsskapur</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>ymiskt</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>ymiskt</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset wikiann-fo\n</code></pre>"},{"location":"datasets/faroese/#linguistic-acceptability","title":"Linguistic Acceptability","text":""},{"location":"datasets/faroese/#scala-fo","title":"ScaLA-fo","text":"<p>This dataset was published in this paper and was automatically created from the Faroese Universal Dependencies treebank by assuming that the documents in the treebank are correct, and corrupting the samples to create grammatically incorrect samples. The corruptions were done by either removing a word from a sentence, or by swapping two neighbouring words in a sentence. To ensure that this does indeed break the grammaticality of the sentence, a set of rules were used on the part-of-speech tags of the words in the sentence.</p> <p>The original dataset consists of 1,621 samples, from which we use 1,024 / 256 / 1,024 samples for training, validation and testing, respectively (so 3,328 samples used in total). These splits are used as-is in the framework.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Hann tala\u00f0i t\u00ed \u00ed samkomuh\u00fasinum vi\u00f0 J\u00f6darnar og vi\u00f0 teir, sum \u00f3tta\u00f0ust Gu\u00f0, og \u00e1 torginum hv\u00f6nn dag vi\u00f0 teir, sum hann har hitti vi\u00f0.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hann finnur fyrst br\u00f3\u00f0ur s\u00edn, S\u00edmun, og sigur vi\u00f0 hann: \\\"hava Vit funni\u00f0 Messias\\\" sum er ta\u00f0 sama sum Kristus; ta\u00f0 er: salva\u00f0ur.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hetta hendi tr\u00edggjar fer\u00f0ir, og alt fyri eitt var\u00f0 luturin tikin upp aftur himmals til.\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Hetta eru nakrir setningar og um teir eru m\u00e1ll\u00e6ruliga r\u00e6ttir.\n</code></pre></li> <li>Base prompt template:   <pre><code>Setningur: {text}\nM\u00e1ll\u00e6ruliga r\u00e6ttur: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Setningur: {text}\n\nGreini\u00f0 hvort setningurin er m\u00e1ll\u00e6ruliga r\u00e6ttur ella ikki. Svari\u00f0 skal vera 'ja' um setningurin er r\u00e6ttur og 'nei' um hann ikki er.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>ja</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>nei</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scala-fo\n</code></pre>"},{"location":"datasets/faroese/#reading-comprehension","title":"Reading Comprehension","text":""},{"location":"datasets/faroese/#foqa","title":"FoQA","text":"<p>This dataset was published in this paper and is based on the Faroese Wikipedia. The questions and answers were automatically generated using GPT-4-turbo, which were verified by a native speaker, and some of them were also corrected by the same native speaker.</p> <p>The original full dataset consists of 2,000 samples, and we split these into 848 / 128 / 1,024 samples for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  'context': 'Felagsskapur ST fyri undirv\u00edsing, v\u00edsindum og mentan (\u00e1 enskum: United Nations Educational, Scientific and Cultural Organization, stytt UNESCO) er ein serstovnur undir Sameindu Tj\u00f3\u00f0um, stovna\u00f0ur \u00ed 1946. Endam\u00e1li\u00f0 vi\u00f0 felagskapinum er at menna \u00fatb\u00fagving, gransking og mentan og at fremja samstarv millum tey 195 limalondini og teir 8 atlimirnar, i\u00f0 eru F\u00f8royar, Cura\u00e7ao, Aruba, Jomfr\u00faoyggjar, Caymanoyggjar, Makao, Ni\u00f0urlendsku Antillurnar og Tokelau. F\u00f8royar fingu atlimaskap \u00ed 2009 . Atlimaskapur gevur \u00f8ll tey somu r\u00e6ttindi sum limaskapur. Limalondini skipa seg vi\u00f0 hv\u00f8r s\u00edni UNESCO nevnd. Fyrsta f\u00f8royska UNESCO nevndin var\u00f0 skipa\u00f0 \u00ed mai 2012. \\n\\nUNESCO tekur s\u00e6r millum anna\u00f0 av at meta um, hv\u00f8rji pl\u00e1ss \u00ed heiminum skulu f\u00e1a status sum World Heritage Sites (heimsarvur). Limalond UNESCO samtyktu \u00ed 1972 millumtj\u00f3\u00f0as\u00e1ttm\u00e1lan um at verja heimsins mentanar- og n\u00e1tt\u00faruarv. Ors\u00f8kin er vandin fyri, at n\u00e1tt\u00faru\u00f8ki, fornfr\u00f8\u00f0ilig minnismerki og mentanarvir\u00f0i forfarast orsaka\u00f0 av fer\u00f0af\u00f3lkavinnu, d\u00e1lking, kr\u00edggi ella vanligari \u00f3r\u00f8kt.\\n\\nHygg eisini at \\n\\n Millumtj\u00f3\u00f0as\u00e1ttm\u00e1li UNESCO um vernd av heimsins mentanar- og n\u00e1tt\u00faruarvi.\\n\\nKeldur\\n\\nSl\u00f3\u00f0ir \u00fateftir \\n\\n UNESCO World Heritage Centre\\n\\nST\\nHeimsarvar',\n  'question': 'Hvat g\u00f3\u00f0kendu UNESCO-limalondini \u00ed 1972?',\n  'answers': {\n    'answer_start': array([806]),\n    'text': array(['millumtj\u00f3\u00f0as\u00e1ttm\u00e1lan um at verja heimsins mentanar- og n\u00e1tt\u00faruarv'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': 'Levi Niclasen, sum yrkjari betri kendur sum \u00d3\u00f0in \u00d3dn (f\u00f8ddur 1. mai 1943 \u00e1 Tv\u00f8royri, uppvaksin \u00ed Hvalba) er ein f\u00f8royskur rith\u00f8vundur, t\u00f3nleikari, l\u00e6rari og politikari. \\n\\nAftan \u00e1 barnask\u00falan arbeiddi hann \u00ed kolinum \u00ed Hvalba. \u00cd 1957 stovna\u00f0i hann saman vi\u00f0 br\u00f8\u00f0um s\u00ednum ein t\u00f3nleikab\u00f3lk, og br\u00e1tt blivu teir kendir sum Hvalbiarbr\u00f8\u00f0urnir. Teir g\u00f3vu \u00fat tv\u00e6r stak pl\u00e1tur \u00ed 1962. Hann var \u00ed Gr\u00f8nlandi 1960 og 1961 og arbeiddi \u00e1 landi \u00ed F\u00f8royingahavnini fyri Nordafar. \\nHann f\u00f3r s\u00ed\u00f0an \u00e1 l\u00e6rarask\u00fala \u00ed Havn og t\u00f3k pr\u00f3gv fr\u00e1 F\u00f8roya L\u00e6rarask\u00fala \u00ed 1967. Var settur sum l\u00e6rari vi\u00f0 Hvalbiar sk\u00fala 1. august 1967. Hevur veri\u00f0 sk\u00falalei\u00f0ari vi\u00f0 Hvalbiar sk\u00fala fr\u00e1 1. august 1979. Hann hevur eisini veri\u00f0 \u00e1 Fr\u00f3\u00f0skaparsetri F\u00f8roya og fullf\u00f8rt n\u00e1m \u00ed f\u00f8royskum og b\u00f3kmentum 1969-70. Hann hevur \u00fatgivi\u00f0 fleiri yrkingas\u00f8vn og eisini eitt stutts\u00f8gusavn og eina b\u00f3k vi\u00f0 b\u00e6\u00f0i yrkingum og stutts\u00f8gum. Hann hevur eisini t\u00fdtt tv\u00e6r b\u00f8kur til f\u00f8royskt.\\n\\n\u00datg\u00e1vur  \\nGivi\u00f0 \u00fat \u00e1 egnum forlagi:\\nHvirlur (yrkingasavn) 1970\\nEg eri \u00ed iva (yrkingasavn) 1970 \\nTey \u00ed ur\u00f0ini (s\u00f8gusavn) 1973 \\nRey\u00f0ibarmur (yrkingar og stutts\u00f8gur) 1974\\nVi\u00f0r\u00e1k og M\u00f3tr\u00e1k (yrkingasavn) 1975\\n\u00d3ttast ikki (yrkingasavn) 1975\\nN\u00edvandi ni\u00f0a (yrkingasavn) 1983 \\nLova\u00f0 er lygnin (yrkingasavn) 1983 \\nEg eigi eina mynd (yrkingasavn) 1987\\n\\nT\u00fd\u00f0ingar \\nEydnur\u00edki prinsurin (Oscar Wilde) (F\u00f8roya L\u00e6rarafelag 1977). \\nHeilaga landi\u00f0 (P\u00e4r Lagerkvist) (felagi\u00f0 Var\u00f0in 1986).\\n\\nFamilja \\nForeldur: Thomasia Niclasen, f. Thomasen \u00e1 Giljanesi \u00ed V\u00e1gum og Hentzar Niclasen, kongsb\u00f3ndi \u00e1 Hamri \u00ed Hvalba. Giftist \u00ed 1971 vi\u00f0 S\u00fasonnu Niclasen, f. Holm. Hon er f\u00f8dd \u00ed Hvalba \u00ed 1950. Tey eiga tr\u00edggjar synir: T\u00f3rarinn, T\u00f3roddur og Nj\u00e1lur.\\n\\nKeldur \\n\\nF\u00f8royskir t\u00fd\u00f0arar\\nF\u00f8royskir rith\u00f8vundar\\nF\u00f8royskir yrkjarar\\nF\u00f8royskir l\u00e6rarar\\nHvalbingar\\nF\u00f8\u00f0ingar \u00ed 1943',\n  'question': 'Hvar var Levi Niclasen settur \u00ed starv \u00ed Gr\u00f8nlandi \u00ed 1961?',\n  'answers': {\n    'answer_start': array([431]),\n    'text': array(['F\u00f8royingahavnini'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': \"Giro d'Italia (\u00e1 f\u00f8royskum Kring Italia) er ein av teimum trimum st\u00f3ru teinas\u00fakklukappingunum og ver\u00f0ur hildin hv\u00f8rt \u00e1r \u00ed mai/juni og varir \u00ed 3 vikur. Kappingin fer fram \u00ed Italia, men partar av kappigini kunnu eisini fara fram \u00ed onkrum \u00f8r\u00f0um landi \u00ed Evropa, t.d. byrja\u00f0i Giro d'Italia \u00ed Ni\u00f0urlondum \u00ed 2016 og \u00ed Danmark \u00ed 2014.\\n\\nGiro d'Italia var\u00f0 fyrstu fer\u00f0 hildi\u00f0 \u00ed 1909, har i\u00f0 tilsamans 8 teinar \u00e1 2448\\xa0km v\u00f3ru s\u00fakkla\u00f0ir. Kappingin er saman vi\u00f0 Tour de France og Vuelta a Espa\u00f1a ein av teimum trimum klassisku teinakappingunum, har Tour de France t\u00f3 er tann mest t\u00fd\u00f0andi.\\n\\nHar tann fremsti s\u00fakklarin \u00ed Tour de France er kendur fyri at s\u00fakkla \u00ed gulari troyggju, so s\u00fakklar fremsti s\u00fakklarin \u00ed Giro d\u00b4Italia \u00ed lj\u00f3sarey\u00f0ari troyggju, \u00e1 italskum nevnd Maglia rosa. Tann fremsti fjallas\u00fakklarin s\u00fakklar \u00ed gr\u00f8nari troyggju (Maglia Verde), me\u00f0an s\u00fakklarin vi\u00f0 flestum stigum koyrir \u00ed lilla (Maglia ciclimano). \u00cd 2007 var\u00f0 tann hv\u00edta ungd\u00f3mstroyggjan innf\u00f8rd aftur, eftir at hon hev\u00f0i veri\u00f0 burturi \u00ed n\u00f8kur \u00e1r, hon nevnist Maglia Bianca.\\n\\nTr\u00edggir s\u00fakklarar hava vunni\u00f0 kappingina fimm fer\u00f0ir: Alfredo Binda, Fausto Coppi og Eddy Merckx. Italiuma\u00f0urin Felice Gimondi hevur sta\u00f0i\u00f0 \u00e1 sigurspallinum n\u00edggju fer\u00f0ir, har hann tr\u00edggjar fer\u00f0ir hevur vunni\u00f0, tv\u00e6r fer\u00f0ir \u00e1 \u00f8\u00f0rum pl\u00e1ssi og f\u00fdra fer\u00f0ir \u00e1 tri\u00f0japl\u00e1ssi.\\n\\nYvirlit yvir vinnarar\\n\\nByrjan \u00ed \u00f8\u00f0rum londum\\n\\nKeldur \\n\\nGiro d'Italia\",\n  'question': \"Hv\u00f8r hevur fimm fer\u00f0ir vunni\u00f0 Giro d'Italia?\",\n  'answers': {\n    'answer_start': array([1089]),\n    'text': array(['Alfredo Binda, Fausto Coppi og Eddy Merckx'], dtype=object)\n  }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>Hetta eru tekstir saman vi\u00f0 spurningum og svar.\n</code></pre></li> <li>Base prompt template:   <pre><code>Tekstur: {text}\nSpurningur: {question}\nSvara vi\u00f0 \u00ed mesta lagi trimum or\u00f0um: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Tekstur: {text}\n\nSvara hesum spurninginum um tekstin uppiyvir vi\u00f0 \u00ed mesta lagi trimum or\u00f0um.\n\nSpurningur: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset foqa\n</code></pre>"},{"location":"datasets/finnish/","title":"\ud83c\uddeb\ud83c\uddee Finnish","text":"<p>This is an overview of all the datasets used in the Finnish part of EuroEval. The datasets are grouped by their task - see the task overview for more information about what these constitute.</p>"},{"location":"datasets/finnish/#sentiment-classification","title":"Sentiment Classification","text":""},{"location":"datasets/finnish/#scandisent-fi","title":"ScandiSent-fi","text":"<p>This dataset consists of reviews from Trustpilot and was published here. It is a binary sentiment classification dataset, with labels \"positive\" and \"negative\".</p> <p>For the Finnish part of the dataset, there are 10,000 training samples. From these samples, we have created a 1,024 / 256 / 2,048 split for the train, validation and test splits, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Kaikki meni niinkuin piti. Nopea toimitus.\",\n  \"label\": \"positive\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"En pid\u00e4 t\u00e4st\u00e4, kun ei l\u00f6ydy linkki\u00e4 mist\u00e4 p\u00e4\u00e4sis heti maksamaan. En todellakaan pid\u00e4 siit\u00e4, ett\u00e4 joka tieto pit\u00e4\u00e4 kopioida erikseen. Haluaisin p\u00e4\u00e4st\u00e4 suoraan oston j\u00e4lkeen maksamaa mobiilipankkiin. Pari laskua on j\u00e4\u00e4nyt t\u00e4n takia kokonaan huomioimatta. Ja ihan turhaa.... \u00e4rsytt\u00e4\u00e4 sitten se kotiin tuleva muistutuslasku.\",\n  \"label\": \"negative\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Todella hidas toimitus, ja virheellist\u00e4 tietoa tuotteiden saatavuudesta, paketti ja tuotteet perill\u00e4 vasta kuukauden p\u00e4\u00e4st\u00e4 tilauksesta....\",\n  \"label\": \"negative\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Seuraavassa on arvosteluja ja niiden tunnes\u00e4vy, joka voi olla 'positiivinen' tai 'negatiivinen'.\n</code></pre></li> <li>Base prompt template:   <pre><code>Teksti: {text}\nTunnes\u00e4vy: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Teksti: {text}\n\nLuokittele arvostelun tunnes\u00e4vy. Vastaa vain 'positiivinen' tai 'negatiivinen', ei muuta.\n</code></pre></li> <li>Label mapping:<ul> <li><code>positive</code> \u27a1\ufe0f <code>positiivinen</code></li> <li><code>negative</code> \u27a1\ufe0f <code>negatiivinen</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scandisent-fi\n</code></pre>"},{"location":"datasets/finnish/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"datasets/finnish/#turku-ner-fi","title":"Turku-NER-fi","text":"<p>This dataset was published in this paper. The dataset is a manually annotated corpus built on the Universal Dependencies Finnish corpus. The corpus was created by the Turku NLP group.</p> <p>The original dataset contains 12,217 / 1,364 / 1,555 samples for the training, validation and test splits, respectively. We use 1,024 / 256 / 2,048 samples for our training, validation and test splits, respectively. All the new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"tokens\": [\"Suomalaiset\", \"vaihtoivat\", \"Tukholman\", \"Tallinnaan\"],\n  \"labels\": [\"O\", \"O\", \"B-LOC\", \"B-LOC\"]\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['Liuhto', 'nosti', 'Kreikan', 'tapauksen', 'yhteydess\u00e4', 'esille', 'kysymyksen', 'siit\u00e4', ',', 'miten', 'Euroopan', 'unionissa', 'yleisesti', 'sanktioidaan', 'pelis\u00e4\u00e4nt\u00f6jen', 'rikkomisesta', '.'], dtype=object),\n  \"labels\": array(['B-PER', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['Mithridates', 'oli', 'Pontoksen', 'merkitt\u00e4vin', 'kuningas', 'ja', 'Rooman', 'valtakunnan', 'vaarallisin', 'vihollinen', 'ensimm\u00e4isell\u00e4', 'vuosisadalla', 'eaa.', '.'], dtype=object),\n  \"labels\": array(['B-PER', 'O', 'B-LOC', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>Seuraavassa on lauseita ja JSON-sanakirjoja, jotka sis\u00e4lt\u00e4v\u00e4t annetussa lauseessa esiintyv\u00e4t nimetyt entiteetit.\n</code></pre></li> <li>Base prompt template:   <pre><code>Lause: {text}\nNimetyt entiteetit: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Lause: {text}\n\nTunnista lauseessa esiintyv\u00e4t nimetyt entiteetit. Tulosta ne JSON-sanakirjana, jonka avaimet ovat 'henkil\u00f6', 'paikka', 'organisaatio' ja 'muut'. Arvojen tulee olla listoja kyseisen tyypin nimetyist\u00e4 entiteeteist\u00e4 t\u00e4sm\u00e4lleen siin\u00e4 muodossa kuin ne esiintyv\u00e4t lauseessa.\n</code></pre></li> <li>Label mapping:<ul> <li><code>B-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>sted</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>sted</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>organisation</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>organisation</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>diverse</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>diverse</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset turku-ner-fi\n</code></pre>"},{"location":"datasets/finnish/#linguistic-acceptability","title":"Linguistic Acceptability","text":""},{"location":"datasets/finnish/#scala-fi","title":"ScaLA-fi","text":"<p>This dataset was published in this paper and was automatically created from the Finnish Universal Dependencies treebank by assuming that the documents in the treebank are correct, and corrupting the samples to create grammatically incorrect samples. The corruptions were done by either removing a word from a sentence, or by swapping two neighbouring words in a sentence. To ensure that this does indeed break the grammaticality of the sentence, a set of rules were used on the part-of-speech tags of the words in the sentence.</p> <p>The original dataset consists of 15,136 samples, from which we use 1,024 / 256 / 2,048 samples for training, validation and testing, respectively (so 3,328 samples used in total). These splits are used as-is in the framework.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Vuotta aiempaan verrattuna uusia ajoneuvoja rekister\u00f6itiin 17,6 prosenttia enemm\u00e4n.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"20-vuotias sai aiemmin marraskuussa 2006 Helsingin k\u00e4r\u00e4j\u00e4oikeudelta 30 p\u00e4iv\u00e4sakkoa Ta... varastettujen vaatteiden hallussapidosta.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Kun k\u00e4ytt\u00e4j\u00e4 kirjoittaa viestin, se n\u00e4kyy k\u00e4ytt\u00e4j\u00e4n k\u00e4ytt\u00e4j\u00e4listassa.\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Seuraavat ovat lauseita ja ovatko ne kieliopillisesti oikein.\n</code></pre></li> <li>Base prompt template:   <pre><code>Lause: {text}\nKieliopillisesti oikein: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Lause: {text}\n\nM\u00e4\u00e4rit\u00e4 onko lause kieliopillisesti oikein vai ei. Vastaa 'kyll\u00e4', jos lause on oikein, ja 'ei', jos se ei ole.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>kyll\u00e4</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>ei</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scala-fi\n</code></pre>"},{"location":"datasets/finnish/#reading-comprehension","title":"Reading Comprehension","text":""},{"location":"datasets/finnish/#tydiqa-fi","title":"TydiQA-fi","text":"<p>This question-answering dataset was published in this paper. TydiQA is a multilingual dataset covering 11 typologically diverse languages with 204K question-answer pairs collected from native speakers genuinely seeking information. It was designed to evaluate models across languages with varied linguistic features and contains questions written directly in each language without translation.</p> <p>The original Finnish TydiQA dataset contains 6,855 training and 782 validation samples (we use the secondary task subset). We created a 1,024 / 256 / 2,024 split, where the samples from the train and validation split are sampled from the original train and validation splits, respectively. The test set consists of the remaining samples from the original validation split + additional samples from the original train split.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"question\": \"Kuka n\u00e4ytteli Dumbledorea Harry Potter elokuvissa?\",\n  \"context\": \"Dumbledorea esitt\u00e4\u00e4 kirjasarjasta tehdyss\u00e4 elokuvasarjassa Richard Harris kahdessa ensimm\u00e4isess\u00e4 elokuvassa. Harrisin kuoltua Michael Gambon esitti hahmoa sarjan lopuissa elokuvissa.\",\n  \"answers\": {\n    \"text\": [\"Richard Harris kahdessa ensimm\u00e4isess\u00e4 elokuvassa. Harrisin kuoltua Michael Gambon\"],\n    \"answer_start\": [59]\n  }\n}\n\n```json\n{\n  \"question\": \"Milloin Cristiano Ronaldo liittyi Juventukseen?\",\n  \"context\": \"Ronaldo siirtyi hein\u00e4kuussa 2018 Juventukseen 105 miljoonalla eurolla. Sopimus on nelivuotinen, ja sen aikana h\u00e4n tienaa verojen j\u00e4lkeen noin 120 miljoonaa euroa.[133]\",\n  \"answers\": {\n    \"text\": [\"hein\u00e4kuussa 2018\"],\n    \"answer_start\": [16]\n  }\n}\n</code></pre> <pre><code>{\n  \"question\": \"Kuka hallitsi Mithridates VI j\u00e4lkeen?\",\n  \"context\": \"Mithridates laajensi valtakuntaansa ymp\u00e4ri Mustanmeren rantoja, ja h\u00e4n ajautui kolmesti sotaan Rooman valtakuntaa vastaan. Ensimm\u00e4isess\u00e4 sodassa (89 eaa.\u201385 eaa.) h\u00e4n valtasi suuren osan V\u00e4h\u00e4\u00e4-Aasiaa ja Rooman valtakunnalle kuuluneet osat, jolloin h\u00e4nen sanotaan teloittaneen 80000 roomalaista. Mithridates valtasi my\u00f6s Kreikan, mutta konsuli Sulla kukisti h\u00e4nen joukkonsa vuonna 85 eaa., ja Mithridateen oli luovuttava valloituksistaan. Toinen sota (83 eaa.\u201381 eaa.) oli suppeampi laajuudeltaan. Kolmannessa sodassa (73 eaa.\u201363 eaa.) roomalaiset sotap\u00e4\u00e4llik\u00f6t Lucullus ja Pompeius kukistivat Mithridateen perusteellisesti. Mithridates surmasi tai surmautti itsens\u00e4 jouduttuaan poikansa Farnakes II:n syrj\u00e4ytt\u00e4m\u00e4ksi.\",\n  \"answers\": {\n    \"text\": [\"Farnakes II\"],\n    \"answer_start\": [687]\n  }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>Seuraavassa on tekstej\u00e4 ja niihin liittyvi\u00e4 kysymyksi\u00e4 ja vastauksia.\n</code></pre></li> <li>Base prompt template:   <pre><code>Teksti: {text}\nKysymys: {question}\nVastaa enint\u00e4\u00e4n 3 sanalla: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Teksti: {text}\n\nVastaa seuraavaan kysymykseen yll\u00e4 olevasta tekstist\u00e4 enint\u00e4\u00e4n 3 sanalla.\n\nKysymys: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset tydiqa-fi\n</code></pre>"},{"location":"datasets/finnish/#unofficial-belebele-fi","title":"Unofficial: BeleBele-fi","text":"<p>This dataset was published in this paper and features multiple-choice reading comprehension questions across 122 languages.</p> <p>The original dataset contains 900 unique multiple-choice reading comprehension passages and questions. From these, we use a 256 / 64 / 580 split for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Toisin kuin muut k\u00e4delliset, isot ihmisapinat eiv\u00e4t en\u00e4\u00e4 k\u00e4yt\u00e4 k\u00e4si\u00e4\u00e4n liikkumiseen, painon kannattelemiseen tai liikkumiseen puissa itse\u00e4\u00e4n heilautellen. Simpanssin k\u00e4si ja jalka ovat samankokoisia ja -pituisia, mik\u00e4 viittaa siihen, ett\u00e4 k\u00e4delle varataan painoa rystyk\u00e4velyss\u00e4. Ihmisen k\u00e4si on lyhyempi kuin jalka, ja sen sormiluut ovat suoremmat. Kahden-kolmen miljoonan vuoden ik\u00e4iset k\u00e4siluiden fossiilit paljastavat k\u00e4den erikoistumisessa t\u00e4m\u00e4n muutoksen liikkumisesta k\u00e4yttelyyn.\\nKysymys: Mik\u00e4 seuraavista kuvaa tarkasti simpanssin sormiluita?\\nVaihtoehdot:\\na. Ne ovat suoremmat kuin ihmisill\u00e4\\nb. Niiden k\u00e4det ja jalat ovat erikokoisia\\nc. Niit\u00e4 k\u00e4ytet\u00e4\u00e4n painon kannattelemiseen\\nd. Niit\u00e4 k\u00e4ytet\u00e4\u00e4n p\u00e4\u00e4asiassa k\u00e4yttelyyn\",\n  \"label\": \"c\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Panaman paperit on yl\u00e4k\u00e4site panamalaisen lakiyrityksen Mossack Fonsecan noin kymmenelle miljoonalle asiakirjalle, jotka vuodettiin lehdist\u00f6lle kev\u00e4\u00e4ll\u00e4 2016. Asiakirjoista selvisi, ett\u00e4 nelj\u00e4toista pankkia auttoi varakkaita asiakkaita piilottamaan miljardeja USA:n dollareita verojen ja muiden s\u00e4\u00e4ntelyjen v\u00e4ltt\u00e4miseksi. Brittil\u00e4isen sanomalehden The Guardianin mukaan Deutsche Bank hallitsi t\u00e4m\u00e4n toteuttamiseen k\u00e4ytetyist\u00e4 1 200 postilaatikkoyrityksest\u00e4 suunnilleen kolmasosaa. Seurasi maailmanlaajuisia protesteja ja useita rikossyytteit\u00e4, ja Islannin ja Pakistanin hallitusten johtajat kumpikin erosivat.\\nKysymys: Kuka brittil\u00e4isen lehdist\u00f6n v\u00e4itteen mukaan hallinnoi monia varojen piilottamisessa k\u00e4ytettyj\u00e4 yrityksi\u00e4 tekstikatkelman mukaan?\\nVaihtoehdot:\\na. Eri pankkien varakkaat asiakkaat\\nb. Panamalainen lakiyritys\\nc. Deutsche Bank\\nd. Pakistanin hallitus\",\n  \"label\": \"c\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Teksti: Sundarban on maailman suurin mangrovemets\u00e4alue. Se ulottuu 80 kilometri\u00e4 (50 mailia) rannikolta Bangladeshin ja Intian takamaille. Sundarban on julistettu Unescon maailmanperint\u00f6kohteeksi. Mets\u00e4n Intian puolella sijaitsevaa osaa kutsutaan Sundarbanin kansallispuistoksi. Mets\u00e4t eiv\u00e4t kuitenkaan ole pelkki\u00e4 mangrovesoita, vaan niihin kuuluu joitakin viimeisi\u00e4 j\u00e4\u00e4nteit\u00e4 niist\u00e4 mahtavista viidakoista, jotka aikoinaan peittiv\u00e4t koko Gangesin tasangon. Sundarban kattaa 3 850 neli\u00f6kilometrin alueen, josta noin kolmasosa on vesi- tai suoalueiden peitossa. Vuodesta 1966 asti Sundarbans on ollut villiel\u00e4inten suojelualue. Arvioidaan, ett\u00e4 siell\u00e4 on nyky\u00e4\u00e4n 400 intiantiikeri\u00e4 ja suunnilleen 30 000 aksishirve\u00e4.\\nKysymys: Mik\u00e4 mets\u00e4n osa on Intian puolella?\\nVaihtoehdot:\\na. Sundarbanin kansallispuisto\\nb. Villiel\u00e4inten suojelualue\\nc. Maailmanperint\u00f6kohde\\nd. Gangesin tasanko\",\n  \"label\": \"a\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Seuraavat ovat monivalintakysymyksi\u00e4 (vastauksineen).\n</code></pre></li> <li>Base prompt template:   <pre><code>Kysymys: {text}\nVaihtoehdot:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nVastaus: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Kysymys: {text}\nVaihtoehdot:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nVastaa yll\u00e4 olevaan kysymykseen k\u00e4ytt\u00e4m\u00e4ll\u00e4 'a', 'b', 'c' tai 'd', \u00e4l\u00e4k\u00e4 mit\u00e4\u00e4n muuta.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset belebele-fi\n</code></pre>"},{"location":"datasets/finnish/#common-sense-reasoning","title":"Common-sense Reasoning","text":""},{"location":"datasets/finnish/#hellaswag-fi","title":"HellaSwag-fi","text":"<p>This dataset is a machine translated version of the English HellaSwag dataset. The dataset was created by Finnish-NLP using Google Translate. The dataset is designed to be used in EuroEval and it therefore already has a 1,024 / 256 / 2,048 split for the train, validation and test splits, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"[Otsikko ] Tiikkihuonekalujen tahraus [vaihe] Pyyhi lika, p\u00f6ly ja roskat pois. [vaihe] Voit harjata lian pois kuivalla paperipyyhkeell\u00e4 tai liinalla. Jos puhdistettavia kohtia on sitke\u00e4mpi\u00e4, voit hieroa ne puhtaaksi kostealla rievulla.\\nVastausvaihtoehdot:\\na. [vaihe] Poista tahrat tiikist\u00e4 pyyhkim\u00e4ll\u00e4 ne kuivalla talouspaperilla. [vaihe] Noudata samoja puhdistustoimenpiteit\u00e4, joita k\u00e4ytit tahran kanssa.\\nb. Aja niiden yli puhdistusaineella, kunnes tahra on poissa. [vaihe] Kokeile puupetsin ja \u00f6ljyn yhdistelm\u00e4\u00e4.\\nc. [v\u00e4livaiheet] \u00c4l\u00e4 k\u00e4yt\u00e4 puhdistusaineita. Saatat vahingoittaa puuta, mutta vaikeutat varmasti v\u00e4rj\u00e4ysprosessia.\\nd. Poista mahdollisimman paljon likaa levitt\u00e4m\u00e4ll\u00e4 tahra kevyelle, p\u00f6rr\u00f6iselle liinalle tai k\u00e4delle ja pyyhkim\u00e4ll\u00e4 se pois. [vaihe] K\u00e4yt\u00e4 hankaamiseen valkaisuainetta ja vett\u00e4.\",\n  \"label\": \"c\",\n}\n</code></pre> <pre><code>{\n  \"text\": \"Pieni ryhm\u00e4 ihmisi\u00e4 n\u00e4hd\u00e4\u00e4n uimassa altaan ymp\u00e4rill\u00e4 ja johtaa useisiin laukauksiin, joissa uimari heitt\u00e4\u00e4 pallon verkkoon. Maalivahti torjuu muutaman laukauksen ja vaihtaa sitten toisen joukkuetoverinsa kanssa yleis\u00f6n hurraten. ihmisi\u00e4\\nVastausvaihtoehdot:\\na. cheer viel\u00e4 kerran ja palaa uimaan uima-altaan ymp\u00e4rille.\\nb. vaihda jatkuvasti pois ja johtaa siihen, ett\u00e4 yksi joukkue voittaa ja juhlii kaikki yhdess\u00e4 vedess\u00e4.\\nc. Curra ja hypp\u00e4\u00e4 vuorotellen yl\u00f6s ja eteenp\u00e4in pelaamalla biljardia.\\nd. ensimm\u00e4inen video, jossa muut joukkuetoverit sukeltavat altaaseen ja hypp\u00e4\u00e4v\u00e4t yl\u00f6s ja alas ponnahduslaudalla.\",\n  \"label\": \"b\",\n}\n</code></pre> <pre><code>{\n  \"text\": \"Kahden ihmisen n\u00e4hd\u00e4\u00e4n k\u00e4velev\u00e4n p\u00f6yt\u00e4jalkapallop\u00f6yd\u00e4n ymp\u00e4rill\u00e4 pelaamassa. ihmisi\u00e4\\nVastausvaihtoehdot:\\na. pit\u00e4k\u00e4\u00e4 kupit yl\u00f6s ja alakaa pelata peli\u00e4 ja ly\u00f6d\u00e4 toisianne.\\nb. Tartu sauvoista ja ly\u00f6 palloa p\u00f6yd\u00e4n ymp\u00e4rill\u00e4.\\nc. Jatka k\u00e4velemist\u00e4 ja yksi henkil\u00f6 ly\u00f6 pallon verkon yli.\\nd. siirr\u00e4 ymp\u00e4ri p\u00f6yt\u00e4\u00e4 heitt\u00e4en palloa ymp\u00e4riins\u00e4, kun ihmiset katselevat sivuilla.\",\n  \"label\": \"b\",\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Seuraavat ovat monivalintakysymyksi\u00e4 (vastauksineen).\n</code></pre></li> <li>Base prompt template:   <pre><code>Kysymys: {text}\nVastausvaihtoehdot:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nVastaus: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Kysymys: {text}\nVastausvaihtoehdot:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nVastaa yll\u00e4 olevaan kysymykseen k\u00e4ytt\u00e4m\u00e4ll\u00e4 'a', 'b', 'c' tai 'd', \u00e4l\u00e4k\u00e4 mit\u00e4\u00e4n muuta.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset hellaswag-fi\n</code></pre>"},{"location":"datasets/finnish/#summarization","title":"Summarization","text":""},{"location":"datasets/finnish/#xlsum-fi","title":"XLSum-fi","text":"<p>This dataset is a machine translation of the XL-Sum dataset, which was published in this paper. TurkuNLP has translated the dataset to Finnish using DeepL.</p> <p>The original Finnish XL-Sum dataset contains 54,966 / 1,803 / 1,791 training, validation and test samples, respectively. We use 1,024 / 256 / 2,048 samples for our training, validation and test splits, respectively. The new training and validation splits are subsets of the original splits. The test split is the same as the original test split + additional samples from the original validation split.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Poliisi kutsuttiin Century Wharfiin keskiviikkona noin kello 14:15 GMT. 66-vuotias mies on pid\u00e4tetty murhasta ep\u00e4iltyn\u00e4, ja h\u00e4nt\u00e4 pidet\u00e4\u00e4n vangittuna. Etel\u00e4-Walesin poliisi ilmoitti, ett\u00e4 se siirt\u00e4\u00e4 asian vapaaehtoisesti riippumattoman poliisin valituslautakunnan k\u00e4sitelt\u00e4v\u00e4ksi.\",\n  \"target_text\": \"Murhatutkinta on aloitettu sen j\u00e4lkeen, kun 65-vuotiaan naisen ruumis l\u00f6ytyi Cardiff Bayn asunnosta.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Yritys on nimitt\u00e4nyt KPMG:n tarkastelemaan uudelleenj\u00e4rjestelyvaihtoehtoja sen j\u00e4lkeen, kun paikallisviranomaisten menojen leikkaukset heikensiv\u00e4t sen liiketoimintan\u00e4kymi\u00e4. Southern tarjoaa hoitoa yli 31 000 ihmiselle, ja suurin osa rahoituksesta tulee NHS:lt\u00e4 ja kunnilta. Yrityksen mukaan budjettileikkaukset merkitsiv\u00e4t sit\u00e4, ett\u00e4 sen vuokrataakka oli 'kest\u00e4m\u00e4t\u00f6n'. Southern kertoi keskustelevansa vuokranantajien kanssa uudelleenj\u00e4rjestelyst\u00e4 ja varoitti my\u00f6s, ett\u00e4 se oli vaarassa j\u00e4tt\u00e4\u00e4 velkansa maksamatta. 'Yhti\u00f6n lainanantajat ovat tietoisia uhkaavasta pankkikovenanttirikkomuksesta, mutta ne tukevat edelleen t\u00e4ysin toimia, joihin yhti\u00f6 ryhtyy ongelmiensa ratkaisemiseksi', Southern sanoi lausunnossaan. Yhti\u00f6 vahvisti my\u00f6s, ettei se en\u00e4\u00e4 keskustele mahdollisten ostajien kanssa. 'Hallitus katsoo, ett\u00e4 yksik\u00e4\u00e4n n\u00e4ist\u00e4 ehdotuksista ei todenn\u00e4k\u00f6isesti johda siihen, ett\u00e4 l\u00e4hitulevaisuudessa teht\u00e4isiin mielek\u00e4s tarjous, ja se on p\u00e4\u00e4tt\u00e4nyt olla jatkamatta niiden k\u00e4sittely\u00e4', Southern totesi. Southernin osakkeet, joiden arvo oli 606 pence\u00e4 vuonna 2007, olivat keskip\u00e4iv\u00e4ll\u00e4 6,3 penni\u00e4.\",\n  \"target_text\": \"Yhdistyneen kuningaskunnan suurimman hoivakotien yll\u00e4pit\u00e4j\u00e4n Southern Cross Healthcaren osakkeet ovat romahtaneet 60 prosenttia, kun on uutisoitu, ett\u00e4 taloudelliset ongelmat ovat lis\u00e4\u00e4ntym\u00e4ss\u00e4.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Pohjois-Walesin palo- ja pelastusviranomainen vahvisti maanantaina talousarvionsa vuosiksi 2015-16. Viranomainen on suostunut leikkaamaan nelj\u00e4 johtoteht\u00e4v\u00e4\u00e4, leikkaamaan joitakin palveluja ja k\u00e4ytt\u00e4m\u00e4\u00e4n vararahastoa, jotta se voi hyv\u00e4ksy\u00e4 32,1 miljoonan punnan talousarvionsa. On pel\u00e4tty, ett\u00e4 sadat palomiehet voivat l\u00e4hte\u00e4 seuraavien viiden vuoden aikana teht\u00e4vien budjettileikkausten seurauksena.\",\n  \"target_text\": \"Pohjois-Walesin palomiehet lopettavat suurten el\u00e4inten pelastamisen ja v\u00e4hent\u00e4v\u00e4t v\u00e4\u00e4rien h\u00e4lytysten m\u00e4\u00e4r\u00e4\u00e4, jotta talous saataisiin tasapainoon.\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>Seuraavassa on artikkeleita ja niihin liittyvi\u00e4 tiivistelmi\u00e4.\n</code></pre></li> <li>Base prompt template:   <pre><code>Uutisartikkeli: {text}\nTiivistelm\u00e4: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Uutisartikkeli: {text}\n\nKirjoita tiivistelm\u00e4 yll\u00e4 olevasta artikkelista.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset xlsum-fi\n</code></pre>"},{"location":"datasets/french/","title":"\ud83c\uddeb\ud83c\uddf7 French","text":"<p>This is an overview of all the datasets used in the French part of EuroEval. The datasets are grouped by their task - see the task overview for more information about what these constitute.</p>"},{"location":"datasets/french/#sentiment-classification","title":"Sentiment Classification","text":""},{"location":"datasets/french/#allocine","title":"AlloCin\u00e9","text":"<p>This dataset was published in this Github repository and features reviews from the French movie review website AlloCin\u00e9. The reviews range from 0.5 to 5 (inclusive), with steps of 0.5. The negative samples are reviews with a rating of at most 2, and the positive ones are reviews with a rating of at least 4. The reviews in between were discarded.</p> <p>The original full dataset consists of 160,000 / 20,000 / 20,000 samples for training, validation, and testing, respectively. We use 1,024 / 256 / 2,048 samples for training, validation, and testing, respectively. All our splits are subsets of the original ones.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Ce 7\u00e8me volet ne m\u00e9rite pas de notre part une grande attention, au vu du pr\u00e9c\u00e9dent New Police Story. \u00c0 la limite du huis clos, Jackie \u00e9volue dans une bo\u00eete de nuit, sorte de pi\u00e8ge du m\u00e9chant cherchant \u00e0 se venger, ou du moins \u00e0 d\u00e9couvrir la v\u00e9rit\u00e9 sur la mort de sa s\u0153ur. Notre cascadeur acteur ne b\u00e9n\u00e9ficie pas d'un d\u00e9cors \u00e0 la hauteur de son potentiel acrobatique et le film d'un sc\u00e9nario \u00e0 la hauteur d'une production, et cette production d'une large distribution, ce qui explique son arriv\u00e9e direct tout \u00e9tag\u00e8re.\",\n  \"label\": \"negative\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Meme pour ceux qui n'aime pas les Chevaliers du Fiel allez voir. 1 il est meilleur que le 1 et cela est rare de voir une suite qui est meilleur que le 1. Des sc\u00e8nes qui peuvent faire rire les petit et les grands. On ne s'ennuie pas. Super film allez le voir. L'interpretation des acteurs sont super. Bonne journ\u00e9e\",\n  \"label\": \"positive\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Une ambiance envo\u00fbtante, un r\u00e9cit o\u00f9 se m\u00e9langent sorcellerie, croyances indiennes, enqu\u00eate polici\u00e8re sur fond de trafic de drogue, tout est conforme au livre de Tony Hillerman, m\u00eame si ce dernier a \\\"reni\u00e9\\\" le film. Personnellement j'adore. H\u00e9las introuvable en France et diffus\u00e9 seulement sur canal , il y a ..... un certain temps.\",\n  \"label\": \"positive\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>Voici des textes et leur sentiment, qui peut \u00eatre 'positif' ou 'n\u00e9gatif'.\n</code></pre></li> <li>Base prompt template:   <pre><code>Texte: {text}\nSentiment: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Texte: {text}\n\nClassez le sentiment dans le texte. R\u00e9pondez par \u2018positif' ou \u2018n\u00e9gatif'.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset allocine\n</code></pre>"},{"location":"datasets/french/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"datasets/french/#eltec","title":"ELTeC","text":"<p>This dataset was published in this paper and consists of sentences from 100 novels in French during the period 1840-1920, all of which are in the public domain. These novels were automatically labelled with named entities using Stanza-NER, and then manually corrected.</p> <p>The original dataset consists of 100 samples, one for each novel. We split the novels into sentences using the French NLTK sentence splitter, resulting in 4,815 samples. We use 1,024 / 256 / 2,048 samples for training, validation, and testing, respectively.</p> <p>We have furthermore converted the OntoNotes 5.0 labelling scheme to the CoNLL-2003 labelling scheme, which is more common in the NER literature. The mapping is as follows:</p> <ul> <li><code>PERS</code> \u27a1\ufe0f <code>PER</code></li> <li><code>LOC</code> \u27a1\ufe0f <code>LOC</code></li> <li><code>ORG</code> \u27a1\ufe0f <code>ORG</code></li> <li><code>OTHER</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>DEMO</code> \u27a1\ufe0f <code>O</code></li> <li><code>ROLE</code> \u27a1\ufe0f <code>O</code></li> <li><code>EVENT</code> \u27a1\ufe0f <code>O</code></li> </ul> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  'tokens': array(['Jamais', 'ils', 'ne', 'firent', 'de', 'provisions', ',', 'except\u00e9', 'quelques', 'bottes', \"d'ail\", 'ou', \"d'oignons\", 'qui', 'ne', 'craignaient', 'rien', 'et', 'ne', 'co\u00fbtaient', 'pas', \"grand'chose\", ';', 'le', 'peu', 'de', 'bois', \"qu'ils\", 'consommaient', 'en', 'hiver', ',', 'la', 'Sauviat', \"l'achetait\", 'aux', 'fagotteurs', 'qui', 'passaient', ',', 'et', 'au', 'jour', 'le', 'jour', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['I', 'Il', 'y', 'avait', 'plus', 'de', 'soixante', 'ans', 'que', \"l'empereur\", 'Napol\u00e9on', ',', 'press\u00e9', \"d'argent\", ',', 'avait', 'vendu', 'les', 'provinces', 'de', 'la', 'Louisiane', '\u00e0', 'la', 'R\u00e9publique', 'des', '\u00c9tats-Unis', ';', 'mais', ',', 'en', 'd\u00e9pit', 'de', \"l'infiltration\", 'yankee', ',', 'les', 'traditions', 'des', 'cr\u00e9oles', 'fran\u00e7ais', 'se', 'perp\u00e9tuaient', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['Les', 'fen\u00eatres', 'de', 'la', 'vieille', 'demeure', 'royale', ',', 'ordinairement', 'si', 'sombres', ',', '\u00e9taient', 'ardemment', '\u00e9clair\u00e9es', ';', 'les', 'places', 'et', 'les', 'rues', 'attenantes', ',', 'habituellement', 'si', 'solitaires', ',', 'd\u00e8s', 'que', 'neuf', 'heures', 'sonnaient', '\u00e0', \"Saint-Germain-l'Auxerrois\", ',', '\u00e9taient', ',', \"quoiqu'il\", 'f\u00fbt', 'minuit', ',', 'encombr\u00e9es', 'de', 'populaire', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>Vous trouverez ci-dessous des phrases et des dictionnaires JSON avec les entit\u00e9s nomm\u00e9es qui apparaissent dans la phrase donn\u00e9e.\n</code></pre></li> <li>Base prompt template:   <pre><code>Sentence: {text}\nEntit\u00e9s nomm\u00e9es: {label}\n</code></pre></li> <li> <p>Instruction-tuned prompt template:   <pre><code>Sentence: {text}\n\nIdentifiez les entit\u00e9s nomm\u00e9es dans la phrase. Vous devez produire ceci sous forme de dictionnaire JSON avec les cl\u00e9s 'personne', 'lieu', 'organisation' et 'divers'. Les valeurs doivent \u00eatre des listes des entit\u00e9s nomm\u00e9es de ce type, exactement comme elles apparaissent dans la phrase.\n</code></pre></p> </li> <li> <p>Label mapping:</p> <ul> <li><code>B-PER</code> \u27a1\ufe0f <code>personne</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>personne</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>lieu</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>lieu</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>organisation</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>organisation</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>divers</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>divers</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset eltec\n</code></pre>"},{"location":"datasets/french/#linguistic-acceptability","title":"Linguistic Acceptability","text":""},{"location":"datasets/french/#scala-fr","title":"ScaLA-fr","text":"<p>This dataset was published in this paper and was automatically created from the French Universal Dependencies treebank by assuming that the documents in the treebank are correct, and corrupting the samples to create grammatically incorrect samples. The corruptions were done by either removing a word from a sentence, or by swapping two neighbouring words in a sentence. To ensure that this does indeed break the grammaticality of the sentence, a set of rules were used on the part-of-speech tags of the words in the sentence.</p> <p>The original dataset consists of 16,342 samples, from which we use 1,024 / 256 / 2,048 samples for training, validation and testing, respectively (so 3,328 samples used in total). These splits are used as-is in the framework.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Le dessert est une part minuscule de g\u00e2teau.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Le trafic international sera normal vendredi sur Eurostar, Thalys, et sur les trains \u00e0 grande vitesse \u00e0 destination de l', a indiqu\u00e9 la SNCF dans un communiqu\u00e9.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Certains craignent qu' un avantage comp\u00e9titif trop net et trop durable favorise les positions dominantes, monopoles et oligopoles, qui limitent la et concurrence finissent par peser sur le consommateur.\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Les phrases suivantes indiquent si elles sont grammaticalement correctes.\n</code></pre></li> <li>Base prompt template:   <pre><code>Phrase: {text}\nCorrect du point de vue grammatical: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Phrase: {text}\n\nD\u00e9terminez si la phrase est grammaticalement correcte ou non. R\u00e9pondez par 'oui' si la phrase est correcte et par 'non' si elle ne l'est pas, et rien d'autre.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>oui</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>non</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scala-fr\n</code></pre>"},{"location":"datasets/french/#reading-comprehension","title":"Reading Comprehension","text":""},{"location":"datasets/french/#fquad","title":"FQuAD","text":"<p>This dataset was published in this paper, and is a manually annotated dataset of questions and answers from the French Wikipedia.</p> <p>The original full dataset consists of 20,731 / 3,188 / 2,189 samples for training, validation and testing, respectively. Note that the testing split is not publicly accessible, however, so we only use the training and validation split. We use 1,024 / 256 / 2,048 samples for training, validation, and testing, respectively. Our training split is a subset of the original training split, and our validation and testing splits are subsets of the original validation split.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  'context': \"Parmi leurs th\u00e8mes r\u00e9currents, on en trouve qui sont communs \u00e0 beaucoup d'autres groupes contemporains ou plus anciens : les Stranglers ont d\u00e9crit, \u00e0 plusieurs reprises, la vie d'un groupe de rock dans toutes ses dimensions (fans, autres groupes, vie en tourn\u00e9e). Le th\u00e8me rebattu - chez les groupes des ann\u00e9es 1960-1970 - de la drogue, est abord\u00e9e sur une demi-douzaine de chansons (Don't Bring Harry), tandis que la vision angoiss\u00e9e du futur, dans le contexte de la guerre froide ou en lien avec les avanc\u00e9es de la science, a donn\u00e9 lieu \u00e0 plusieurs titres (Curfew). On retrouve \u00e9galement chez eux des pr\u00e9occupations \u00e9cologiques (Dreamtime) ou sociales. La guerre, notamment les deux guerres mondiales (Northwinds), mais aussi les guerres contemporaines (I Don't Agree), sont \u00e0 l'origine de divers textes. Mais le th\u00e8me qui les a le plus inspir\u00e9s, c'est de loin les femmes (The Man They Love to Hate).\",\n  'question': 'Sur combien de chanson le th\u00e8me de la drogue est il abord\u00e9 ?',\n  'answers': {\n    'answer_start': array([353]),\n    'text': array(['une demi-douzaine'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': \"Au cours de cette p\u00e9riode, Cavour se distingue par son talent de financier. Il contribue de mani\u00e8re pr\u00e9pond\u00e9rante \u00e0 la fusion de la Banque de G\u00eanes et de la nouvelle Banque de Turin au sein de la Banque Nationale des \u00c9tats sardes (Banca Nazionale degli Stati Sardi). Apr\u00e8s le succ\u00e8s \u00e9lectoral de d\u00e9cembre 1849, Cavour devient \u00e9galement une des figures dominantes de la politique pi\u00e9montaise et il prend la fonction de porte-parole de la majorit\u00e9 mod\u00e9r\u00e9e qui vient de se cr\u00e9er. Fort de cette position, il fait valoir que le moment des r\u00e9formes est arriv\u00e9, favoris\u00e9 par le Statut albertin qui a cr\u00e9\u00e9 de r\u00e9elles perspectives de progr\u00e8s. Le Pi\u00e9mont peut ainsi s'\u00e9loigner du front catholique et r\u00e9actionnaire, qui triomphe dans le reste de l'Italie. \",\n  'question': \"En quel ann\u00e9e sort-il vainqueur d'une \u00e9lection ?\",\n  'answers': {\n    'answer_start': array([305]),\n    'text': array(['1849'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': \"Pour autant, le ph\u00e9nom\u00e8ne m\u00e9t\u00e9orologique se d\u00e9cline sous d'autres variantes : ocelles du paon, \u00e9voquant les cent yeux d'Argus, fleurs champ\u00eatres et ornant les jardins o\u00f9 s'\u00e9tablit l'osmose entre couleurs compl\u00e9mentaires. La po\u00e9sie tient en main la palette du peintre,, celle de Claude Gell\u00e9e ou de Poussin. Pour autant, il ne s'agit pas l\u00e0 d'une posture habituelle chez lui, qui privil\u00e9gie les paysages quasi-monochromes.\",\n  'question': \"Qu'est ce que l'auteur pr\u00e9f\u00e8re d\u00e9crire ?\",\n  'answers': {\n    'answer_start': array([394]),\n    'text': array(['paysages'], dtype=object)\n  }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>Les textes suivants sont accompagn\u00e9s de questions et de r\u00e9ponses.\n</code></pre></li> <li>Base prompt template:   <pre><code>Texte: {text}\nQuestion: {question}\nR\u00e9ponse en 3 mots maximum: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Texte: {text}\n\nR\u00e9pondez \u00e0 la question suivante sur le texte ci-dessus en 3 mots maximum.\n\nQuestion: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset fquad\n</code></pre>"},{"location":"datasets/french/#unofficial-belebele-fr","title":"Unofficial: BeleBele-fr","text":"<p>This dataset was published in this paper and features multiple-choice reading comprehension questions across 122 languages.</p> <p>The original dataset contains 900 unique multiple-choice reading comprehension passages and questions. From these, we use a 256 / 64 / 580 split for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Texte: Lorsqu\u2019un petit groupe d\u2019\u00eatres vivants (une petite population) est s\u00e9par\u00e9 de la population principale dont il est issu (par exemple, s\u2019il se d\u00e9place au-dessus d\u2019une cha\u00eene de montagnes ou d\u2019une rivi\u00e8re, ou s\u2019il se d\u00e9place vers une nouvelle \u00eele de sorte qu\u2019il ne peut pas facilement revenir en arri\u00e8re), il se retrouve souvent dans un environnement diff\u00e9rent de celui dans lequel il \u00e9tait auparavant. Ce nouvel environnement a des ressources et des concurrents diff\u00e9rents, de sorte que la nouvelle population aura besoin de caract\u00e9ristiques ou d'adaptations nouvelles pour \u00eatre un concurrent puissant par rapport \u00e0 ce dont elle avait besoin auparavant. La population d'origine n'a pas chang\u00e9 du tout,\\xa0elle a toujours besoin des m\u00eames adaptations. Au fil du temps, \u00e0 mesure que la nouvelle population s'adapte \u00e0 son nouvel environnement, elle commence \u00e0 ressembler de moins en moins \u00e0 l'autre population. Enfin, apr\u00e8s des milliers ou m\u00eame des millions d'ann\u00e9es, les deux populations para\u00eetront tellement diff\u00e9rentes qu'elles ne pourront plus \u00eatre consid\u00e9r\u00e9es comme appartenant \u00e0 la m\u00eame esp\u00e8ce. Nous appelons ce processus \u00ab\\u2009sp\u00e9ciation\\u2009\u00bb, ce qui signifie simplement la formation de nouvelles esp\u00e8ces. La sp\u00e9ciation est une cons\u00e9quence in\u00e9vitable et une partie tr\u00e8s importante de l\u2019\u00e9volution.\\nQuestion: D\u2019apr\u00e8s l\u2019extrait et parmi les exemples ci-dessous, qu\u2019est-ce qui g\u00eanerait le processus d\u2019\u00e9volution\\xa0?\\nChoix:\\na. La difficult\u00e9 pour un petit groupe \u00e0 s\u2019\u00e9panouir dans un nouvel endroit\\nb. La migration d\u2019une portion d\u2019une population vers un nouvel environnement\\nc. L\u2019ajustement par une population de son adaptation \u00e0 un nouvel environnement\\nd. Le fait qu\u2019une population finisse par devenir deux populations distinctes\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Texte: Le pillage g\u00e9n\u00e9ralis\u00e9 se serait poursuivi pendant la nuit, les forces de l'ordre n'\u00e9tant pas pr\u00e9sentes dans les rues de Bichkek. Un observateur a d\u00e9crit Bichkek comme \u00e9tant en train de sombrer dans un \u00e9tat d\u2019\u00ab anarchie \u00bb, tandis que la population se d\u00e9pla\u00e7ait en bandes dans les rues et pillait les magasins de biens de consommation. Plusieurs habitants de Bichkek ont reproch\u00e9 les manifestants du sud d'\u00eatre responsables de l'anarchie.\\nQuestion: Qui a accus\u00e9 les manifestants du sud de pillage\\xa0?\\nChoix:\\na. Des habitants de Bichkek\\nb. Les forces de l\u2019ordre\\nc. Les anarchistes\\nd. Des bandes de personnes\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Texte: Dans de nombreuses r\u00e9gions du monde, faire un signe de la main est un geste amical signifiant \u00ab\\u2009bonjour\\u2009\u00bb. En revanche, en Malaisie, du moins chez les Malais des zones rurales, cela signifie \u00ab viens par ici \u00bb, comme le fait de plier l'index vers soi, geste utilis\u00e9 dans certains pays occidentaux, et il ne devrait \u00eatre utilis\u00e9 qu'en ce sens. De m\u00eame, un voyageur britannique en Espagne pourrait confondre un signe d'adieu fait par une personne qui tourne la paume de sa main vers elle-m\u00eame (plut\u00f4t que vers la personne \u00e0 qui elle adresse le signe) avec une invitation \u00e0 revenir.\\nQuestion: Dans les zones rurales de la Malaisie, quel geste signifie \u00ab viens par ici \u00bb ?\\nChoix:\\na. Plier l\u2019index\\nb. Faire un signe de la main\\nc. Faire un \u00ab high five \u00bb\\nd. Lever le pouce\",\n  \"label\": \"b\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Les questions suivantes sont des questions \u00e0 choix multiples (avec r\u00e9ponses).\n</code></pre></li> <li>Base prompt template:   <pre><code>Question: {text}\nChoix:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nR\u00e9ponse: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Question: {text}\nChoix:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nR\u00e9pondez \u00e0 la question ci-dessus par 'a', 'b', 'c' ou 'd', et rien d'autre.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset belebele-fr\n</code></pre>"},{"location":"datasets/french/#knowledge","title":"Knowledge","text":""},{"location":"datasets/french/#mmlu-fr","title":"MMLU-fr","text":"<p>This dataset is a machine translated version of the English MMLU dataset and features questions within 57 different topics, such as elementary mathematics, US history and law. The translation to French was done by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 269 / 1,410 / 13,200 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). These splits are new and there can thus be some overlap between the original validation and test sets and our validation and test sets.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"En 2013, la part des personnes en Ethiopie qui pensent que les partis politiques sont corrompus est\\nChoix:\\na. 24%\\nb. 44%\\nc. 64%\\nd. 84%\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Combien de nombres entiers positifs et n\u00e9gatifs $12$ est-il un multiple?\\nChoix:\\na. 3\\nb. 12\\nc. 4\\nd. 6\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Quelle affirmation suivante concernant les r\u00e9actions d\u00e9pendantes de la lumi\u00e8re de la photosynth\u00e8se est correcte?\\nChoix:\\na. Ils fournissent le carbone qui est incorpor\u00e9 dans le sucre.\\nb. Ils produisent du PGA, qui est converti en glucose par la fixation du carbone dans les r\u00e9actions ind\u00e9pendantes de la lumi\u00e8re.\\nc. L'eau est s\u00e9par\u00e9e en fournissant des ions hydrog\u00e8ne et des \u00e9lectrons \u00e0 la NADP pour un stockage temporaire.\\nd. Ils se produisent dans le stroma des chloroplastes.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Les questions suivantes sont des questions \u00e0 choix multiples (avec r\u00e9ponses).\n</code></pre></li> <li>Base prompt template:   <pre><code>Question: {text}\nChoix:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nR\u00e9ponse: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Question: {text}\nChoix:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nR\u00e9pondez \u00e0 la question ci-dessus par 'a', 'b', 'c' ou 'd', et rien d'autre.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset mmlu-fr\n</code></pre>"},{"location":"datasets/french/#common-sense-reasoning","title":"Common-sense Reasoning","text":""},{"location":"datasets/french/#hellaswag-fr","title":"HellaSwag-fr","text":"<p>This dataset is a machine translated version of the English HellaSwag dataset. The original dataset was based on both video descriptions from ActivityNet as well as how-to articles from WikiHow. The dataset was translated by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 9,310 samples. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"[header] Comment dire \u00e0 vos enfants que vous allez divorcer [title] Contr\u00f4lez vos \u00e9motions. [step] Vos enfants seront probablement en col\u00e8re et boulevers\u00e9s lorsque vous leur annoncerez le divorce, essayez donc de ne pas r\u00e9agir de la m\u00eame mani\u00e8re. Attendez de rompre la nouvelle lorsque vous pourrez discuter du sujet de mani\u00e8re efficace et rester ma\u00eetre de vos \u00e9motions.\\nChoix:\\na. Rappelez-vous, le but de la discussion est d'\u00eatre l\u00e0 pour les enfants - ils ne devraient pas avoir \u00e0 vous r\u00e9conforter. [title] Essayez de le faire ensemble, si possible.\\nb. [substeps] Trouvez un moyen d'\u00e9viter que vos enfants ne vous agressent verbalement. Assurez-vous d'\u00eatre calme et pos\u00e9 et ne donnez pas l'impression que la nouvelle du divorce est quelque chose qui vous d\u00e9range.\\nc. [substeps] Si vos enfants ont du mal \u00e0 comprendre la nouvelle \u00e0 distance, posez-leur des questions lors d'une conversation intime et priv\u00e9e. Laissez-les utiliser les questions pour traiter et comprendre ce qu'ils ressentent \u00e0 propos de l'annonce.\\nd. [substeps] Si vous ne voulez pas qu'ils le sachent imm\u00e9diatement, partez en silence et r\u00e9fl\u00e9chissez un peu plus longtemps avant de leur dire. Cherchez un endroit confortable pour vous deux pour parler en priv\u00e9, afin que vous puissiez tous deux prendre du temps pour traiter vos sentiments et accepter la situation.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Certains stands servent des hot-dogs aux gens alors qu'ils p\u00eachent sur la glace. Un petit gar\u00e7on et une petite fille tentent d'attraper un poisson. ils\\nChoix:\\na. attrapent un poisson et continuent de nager.\\nb. sont interview\u00e9s pendant qu'ils p\u00eachent.\\nc. essaient \u00e0 plusieurs reprises, errant tout pr\u00e8s de leur poisson.\\nd. sont rapidement emport\u00e9s par le courant alors qu'ils luttent pour s'\u00e9loigner du banc de la rivi\u00e8re et pagayent pour \u00e9chapper \u00e0 de l\u00e9g\u00e8res infestations de poissons dans l'eau\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"[header] Comment se calmer [title] Respirer. [step] Respirer. Lentement.\\nChoix:\\na. Concentrez-vous sur votre respiration et d\u00e9tendez votre corps. Continuez \u00e0 inspirer et expirer lentement par le nez, en mettant une pression sur votre diaphragme et vos muscles fessiers (vos poumons).\\nb. Si votre c\u0153ur bat vite ou fort, vous pourriez \u00eatre en danger de tachycardie, d'AVC ou de toute autre crise cardiaque. [title] Allongez-vous sur le dos et inspirez et expirez profond\u00e9ment.\\nc. Inspirez pendant 5 secondes; retenez votre souffle pendant 5 secondes, puis expirez pendant 5 secondes. Cela fonctionne parce que vous faites l'oppos\u00e9 de ce qu'une personne excit\u00e9e ferait.\\nd. Inspirez pendant un compte de cinq et abaissez-vous. Expirez, expirez quatre fois de plus, aussi profond\u00e9ment que vous pouvez sentir, et r\u00e9p\u00e9tez pour un total de dix.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Les questions suivantes sont des questions \u00e0 choix multiples (avec r\u00e9ponses).\n</code></pre></li> <li>Base prompt template:   <pre><code>Question: {text}\nChoix:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nR\u00e9ponse: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Question: {text}\nChoix:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nR\u00e9pondez \u00e0 la question ci-dessus par 'a', 'b', 'c' ou 'd', et rien d'autre.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset hellaswag-fr\n</code></pre>"},{"location":"datasets/french/#summarization","title":"Summarization","text":""},{"location":"datasets/french/#orange-sum","title":"Orange Sum","text":"<p>This dataset was published in this paper and consists of news articles from Orange Actu. The summaries were written by the journalists themselves (the \"abstract\" field in the original dataset).</p> <p>The original full dataset consists of 21,401 / 1,500 / 1,500 samples for training, validation and testing, respectively. We use 1,024 / 256 / 1,024 samples for training, validation, and testing, respectively. All our splits are subsets of the original ones.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"R\u00e9clam\u00e9 puis annonc\u00e9 par Emmanuel Macron, le d\u00e9bat parlementaire sur l'immigration s'est ouvert ce lundi 7 octobre avec une allocution d'Edouard Philippe devant les d\u00e9put\u00e9s. Le Premier ministre a commenc\u00e9 son discours en empruntant les mots d'un de ses pr\u00e9d\u00e9cesseurs, Michel Rocard. Il a ensuite fait \u00e9tat d'un syst\u00e8me fran\u00e7ais d'asile \\\"satur\u00e9\\\". \\\"En 2018, la France a enregistr\u00e9 le record de 123.000 demandes d'asile\\\", a t-il rappel\u00e9, estimant que la France \\\"n'a pas atteint tous\\\" ses objectifs en mati\u00e8re de politique migratoire et de lutte contre l'immigration irr\u00e9guli\u00e8re. \\\"La question d'un pilotage par objectifs de l'admission au s\u00e9jour n'est pas tabou. Je n'ai pas peur de r\u00e9fl\u00e9chir \u00e0 l'id\u00e9e de quotas. Il nous faut donc regarder sujet apr\u00e8s sujet. On sait depuis longtemps que les quotas ne s'appliquent ni \u00e0 l'asile ni \u00e0 l'immigration familiale. Pour autant, celle-ci ne pourrait \u00e9chapper \u00e0 toute ma\u00eetrise. Il faut lutter contre les abus et les fraudes, et resserrer les crit\u00e8res l\u00e0 o\u00f9 cela s'impose\\\" a t-il poursuivi.Le Premier ministre a en revanche balay\u00e9 l'id\u00e9e de la fin du droit du sol, r\u00e9clam\u00e9e par des \u00e9lus de droite. \\\"Je ne vois pas bien en quoi \u00e0 l'\u00e9chelle du pays, la fin du droit du sol serait une r\u00e9ponse\\\". Il a \u00e9galement adress\u00e9 une critique virulente \u00e0 l'\u00e9gard de la th\u00e9orie de \\\"l'immigration de remplacement\\\", un \\\"vocable d'une laideur certaine qui fait appel aux ressorts les plus d\u00e9testables du complotisme.Ces th\u00e9ories \\\"inspiraient encore r\u00e9cemment des discours dont j'ai eu l'occasion de dire qu'ils \u00e9taient profond\u00e9ment contraires \u00e0 l'id\u00e9e dont nous nous faisons de la France et de la R\u00e9publique\\\" a t-il encore ass\u00e9n\u00e9, en r\u00e9f\u00e9rence \u00e0 la r\u00e9cente \\\"Convention de la droite\\\" organis\u00e9e le 28 septembre dernier autour de Marion Mar\u00e9chal et Eric Zemmour.\",\n  \"target_text\": \"Le Premier ministre a ouvert ce lundi 7 octobre le d\u00e9bat sur l'immigration \u00e0 l'Assembl\u00e9e nationale, d\u00e9clarant que le syst\u00e8me fran\u00e7ais d'asile est aujourd'hui \\\"satur\u00e9\\\". Il a au passage pourfendu la th\u00e9orie de \\\"l'immigration de remplacement\\\", qui fait selon lui appel \\\"aux ressorts les plus d\u00e9testables du complotisme\\\".\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Un supermarch\u00e9 a \u00e9t\u00e9 d\u00e9truit par une explosion, samedi 2 janvier, \u00e0 Grasse, dans les Alpes-Maritimes, a rapport\u00e9 France 3. Aucun bless\u00e9 n'est \u00e0 d\u00e9plorer.L'explosion s'est produite vers 6h du matin dans ce supermarch\u00e9 Aldi de Grasse. Elle a \u00e9t\u00e9 suivie par un violent incendie. Le b\u00e2timent a \u00e9t\u00e9 \\\"totalement d\u00e9truit\\\", selon le maire de la ville, qui a \u00e9voqu\u00e9 une cause \\\"accidentelle\\\" sur sa page Facebook. Une centaine de pompiers, ainsi que des policiers ont \u00e9t\u00e9 mobilis\u00e9s pour lutter contre le sinistre et s\u00e9curiser le p\u00e9rim\u00e8tre.Selon Nice-Matin, deux employ\u00e9es du supermarch\u00e9 ont \u00e9t\u00e9 souffl\u00e9es par l'explosion en allumant la lumi\u00e8re au moment d'arriver sur leur lieu de travail. Aucune des deux n'a \u00e9t\u00e9 bless\u00e9e physiquement, mais elles sont tr\u00e8s choqu\u00e9es.Vers 9h, le feu \u00e9tait ma\u00eetris\u00e9, a indiqu\u00e9 \u00e0 France 3 un porte-parole du Service d'incendie et de secours des Alpes-Maritimes. Soixante pompiers et 40 engins de secours \u00e9taient toujours mobilis\u00e9s sur place.\",\n  \"target_text\": \"Une centaine de pompiers ont \u00e9t\u00e9 mobilis\u00e9s pour lutter contre l'incendie.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Trois ans et demi apr\u00e8s la d\u00e9cision des Britanniques de quitter l'Union europ\u00e9enne, le Brexit est finalement intervenu vendredi 31 janvier. Une mesure qui va s\u00e9rieusement changer la donne pour les Britanniques qui si\u00e8gent aujourd'hui dans les conseils municipaux en France. Comme tous les citoyens europ\u00e9ens, les Britanniques avaient jusqu'\u00e0 pr\u00e9sent le droit de vote et d'\u00e9ligibilit\u00e9 aux \u00e9lections municipales fran\u00e7aises. Actuellement sur 2.493 conseillers \u00e9trangers, 757 viennent du Royaume-Uni, soit environ 30%, selon le R\u00e9pertoire national des \u00e9lus. Ils sont nettement plus nombreux que les Belges (544 \u00e9lus) et les Portugais (357). Ils r\u00e9sident pour la plupart dans un grand quart Sud-Ouest de la France : Charente (70 \u00e9lus), Dordogne (59), Aude (52), Haute-Vienne (40), Lot-et-Garonne (31), H\u00e9rault (30), Deux S\u00e8vres (28), Gers (26), Lot (23)...Or, avec le Brexit, ils ne pourront pas briguer de nouveau mandat, \u00e0 moins d'avoir acquis une autre nationalit\u00e9 europ\u00e9enne depuis les derni\u00e8res \u00e9lections. C'est notamment le cas \u00e0 Poupas, village de 85 habitants dans le Tarn-et-Garonne, o\u00f9 deux des trois conseillers municipaux britanniques, sur les 11 au total que compte la commune, ont obtenu la nationalit\u00e9 fran\u00e7aise. Le droit \\\"de payer et de se taire\\\"Pour certaines petites communes, o\u00f9 il est souvent difficile de trouver des candidats, c'est un vrai casse-t\u00eate. \u00c0 Perriers-en-Beauficel, dans la Manche, Patrick Head , originaire du Wiltshire (sud de l'Angleterre), va ainsi terminer son mandat. Le sexag\u00e9naire avait rafl\u00e9 pas moins de 89,74% des suffrages dans ce petit village normand, o\u00f9 il a \u00e9lu domicile en 2004. Soit le meilleur score de cette commune de 216 habitants, o\u00f9 les \u00e9lecteurs peuvent rayer ou ajouter un nom. \\\"\u00c7a va nous manquer car Patrick nous aidait beaucoup\\\", regrette la maire Lydie Brionne, qui explique que son colistier faisait \\\"le lien\\\" avec la cinquantaine de Britanniques install\u00e9s dans ce coin de campagne normande. \u00c0 Perriers-en-Beauficel, sur les onze \u00e9lus de 2014, deux sont Britanniques. \\\"Il va falloir trouver deux nouveaux candidats. C'est difficile de trouver des gens motiv\u00e9s dans une petite commune\\\", souligne la maire, par ailleurs \u00e9leveuse de vaches laiti\u00e8res. \\\"Depuis 20 ans, beaucoup de Britanniques se sont install\u00e9s, ils ont repeupl\u00e9 la commune, \u00e7a a donn\u00e9 du dynamisme\\\", raconte l'\u00e9lue. Avec le Brexit, \\\"j'ai peur qu'ils soient oblig\u00e9s de repartir.\\\"Loin d'\u00eatre isol\u00e9, le cas de ce village normand se retrouve partout o\u00f9 les Britanniques sont fortement implant\u00e9s. \u00c0 Bellegarde-du-Raz\u00e8s, commune de 240 habitants dans l'Aude, les deux \u00e9lus d'Outre-Manche \\\"apportent une valeur ajout\u00e9e\\\" au village, avec \\\"leur importante implication dans le milieu associatif\\\", estime le maire Gilbert De Paoli. L'\u00c9cossaise Alisson Mackie, 63 ans, install\u00e9e depuis 2011, est d\u00e9pit\u00e9e de ne plus pouvoir se repr\u00e9senter en mars. \\\"On a construit notre maison ici, on paye des imp\u00f4ts ici, on consomme ici mais on a \u00e9t\u00e9 ray\u00e9s des listes \u00e9lectorales\\\", d\u00e9plore-t-elle.\u00c0 Jouac, village de 180 habitants en Haute-Vienne, la maire Virginie Windridge, 39 ans, elle-m\u00eame mari\u00e9e \u00e0 un Britannique, trouve aussi \\\"tr\u00e8s injuste que des gens qui sont l\u00e0 depuis des ann\u00e9es, payent des imp\u00f4ts et contribuent \u00e0 la vie de la commune, aient du jour au lendemain le droit 'de payer et de se taire'\\\". \\\"C'est dur \u00e0 avaler\\\", dit-elle.Les deux \u00e9lus britanniques actuels ont \\\"un apport important\\\", souligne la maire. \\\"D\u00e9j\u00e0 ils sont un relais avec la communaut\u00e9 britannique de la commune. Et puis ils apportent des id\u00e9es diff\u00e9rentes, une autre fa\u00e7on de fonctionner, de voir les choses\\\", d\u00e9crit Mme Windridge. \\\"Ils am\u00e8nent parfois un regard sur ce qui existe ou se fait ailleurs, une autre perspective\\\". \\\"Et, il faut bien le dire, culturellement, quelquefois, les Britanniques sont plus ouverts aux changements que nous, ont un peu moins peur de l'inconnu\\\", ajoute-t-elle en donnant en exemple la d\u00e9cision d'\u00e9teindre l'\u00e9clairage public nocturne. \\\"Les \u00e9lus britanniques \u00e9taient naturellement les plus ouverts sur cette id\u00e9e-l\u00e0, ils voyaient de suite le gagnant-gagnant, pour l'environnement et le budget de la commune\\\", estime-t-elle.\",\n  \"target_text\": \"\u00c0 l'heure actuelle, plus de 750 Britanniques si\u00e8gent dans les conseils municipaux en France. Or, avec la sortie du Royaume-Uni de l'Union europ\u00e9enne, ils ne pourront pas se repr\u00e9senter en mars prochain.\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>Les articles suivants sont accompagn\u00e9s d'un r\u00e9sum\u00e9.\n</code></pre></li> <li>Base prompt template:   <pre><code>Article de presse: {text}\nR\u00e9sum\u00e9: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Article de presse: {text}\n\nR\u00e9digez un r\u00e9sum\u00e9 de l'article ci-dessus.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset orange-sum\n</code></pre>"},{"location":"datasets/german/","title":"\ud83c\udde9\ud83c\uddea German","text":"<p>This is an overview of all the datasets used in the German part of EuroEval. The datasets are grouped by their task - see the task overview for more information about what these constitute.</p>"},{"location":"datasets/german/#sentiment-classification","title":"Sentiment Classification","text":""},{"location":"datasets/german/#sb10k","title":"SB10k","text":"<p>This dataset was published in this paper and is based on German tweets, which were manually annotated by three annotators.</p> <p>The original full dataset consists of 1,840 / 324 / 870 samples, and we use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively. The splits are new and there can thus be some overlap between the original validation and test sets and our validation and test sets.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"ALEMANHA (4-5-1): Neuer; Schmelzer, Hummels, Mertesacker, Lahm; G\u00fcndogan, Khedira, \u00d6zil, M\u00fcller, Reus; Klose\",\n  \"label\": \"positive\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"@user ok. Bin jetzt dann hernach gleich nochmal weg, aber schreib ruhig.\",\n  \"label\": \"neutral\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"@user Schw\u00fcle 34\u00b0, Tendenz steigend. #schrecklich\",\n  \"label\": \"negative\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Im Folgenden sind Tweets und ihre Stimmung aufgef\u00fchrt, die 'positiv', 'neutral' oder 'negativ' sein kann.\n</code></pre></li> <li>Base prompt template:   <pre><code>Tweet: {text}\nStimmungslage: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Tweet: {text}\n\nKlassifizieren Sie die Stimmung im Tweet. Antworten Sie mit 'positiv', 'neutral' oder 'negativ'.\n</code></pre></li> <li>Label mapping:<ul> <li><code>positive</code> \u27a1\ufe0f <code>positiv</code></li> <li><code>neutral</code> \u27a1\ufe0f <code>neutral</code></li> <li><code>negative</code> \u27a1\ufe0f <code>negativ</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset sb10k\n</code></pre>"},{"location":"datasets/german/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"datasets/german/#germeval","title":"GermEval","text":"<p>This dataset was published in this paper and is based on German Wikipedia as well as news articles, and was manually annotated. It roughly follows the CoNLL-2003 format, but also allows overlapping entities and derived entities (such as \"English\" for \"England\"). We remove the derived entities and convert the partially overlapping entities to non-overlapping entities (e.g., <code>B-ORGpart</code> to <code>B-ORG</code>).</p> <p>The original full dataset consists of 24,000 / 2,200 / 5,100 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 1,024 split for training,</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  'tokens': array(['Am', 'Ende', 'der', 'Saison', '2006/07', 'soll', 'es', 'f\u00fcr', 'die', 'L\u00f6wen', 'wieder', 'zu', 'einem', 'Europapokal-Platz', 'reichen', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'B-LOC', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['In', 'einer', 'Stichwahl', 'gegen', 'seinen', 'Vorg\u00e4nger', 'Georg', 'Kronawitter', 'wurde', 'Erich', 'Kiesl', 'am', '1.', 'April', '1984', 'abgew\u00e4hlt', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['Noch', 'im', '13.', 'Jahrhundert', 'wurde', 'sie', 'in', 'manchen', 'Handschriften', 'mit', 'der', 'Christherre-Chronik', 'verschmolzen', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O'], dtype=object)\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>Es folgen S\u00e4tze und JSON-W\u00f6rterb\u00fccher mit den benannten Entit\u00e4ten, die in der angegebenen Phrase vorkommen.\n</code></pre></li> <li>Base prompt template:   <pre><code>Satz: {text}\nBenannte Entit\u00e4ten: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Satz: {text}\n\nIdentifizieren Sie die benannten Entit\u00e4ten im Satz. Sie sollten dies als JSON-W\u00f6rterbuch mit den Schl\u00fcsseln 'person', 'ort', 'organisation' und 'verschiedenes' ausgeben. Die Werte sollten Listen der benannten Entit\u00e4ten dieses Typs sein, genau wie sie im Satz erscheinen.\n</code></pre></li> <li>Label mapping:<ul> <li><code>B-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>ort</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>ort</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>organisation</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>organisation</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>verschiedenes</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>verschiedenes</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset germeval\n</code></pre>"},{"location":"datasets/german/#linguistic-acceptability","title":"Linguistic Acceptability","text":""},{"location":"datasets/german/#scala-de","title":"ScaLA-de","text":"<p>This dataset was published in this paper and was automatically created from the German Universal Dependencies treebank by assuming that the documents in the treebank are correct, and corrupting the samples to create grammatically incorrect samples. The corruptions were done by either removing a word from a sentence, or by swapping two neighbouring words in a sentence. To ensure that this does indeed break the grammaticality of the sentence, a set of rules were used on the part-of-speech tags of the words in the sentence.</p> <p>The original dataset consists of 15,590 samples, from which we use 1,024 / 256 / 2,048 samples for training, validation and testing, respectively (so 3,328 samples used in total). These splits are used as-is in the framework.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Im In dem Sommer drau\u00dfen zu sitzen ist immer wieder eine \\\"Wonne\\\", so man noch einen Platz bekommt\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Eine 65 m lange Betonmauer tr\u00e4gt nachts einen Leucht - Schriftzug \\\"HOSTAL HOSTILE HOTEL HOSTAGE GOSTIN OSTILE HOSTEL HOSTIL HOST\\\", was in seinem etymologischen Wortspiel so viel bedeutet, dass aus einem feindlichen ein gastfreundlicher Ort geworden ist, in Anspielung auf das auf dem Gel\u00e4nde des ehemaligen Frauenlagers genau gegen\u00fcber liegende Novotel Goldene Bremm (heute Mercure Saarbr\u00fccken - S\u00fcd), das konzeptionell insoweit in die Idee einbezogen ist.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Allerdings wurde nachgewiesen, dass sich der ebenfalls in Extremlebensr\u00e4umen vorkommende Nematode Halicephalobus mephisto im in dem Labor bevorzugt Desulforudis audaxviator ern\u00e4hrt, wenn er eine Wahl hat (Alternative: E. coli).\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Die folgenden S\u00e4tze und ob sie grammatikalisch korrekt sind.\n</code></pre></li> <li>Base prompt template:   <pre><code>Satz: {text}\nGrammatikalisch richtig: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Satz: {text}\n\nBestimmen Sie, ob der Satz grammatikalisch korrekt ist oder nicht. Antworten Sie mit 'ja', wenn der Satz korrekt ist und 'nein', wenn er es nicht ist.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>ja</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>nein</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scala-de\n</code></pre>"},{"location":"datasets/german/#reading-comprehension","title":"Reading Comprehension","text":""},{"location":"datasets/german/#germanquad","title":"GermanQuAD","text":"<p>This dataset was published in this paper and is based on German Wikipedia articles, and was manually annotated.</p> <p>The original full dataset consists of 11,518 / 2,204 samples for training and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). These splits are new and there can thus be some overlap between the original validation and test sets and our validation and test sets.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  'context': \"Mali\\n\\n=== Verwaltungsgliederung ===\\nDer Staat gliedert sich in zehn Regionen und den Hauptstadtdistrikt. Diese teilen sich in 49 Kreise ''(cercles)'' und 703 Gemeinden ''(communes)''. Die Regionen sind nach ihren Hauptst\u00e4dten benannt. Zwei dieser zehn Regionen, M\u00e9naka und Taoud\u00e9nit, wurden 2012 per Gesetzesbeschluss gebildet. Die Einrichtung ist seit 2016 im Gange.\\nDie Angaben der Regionen Gao und Timbuktu, aus denen die Regionen M\u00e9naka und Taoud\u00e9nit ausgegliedert wurden, spiegeln noch den Stand vor der Aufspaltung wider.\\nUm auch Fl\u00fcchtlinge und vor allem Nomaden in das Verwaltungssystem eingliedern zu k\u00f6nnen, entstanden sogenannte ''Fractions'' (''Fractions Nomades'', ein Begriff, den schon die Kolonialregierung nutzte), die es dementsprechend vor allem im Norden in der N\u00e4he von D\u00f6rfern gibt. Seit den gro\u00dfen Trockenphasen entstanden durch Wanderungsbewegungen solche Verwaltungseinheiten allerdings auch verst\u00e4rkt im S\u00fcden.\",\n  'question': 'Wie viele verschiedene Regionen hat Mali? ',\n  'answers': {\n    'answer_start': array([63], dtype=int32),\n    'text': array(['zehn Regionen und den Hauptstadtdistrikt'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': 'Iran\\n\\n=== Automobilindustrie ===\\nIn der Automobilindustrie waren 2010 rund 500.000 Menschen besch\u00e4ftigt, damit ist die Branche der zweitgr\u00f6\u00dfte Arbeitgeber nach der \u00d6lindustrie und der Iran der gr\u00f6\u00dfte Automobilproduzent im Mittleren Osten. 2012 ist die Automobilproduktion des Iran jedoch scharf eingebrochen; es wurden nur noch 989.110 Fahrzeuge produziert \u2013 40 Prozent weniger als 2011. Darunter fallen 848.000 PKW und 141.110 Nutzfahrzeuge.\\nDie beiden gr\u00f6\u00dften Automobilhersteller sind die staatliche SAIPA \u2013 derzeit im Privatisierungsprozess \u2013 und Iran Khodro (IKCO). Die IKCO produziert neben einheimischen Modellen wie Dena und Runna in Lizenz Modelle u.\\xa0a. von Peugeot. SAIPA hat die IKCO im Jahr 2010 das erste Mal in der Rangfolge \u00fcberholt. Nach Ansicht des Business Monitor International\u2019s Iran Autos Report wird sich die Belastbarkeit der iranischen Automobilindustrie erst in den n\u00e4chsten Jahren zeigen, wenn der einheimische Markt ges\u00e4ttigt ist und der Iran zunehmend auf dem internationalen Markt agiert, denn bisher ist der Produktionsanstieg noch \u00fcberwiegend auf die Unterst\u00fctzung der Regierung zur\u00fcckzuf\u00fchren. 12,64 % der zugelassenen Kraftfahrzeuge werden mit Gas betrieben. Der Iran liegt damit weltweit an f\u00fcnfter Stelle der Nutzung von gasbetriebenen Kraftfahrzeugen.\\nDer schwedische LKW-Produzent Scania er\u00f6ffnete 2011 eine neue Produktionslinie in Qazvin und l\u00f6st damit Daimler-Chrysler ab, das seine Gesch\u00e4ftskontakte mit dem Iran abgebrochen hat.',\n  'question': 'Wie hei\u00dfen die Automodelle von Iran Khodro?',\n  'answers': {\n    'answer_start': array([622], dtype=int32),\n    'text': array([' Dena und Runna'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': 'Griechenland\\n\\n=== Klima ===\\nGriechenland hat \u00fcberwiegend ein mediterranes Klima mit feucht-milden Wintern und trocken-hei\u00dfen Sommern. An der K\u00fcste ist es im Winter sehr mild und es regnet h\u00e4ufig; Schnee f\u00e4llt nur selten. Die Sommer sind relativ hei\u00df und es gibt nur gelegentlich Sommergewitter. Mit 48\u00b0 wurde 1977 in Griechenland der kontinentaleurop\u00e4ische Hitzerekord gemessen.\\nIm Landesinneren ist es vor allem im Winter deutlich k\u00fchler und es gibt h\u00e4ufig Nachtfrost, manchmal auch starke Schneef\u00e4lle. Der Fr\u00fchling ist kurz, verw\u00f6hnt aber \u201emit einem Feuerwerk aus Lavendel und Anemonen, Klatschmohn und Kamille\u201c. Im Sommer ist es \u00e4hnlich wie an der K\u00fcste hei\u00df und trocken. Die j\u00e4hrlichen Niederschl\u00e4ge schwanken zwischen 400 und 1000\\xa0mm. Da Griechenland sehr gebirgig ist, ist Wintersport durchaus m\u00f6glich, es existieren 19 Wintersportgebiete unterschiedlicher Gr\u00f6\u00dfe. Ein kleiner Teil im Nordwesten des Festlandes liegt in der gem\u00e4\u00dfigten Klimazone.',\n  'question': 'Wie oft schneit es in Griechenland?',\n  'answers': {\n    'answer_start': array([209], dtype=int32),\n    'text': array(['nur selten'], dtype=object)\n  }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>Im Folgenden finden Sie Texte mit den dazugeh\u00f6rigen Fragen und Antworten.\n</code></pre></li> <li>Base prompt template:   <pre><code>Text: {text}\nFragen: {question}\nFragen Antwort in maximal 3 W\u00f6rtern: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Text: {text}\n\nBeantworten Sie die folgende Frage zum obigen Text in h\u00f6chstens 3 W\u00f6rtern.\n\nFrage: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset germanquad\n</code></pre>"},{"location":"datasets/german/#unofficial-belebele-de","title":"Unofficial: BeleBele-de","text":"<p>This dataset was published in this paper and features multiple-choice reading comprehension questions across 122 languages.</p> <p>The original dataset contains 900 unique multiple-choice reading comprehension passages and questions. From these, we use a 256 / 64 / 580 split for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Text: Es gibt viele Dinge, die Sie vor und w\u00e4hrend einer Reise ber\u00fccksichtigen m\u00fcssen. Erwarten Sie nicht, dass die Dinge beim Reisen genau so sind wie \u201ezuhause\u201c. Umgangsformen, Gesetze, Essen, Verkehr, Unterk\u00fcnfte, Standards, Spache und so weiter werden zu einem gewissen Grad anders sein als dort, wo Sie leben. Dies ist etwas, was man immer im Hinterkopf behalten sollte, um Entt\u00e4uschung oder gar Abneigung \u00fcber lokale Vorgehensweisen zu vermeiden.\\nFragen: Was kann Reisenden dem Abschnitt nach helfen, Entt\u00e4uschung beim Besuch neuer Orte zu vermeiden?\\nAntwortm\u00f6glichkeiten:\\na. \u00c4hnliche Standards wie zuhause erwarten\\nb. Essen probieren, das ungewohnt ist\\nc. Die gleichen Gesetze wie zuhause einhalten\\nd. Nicht vorher nach Unterk\u00fcnften recherchieren\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Text: Genehmigungen m\u00fcssen im Voraus bestellt werden. Sie ben\u00f6tigen eine Genehmigung, um in La Sirena zu \u00fcbernachten. Sirena ist die einzige Rangerstation, die neben Zelten auch \u00dcbernachtung im Schlafsaal und warme Mahlzeiten anbietet. La Leona, San Pedrillo und Los Patos bieten nur Camping ohne Verpflegung an. Es ist m\u00f6glich, eine Parklizenz direkt bei der Rangerstation in Puerto Jim\u00e9nez zu bekommen, aber sie akzeptieren keine Kreditkarten Die Parkverwaltung (MINAE) stellt Genehmigungen  f\u00fcr den Park nicht fr\u00fcher als einen Monat vor der geplanten Ankunft aus. CafeNet El Sol bietet einen Reservierungsservice gegen eine Geb\u00fchr von 30 US-Dollar bzw. 10 US-Dollar f\u00fcr Tageskarten an. Einzelheiten dazu findet man auf deren Corcovado-Seite.\\nFragen: Welche der folgenden Rangerstationen bietet zwei \u00dcbernachtungsm\u00f6glichkeiten an?\\nAntwortm\u00f6glichkeiten:\\na. Sirena\\nb. Los Patos\\nc. La Leona\\nd. San Pedrillo\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Text: Naturnaher Tourismus zieht Leute an, die daran interessiert sind, Naturgebiete zu besuchen, um die Landschaft zu genie\u00dfen, einschlie\u00dflich der wilden Pflanzen und Tiere. Beispiele f\u00fcr Aktivit\u00e4ten vor Ort sind Jagen, Angeln, Fotografie, Vogelbeobachtung, der Besuch von Parks und das Lernen von Informationen \u00fcber das \u00d6kosystem. Ein Beispiel daf\u00fcr ist der Besuch, das Fotografieren und das Studieren von Orangutangs in Borneo.\\nFragen: Welche der folgenden Aktivit\u00e4ten ist kein Beispiel f\u00fcr naturnahen Tourismus?\\nAntwortm\u00f6glichkeiten:\\na. Wandern zu einem Wasserfall\\nb. Fotografieren von Wildblumen\\nc. Besuch eines Wissenschaftsmuseum\\nd. Fliegenfischen\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Die folgenden Fragen sind Multiple-Choice-Fragen (mit Antworten).\n</code></pre></li> <li>Base prompt template:   <pre><code>Frage: {text}\nAntwort: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Frage: {text}\nAntwortm\u00f6glichkeiten:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBeantworten Sie die obige Frage mit 'a', 'b', 'c' oder 'd', und nichts anderes.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset belebele-de\n</code></pre>"},{"location":"datasets/german/#knowledge","title":"Knowledge","text":""},{"location":"datasets/german/#mmlu-de","title":"MMLU-de","text":"<p>This dataset is a machine translated version of the English MMLU dataset and features questions within 57 different topics, such as elementary mathematics, US history and law. The translation to German was done by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 269 / 1,410 / 13,200 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). These splits are new and there can thus be some overlap between the original validation and test sets and our validation and test sets.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Teotihuac\u00e1n wurde im Becken von Mexiko bekannt, nachdem sein Rivale Cuicuilco,\\nAntwortm\u00f6glichkeiten:\\na. von einem Vulkanausbruch gel\u00e4hmt wurde.\\nb. einem B\u00fcrgerkrieg unter seinen herrschenden Familien erlag.\\nc. unter einer Ernteplage litt.\\nd. von einem Hurrikan an der Golfk\u00fcste \u00fcberschwemmt wurde.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Wer von den folgenden ist der industrielle Philanthrop?\\nAntwortm\u00f6glichkeiten:\\na. Frederick Taylor\\nb. Seebohm Rowntree\\nc. Henry Ford\\nd. Max Weber\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Verglichen mit der Varianz der Maximum-Likelihood-Sch\u00e4tzung (MLE) ist die Varianz der Maximum-A-Posteriori (MAP)-Sch\u00e4tzung ________\\nAntwortm\u00f6glichkeiten:\\na. h\u00f6her\\nb. gleich\\nc. niedriger\\nd. es kann jede der obigen Optionen sein\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Die folgenden Fragen sind Multiple-Choice-Fragen (mit Antworten).\n</code></pre></li> <li>Base prompt template:   <pre><code>Frage: {text}\nAntwort: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Frage: {text}\nAntwortm\u00f6glichkeiten:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBeantworten Sie die obige Frage mit 'a', 'b', 'c' oder 'd', und nichts anderes.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset mmlu-de\n</code></pre>"},{"location":"datasets/german/#unofficial-arc-de","title":"Unofficial: ARC-de","text":"<p>This dataset is a machine translated version of the English ARC dataset and features US grade-school science questions. The translation to German was done by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 1,110 / 297 / 1,170 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively (so 2,304 samples used in total). All new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Callahan zitiert die Ergebnisse des Oregon Death with Dignity Legal Defense and Education Center, wonach es \\\"nach vier vollen Jahren keine Missteps, Missbr\u00e4uche oder Zwangstendenzen\\\" bez\u00fcglich der Gesetze zur Euthanasie gab. Er argumentiert dagegen, dass\\nAntwortm\u00f6glichkeiten:\\na. sie dies ohne eine anonyme Umfrage nicht sicher wissen k\u00f6nnen.\\nb. andere Studien haben widerspr\u00fcchliche Ergebnisse gefunden.\\nc. selbst wenn das Ergebnis wahr ist, ist es irrelevant f\u00fcr den moralischen Status der Euthanasie.\\nd. die Ergebnisse sind verd\u00e4chtig, weil die Studie von Bef\u00fcrwortern der Euthanasie durchgef\u00fchrt wurde.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>  \"text\": \"Eine Frau besa\u00df ein Land im absoluten Besitz. Die Frau \u00fcbertrug das Land an einen Freund \u201cauf Lebenszeit\u201d und als der Freund starb, sollte das Land an den Nachbarn der Frau \\\"und ihre Erben\\\" weitergegeben werden. Der Nachbar starb und in ihrem ordnungsgem\u00e4\u00df beglaubigten Testament vermachte sie ihre gesamte Hinterlassenschaft an eine \u00f6rtliche Wohlt\u00e4tigkeitsorganisation. Wenn sie intestat gestorben w\u00e4re, w\u00e4re ihre Tochter ihre einzige Erbin gewesen. Ein Jahr nach dem Tod des Nachbarn \u00fcbertrug dessen Tochter durch eine Quitclaims-Erkl\u00e4rung jegliches Interesse, das sie an dem Land haben k\u00f6nnte, an die Freundin der Frau. Das Common Law Rule Against Perpetuities ist in der Gerichtsbarkeit unver\u00e4ndert. Es gibt keine weiteren anwendbaren Gesetze. Wer hat welches Interesse an dem Land?\\nAntwortm\u00f6glichkeiten:\\na. Der Freund hat einen absoluten Besitz, weil sein Lebensbesitz mit dem verbleibenden Teil verschmolzen ist, den ihm die Tochter \u00fcbertragen hat.\\nb. Der Freund hat einen Lebensbesitz und die Wohlt\u00e4tigkeitsorganisation hat einen unverfallbaren Rest, weil das Interesse des Nachbarn \u00fcbertragbar war.\\nc. Der Freund hat einen Lebensbesitz und die Tochter hat einen unverfallbaren Rest, weil die Erkl\u00e4rung der Frau ein Interesse an den Erben des Nachbarn schuf.\\nd. Der Freund hat einen Lebensbesitz und die Frau hat einen R\u00fcckfall, weil das Restinteresse des Nachbarn gem\u00e4\u00df dem Rule Against Perpetuities ung\u00fcltig war.\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"F\u00fcr welches dieser beiden Szenarien tut der Hauptcharakter (der ich/mich/mein verwendet) etwas, das nach den gew\u00f6hnlichen moralischen Standards in den USA im Jahr 2020 eindeutig moralisch falsch ist? Szenario 1 | Ich gehe gerne in die Kirche, weil es mich in h\u00f6chstem Ma\u00dfe Gott nahe f\u00fchlen l\u00e4sst. Szenario 2 | Ich habe den Test meines Nachbarn angesehen, um eine gute Note zu bekommen.\\nAntwortm\u00f6glichkeiten:\\na. Falsch, Falsch\\nb. Falsch, Nicht falsch\\nc. Nicht falsch, Falsch\\nd. Nicht falsch, Nicht falsch\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Die folgenden Fragen sind Multiple-Choice-Fragen (mit Antworten).\n</code></pre></li> <li>Base prompt template:   <pre><code>Frage: {text}\nAntwort: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Frage: {text}\nAntwortm\u00f6glichkeiten:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBeantworten Sie die obige Frage mit 'a', 'b', 'c' oder 'd', und nichts anderes.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset arc-de\n</code></pre>"},{"location":"datasets/german/#common-sense-reasoning","title":"Common-sense Reasoning","text":""},{"location":"datasets/german/#hellaswag-de","title":"HellaSwag-de","text":"<p>This dataset is a machine translated version of the English HellaSwag dataset. The original dataset was based on both video descriptions from ActivityNet as well as how-to articles from WikiHow. The dataset was translated by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 9,310 samples. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"[header] Wie man sich trennt, wenn Kinder involviert sind [title] Erstellen Sie einen Trennungsplan mit Ihrem Partner. [step] Sie sollten sich auch auf das Gespr\u00e4ch mit Ihren Kindern vorbereiten, indem Sie vorher mit Ihrem Partner einen Plan f\u00fcr die Zukunft erstellen. Sie sollten gemeinsam besprechen, wer wo leben wird, wer f\u00fcr bestimmte t\u00e4gliche Bed\u00fcrfnisse und Aktivit\u00e4ten der Kinder verantwortlich sein wird und wann der offizielle Scheidungsprozess beginnen wird.\\nAntwortm\u00f6glichkeiten:\\na. Indem Sie hier\u00fcber klare Vorstellungen haben, k\u00f6nnen Sie Ihre Kinder besser beruhigen und einheitlich auftreten. [substeps] Zum Beispiel, k\u00f6nnten Sie vereinbaren, dass Ihr Partner auszieht und in einer nahegelegenen Wohnung oder einem anderen Haus lebt.\\nb. Sie beide sollten Ihre Aktionen in den Monaten bis zur Eheschlie\u00dfung sowie dar\u00fcber, wie Sie alles tun werden, planen, sobald das Kind wieder mit seinem Vater vereint ist. [title] Entscheiden Sie, was Sie mit dem Kind machen werden.\\nc. Stellen Sie sicher, dass Ihr Partner einverstanden ist und zustimmt, immer Pausen zu machen. [substeps] Sie sollten sich nun auf die Urlaubsdaten und Reisepl\u00e4ne einigen, zu denen Ihre Kinder gehen werden.\\nd. Der erste Schritt zu diesem Plan ist, ein Telefongespr\u00e4ch zu vereinbaren, damit Sie mit Ihrem Partner pers\u00f6nlich sprechen k\u00f6nnen. Sprechen Sie ruhig und deutlich, um den Ton f\u00fcr dieses Gespr\u00e4ch zu setzen.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"[header] Wie man Festival-Make-up macht [title] Bereiten Sie Ihr Gesicht vor. [step] Bevor Sie Ihr Augen-Make-up auftragen, m\u00fcssen Sie eine Basis schaffen. Dies hilft sicherzustellen, dass Ihr Augen-Make-up den ganzen Tag h\u00e4lt.\\nAntwortm\u00f6glichkeiten:\\na. [substeps] Zeichnen Sie eine runde, quadratische oder diagonale Linie um Ihr Auge. Verfolgen Sie den Kreis um Ihr Auge und ziehen Sie dann einen rechteckigen Streifen in der Mitte.\\nb. [substeps] Beginnen Sie mit einem sauberen, mit Feuchtigkeit versorgten Gesicht. Reinigen Sie Ihr Gesicht zun\u00e4chst mit einem sanften Reinigungsmittel und tragen Sie dann einen leichten Feuchtigkeitsspender auf Ihr Gesicht und Ihren Hals auf, um das Erscheinungsbild feiner Linien zu reduzieren.\\nc. Bevor Sie Lidschatten auftragen, w\u00e4hlen Sie einen einzelnen Lidschatten aus und messen Sie ihn so aus, dass er etwas gr\u00f6\u00dfer ist als das Auge, das Sie verblenden m\u00f6chten. Tragen Sie den Lidschatten auf die Spitze jedes Auges auf und streichen Sie mit einem Verblendpinsel dar\u00fcber.\\nd. Make-up am fr\u00fchen Morgen zu tragen ist nicht immer eine Option, aber Sie k\u00f6nnen es am Abend tun. [substeps] Duschen Sie, um Ihre Haut sauber und mit Feuchtigkeit versorgt zu halten.\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Wir sehen einen Mann in einem Orchester Grimassen schneiden. Der Mann steht dann auf und spielt die Violine. Wir sehen Menschen an Spinden. wir\\nAntwortm\u00f6glichkeiten:\\na. sehen Menschen in einem Bus.\\nb. sehen Menschen beim \u00dcben von Kampfsport und Musik spielen.\\nc. kehren zum Mann zur\u00fcck, der die Violine spielt.\\nd. sehen den Mann am Keyboard wieder.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Die folgenden Fragen sind Multiple-Choice-Fragen (mit Antworten).\n</code></pre></li> <li>Base prompt template:   <pre><code>Frage: {text}\nAntwort: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Frage: {text}\nAntwortm\u00f6glichkeiten:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBeantworten Sie die obige Frage mit 'a', 'b', 'c' oder 'd', und nichts anderes.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset hellaswag-de\n</code></pre>"},{"location":"datasets/german/#summarization","title":"Summarization","text":""},{"location":"datasets/german/#mlsum-de","title":"MLSum-de","text":"<p>This dataset was published in this paper and features news articles and their summaries in five languages, including German. The German part of the dataset is based on news articles from S\u00fcddeutsche Zeitung, with human-written summaries.</p> <p>The original full dataset consists of 221,000 / 11,400 / 10,700 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). All new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Jede neue Schlagzeile ein Stich ins Herz: F\u00fchrende Muslime beklagen in einem offenen Brief die wachsende \\\"Feindseligkeit\\\" gegen Migranten in Deutschland. Sie fordern Bundespr\u00e4sident Wulff auf, Stellung zu beziehen. In einem offenen Brief haben 15 namhafte deutsche Muslime Bundespr\u00e4sident Christian Wulff aufgefordert, in der schwelenden Debatte um Integrationsprobleme Stellung zu beziehen. Ausl\u00f6ser der Kontroverse war das Buch Deutschland schafft sich ab des SPD-Politikers und scheidenden Bundesbankvorstandes Thilo Sarrazin. Detailansicht \u00f6ffnen In der von SPD-Politiker und Noch-Bundesbanker Thilo Sarrazin ausgel\u00f6sten Integrationsdebatte fordern namhafte deutsche Muslime nun von Bundespr\u00e4sident Christian Wulff, Stellung zu beziehen. (Foto: dpa) Intellektuelle wie der Regisseur Fatih Akin und der Schriftsteller Feridun Zaimoglu beklagten in dem in der taz ver\u00f6ffentlichten Brief wachsende \\\"Feindseligkeit\\\" gegen Muslime in Deutschland. W\u00f6rtlich hei\u00dft es: \\\"F\u00fcr Musliminnen und Muslime ist derzeit nicht einmal der Gang zum Zeitungsh\u00e4ndler leicht, weil sie nie wissen, welche Schlagzeile, welches stereotype Bild sie dort erwartet.\\\" Die Unterzeichner erinnerten Wulff an seine Antrittsrede, in der er die Chancen der Integration betont hatte. \\\"Wir bitten Sie, gerade in der derzeitigen angespannten Stimmung f\u00fcr die Leits\u00e4tze einer offenen, von gegenseitigem Respekt gepr\u00e4gten demokratischen Kultur einzustehen und \u00f6ffentlich f\u00fcr sie zu werben\\\", hei\u00dft es in dem Appell an Wulff. Ausl\u00f6ser f\u00fcr den offenen Brief sei der Aufruf der Bild-Zeitung gewesen, an Pr\u00e4sident Wulff zu schreiben, sagte Shermin Langhoff, Intendantin des Berliner Theaters Ballhaus Naunynstra\u00dfe. \\\"Wir dachten uns, das k\u00f6nnen wir nicht so stehen lassen\\\", sagte die Mitunterzeichnerin zur SZ. Sie sprach von \\\"biologistischen Wahnthesen\\\" Sarrazins und hofft auf ein \\\"Wort der Vernunft\\\" aus Bellevue. Auch andere Unterzeichnerinnen setzen darauf, dass sich das Staatsoberhaupt in die Debatte einschaltet. Aylin Selcuk, Initiatorin des Vereins Deukische Generation, w\u00fcnscht sich ein starkes Zeichen Wulffs. Der Pr\u00e4sident m\u00f6ge zeigen, dass die Muslime in Deutschland dazugeh\u00f6ren. \\\"Wir bitten Sie: Bekennen Sie sich zu uns.\\\" Lamya Kaddor vom Liberal-Islamischen Bund sprach von einem \\\"\u00f6ffentlichen Bekenntnis\\\" des Pr\u00e4sidenten. In der laufenden Debatte gehe es nicht nur um Muslime, sondern um den \\\"Zusammenhalt in der Gesellschaft\\\", warnte Selcuk. Die Studentin hatte Sarrazin nach seinen \u00c4u\u00dferungen zur vererbten Intelligenz wegen Volksverhetzung angezeigt. Seitdem erreichten sie unz\u00e4hlige E-Mails, in denen sie geschm\u00e4ht und bedroht werde, sagte Selcuk. Nun hofft sie auf Wulff. \\\"Wir werden dieses Land nicht aufgeben\\\", hei\u00dft es in dem Brief an Christian Wulff. \\\"Dieses Land ist unsere Heimat und Sie sind unser Pr\u00e4sident.\\\"\",\n  \"target_text\": \"Jede neue Schlagzeile ein Stich ins Herz: F\u00fchrende Muslime beklagen in einem offenen Brief die wachsende \\\"Feindseligkeit\\\" gegen Migranten in Deutschland. Sie fordern Bundespr\u00e4sident Wulff auf, Stellung zu beziehen.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hoch flog der erste Schl\u00e4ger in die Luft, und viele andere Gegenst\u00e4nde folgten ihm. \u00dcberall auf dem Eis lag die Ausr\u00fcstung der deutschen Mannschaft zerstreut, Handschuhe, Helme, Schl\u00e4ger, weg damit, wer braucht so etwas schon, wenn er hemmungslos jubeln kann? In einer Ecke des Eises versammelten sich die Spieler der deutschen Eishockey-Mannschaft. Sie h\u00fcpften und tanzten und schrien, und wenn es nicht zu den Gepflogenheiten des Sports z\u00e4hlen w\u00fcrde, irgendwann zum H\u00e4ndesch\u00fctteln mit dem Gegner in der Mitte des Feldes zu erscheinen, dann h\u00e4tten sie wahrscheinlich noch eine ganze Weile so weitergemacht. Es war nun wirklich ein sporthistorischer Moment, den das Team des Deutschen Eishockey-Bundes (DEB) dort zelebrierte. Mit 4:3 (1:0, 3:1, 0:2) hatte es in einem ph\u00e4nomenalen Spiel den Rekord-Olympiasieger Kanada bezwungen und sich damit f\u00fcr das Finale des Turniers gegen die Olympischen Athleten aus Russland (5.10 Uhr MEZ) qualifiziert. Zum ersten Mal \u00fcberhaupt kann eine deutsche Mannschaft Olympiasieger werden, es ist der gr\u00f6\u00dfte Erfolg in der Geschichte des deutschen Eishockeys. \\\"Verr\u00fcckt, ne, verr\u00fcckt, verr\u00fcckte Welt\\\", sagte Bundestrainer Marco Sturm: \\\"Das ist einmalig.\\\" Ein ohnehin schon irres Turnier kulminiert in diesem 4:3 im Halbfinale Ja, einmalig war es in der Tat, was seine Mannschaft da geleistete hatte. Und es war interessant mitzuerleben, wie nach dem Spiel ein Akteur nach dem anderen in die Kabine trottete und sich unterwegs kurz den Journalisten stellte. Da war etwa der Torwart Danny aus den Birken, der v\u00f6llig ausgelaugt war. Oder Defensivspieler Moritz M\u00fcller, der seine Tr\u00e4nen kaum halten konnte. Oder die NHL-gest\u00e4hlten Routiniers Christian Erhoff und Marcel Goc, die schon so viel erlebt haben, aber so etwas wie an diesem Abend dann doch noch nicht. Keiner hatte schon so recht begriffen, was da geschehen war, und keiner wollte zu gro\u00dfen sportfachlichen Analysen ansetzen, als es um die Gr\u00fcnde f\u00fcr den Erfolg ging. Ein jeder sagte nur: Team. Mannschaft. Teamgeist. Mannschaftsgeist. Diese W\u00f6rter fallen oft im Sport, aber soweit sich das von au\u00dfen beurteilen l\u00e4sst, trifft das bei den Eishockey-Spielern tats\u00e4chlich zu. Sturm hat in den drei Jahren eine bemerkenswerte Mannschaft geformt, die ohnehin ein irres Turnier spielt. Das knappe 0:1 gegen Schweden in der Vorrunde, der Penalty-Sieg \u00fcber Norwegen, der Erfolg nach Verl\u00e4ngerung gegen die Schweiz, das denkw\u00fcrdige 4:3 gegen Schweden im Viertelfinale. Aber all das kulminierte jetzt in diesem 4:3 gegen Kanada im Halbfinale. In einem \\\"Jahrhundertspiel\\\", wie Alfons H\u00f6rmann, Pr\u00e4sident des Deutschen Olympischen Sportbundes, nicht ganz zu Unrecht schw\u00e4rmte.\",\n  \"target_text\": \"Nach dem sensationellen 4:3-Sieg gegen Kanada kann das deutsche Eishockey-Team erstmals Olympiasieger werden. Im Finale ist der Gegner der Favorit - doch die Mannschaft von Marco Sturm glaubt an sich.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Monatelang haben Sicherheitsbeh\u00f6rden nach Salah Abdeslam gefahndet. Jetzt ist der 26-j\u00e4hrige Terrorverd\u00e4chtige festgenommen worden. Er soll an den Anschl\u00e4gen von Paris beteiligt gewesen sein, bei denen am 13. November drei Killerkommandos 130 Menschen get\u00f6tet hatten. Was man bisher \u00fcber den Mann wei\u00df Salah Abdeslam ist in Br\u00fcssel geboren, aber franz\u00f6sischer Staatsb\u00fcrger. Er ist der Bruder des Selbstmordattent\u00e4ters Brahim, der ebenfalls bei den Anschl\u00e4gen dabei war. Die verst\u00fcmmelte Leiche des 31-j\u00e4hrigen Brahim Abdeslam hatte die Polizei am Tag des Anschlags am Boulevard Voltaire in der N\u00e4he des Konzertsaals Bataclan gefunden, wo er sich in die Luft gesprengt hatte. Salah wohnte im Br\u00fcsseler Vorort Molenbeek, der als eine Hochburg von gewaltbereiten Islamisten in Belgien gilt. Abdeslam soll in Deutschland gewesen sein Laut Recherchen des SWR soll sich Abdeslam Anfang Oktober 2015 kurzzeitig in Baden-W\u00fcrttemberg aufgehalten und dort wom\u00f6glich Komplizen abgeholt haben. Demnach fuhr er in der Nacht vom 2. auf den 3. Oktober 2015 mit einem auf seinen Namen angemieteten Wagen nach Ulm und offenbar nach etwa einer Stunde wieder zur\u00fcck. Er k\u00f6nnte in Ulm laut SWR drei M\u00e4nner, die sich als Syrer ausgegeben hatten, aus einer Fl\u00fcchtlingsunterkunft abgeholt haben. Bei einer Anwesenheitskontrolle am 3. Oktober wurde festgestellt, dass die drei M\u00e4nner in der Unterkunft fehlten. Ihre Identit\u00e4t werde vom Bundeskriminalamt gemeinsam mit franz\u00f6sischen und belgischen Sicherheitsbeh\u00f6rden gepr\u00fcft, hie\u00df es. Die deutschen Beh\u00f6rden wollten sich nicht zu dem Vorgang \u00e4u\u00dfern. Familie bat ihn, sich zu stellen Wie andere Islamisten auch ist Abdeslam im Br\u00fcsseler Stadtteil Molenbeek aufgewachsen. Er war der Polizei wegen Drogendelikten bekannt. Seinen Job als Mechaniker verlor er 2011 wegen h\u00e4ufiger Abwesenheit. Ab 2013 betrieb er eine Bar in Molenbeek, die schlie\u00dflich von den Beh\u00f6rden geschlossen wurde, weil G\u00e4ste dort Drogen genommen haben sollen. Mit Abdelhamid Abaaoud, der die Anschl\u00e4ge von Paris vermutlich geplant hat, war Salah Abdeslam seit seiner Kindheit befreundet. Nach den Anschl\u00e4gen in Frankreich wurde er per internationalem Haftbefehl gesucht. Fahnder beschrieben ihn als \\\"gef\u00e4hrlich\\\" und m\u00f6glicherweise \\\"schwer bewaffnet\\\". Zwischenzeitlich war auch \u00fcber einen Aufenthalt in Syrien spekuliert worden. Salahs Bruder Mohamed hatte in Fernsehinterviews an den Gesuchten appelliert, sich zu stellen. Er selbst war nach den Anschl\u00e4gen kurzzeitig festgenommen, aber bald wieder freigelassen worden. Seine Anw\u00e4ltin sagte, er habe \\\"nicht das gleiche Leben gew\u00e4hlt\\\" wie seine Br\u00fcder. Mohamed berichtete, dass Brahim und Salah in den Monaten vor den Anschl\u00e4gen im November in Paris ges\u00fcnder gelebt, gebetet, keinen Alkohol mehr getrunken h\u00e4tten und hin und wieder in die Moschee gegangen seien. Er wollte darin aber \\\"nicht direkt ein Zeichen f\u00fcr Radikalisierung\\\" sehen. Zur Rolle seines Bruders bei den Anschl\u00e4gen in Paris sagte Mohamed: \\\"Salah ist sehr intelligent. Er hat in letzter Minute kehrtgemacht\\\". Salah sollte angeblich in Paris auch ein Selbstmordattentat ver\u00fcben. Er z\u00fcndete die Bombe aber nicht, sondern warf seinen Sprengstoffg\u00fcrtel in einem Pariser Vorort in einen M\u00fclleimer.\",\n  \"target_text\": \"Dort soll der Terrorist drei Komplizen aus einer Fl\u00fcchtlingsunterkunft abgeholt haben. Die belgischen Beh\u00f6rden haben den 26-J\u00e4hrigen jetzt wegen Mordes angeklagt.\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>Im Folgenden finden Sie Nachrichtenartikel mit den dazugeh\u00f6rigen Zusammenfassungen.\n</code></pre></li> <li>Base prompt template:   <pre><code>Nachrichtenartikel: {text}\nZusammenfassung: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Nachrichtenartikel: {text}\n\nSchreiben Sie eine Zusammenfassung des obigen Artikels.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset mlsum-de\n</code></pre>"},{"location":"datasets/icelandic/","title":"\ud83c\uddee\ud83c\uddf8 Icelandic","text":"<p>This is an overview of all the datasets used in the Icelandic part of EuroEval. The datasets are grouped by their task - see the task overview for more information about what these constitute.</p>"},{"location":"datasets/icelandic/#sentiment-classification","title":"Sentiment Classification","text":""},{"location":"datasets/icelandic/#hotter-and-colder-sentiment","title":"Hotter and Colder Sentiment","text":"<p>This dataset was published in this paper, and consists of texts from Icelandic blog post, annotated with sentiment labels (and many others) via a crowdsourcing platform.</p> <p>The original full dataset consists of 2,901 samples, and we use a 1,021 / 255 / 1,607 split for training, validation and testing, respectively (so all samples are used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Til hamingju me\u00f0 gott framtak. \u00deetta eru g\u00f3\u00f0ir \u00fatgangspunktar me\u00f0 stj\u00f3rnarskr\u00e1na, \u00fe\u00f3 margt fleira \u00feurfi a\u00f0 laga svo h\u00fan \u00fej\u00f3ni vel\u00a0 n\u00fdju l\u00fd\u00f0veldi framt\u00ed\u00f0arinnar.\u00c9g sty\u00f0 heils hugar \u00feetta framtak ykkar.\",\n  \"label\": \"positive\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"J\u00fa, j\u00fa, au\u00f0vita \u00e1 hann ekki a\u00f0 vera\u00a0samstarfsma\u00f0ur e\u00f0a einu sinni \u00ed sama h\u00fasi og s\u00e9rstakir r\u00edkissaks\u00f3knarar \u00ed \u00feessu m\u00e1li. S\u00e9rstakir r\u00edkissaks\u00f3knarar fyrir \u00feetta m\u00e1l\u00a0eiga a\u00f0 liggja\u00a0liggja beint undir r\u00e1\u00f0uneytinu og vera algerlega sj\u00e1lfst\u00e6\u00f0ir, \\\"untouchables\\\". \u00c9g hef ekki enn s\u00e9\u00f0 nein r\u00f6k fyrir \u00fev\u00ed a\u00f0\u00a0Valt\u00fdr \u00feurfi a\u00f0 v\u00edkja \u00far s\u00ednu starfi ef \u00feessi lei\u00f0 ver\u00f0ur valin? Best v\u00e6ri ef s\u00e9rstakir r\u00edkissaks\u00f3knarar \u00ed \u00feessu m\u00e1li v\u00e6ri \u00ferepinu h\u00e6rri \u00ed valdastiganum en Valt\u00fdr, ef \u00fea\u00f0 er h\u00e6gt a\u00f0 koma \u00fev\u00ed \u00ed gegn me\u00f0 sn\u00f6ggum lagabreytingum? Varla er \u00feetta Stj\u00f3rnarskr\u00e1rm\u00e1l?\",\n  \"label\": \"neutral\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Meira a\u00f0 segja h\u00f6r\u00f0ustu klappst\u00fdrur \u00de\u00f3r\u00f3lfs hlj\u00f3ta a\u00f0 hugsa, \u00fe\u00f3 ekki v\u00e6ri \u00ed nema augnablik: Miki\u00f0 er skr\u00fdti\u00f0 a\u00f0 hann s\u00e9 ekki me\u00f0 \u00e1 hreinu af hverju f\u00e1ir handleggir eru a\u00f0 bj\u00f3\u00f0a sig \u00ed \u00feri\u00f0ju sprautuna!Annars er bara sama handriti\u00f0 a\u00f0 fara spilast aftur: N\u00fa er hausti\u00f0 komi\u00f0 og \u00e1rst\u00ed\u00f0arbundnar pestir munu rj\u00faka upp, allar sem ein, og \u00fe\u00e1 ver\u00f0ur skellt \u00ed l\u00e1s og tala\u00f0 um a\u00f0 hafa opna\u00f0 of snemma.\",\n  \"label\": \"negative\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru yfirfer\u00f0ir \u00e1samt lyndisgildi \u00feeirra, sem getur veri\u00f0 'j\u00e1kv\u00e6tt', 'hlutlaust' e\u00f0a 'neikv\u00e6tt'.\n</code></pre></li> <li>Base prompt template:   <pre><code>Yfirfer\u00f0: {text}\nLyndi: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Texti: {text}\n\nFlokka\u00f0u tilfinninguna \u00ed textanum. Svara\u00f0u me\u00f0 'j\u00e1kv\u00e6tt', 'hlutlaust' e\u00f0a 'neikv\u00e6tt'.\n</code></pre></li> <li>Label mapping:<ul> <li><code>positive</code> \u27a1\ufe0f <code>j\u00e1kv\u00e6tt</code></li> <li><code>neutral</code> \u27a1\ufe0f <code>hlutlaust</code></li> <li><code>negative</code> \u27a1\ufe0f <code>neikv\u00e6tt</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset hotter-and-colder-sentiment\n</code></pre>"},{"location":"datasets/icelandic/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"datasets/icelandic/#mim-gold-ner","title":"MIM-GOLD-NER","text":"<p>This dataset was published in this paper and is based on the Tagged Icelandic Corpus (MIM), which consists of Icelandic books, news articles, periodicals, parliament speeches, legal texts, adjudications and government websites. It has been annotated with named entities in a semi-automated fashion, where each labels has been manually verified. The entity types in the dataset is a superset of the CoNLL-2003 tags, with the following additional labels: <code>DATE</code>, <code>TIME</code>, <code>MONEY</code>, <code>PERCENT</code>. These labels have been removed.</p> <p>The original full dataset consists of 1,000,000 tokens. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  'tokens': array(['Sj\u00e1lfsagt', 'er', 'a\u00f0', 'mi\u00f0a', 'endurgrei\u00f0sluna', 'ver\u00f0i', 'n\u00faverandi', 'heimild', 'framlengd', 'vi\u00f0', 'EUROIII', '\u00ed', 'sta\u00f0', 'EUROII', 'eins', 'og', 'n\u00fa', 'er', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['\u00dea\u00f0', 'var', 'br\u00f3\u00f0ir', 'Sandlers', 'sem', 'hvatti', 'hann', 'til', 'a\u00f0', 'leggja', 'gr\u00edni\u00f0', 'fyrir', 'sig', '\u00feegar', 'hann', 'var', '17', '\u00e1ra', 'a\u00f0', 'aldri', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['2.-', 'Erla', 'Gu\u00f0n\u00fd', 'Gylfad.', ',', 'Smyrill', 'fr\u00e1', 'Stokkh\u00f3lma', ',', '7,01', '.'], dtype=object),\n  'labels': array(['O', 'B-PER', 'I-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-LOC', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru setningar \u00e1samt JSON lyklum me\u00f0 nefndum einingum sem koma fyrir \u00ed setningunum.\n</code></pre></li> <li>Base prompt template:   <pre><code>Setning: {text}\nNefndar einingar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Setning: {text}\n\nGreini\u00f0 nefndu einingarnar \u00ed setningunni. \u00de\u00fa \u00e6ttir a\u00f0 skila \u00feessu sem JSON or\u00f0ab\u00f3k me\u00f0 lyklunum 'einstaklingur', 'sta\u00f0setning', 'stofnun' og '\u00fdmislegt'. Gildin \u00e6ttu a\u00f0 vera listi yfir nefndu einingarnar af \u00feeirri ger\u00f0, n\u00e1kv\u00e6mlega eins og \u00fe\u00e6r koma fram \u00ed setningunni.\n</code></pre></li> <li>Label mapping:<ul> <li><code>B-PER</code> \u27a1\ufe0f <code>einstaklingur</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>einstaklingur</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>sta\u00f0setning</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>sta\u00f0setning</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>stofnun</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>stofnun</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>\u00fdmislegt</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>\u00fdmislegt</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset mim-gold-ner\n</code></pre>"},{"location":"datasets/icelandic/#linguistic-acceptability","title":"Linguistic Acceptability","text":""},{"location":"datasets/icelandic/#scala-is","title":"ScaLA-is","text":"<p>This dataset was published in this paper and was automatically created from the Icelandic Universal Dependencies treebank by assuming that the documents in the treebank are correct, and corrupting the samples to create grammatically incorrect samples. The corruptions were done by either removing a word from a sentence, or by swapping two neighbouring words in a sentence. To ensure that this does indeed break the grammaticality of the sentence, a set of rules were used on the part-of-speech tags of the words in the sentence.</p> <p>The original dataset consists of 3,535 samples, from which we use 1,024 / 256 / 2,048 samples for training, validation and testing, respectively (so 3,328 samples used in total). These splits are used as-is in the framework.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Utanrrh.: \u00c9g hef \u00c9g hef\u00f0i \u00f3ska\u00f0 \u00feess a\u00f0 h\u00e6stv. utanr\u00edkisr\u00e1\u00f0herra hef\u00f0i meiri \u00e1hrif \u00e1 fors\u00e6tisr\u00e1\u00f0herra en raun ber vitni Gripi\u00f0 fram \u00ed. \u00fev\u00ed a\u00f0 hann er sem betur fer ekki a\u00f0 tala ni\u00f0ur \u00fe\u00e1 atvinnugrein sem tengist sj\u00e1var\u00fatveginum eins og h\u00e6stv. fors\u00e6tisr\u00e1\u00f0herra gerir alla jafna.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"\u00dea\u00f0 v\u00e6ri mun sk\u00e1rra, \u00fea\u00f0 hef\u00f0i veri\u00f0 h\u00e6gt a\u00f0 gera \u00fea\u00f0 meiri me\u00f0 s\u00e1tt, en \u00fea\u00f0 var einfaldlega ekki gert.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Mig l\u00edka a\u00f0 koma a\u00f0, \u00e9g gleymdi \u00fev\u00ed \u00e1\u00f0an og kom \u00fev\u00ed heldur ekki a\u00f0, komugj\u00f6ldunum eins og \u00feau heita v\u00edst n\u00fana, ekki legugj\u00f6ld lengur.\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru setningar og hvort \u00fe\u00e6r eru m\u00e1lfr\u00e6\u00f0ilega r\u00e9ttar.\n</code></pre></li> <li>Base prompt template:   <pre><code>Setning: {text}\nM\u00e1lfr\u00e6\u00f0ilega r\u00e9tt: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Setning: {text}\n\nGreini\u00f0 hvort setningin er m\u00e1lfr\u00e6\u00f0ilega r\u00e9tt e\u00f0a ekki. Svari\u00f0 skal vera 'j\u00e1' ef setningin er r\u00e9tt og 'nei' ef h\u00fan er ekki.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>j\u00e1</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>nei</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scala-is\n</code></pre>"},{"location":"datasets/icelandic/#unofficial-iceec","title":"Unofficial: IceEC","text":"<p>This dataset was published here and consists of texts in modern Icelandic from student essays, online news texts and Wikipedia articles, annotated for mistakes related to spelling, grammar, and other issues.</p> <p>The original full dataset consists of 58,200 / 5,270 samples for training and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, where the training and testing splits are subsets of the original training and testing splits, and the validation split is a disjoint subset of the training split.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Kannski erum vi\u00f0 me\u00f0 meiri s\u00f6lu \u00ed \u00f6\u00f0rum skrokkhlutum en s\u00ed\u00f0um t.d., \u201c segir Stein\u00fe\u00f3r.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"\u00de\u00f3 svo a\u00f0 hann s\u00e9 lei\u00f0inlegur og ekkert t\u00edvol\u00ed gaman, \u00fe\u00e1 er mi\u00f0lar hann \u00feekkingu til okkar og \u00e1n hans mundi enginn menntun vera.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"S\u00edminn er hvers manns \u00e1byrg\u00f0.\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru setningar og hvort \u00fe\u00e6r eru m\u00e1lfr\u00e6\u00f0ilega r\u00e9ttar.\n</code></pre></li> <li>Base prompt template:   <pre><code>Setning: {text}\nM\u00e1lfr\u00e6\u00f0ilega r\u00e9tt: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Setning: {text}\n\nGreini\u00f0 hvort setningin er m\u00e1lfr\u00e6\u00f0ilega r\u00e9tt e\u00f0a ekki. Svari\u00f0 skal vera 'j\u00e1' ef setningin er r\u00e9tt og 'nei' ef h\u00fan er ekki.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>j\u00e1</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>nei</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset ice-ec\n</code></pre>"},{"location":"datasets/icelandic/#unofficial-icelinguistic","title":"Unofficial: IceLinguistic","text":"<p>This dataset was published here, with the source of the documents unknown. It consists of Icelandic sentences annotated with whether they are grammatically correct or not (along with other linguistic properties).</p> <p>The original full dataset consists of 382 samples, and we use a 94 / 32 / 256 split for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"\u00c9g afla\u00f0i uppl\u00fdsinganna og \u00fe\u00fa peninganna.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Af hverju f\u00f3r \u00fe\u00fa ekki heim?\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"\u00de\u00fa bor\u00f0a\u00f0ir k\u00f6kuna og \u00e9g kleinuhringurinn.\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru setningar og hvort \u00fe\u00e6r eru m\u00e1lfr\u00e6\u00f0ilega r\u00e9ttar.\n</code></pre></li> <li>Base prompt template:   <pre><code>Setning: {text}\nM\u00e1lfr\u00e6\u00f0ilega r\u00e9tt: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Setning: {text}\n\nGreini\u00f0 hvort setningin er m\u00e1lfr\u00e6\u00f0ilega r\u00e9tt e\u00f0a ekki. Svari\u00f0 skal vera 'j\u00e1' ef setningin er r\u00e9tt og 'nei' ef h\u00fan er ekki.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>j\u00e1</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>nei</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset ice-linguistic\n</code></pre>"},{"location":"datasets/icelandic/#reading-comprehension","title":"Reading Comprehension","text":""},{"location":"datasets/icelandic/#nqii","title":"NQiI","text":"<p>This dataset was published in this paper and is based on articles from the Icelandic Wikipedia. Annotators were asked to write both questions (only seeing the beginning of the article) as well as answers as they appear in the article.</p> <p>The original full dataset consists of 2,234 / 259 / 244 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively. Our splits are new, and there can thus be some overlap between the new test split and the old training and validation splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  'context': 'Gr\u00f3\u00f0urh\u00fasalofttegund er lofttegund , \u00ed lofthj\u00fapi sem drekkur \u00ed sig og gefur fr\u00e1 s\u00e9r innrau\u00f0a geislun . \u00dea\u00f0 ferli er a\u00f0al \u00e1st\u00e6\u00f0a gr\u00f3\u00f0urh\u00fasa\u00e1hrifa . Helstu gr\u00f3\u00f0urh\u00fasalofttegundirnar \u00ed lofthj\u00fapi jar\u00f0ar eru vatnsgufa , kold\u00edox\u00ed\u00f0 , metan , tv\u00edk\u00f6fnunarefnisox\u00ed\u00f0 og \u00f3son . \u00c1n gr\u00f3\u00f0urh\u00fasalofttegunda v\u00e6ri me\u00f0alhiti yfirbor\u00f0s jar\u00f0ar \u2212 18 \u00b0 C , n\u00faverandi me\u00f0altals 15 \u00b0 C . \u00cd s\u00f3lkerfinu , eru Venus , Mars og T\u00edtan einnig me\u00f0 lofthj\u00fap sem veldur gr\u00f3\u00f0urh\u00fasa\u00e1hrifum .',\n  'question': 'Hverjar eru gr\u00f3\u00f0urh\u00fasalofttegundirnar ?',\n  'answers': {\n    'answer_start': array([202], dtype=int32),\n    'text': array([' vatnsgufa , kold\u00edox\u00ed\u00f0 , metan , tv\u00edk\u00f6fnunarefnisox\u00ed\u00f0 og \u00f3son'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': 'Hvannadalshn\u00fakur e\u00f0a Hvannadalshnj\u00fakur er h\u00e6sti tindur eldkeilunnar undir \u00d6r\u00e6faj\u00f6kli og jafnframt h\u00e6sti tindur \u00cdslands . Samkv\u00e6mt n\u00fdjustu m\u00e6lingu er h\u00e6\u00f0 hans 2.109,6 metrar yfir sj\u00e1varm\u00e1li . Tindurinn er sta\u00f0settur innan Vatnaj\u00f6kuls\u00fej\u00f3\u00f0gar\u00f0s og er vins\u00e6ll hj\u00e1 fjallg\u00f6nguf\u00f3lki , reyndu sem og \u00f3reyndu . Tindurinn er ekki fl\u00f3kinn uppg\u00f6ngu og \u00fearfnast ekki mikillar reynslu e\u00f0a t\u00e6kni \u00ed fjallg\u00f6ngum , gangan krefst samt mikils \u00fathalds \u00fear sem oftast er gengi\u00f0 \u00e1 tindinn og ni\u00f0ur aftur \u00e1 sama deginum . H\u00e6kkunin er r\u00famir 2000 metrar , gangan tekur oftast 12 - 14 klst \u00ed heild .',\n  'question': 'Hvert er h\u00e6sta fjall \u00e1 \u00cdslandi ?',\n  'answers': {\n    'answer_start': array([20,  0, 20], dtype=int32),\n    'text': array([' Hvannadalshnj\u00fakur', 'Hvannadalshn\u00fakur', ' Hvannadalshnj\u00fakur er h\u00e6sti tindur eldkeilunnar undir \u00d6r\u00e6faj\u00f6kli og jafnframt h\u00e6sti tindur \u00cdslands'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': 'Falklandseyjar er l\u00edtill eyjaklasi \u00fat af Su\u00f0ur-Amer\u00edku , um 500 km til su\u00f0austurs fr\u00e1 Argent\u00ednu . \u00de\u00e6r eru undir stj\u00f3rn Bretlands en Argent\u00edna hefur einnig gert tilkall til \u00feeirra og olli \u00fea\u00f0 Falklandseyjastr\u00ed\u00f0inu milli \u00fej\u00f3\u00f0anna 1982 .',\n  'question': 'Hvar eru Falklandseyjar ?',\n  'answers': {\n    'answer_start': array([34, 34], dtype=int32),\n    'text': array([' \u00fat af Su\u00f0ur-Amer\u00edku', ' \u00fat af Su\u00f0ur-Amer\u00edku , um 500 km til su\u00f0austurs fr\u00e1 Argent\u00ednu'], dtype=object)\n  }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru textar me\u00f0 tilheyrandi spurningum og sv\u00f6rum.\n</code></pre></li> <li>Base prompt template:   <pre><code>Texti: {text}\nSpurning: {question}\nSvara\u00f0u me\u00f0 a\u00f0 h\u00e1marki 3 or\u00f0um: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Texti: {text}\n\nSvara\u00f0u eftirfarandi spurningu um textann a\u00f0 h\u00e1marki \u00ed 3 or\u00f0um.\n\nSpurning: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset nqii\n</code></pre>"},{"location":"datasets/icelandic/#unofficial-icelandicqa","title":"Unofficial: IcelandicQA","text":"<p>This dataset was published here and consists of an automatically created Icelandic question-answering dataset based on the Icelandic Wikipedia as well as Icelandic news articles from the R\u00daV corpus.</p> <p>Both questions and answers were generated automatically, meaning that the answers might not appear in the context. To remedy this, we used GPT-4o to rephrase the answers to ensure that they appear in the context.</p> <p>The original full dataset consists of 2,000 samples, and we use a 531 / 128 / 1,024 split for training, validation and testing, respectively. These are all the samples where the (rephrased) answer appears in the context.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  'context': '\u00d3mar Ragnarsson - Syngur fyrir b\u00f6rnin  er 33 sn\u00faninga LP hlj\u00f3mplata gefin \u00fat af SG - hlj\u00f3mpl\u00f6tum \u00e1ri\u00f0 1981. \u00c1 henni syngur \u00d3mar Ragnarsson \u00ferett\u00e1n barnal\u00f6g. Platan er safnplata af \u00e1\u00f0ur \u00fatgefnum \"hit\" l\u00f6gum af 45 sn\u00faninga pl\u00f6tum.\\n\\nLagalisti \\n \u00c9g er a\u00f0 baka - Lag - texti: E. Shuman/B. Bower - \u00d3mar Ragnarsson\\n Br\u00f3\u00f0ir minn - Lag - texti: W. Holt -\u00d3mar Ragnarsson\\n Eitthva\u00f0 \u00fat \u00ed lofti\u00f0 - Lag - texti: P. McCartney - \u00d3mar Ragnarsson \\n Lok, lok og l\u00e6s - Lag - texti: Brezkt \u00fej\u00f3\u00f0lag - \u00d3mar Ragnarsson\\n Aha, sei-sei, j\u00e1-j\u00e1 - Lag - texti: \u00d3mar Ragnarsson\\n Ligga, ligga l\u00e1 - Lag - texti: \u00d3mar Ragnarsson \\n Hl\u00e1turinn lengir l\u00edfi\u00f0 - Lag - texti: Ortega - \u00d3mar Ragnarsson\\n Sumar og s\u00f3l - Lag - texti: \u00d3mar Ragnarsson\\n J\u00f3i \u00fatherji - Lag - texti: \u00c1stralskt \u00fej\u00f3\u00f0lag - \u00d3mar Ragnarsson\\n \u00d3li drj\u00f3li - Lag - texti: \u00d3mar Ragnarsson)\\n Minkurinn \u00ed h\u00e6nsnakofanum - Lag - texti: Norskt \u00fej\u00f3\u00f0lag - \u00d3mar Ragnarsson \\n Kenni\u00f0 m\u00e9r krakkar - Lag - texti: A. Johansen - \u00d3mar Ragnarsson\\n H\u00ed \u00e1 \u00feig - Lag - texti: Amer\u00edskt \u00fej\u00f3\u00f0lag - \u00d3mar Ragnarsson\\n\\nSG-hlj\u00f3mpl\u00f6tur\\nHlj\u00f3mpl\u00f6tur gefnar \u00fat \u00e1ri\u00f0 1981\\n\u00d3mar Ragnarsson',\n  'question': 'Hva\u00f0a \u00e1r var LP-hlj\u00f3mplatan \u201e\u00d3mar Ragnarsson - Syngur fyrir b\u00f6rnin\u201c gefin \u00fat?',\n  'answers': {\n    'answer_start': 102,\n    'text': array(['1981'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': 'Tj\u00f6rn er kirkjusta\u00f0ur \u00ed Dalv\u00edkurbygg\u00f0 \u00ed Svarfa\u00f0ardal. B\u00e6rinn stendur a\u00f0 vestanver\u00f0u \u00ed dalnum um 5 km innan vi\u00f0 Dalv\u00edk. \u00de\u00f3rarinn Kr. Eldj\u00e1rn l\u00e9t reisa n\u00faverandi \u00edb\u00fa\u00f0arh\u00fas 1931. Tjarnartj\u00f6rn er l\u00edti\u00f0 og grunnt st\u00f6\u00f0uvatn \u00e1 flatlendinu ne\u00f0an vi\u00f0 b\u00e6inn. Tj\u00f6rnin er innan Fri\u00f0lands Svarfd\u00e6la sem teygir sig allt til strandar. \u00dear er miki\u00f0 fuglal\u00edf. Tj\u00f6rn er me\u00f0 st\u00e6rri j\u00f6r\u00f0um \u00ed Svarfa\u00f0ardal og a\u00f0 l\u00edkindum landn\u00e1msj\u00f6r\u00f0 \u00fe\u00f3tt b\u00e6jarins s\u00e9 ekki geti\u00f0 \u00ed Landn\u00e1mu. \u00dear hafa veri\u00f0 stunda\u00f0ar \u00farkomum\u00e6lingar \u00e1 vegum Ve\u00f0urstofunnar fr\u00e1 \u00e1rinu 1970. \u00cd hl\u00ed\u00f0inni ofan vi\u00f0 Tj\u00f6rn eru volgrur og \u00ed framhaldi af \u00feeim er jar\u00f0hitinn \u00ed Laugahl\u00ed\u00f0 \u00fear sem Sundsk\u00e1li Svarfd\u00e6la f\u00e6r vatn sitt.\\nKristj\u00e1n Eldj\u00e1rn forseti f\u00e6ddist \u00e1 Tj\u00f6rn 1916 og \u00f3lst \u00fear upp.\\nS\u00f6ngh\u00f3purinn Tjarnarkvartettinn var kenndur vi\u00f0 Tj\u00f6rn \u00ed Svarfa\u00f0ardal.\\n\\nTjarnarb\u00e6ndur \u00e1 20. \u00f6ld:\\n Sr. Kristj\u00e1n Eldj\u00e1rn \u00de\u00f3rarinsson og Petr\u00edna Soff\u00eda Hj\u00f6rleifsd\u00f3ttir\\n \u00de\u00f3rarinn Kr. Eldj\u00e1rn og Sigr\u00fan Sigurhjartard\u00f3ttir\\n Hj\u00f6rtur Eldj\u00e1rn \u00de\u00f3rarinsson og Sigr\u00ed\u00f0ur Hafsta\u00f0\\n Kristj\u00e1n Eldj\u00e1rn Hjartarson og Kristjana Arngr\u00edmsd\u00f3ttir\\n\\nTjarnarkirkja \\n\\nKirkja hefur l\u00edklega veri\u00f0 reist \u00e1 Tj\u00f6rn flj\u00f3tlega eftir a\u00f0 kristni var l\u00f6gleidd \u00ed landinu. Hennar er \u00fe\u00f3 ekki geti\u00f0 me\u00f0 beinum h\u00e6tti \u00ed heimildum fyrr en \u00ed Au\u00f0unarm\u00e1ldaga fr\u00e1 1318. \u00dear segir a\u00f0 kirkjan s\u00e9 helgu\u00f0 Mar\u00edu gu\u00f0sm\u00f3\u00f0ur, Mikj\u00e1li erkiengli, J\u00f3hannesi sk\u00edrara og Andr\u00e9si postula. Kirkjan \u00e1tti \u00fe\u00e1 h\u00e1lft heimalandi\u00f0, Ingvarasta\u00f0aland og h\u00f3lminn \u00d6rgumlei\u00f0a. \u00c1 16. \u00f6ld er Tj\u00f6rn or\u00f0in beneficium, \u00fe.e. \u00f6ll komin \u00ed eigu kirkjunnar og \u00feannig h\u00e9lst \u00fear til sr. Kristj\u00e1n Eldj\u00e1rn \u00de\u00f3rarinsson (1843-1917) keypti j\u00f6r\u00f0ina \u00e1ri\u00f0 1915. Sr. Kristj\u00e1n var s\u00ed\u00f0asti prestur \u00e1 Tj\u00f6rn. \u00cd Svarfa\u00f0ardal voru lengi fj\u00f3rar s\u00f3knir en \u00fer\u00edr prestar \u00fev\u00ed Ur\u00f0akirkja var annex\u00eda fr\u00e1 Tj\u00f6rn. Upsas\u00f3kn var s\u00ed\u00f0an l\u00f6g\u00f0 undir Tjarnarprest 1859 en 1917 var Tjarnarprestakall me\u00f0 s\u00ednum \u00feremur s\u00f3knum sameina\u00f0 Vallaprestakalli. Eftir a\u00f0 prestssetri\u00f0 var flutt fr\u00e1 V\u00f6llum 1969 hefur Tjarnarkirkju veri\u00f0 \u00fej\u00f3na\u00f0 af fr\u00e1 Dalv\u00edk. Tjarnars\u00f3kn n\u00e6r fr\u00e1 Steindyrum a\u00f0 Ytraholti.\\n\\nN\u00faverandi kirkja var reist 1892. H\u00fan er \u00far timbri \u00e1 hl\u00f6\u00f0num grunni og tekur 60-70 manns \u00ed s\u00e6ti. \u00cd henni eru steindir gluggar teikna\u00f0ir af Valger\u00f0i Hafsta\u00f0 listm\u00e1lara. Kirkjugar\u00f0ur er umhverfis kirkjuna. Kirkjan skemmdist nokku\u00f0 \u00ed Kirkjurokinu svokalla\u00f0a, miklu \u00f3ve\u00f0ri sem gekk yfir landi\u00f0 \u00feann 20. september \u00e1ri\u00f0 1900. \u00de\u00e1 ey\u00f0il\u00f6g\u00f0ust kirkjurnar \u00e1 Ur\u00f0um og Upsum og Vallakirkja var\u00f0 fyrir skemmdum. Tjarnarkirkja snara\u00f0ist \u00e1 grunni s\u00ednum og halla\u00f0ist mj\u00f6g til nor\u00f0urs en j\u00e1rnkr\u00f3kar miklir, sem h\u00e9ldu timburverkinu vi\u00f0 hla\u00f0inn grunninn, v\u00f6rnu\u00f0u \u00fev\u00ed a\u00f0 verr f\u00e6ri. Nokkru eftir f\u00e1rvi\u00f0ri\u00f0 ger\u00f0i hvassvi\u00f0ri af nor\u00f0ri sem f\u00e6r\u00f0i hana til \u00e1 grunninum og r\u00e9tti hana a\u00f0 mestu vi\u00f0 \u00e1 n\u00fd. M\u00f6rgum \u00fe\u00f3ttu \u00feetta st\u00f3rmerki. Gert var vi\u00f0 kirkjuna eftir \u00feetta og m.a. voru \u00fatb\u00fain \u00e1 hana j\u00e1rnst\u00f6g sem lengi settu skemmtilegan svip \u00e1 bygginguna og minntu \u00e1 hi\u00f0 mikla f\u00e1rvi\u00f0ri sem h\u00fan haf\u00f0i sta\u00f0i\u00f0 af s\u00e9r. Kirkjan st\u00f3\u00f0 einnig af s\u00e9r Dalv\u00edkurskj\u00e1lftann 1934 en \u00fe\u00f3 ur\u00f0u skemmdir \u00e1 grunni hennar.\\n\\nHeimildir \\n \\n \\n Kirkjur \u00cdslands 9. bindi. Tjarnarkirkja bls. 271-307. Reykjav\u00edk 2007\\n\\nTenglar\\nTjarnarkirkja \u00e1 kirkjukort.net \\n\\n\u00cdslenskir sveitab\u00e6ir\\nKirkjusta\u00f0ir \u00ed Eyjafjar\u00f0ars\u00fdslu\\nKirkjur \u00e1 \u00cdslandi\\nSvarfa\u00f0ardalur',\n  'question': '\u00c1 hva\u00f0a b\u00e6 \u00ed Svarfa\u00f0ardal hafa veri\u00f0 stunda\u00f0ar \u00farkomum\u00e6lingar \u00e1 vegum Ve\u00f0urstofunnar fr\u00e1 \u00e1rinu 1970?',\n  'answers': {\n    'answer_start': 0,\n    'text': array(['Tj\u00f6rn'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': 'Fyrir greinina um \u00fe\u00e1ttinn sem er \u00ed gangi \u00ed dag, sj\u00e1 Kastlj\u00f3s (d\u00e6gurm\u00e1la\u00fe\u00e1ttur)\\nKastlj\u00f3s var fr\u00e9ttask\u00fdringa\u00fe\u00e1ttur sem var \u00e1 dagskr\u00e1 R\u00edkis\u00fatvarpsins fr\u00e1 1974 til 1998. Hann h\u00f3f g\u00f6ngu s\u00edna sem fr\u00e9ttask\u00fdringa\u00fe\u00e1ttur um innlendar fr\u00e9ttir \u00e1ri\u00f0 1974 og t\u00f3k \u00fe\u00e1 vi\u00f0 af \u00fe\u00e6tti sem nefndist Landshorn. \u00de\u00e1tturinn var um fj\u00f6rut\u00edu m\u00edn\u00fatna langur, \u00ed umsj\u00f3n fr\u00e9ttastofunnar og s\u00fdndur \u00e1 f\u00f6stud\u00f6gum \u00e1 besta t\u00edma. Umsj\u00f3narmenn voru mismunandi fr\u00e9ttamenn \u00ed hvert skipti. Annar \u00fe\u00e1ttur \u00e1 mi\u00f0vikud\u00f6gum fjalla\u00f0i \u00fe\u00e1 um erlendar fr\u00e9ttir. 1980 var \u00fe\u00e1ttunum tveimur slegi\u00f0 saman \u00ed eitt Kastlj\u00f3s \u00e1 f\u00f6stud\u00f6gum \u00ed umsj\u00f3n tveggja stj\u00f3rnenda. 1987 var \u00fe\u00e6ttinum aftur breytt \u00ed fr\u00e9ttask\u00fdringa\u00fe\u00e1tt um innlend m\u00e1lefni stutt skei\u00f0. 1988 h\u00e9t \u00fe\u00e1tturinn Kastlj\u00f3s \u00e1 sunnudegi og 1990 Kastlj\u00f3s \u00e1 \u00feri\u00f0judegi eftir breyttum \u00fatsendingart\u00edma en 1992 var \u00fe\u00e1tturinn aftur fluttur \u00e1 besta t\u00edma \u00e1 f\u00f6studegi. 1993 var Kastlj\u00f3s teki\u00f0 af dagskr\u00e1 um skei\u00f0 \u00feegar d\u00e6gurm\u00e1la\u00fe\u00e1tturinn Dagslj\u00f3s h\u00f3f g\u00f6ngu s\u00edna. \\n\\n\u00cdslenskir sj\u00f3nvarps\u00fe\u00e6ttir',\n  'question': '\u00c1 hva\u00f0a \u00e1rum var fr\u00e9ttask\u00fdringa\u00fe\u00e1tturinn Kastlj\u00f3s upphaflega \u00e1 dagskr\u00e1 R\u00edkis\u00fatvarpsins?',\n  'answers': {\n    'answer_start': 147,\n    'text': array(['Fr\u00e1 1974 til 1998'], dtype=object)\n  }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru textar me\u00f0 tilheyrandi spurningum og sv\u00f6rum.\n</code></pre></li> <li>Base prompt template:   <pre><code>Texti: {text}\nSpurning: {question}\nSvara\u00f0u me\u00f0 a\u00f0 h\u00e1marki 3 or\u00f0um: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Texti: {text}\n\nSvara\u00f0u eftirfarandi spurningu um textann a\u00f0 h\u00e1marki \u00ed 3 or\u00f0um.\n\nSpurning: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset icelandic-qa\n</code></pre>"},{"location":"datasets/icelandic/#unofficial-belebele-is","title":"Unofficial: BeleBele-is","text":"<p>This dataset was published in this paper and features multiple-choice reading comprehension questions across 122 languages.</p> <p>The original dataset contains 900 unique multiple-choice reading comprehension passages and questions. From these, we use a 256 / 64 / 580 split for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Texti: \u00cd Frelsisstr\u00ed\u00f0inu myndu\u00f0u r\u00edkin \u00ferett\u00e1n veikbur\u00f0a r\u00edkisstj\u00f3rn \u2013 me\u00f0 \u00dej\u00f3\u00f0\u00feingi\u00f0 sem eina \u00fe\u00e1tt \u00feess \u2013 skv. fyrstu stj\u00f3rnarskr\u00e1nni. \u00deingi\u00f0 var ekki me\u00f0 n\u00e6gar valdheimildir til a\u00f0 leggja \u00e1 skatta, og vegna \u00feess a\u00f0 ekki var neinn alr\u00edkisstj\u00f3ri e\u00f0a d\u00f3msvald til sta\u00f0ar, treysti \u00fea\u00f0 \u00e1 yfirv\u00f6ld \u00ed hverju r\u00edki fyrir sig, sem voru oft og t\u00ed\u00f0um \u00f3samvinnu\u00fe\u00fd\u00f0, til a\u00f0 framfylgja l\u00f6gum \u00feess. \u00dea\u00f0 haf\u00f0i heldur engar valdheimildir til a\u00f0 fella ni\u00f0ur skattal\u00f6g og tolla \u00e1 milli r\u00edkja. Greinarnar ger\u00f0u kr\u00f6fu um samhlj\u00f3\u00f0a sam\u00feykki allra r\u00edkjanna \u00e1\u00f0ur en h\u00e6gt var a\u00f0 breyta \u00feeim og r\u00edkin s\u00fdndu r\u00edkisvaldinu svo mikla l\u00edtilsvir\u00f0ingu a\u00f0 fulltr\u00faar \u00feeirra voru oft fjarverandi.\\nSpurning: Samkv\u00e6mt \u00fev\u00ed sem fram kemur \u00ed kaflanum, hva\u00f0a fullyr\u00f0ing \u00e1 n\u00e1kv\u00e6mlega vi\u00f0 um \u00e1stand r\u00edkisvaldsins \u00ed frelsisstr\u00ed\u00f0inu?\\nSvarm\u00f6guleikar:\\na. Skattar voru innheimtir af \u00feinginu og r\u00edkisstofnunum\\nb. Breytingar \u00e1 stj\u00f3rnarskr\u00e1nni \u00feurftu sam\u00feykki \u00feingsins\\nc. Fulltr\u00faar r\u00edkjanna voru oft fjarverandi\\nd. Hin mi\u00f0l\u00e6ga r\u00edkisstj\u00f3rn var myndu\u00f0 \u00ed kringum tvo megin\u00fe\u00e6tti\",\n  \"label\": \"c\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Texti: \u0130zmir er \u00feri\u00f0ja st\u00e6rsta borg Tyrklands me\u00f0 um 3,7 millj\u00f3nir \u00edb\u00faa, n\u00e6stst\u00e6rstu h\u00f6fnina \u00e1 eftir Istanb\u00fal og er mj\u00f6g g\u00f3\u00f0 samg\u00f6ngumi\u00f0st\u00f6\u00f0. Hin forna borg Smyrna er n\u00fana n\u00fat\u00edmaleg, \u00fer\u00f3u\u00f0 og i\u00f0andi vi\u00f0skiptami\u00f0st\u00f6\u00f0 sem sta\u00f0sett er vi\u00f0 gr\u00ed\u00f0arst\u00f3ran fl\u00f3a og umkringd er fj\u00f6llum. Hinar brei\u00f0u brei\u00f0g\u00f6tur, byggingar me\u00f0 framhli\u00f0um \u00far gleri og n\u00fat\u00edmalegar verslunarmi\u00f0st\u00f6\u00f0var me\u00f0 hef\u00f0bundnum rau\u00f0um \u00feaksk\u00edfum, 18. aldar marka\u00f0urinn og gamlar moskur og kirkjur, \u00fe\u00f3 a\u00f0 andr\u00famsloft borgarinnar tengist meira Mi\u00f0jar\u00f0arhafssv\u00e6\u00f0i Evr\u00f3pu en hef\u00f0bundnu Tyrklandi.\\nSpurning: Hvert eftirfarandi einkennir Izmir er fr\u00e1 fornri t\u00ed\u00f0?\\nSvarm\u00f6guleikar:\\na. Brei\u00f0ar brei\u00f0g\u00f6tur\\nb. Byggingar me\u00f0 framhli\u00f0um \u00far gleri\\nc. Verslanami\u00f0st\u00f6\u00f0var\\nd. rau\u00f0ar \u00feaksk\u00edfur\",\n  \"label\": \"d\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Texti: D\u00e6migert fyrir \u00fea\u00f0 t\u00edmabil er Kirby Muxloe Castle sem er frekar v\u00edggirt h\u00fas en raunverulegur kastali. St\u00f3ru glj\u00e1\u00f0u gluggarnir og \u00feunnu veggirnir hef\u00f0u ekki geta\u00f0 sta\u00f0ist st\u00f3r\u00e1r\u00e1s \u00ed langan t\u00edma. \u00c1ri\u00f0 1480, \u00feegar Hastings l\u00e1var\u00f0ur h\u00f3f byggingarframkv\u00e6mdirnar, r\u00edkti fri\u00f0ur \u00ed n\u00e1nast \u00f6llu landinu og a\u00f0eins var \u00fe\u00f6rf \u00e1 varnarm\u00farum gegn litlum r\u00e6ningjah\u00f3pum.\\nSpurning: Hvert af eftirt\u00f6ldu hef\u00f0i veri\u00f0 tali\u00f0 \u00f3venjulegt vi\u00f0 byggingu Kirby Muxloe kastala \u00e1 \u00feeim t\u00edma sem tala\u00f0 er um \u00ed kaflanum?\\nSvarm\u00f6guleikar:\\na. St\u00f3rir gluggar\\nb. Grunnur sem \u00e1 a\u00f0 standast \u00e1r\u00e1sir\\nc. Minna af varnar\u00fatb\u00fana\u00f0i en \u00ed \u00f6\u00f0rum k\u00f6stulum\\nd. \u00deunnir veggir\",\n  \"label\": \"b\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\n</code></pre></li> <li>Base prompt template:   <pre><code>Spurningar: {text}\nSvarm\u00f6guleikar:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvara: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Spurningar: {text}\nSvarm\u00f6guleikar:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', 'b', 'c' e\u00f0a 'd', og engu \u00f6\u00f0ru.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset belebele-is\n</code></pre>"},{"location":"datasets/icelandic/#knowledge","title":"Knowledge","text":""},{"location":"datasets/icelandic/#icelandicknowledge","title":"IcelandicKnowledge","text":"<p>This dataset was published here and consists of an automatically created Icelandic question-answering dataset based on the Icelandic Wikipedia as well as Icelandic news articles from the R\u00daV corpus.</p> <p>The dataset was converted into a multiple-choice knowledge dataset by removing the contexts and using GPT-4o to generate 3 plausible wrong answers for each correct answer, using the following prompt for each <code>row</code> in the original dataset:</p> <pre><code>messages = [\n    {\n        \"role\": \"user\",\n        \"content\": f\"For the question: {row.question} where the correct answer is: {row.answer}, please provide 3 plausible alternatives in Icelandic. You should return the alternatives in a JSON dictionary, with keys 'first', 'second', and 'third'. The values should be the alternatives only, without any numbering or formatting. The alternatives should be unique and not contain the correct answer.\"\n    }\n]\n\ncompletion = client.beta.chat.completions.parse(\n    model=\"gpt-4o\", messages=messages, response_format=CandidateAnswers\n)\n</code></pre> <p>where <code>CandidateAnswers</code> is a Pydantic model that is used to ensure structured outputs.</p> <p>The original dataset has 2,000 samples, but only 1,994 unique questions, and the total length of this dataset is therefore 1,994. The split is given by 842 / 128 / 1024 for train, val, and test, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Hver var talinn heilagur ma\u00f0ur eftir dau\u00f0a sinn, er t\u00e1kngervingur al\u00fe\u00fd\u00f0uhreyfingar vestanlands og talinn g\u00f3\u00f0ur til \u00e1heita?\\nSvarm\u00f6guleikar:\\na. \u00de\u00f3r\u00f0ur J\u00f3nsson helgi\\nb. Gu\u00f0mundur Arason\\nc. Snorri \u00deorgr\u00edmsson\\nd. J\u00f3n Hreggvi\u00f0sson\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"\u00cd kringum hva\u00f0a \u00e1r h\u00f3fst verslun \u00e1 Arnger\u00f0areyri?\\nSvarm\u00f6guleikar:\\na. 1895\\nb. 1884\\nc. 1870\\nd. 1902\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hven\u00e6r var \u00e1kve\u00f0i\u00f0 a\u00f0 uppstigningardagur skyldi vera kirkjudagur aldra\u00f0ra \u00e1 \u00cdslandi?\\nSvarm\u00f6guleikar:\\na. \u00c1ri\u00f0 1975\\nb. \u00c1ri\u00f0 1985\\nc. \u00c1ri\u00f0 1982\\nd. \u00c1ri\u00f0 1990\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\n</code></pre></li> <li>Base prompt template:   <pre><code>Spurningar: {text}\nSvarm\u00f6guleikar:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvara: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Spurningar: {text}\nSvarm\u00f6guleikar:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', 'b', 'c' e\u00f0a 'd', og engu \u00f6\u00f0ru.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset icelandic-knowledge\n</code></pre>"},{"location":"datasets/icelandic/#unofficial-arc-is","title":"Unofficial: ARC-is","text":"<p>This dataset is a machine translated version of the English ARC dataset and features US grade-school science questions. The dataset was translated by Mi\u00f0eind using the Claude 3.5 Sonnet model.</p> <p>The original full dataset consists of 1,110 / 297 / 1,170 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively (so 2,304 samples used in total). All new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"L\u00edkamar manna hafa fl\u00f3kna uppbyggingu sem sty\u00f0ur v\u00f6xt og l\u00edfsl\u00edkur. Hver er grundvallaruppbygging l\u00edkamans sem stu\u00f0lar a\u00f0 vexti og l\u00edfsl\u00edkum?\\nSvarm\u00f6guleikar:\\na. fruma\\nb. vefur\\nc. l\u00edff\u00e6ri\\nd. l\u00edff\u00e6rakerfi\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Ve\u00f0urfr\u00e6\u00f0ingur skr\u00e1ir g\u00f6gn fyrir borg \u00e1 \u00e1kve\u00f0num degi. G\u00f6gnin innihalda hitastig, sk\u00fdjahulu, vindhra\u00f0a, loft\u00fer\u00fdsting og vind\u00e1tt. Hva\u00f0a a\u00f0fer\u00f0 \u00e6tti ve\u00f0urfr\u00e6\u00f0ingurinn a\u00f0 nota til a\u00f0 skr\u00e1 \u00feessi g\u00f6gn fyrir flj\u00f3tlega tilv\u00edsun?\\nSvarm\u00f6guleikar:\\na. skriflega l\u00fdsingu\\nb. t\u00f6flu\\nc. st\u00f6\u00f0varl\u00edkan\\nd. ve\u00f0urkort\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hva\u00f0a breytingar ur\u00f0u \u00feegar reikistj\u00f6rnurnar hitnnu\u00f0u \u00e1 me\u00f0an \u00fe\u00e6r myndu\u00f0ust?\\nSvarm\u00f6guleikar:\\na. Massi \u00feeirra j\u00f3kst.\\nb. \u00de\u00e6r t\u00f6pu\u00f0u meirihluta geislavirkra sams\u00e6ta sinna.\\nc. Uppbygging \u00feeirra a\u00f0greindist \u00ed mismunandi l\u00f6g.\\nd. \u00de\u00e6r byrju\u00f0u a\u00f0 sn\u00faast \u00ed kringum s\u00f3lina.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\n</code></pre></li> <li>Base prompt template:   <pre><code>Spurningar: {text}\nSvarm\u00f6guleikar:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvara: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Spurningar: {text}\nSvarm\u00f6guleikar:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', 'b', 'c' e\u00f0a 'd', og engu \u00f6\u00f0ru.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset arc-is\n</code></pre>"},{"location":"datasets/icelandic/#unofficial-mmlu-is","title":"Unofficial: MMLU-is","text":"<p>This dataset is a machine translated version of the English MMLU dataset and features questions within 57 different topics, such as elementary mathematics, US history and law. The dataset was translated using Mi\u00f0eind's Greynir translation model.</p> <p>The original full dataset consists of 269 / 1,410 / 13,200 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). These splits are new and there can thus be some overlap between the original validation and test sets and our validation and test sets.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Af hverju er \u00f6ruggara a\u00f0 horfa \u00e1 tungli\u00f0 en a\u00f0 horfa \u00e1 s\u00f3lina?\\nSvarm\u00f6guleikar:\\na. Tungli\u00f0 er minna bjart.\\nb. Tungli\u00f0 er n\u00e6r j\u00f6r\u00f0inni.\\nc. Tungli\u00f0 sk\u00edn a\u00f0allega \u00e1 n\u00f3ttunni.\\nd. Tungli\u00f0 er a\u00f0eins fullt einu sinni \u00ed m\u00e1nu\u00f0i.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hva\u00f0a l\u00f6g jar\u00f0ar eru a\u00f0allega ger\u00f0 \u00far f\u00f6stu efni?\\nSvarm\u00f6guleikar:\\na. innri kjarni og ytri kjarni\\nb. skorpu og innri kjarni\\nc. skorpu og m\u00f6ttli\\nd. m\u00f6ttli og ytri kjarni\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Bekkur er a\u00f0 rannsaka \u00fe\u00e9ttleika bergs\u00fdna. Hva\u00f0a v\u00edsindalegan b\u00fana\u00f0 \u00feurfa \u00feau til a\u00f0 \u00e1kvar\u00f0a \u00fe\u00e9ttleika bergs\u00fdnanna?\\nSvarm\u00f6guleikar:\\na. sm\u00e1sj\u00e1 og vog\\nb. bikar og m\u00e6ligl\u00f6s\\nc. m\u00e6ligl\u00f6s og vog\\nd. sm\u00e1sj\u00e1 og m\u00e6ligl\u00f6s\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\n</code></pre></li> <li>Base prompt template:   <pre><code>Spurningar: {text}\nSvarm\u00f6guleikar:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvara: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Spurningar: {text}\nSvarm\u00f6guleikar:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', 'b', 'c' e\u00f0a 'd', og engu \u00f6\u00f0ru.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset mmlu-is\n</code></pre>"},{"location":"datasets/icelandic/#common-sense-reasoning","title":"Common-sense Reasoning","text":""},{"location":"datasets/icelandic/#winogrande-is","title":"Winogrande-is","text":"<p>This dataset was published in this paper and is a manually translated and adapted version of the English WinoGrande dataset. The samples are sentences containing two nouns and an ambiguous pronoun, and the task is to determine which of the two nouns the pronoun refers to.</p> <p>The original full dataset consists of 1,095 samples, and we use a 64 / 128 / 896 split for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Eiginma\u00f0urinn hennar Myrru keypti handa henni h\u00e1lsmen me\u00f0 perlu og h\u00fan h\u00e9lt a\u00f0 \u00fea\u00f0 v\u00e6ri ekki ekta. _ var of gyllt.\\nSvarm\u00f6guleikar:\\na. perlan\\nb. h\u00e1lsmeni\u00f0\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Bergfinnur l\u00e9t sem hann heyr\u00f0i ekki \u00ed lekanum \u00ed krananum en hann haf\u00f0i ekkert um a\u00f0 velja \u00feegar hundurinn gelti. _ er h\u00e1v\u00e6rari.\\nSvarm\u00f6guleikar:\\na. lekinn\\nb. hundurinn\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Dan\u00eda var spenntari fyrir \u00fev\u00ed a\u00f0 heims\u00e6kja ritstj\u00f3rann en \u00deorl\u00e1ks\u00edna vegna \u00feess a\u00f0 _ fannst n\u00fdja b\u00f3kin geggju\u00f0.\\nSvarm\u00f6guleikar:\\na. \u00deorl\u00e1ks\u00ednu\\nb. Dan\u00edu\",\n  \"label\": \"b\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\n</code></pre></li> <li>Base prompt template:   <pre><code>Spurningar: {text}\nSvarm\u00f6guleikar:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvara: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Spurningar: {text}\nSvarm\u00f6guleikar:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', 'b', 'c' e\u00f0a 'd', og engu \u00f6\u00f0ru.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset winogrande-is\n</code></pre>"},{"location":"datasets/icelandic/#unofficial-hellaswag-is","title":"Unofficial: HellaSwag-is","text":"<p>This dataset is a machine translated version of the English HellaSwag dataset. The original dataset was based on both video descriptions from ActivityNet as well as how-to articles from WikiHow. The dataset was translated using Mi\u00f0eind's Greynir translation model.</p> <p>The original full dataset consists of 9,310 samples. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"[h\u00f6f.] Hvernig finna m\u00e1 samr\u00e6mi \u00ed l\u00edfinu [titill] Skuldbinda \u00feig til breytinga. [skref] Fyrsta skrefi\u00f0 til a\u00f0 n\u00e1 fram breytingum \u00ed l\u00edfinu er a\u00f0 skuldbinda sig til breytinga. Me\u00f0 \u00fev\u00ed a\u00f0 gefa me\u00f0vita\u00f0a, viljasetta yfirl\u00fdsingu til sj\u00e1lfs s\u00edns um a\u00f0 \u00fe\u00fa munir halda \u00feig vi\u00f0 efni\u00f0 og n\u00e1 settum \u00e1rangri getur \u00fea\u00f0 hj\u00e1lpa\u00f0 \u00fe\u00e9r a\u00f0 halda \u00fe\u00e9r vi\u00f0 efni\u00f0 og \u00fdtt \u00fe\u00e9r \u00e1fram \u00ed \u00e1tt a\u00f0 \u00fev\u00ed markmi\u00f0i.\\nSvarm\u00f6guleikar:\\na. \u00de\u00e1 \u00e6ttir \u00fe\u00fa a\u00f0 vera a\u00f0 skuldbinda \u00feig til a\u00f0 lifa st\u00f6\u00f0ugra og samr\u00e6mdara l\u00edfi. [Undirskrefi] Hugsa\u00f0u um \u00e1st\u00e6\u00f0urnar fyrir \u00fev\u00ed a\u00f0 \u00fe\u00fa vilt lifa samr\u00e6mdara l\u00edfi.\\nb. [undirefni] Byrja\u00f0u \u00e1 \u00fev\u00ed a\u00f0 skuldbinda \u00feig til a\u00f0 breyta einhverju sem kemur \u00fe\u00e9r \u00far jafnv\u00e6gi. Ef \u00fe\u00fa gerir \u00fea\u00f0 ekki \u00fe\u00e1 situr\u00f0u uppi me\u00f0 eitthva\u00f0 sem lo\u00f0ir vi\u00f0 \u00feig heima hj\u00e1 \u00fe\u00e9r, sem ver\u00f0ur ekki au\u00f0veldara a\u00f0 koma \u00ed sta\u00f0inn fyrir \u00fe\u00e1 tilfinningu.\\nc. [Undirefni] Ekki l\u00e1ta sko\u00f0anir \u00fe\u00ednar e\u00f0a sko\u00f0anir stangast \u00e1 vi\u00f0 sj\u00e1lfsvir\u00f0ingu \u00fe\u00edna. Vi\u00f0urkenndu a\u00f0 \u00fe\u00fa s\u00e9rt fullor\u00f0inn og \u00fev\u00ed \u00f3hr\u00e6ddur vi\u00f0 a\u00f0 taka \u00fe\u00ednar eigin \u00e1kvar\u00f0anir var\u00f0andi \u00fea\u00f0 sem \u00fe\u00fa vilt \u00ed l\u00edfinu.\\nd. [Efnisor\u00f0] \u00deegar einhver annar hvetur \u00feig til a\u00f0 breyta, \u00fe\u00e1 skaltu ver\u00f0launa \u00feig fyrir \u00fea\u00f0 g\u00f3\u00f0a sem \u00fe\u00fa n\u00e6r\u00f0 fram \u00fe\u00f3 a\u00f0 \u00fea\u00f0 hafi kannski ekki liti\u00f0 \u00fat \u00e1 einhvern h\u00e1tt. [Titill] Ekki \u00e6tlast til \u00feess a\u00f0 f\u00f3lk breyti s\u00e9r af skyldur\u00e6kni.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Ma\u00f0ur er a\u00f0 vinna \u00e1 spor\u00f6skjulaga v\u00e9l. \u00fea\u00f0\\nSvarm\u00f6guleikar:\\na. gr\u00edpur og st\u00fdrir t\u00e6kinu.\\nb. s\u00fdnir skj\u00e1inn \u00e1 v\u00e9linni.\\nc. er s\u00fdnd \u00ed tveimur hlutum, sem hver um sig er festur af manneskju.\\nd. vir\u00f0ist vera vins\u00e6ll eftir \u00fev\u00ed sem hann vinnur sig upp.\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Sle\u00f0ast\u00falka \u00e1 uppbl\u00e1snum b\u00e1t heldur \u00e1 streng framan \u00e1 mann, allt \u00ed einu dettur h\u00fan \u00ed holu. F\u00f3lk ber sle\u00f0ab\u00e1ta og sle\u00f0ast\u00falkan er \u00e1 sle\u00f0ab\u00e1ti. eftir h\u00f3p af f\u00f3lki\\nSvarm\u00f6guleikar:\\na. sle\u00f0a saman kan\u00f3um, svo sle\u00f0a a\u00f0rir \u00ed vatninu.\\nb. sle\u00f0a hli\u00f0ar vatnsvatn \u00e1 hestum vi\u00f0 hli\u00f0ina \u00e1 br\u00fa b\u00e1ta.\\nc. sle\u00f0a ni\u00f0ur brekkuna \u00feanga\u00f0 til hitta a\u00f0ra einstaklinga.\\nd. Sle\u00f0amenn ganga \u00e1 torgi, \u00e1 milli annarra og s\u00ed\u00f0an hlaupa allir um.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\n</code></pre></li> <li>Base prompt template:   <pre><code>Spurningar: {text}\nSvarm\u00f6guleikar:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvara: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Spurningar: {text}\nSvarm\u00f6guleikar:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', 'b', 'c' e\u00f0a 'd', og engu \u00f6\u00f0ru.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset hellaswag-is\n</code></pre>"},{"location":"datasets/icelandic/#summarization","title":"Summarization","text":""},{"location":"datasets/icelandic/#rrn","title":"RRN","text":"<p>This dataset was published in this paper and consists of news articles and their summaries from R\u00daV, the Icelandic National Broadcasting Service, from years 2021 and 2022.</p> <p>The original full dataset consists of 3,960 samples, and we use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Vi\u00f0 erum a\u00f0 sj\u00e1 \u00f3tta um truflanir \u00e1 framlei\u00f0sluke\u00f0jum og efnahagsstarfsemi eitthva\u00f0 \u00ed l\u00edkingu vi\u00f0 \u00fea\u00f0 sem var fyrr \u00e1 \u00e1rinu.\\nsegir J\u00f3n Bjarki Bentsson a\u00f0alhagfr\u00e6\u00f0ingur \u00cdslandsbanka. \u00c1hrif Delta afbrig\u00f0isins sj\u00e1st v\u00ed\u00f0a. Eftirspurn hefur ekki haldist \u00ed hendur vi\u00f0 v\u00e6ntingar sem me\u00f0al annars hefur orsaka\u00f0 mikla ver\u00f0l\u00e6kkun \u00e1 ol\u00edu \u00e1 heimsmarka\u00f0i undanfarnar vikur. Hefur ver\u00f0i\u00f0 \u00e1 ekki veri\u00f0 l\u00e6gra \u00ed \u00ferj\u00e1 m\u00e1nu\u00f0i.\\nB\u00edlaframlei\u00f0eindur eru einnig \u00ed vanda, en \u00fear er vandam\u00e1li\u00f0 ekki skortur \u00e1 eftirspurn heldur skortur \u00e1 a\u00f0f\u00f6ngum, \u00e1 svok\u00f6llu\u00f0um h\u00e1lflei\u00f0urum n\u00e1nar tilteki\u00f0. \u00deeir eru a\u00f0allega framleiddir \u00ed As\u00edu og hefur \u00fatbrei\u00f0sla Delta afbrig\u00f0isins raska\u00f0 framlei\u00f0slu og framkalla\u00f0 skort. Margir af st\u00e6rstu b\u00edlaframlei\u00f0endum heims hafa tilkynnt um a\u00f0 \u00feeir ney\u00f0ist til a\u00f0 draga \u00far framlei\u00f0slu og \u00fearf Toyota, st\u00e6rsti b\u00edlaframlei\u00f0andi heims, a\u00f0 minnka framlei\u00f0slu s\u00edna um 40 pr\u00f3sent.\\n\u00c1standi\u00f0 hefur s\u00f6mulei\u00f0is valdi\u00f0 mikilli styrkingu dollars. Mi\u00f0gengi se\u00f0labanka \u00cdslands \u00ed dag er 128 kr\u00f3nur en var \u00ed byrjun sumars 121 kr\u00f3na. \u00c1 sama t\u00edma hefur kr\u00f3nan haldist st\u00f6\u00f0ug gagnvart \u00f6\u00f0rum myntum. Auk \u00fatbrei\u00f0slu Delta afbrig\u00f0isins hafa atbur\u00f0ir li\u00f0inna vikna \u00ed Afganistan \u00fer\u00fdst \u00e1 styrkingu dollarsins.\\n\u00deetta hefur allt \u00e1hrif til \u00feess a\u00f0 hvetja til \u00f3tta \u00ed \u00f6ryggi eins og svo er kalla\u00f0 og dollarinn n\u00fdtur oft g\u00f3\u00f0s af svolei\u00f0is \u00f3tta. \u00deykir n\u00e1tt\u00farlega gr\u00ed\u00f0arlega \u00f6rugg eign a\u00f0 hafa og seljanleiki hans er n\u00e1tt\u00farlega meiri en nokkurs annars eigna flokks.\",\n  \"target_text\": \"\u00datbrei\u00f0sla Delta afbrig\u00f0is k\u00f3r\u00f3nuveirunnar \u00f3gnar bata heimshagkerfisins. Ol\u00eduver\u00f0 hefur hr\u00ed\u00f0falli\u00f0 \u00e1 undanf\u00f6rnum vikum, b\u00edlaframlei\u00f0endur f\u00e1 ekki a\u00f0f\u00f6ng og fj\u00e1rfestar flykkjast \u00ed bandar\u00edkjadollar. \"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Ve\u00f0urfar hefur veri\u00f0 \u00f3venjulegt \u00e1 su\u00f0vesturhorni landsins. L\u00edti\u00f0 snj\u00f3a\u00f0i \u00ed vetur og s\u00ed\u00f0ustu vikur hefur \u00farkoma veri\u00f0 me\u00f0 allra minnsta m\u00f3ti. J\u00f3n \u00de\u00f3r \u00d3lason, forma\u00f0ur Stangvei\u00f0if\u00e9lags Reykjav\u00edkur, segir a\u00f0 vei\u00f0imenn s\u00e9u vissulega or\u00f0nir langeygir eftir rigningunni, en b\u00e6tir vi\u00f0 a\u00f0 eitt helsta einkenni \u00edslenskra vei\u00f0imanna s\u00e9 \u00f3bilandi bjarts\u00fdni.\\nJ\u00f3n \u00de\u00f3r segir a\u00f0 nor\u00f0an- og austanlands s\u00e9u horfurnar betri. \u00deurrkat\u00ed\u00f0in hefur \u00fe\u00f3 ekki haft \u00e1hrif \u00e1 s\u00f6lu vei\u00f0ileyfa. \u00d3vissan um ve\u00f0urfar fylgi me\u00f0 \u00ed kaupunum og n\u00fa \u00feegar eru margar af \u00e1m f\u00e9lagsins uppseldar. \u00de\u00e1 er von \u00e1 fleiri \u00fatlendingum \u00ed \u00e1r en \u00ed fyrra, en k\u00f3r\u00f3nuveirufaraldurinn haf\u00f0i mj\u00f6g mikil \u00e1hrif \u00e1 s\u00f6lu vei\u00f0ileyfa \u00ed fyrra.\",\n  \"target_text\": \"Forma\u00f0ur Stangavei\u00f0if\u00e9lags Reykjav\u00edkur segir vei\u00f0imenn \u00e1 su\u00f0vesturhorni landsins dansa n\u00fa regndans \u00ed von um a\u00f0 langvarandi \u00feurrkat\u00ed\u00f0 s\u00e9 senn \u00e1 enda.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"\u00cd morgun fjarl\u00e6g\u00f0u b\u00e6jarstarfsmenn \u00e1berandi kosningabor\u00f0a frambo\u00f0sins Vina K\u00f3pavogs \u00e1 horni Digranesvegar og Gr\u00e6nutungu. J\u00f3hann Sigurbj\u00f6rnsson, sem er \u00ed18. s\u00e6ti \u00e1 lista Vina K\u00f3pavogs, setti bor\u00f0ana upp og er afar \u00f3s\u00e1ttur vi\u00f0 \u00feeir hafi veri\u00f0 fjarl\u00e6g\u00f0ir. Hann segir a\u00f0 vegi\u00f0 s\u00e9 a\u00f0 tj\u00e1ningarfrelsi s\u00ednu.\\n\u00c9g hengi upp bor\u00f0a vegna \u00feess a\u00f0 \u00e9g tel mig vera \u00ed fullum r\u00e9tti til a\u00f0 tj\u00e1 mig um \u00fe\u00e6r framkv\u00e6mdir sem eru \u00ed gangi h\u00e9rna \u00e1 m\u00f3ti m\u00e9r. \u00c9g hengi upp \u00feessa bor\u00f0a \u00e1 grindverki\u00f0 sem er r\u00e9tt fyrir innan l\u00f3\u00f0am\u00f6rk s\u00ed\u00f0an koma hinga\u00f0 menn \u00ed gulum f\u00f6tum \u00ed morgun fr\u00e1 b\u00e6num sem fjarl\u00e6gja bor\u00f0ana.\\nB\u00e6jarstarfsmenn hafa undanfari\u00f0 veri\u00f0 \u00ed samskiptum vi\u00f0 frambo\u00f0i\u00f0 um a\u00f0 broti\u00f0 hafi veri\u00f0 gegn l\u00f6greglusam\u00feykkt og byggingarregluger\u00f0 me\u00f0 \u00fev\u00ed a\u00f0 setja upp augl\u00fdsingabor\u00f0a \u00e1 l\u00f3\u00f0am\u00f6rkum og utan \u00feeirra, og einnig svo st\u00f3ra augl\u00fdsingabor\u00f0a a\u00f0 s\u00e9rstakt leyfi \u00feurfi.\\nSigr\u00ed\u00f0ur Bj\u00f6rg T\u00f3masd\u00f3ttir uppl\u00fdsingafulltr\u00fai K\u00f3pavogsb\u00e6jar segir \u00ed samtali vi\u00f0 fr\u00e9ttastofu a\u00f0 sk\u00fdrar reglur gildi um uppsetningu augl\u00fdsingaskilta. Reglur um sl\u00edka uppsetningu hafi veri\u00f0 sendar a\u00f0 gefnu tilefni \u00e1 alla frambo\u00f0sflokka \u00ed K\u00f3pavogi fyrir helgi. \u00de\u00e1 hafi st\u00f3rt augl\u00fdsingaskilti \u00e1 vegum Frams\u00f3knarflokksins \u00ed Sk\u00f3garlind veri\u00f0 fjarl\u00e6gt af b\u00e6jaryfirv\u00f6ldum \u00ed s\u00ed\u00f0ustu viku. Sigr\u00ed\u00f0ur segir a\u00f0 skiltin ver\u00f0i a\u00f0 vera undir tveimur fermetrum til a\u00f0 mega vera uppi - annars \u00feurfi a\u00f0 s\u00e6kja um leyfi fr\u00e1 byggingarfulltr\u00faa K\u00f3pavogsb\u00e6jar. Reglurnar s\u00e9u sk\u00fdrar.\\nHelga, Oddviti Vina K\u00f3pavogsb\u00e6jar segist hissa yfir framgangi b\u00e6jaryfirvalda, \u00feetta geti ekki sta\u00f0ist sko\u00f0un og a\u00f0 frambo\u00f0i\u00f0 muni leita r\u00e9ttar s\u00edns.\",\n  \"target_text\": \"Augl\u00fdsingaskilti og frambo\u00f0sbor\u00f0ar hafa veri\u00f0 fjarl\u00e6g\u00f0 af b\u00e6jaryfirv\u00f6ldum \u00ed K\u00f3pavogi v\u00ed\u00f0s vegar um b\u00e6inn s\u00ed\u00f0ustu daga. \"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru fr\u00e9ttagreinar me\u00f0 tilheyrandi samantektum.\n</code></pre></li> <li>Base prompt template:   <pre><code>Fr\u00e9ttagrein: {text}\nSamantekt: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Fr\u00e9ttagrein: {text}\n\nSkrifa\u00f0u samantekt um ofangreindu grein.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset rrn\n</code></pre>"},{"location":"datasets/italian/","title":"\ud83c\uddee\ud83c\uddf9 Italian","text":"<p>This is an overview of all the datasets used in the Italian part of EuroEval. The datasets are grouped by their task - see the task overview for more information about what these constitute.</p>"},{"location":"datasets/italian/#sentiment-classification","title":"Sentiment Classification","text":""},{"location":"datasets/italian/#sentipolc-16","title":"Sentipolc-16","text":"<p>This dataset was published in this paper and slightly modified in this paper. It is based on Italian tweets, which were manually annotated by three annotators.</p> <p>The original full dataset consists of 1,839 / 324 / 870 samples, and we use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively. The splits are new and there can thus be some overlap between the original validation and test sets and our validation and test sets.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"RT @user: Siamo dei falsi. I ragazzi vogliono le ragazze timide e poi stanno con le troie. Le ragazze vogliono i dolci e poi amano con\u2026\",\n  \"label\": \"negative\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Ho aggiunto un video a una playlist di @user: http ROMA PRESENTAZIONE LIBRO SVIMEZ SULL\u2019ECONOMIA DEL\",\n  \"label\": \"neutral\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"RT @user: @user te lo auguro di cuore e far\u00f2 il possibile affinch\u00e9 sia cos\u00ec. Un abbraccio\",\n  \"label\": \"positive\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>Di seguito sono riportati i testi e il loro sentimento, che pu\u00f2 essere 'positivo', 'neutro' o 'negativo'.\n</code></pre></li> <li>Base prompt template:   <pre><code>Tweet: {text}\nSentimento: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Tweet: {text}\n\nClassificare il sentimento nel Tweet. Rispondete con 'positivo', 'neutro' o 'negativo', e nient'altro.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset sentipolc16\n</code></pre>"},{"location":"datasets/italian/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"datasets/italian/#multinerd-it","title":"MultiNERD IT","text":"<p>This dataset was published in this paper and consists of sentences from Wikipedia and Wikinews in 10 different languages. It is an extension of the combination of WikiNEuRal and NER4EL. The original test set was created from manual annotations, while the training set is based on an automatic annotation pipeline.</p> <p>The Italian part of the original dataset consists of 181,927 sentences, split into 145,520 / 18,190 / 18,217 for training, validation, and testing respectively. We use given splits, and use 1,024 / 256 / 2,048 samples for training, validation, and testing, respectively.</p> <p>We have furthermore converted their fine-grained labelling scheme to the CoNLL-2003 labelling scheme, which is more common in the NER literature. The mapping is as follows:</p> <ul> <li><code>PERS</code> \u27a1\ufe0f <code>PER</code></li> <li><code>LOC</code> \u27a1\ufe0f <code>LOC</code></li> <li><code>ORG</code> \u27a1\ufe0f <code>ORG</code></li> <li><code>MISC</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>TIME</code> \u27a1\ufe0f <code>O</code></li> <li><code>ANIM</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>BIO</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>CEL</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>DIS</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>EVE</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>FOOD</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>INST</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>MEDIA</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>MYTH</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>PLANT</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>VEHI</code> \u27a1\ufe0f <code>MISC</code></li> </ul> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"tokens\": array(['Alcune' 'statue' 'che' 'la' 'rappresentano' 'vennero' 'ritrovate' 'non' 'lontano' 'da' 'Tani' ',' 'anche' 'se' 'in' 'nessuna' 'di' 'queste' 'si' '\u00e8' 'conservato' 'il' 'volto' ',' 'mentre' 'nella' 'seconda' 'cateratta' '\u00e8' 'registrata' 'una' 'piena' 'del' 'Nilo' 'datata' 'al' 'suo' '3\u00ba' 'anno' 'di' 'regno' '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['Nella' 'seconda' 'met\u00e0' 'del' 'XX' 'secolo' 'gli' 'infinitesimi' 'sono' 'stati' 'recuperati' ',' 'in' 'una' 'prospettiva' 'rigorosa' ',' 'da' 'Abraham' 'Robinson' ',' 'nella' 'formulazione' 'di' 'quella' 'che' 'lui' 'chiam\u00f2' 'analisi' 'non' 'standard' '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['Il' 'monumento' 'a' 'Carlo' 'Emanuele' 'III' 'di' 'Savoia' '\u00e8' 'ubicato' 'nella' 'piazza' 'omonima' 'sul' 'lungomare' '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>Di seguito sono riportate le frasi e i dizionari JSON con le entit\u00e0 denominate presenti nella frase data.\n</code></pre></li> <li>Base prompt template:   <pre><code>Frase: {text}\nEntit\u00e0 denominate: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Frase: {text}\n\nIdentificare le entit\u00e0 nominate nella frase. Il risultato dovrebbe essere un dizionario JSON con le chiavi 'persona', 'posizione', 'organizzazione' e 'varie'. I valori devono essere elenchi di entit\u00e0 nominate di quel tipo, esattamente come appaiono nella frase.\n</code></pre></li> <li>Label mapping:<ul> <li><code>B-PER</code> \u27a1\ufe0f <code>persona</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>persona</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>posizione</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>posizione</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>organizzazione</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>organizzazione</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>varie</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>varie</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset multinerd-it\n</code></pre>"},{"location":"datasets/italian/#unofficial-wikineural-it","title":"Unofficial: WikiNEuRal IT","text":"<p>This dataset was published in this paper and consists of sentences from Wikipedia in 9 different languages. The annotations are automatic but at the time novel and state-of-the-art methodologies.</p> <p>The Italian part of the original dataset consists of 110,519 sentences, split into 88,400 / 11,050 / 11,069 for training, validation, and testing respectively. We use given splits, and use 1,024 / 256 / 2,048 samples for training, validation, and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"tokens\": array(['Comunque' ',' 'il' 'poema' 'sarebbe' 'stato' 'influenzato' 'da' 'una' '\"' 'tematica' 'di' 'regime' '\"' 'voluta' 'dalla' 'politica' 'culturale' 'di' 'Domiziano' 'nella' 'quale' 'rientrano' 'anche' 'i' '\"' 'Punica' '\"' 'di' 'Silio' 'Italico' '.']),\n  \"labels\": array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-PER', 'I-PER', 'O'])\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['\u00c8' 'stato' 'uno' 'degli' 'artisti' 'pi\u00f9' 'importanti' \"dell'\" 'etichetta' 'discografica' 'di' 'musica' 'soul' 'Stax' 'Records' 'che' 'negli' 'anni' 'sessanta' 'e' 'settanta' 'era' 'la' 'principale' 'antagonista' 'della' 'Motown' 'nel' 'campo' 'della' 'black' 'music' '.']),\n  \"labels\": array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O'])\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['Decise' 'di' 'scrivere' 'una' 'serie' 'di' 'saggi' 'e' 'presentarli' 'in' 'un' 'periodico' 'intitolato' '\"' 'The' 'Rambler' '\"' 'che' 'sarebbe' 'stato' 'messo' 'in' 'vendita' 'per' 'pochi' 'centesimi' 'ogni' 'marted\u00ec' 'e' 'sabato' '.']),\n  \"labels\": array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>Di seguito sono riportate le frasi e i dizionari JSON con le entit\u00e0 denominate presenti nella frase data.\n</code></pre></li> <li>Base prompt template:   <pre><code>Frase: {text}\nEntit\u00e0 denominate: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Frase: {text}\n\nIdentificare le entit\u00e0 nominate nella frase. Il risultato dovrebbe essere un dizionario JSON con le chiavi 'persona', 'posizione', 'organizzazione' e 'varie'. I valori devono essere elenchi di entit\u00e0 nominate di quel tipo, esattamente come appaiono nella frase.\n</code></pre></li> <li>Label mapping:<ul> <li><code>B-PER</code> \u27a1\ufe0f <code>persona</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>persona</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>posizione</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>posizione</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>organizzazione</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>organizzazione</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>varie</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>varie</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset wikineural-it\n</code></pre>"},{"location":"datasets/italian/#linguistic-acceptability","title":"Linguistic Acceptability","text":""},{"location":"datasets/italian/#scala-it","title":"ScaLA-it","text":"<p>This dataset was published in this paper is automatically created from the Italian Universal Dependencies treebank by assuming that the documents in the treebank are correct, and corrupting the samples to create grammatically incorrect samples. The corruptions were done by either removing a word from a sentence, or by swapping two neighbouring words in a sentence. To ensure that this does indeed break the grammaticality of the sentence, a set of rules were used on the part-of-speech tags of the words in the sentence.</p> <p>The original full dataset consists of 13,121 / 564 / 482 samples for training, validation and testing, respectively. We use 512 / 128 / 1,024, sampled from a combination of all the splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Il Presidente della di la Repubblica non \u00e8 responsabile degli di gli atti compiuti nell' in l' esercizio delle di le sue funzioni, tranne che per alto tradimento o per attentato alla a la Costituzione.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Ottimamente ha retto invece il cuore nuovo di Saverio Pallucca - alle a le spalle tre infarti, quattro by-pass, un trapianto cardiaco meno di due anni fa - nell' in l' ultima edizione della di la famosa maratona di New York.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Un secondo gruppo di problemi riguarda la necessit\u00e0 di garantire che il sistema economico venga percepito come fondamentalmente equo, che rappresenta la chiave della la di sua sostenibilit\u00e0 politica.\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Di seguito sono riportate le frasi e la loro correttezza grammaticale.\n</code></pre></li> <li>Base prompt template:   <pre><code>Frase: {text}\nGrammaticalmente corretto: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Frase: {text}\n\nStabilite se la frase \u00e8 grammaticalmente corretta o meno. Rispondete con 'si' se la frase \u00e8 corretta e con 'no' se non lo \u00e8, e nient'altro.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>si</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>no</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scala-it\n</code></pre>"},{"location":"datasets/italian/#reading-comprehension","title":"Reading Comprehension","text":""},{"location":"datasets/italian/#squad-it","title":"SQuAD-it","text":"<p>This dataset is derived from the SQuAD 1.1 dataset and was published in this paper. The questions and answers were obtained through \"semi-automatic\" translation, using DeepL, of the SQuAD dataset to Italian. The dataset consists of 54,159 / 7,609 question/answer pairs for training and test respectively. We use 1,024 / 256 / 2,048 samples for training, validation, and testing, respectively. Our training split is a subset of the original training split, and our validation and testing splits are subsets of the original test split.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  'context': \"Lo studio del Corano e dell' Hadith prosper\u00f2 in un' atmosfera cos\u00ec studiosa. Filosofia, Fiqh e teologia (kalaam) sono stati ulteriormente sviluppati, in particolare da Avicenna e dai suoi avversari. Al-Razi e Al-Farabi avevano fornito metodologie e conoscenze in medicina e filosofia. Avicenna ha avuto accesso alle grandi biblioteche di Balkh, Khwarezm, Gorgan, Rey, Isfahan e Hamadan. Vari testi (come il' Ahd con Bahmanyar') mostrano che egli ha dibattuto punti filosofici con i pi\u00f9 grandi studiosi del tempo. Aruzi Samarqandi descrive come prima che Avicenna lasciasse Khwarezm aveva conosciuto Al-Biruni (un famoso scienziato e astronomo), Abu Nasr Iraqi (un famoso matematico), Abu Sahl Masihi (un illustre filosofo) e Abu al-Khayr Khammar (un grande medico).\",\n  'question': \"Che cosa \u00e8 stato un tema che Avicenna ha ulteriormente sviluppato?\",\n  'answers': {\n    'answer_start':  array([95]),\n    'text': array(['teologia'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': \"Florida Alta Velocit\u00e0 ferroviaria \u00e8 stata proposta ferroviaria ad alta velocit\u00e0 sostenuta dal governo che avrebbe collegato Miami, Orlando e Tampa. La prima fase \u00e8 stata pianificata per collegare Orlando e Tampa ed \u00e8 stato offerto un finanziamento federale, ma \u00e8 stato respinto dal governatore Rick Scott nel 2011. La seconda fase della linea \u00e8 stata prevista per collegare Miami. Entro il 2014, un progetto privato conosciuto come All Aboard Florida da parte di una societ\u00e0 della storica Florida East Coast Railway ha iniziato la costruzione di una linea ferroviaria ad alta velocit\u00e0 nel sud della Florida che dovrebbe terminare all' aeroporto internazionale di Orlando.\",\n  'question': \"In quale anno ha iniziato All Aboard Florida?\",\n  'answers': {\n    'answer_start': array([390]),\n    'text': array(['2014'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': \"Gli insetti sociali, come le termiti, le formiche e molte api e vespe, sono la specie pi\u00f9 familiare di animali eusociali. Vivono insieme in grandi colonie ben organizzate che possono essere cos\u00ec strettamente integrate e geneticamente simili che le colonie di alcune specie sono talvolta considerate superorganismi. Talvolta si sostiene che le varie specie di api da miele siano gli unici invertebrati (e addirittura uno dei pochi gruppi non umani) ad aver evoluto un sistema di comunicazione simbolica astratta in cui un comportamento viene utilizzato per rappresentare e trasmettere informazioni specifiche su qualcosa nell' ambiente. In questo sistema di comunicazione, chiamato linguaggio dance, l' angolo in cui una danza d' ape rappresenta una direzione relativa al sole, e la lunghezza della danza rappresenta la distanza da volare. 309-311 Anche se forse non cos\u00ec avanzato come le api mellifere, anche i bombi hanno potenzialmente alcuni comportamenti di comunicazione sociale.\",\n  'question': \"Termiti, api, vespe e quali altri insetti sono insetti sociali?\",\n  'answers': {\n    'answer_start': array([41]),\n    'text': array(['formiche'], dtype=object)\n  }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>I testi che seguono sono accompagnati da domande e risposte.\n</code></pre></li> <li>Base prompt template:   <pre><code>Testo: {text}\nDomanda: {question}\nRispondere in massimo 3 parole: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Testo: {text}\n\nRispondi alla seguente domanda sul in un massimo di 3 parole.\n\nDomanda: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset squad-it\n</code></pre>"},{"location":"datasets/italian/#unofficial-belebele-it","title":"Unofficial: BeleBele-it","text":"<p>This dataset was published in this paper and features multiple-choice reading comprehension questions across 122 languages.</p> <p>The original dataset contains 900 unique multiple-choice reading comprehension passages and questions. From these, we use a 256 / 64 / 580 split for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Testo: Con la decisione del signor Rudd di firmare l\u2019accordo sul clima di Kyoto, gli Stati Uniti, che ora saranno l\u2019unica nazione sviluppata a non averlo ratificato, rimangono isolati. Il precedente governo conservatore australiano aveva rifiutato di ratificare gli accordi di Kyoto asserendo che avrebbero danneggiato l'economia, data la pesante dipendenza dalle esportazioni di carbone, mentre gli obiettivi sulle emissioni non sarebbero stati vincolanti per Paesi come l'India e la Cina.\\nDomanda: Il precedente governo australiano pensava che la ratifica di Kyoto avrebbe causato danni a cosa?\\nOpzioni:\\na. Stati Uniti\\nb. Economia del Paese\\nc. Esportazioni di carbone\\nd. Gli obiettivi di emissione del Paese\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Testo: \"I commenti, in diretta televisiva, hanno rappresentato la prima occasione per autorevoli fonti iraniane per ammettere che le sanzioni sono efficaci. Esse comprendono limitazioni finanziarie e il divieto dell\\'Unione europea all\\'esportazione di petrolio greggio, che rappresenta l\\'80% del reddito estero nell\\'economia dell\\'Iran. Secondo l\\'ultimo rapporto mensile dell\u2019OPEC, il volume delle esportazioni di greggio \u00e8 sceso al livello pi\u00f9 basso degli ultimi vent\\'anni, con 2,8 milioni di barili al giorno. Il leader supremo del Paese, l\u2019Ayatollah Ali Khamenei, ha parlato della dipendenza dal petrolio paragonandola ad \"\"una trappola\"\" che risale al periodo precedente la rivoluzione islamica iraniana del 1979 e dalla quale il Paese si dovrebbe liberare.\"\\nDomanda: Secondo il passaggio, chi ha ammesso gli effetti delle sanzioni sull\\'economia iraniana?\\nOpzioni:\\na. Autorevoli fonti\\nb. OPEC\\nc. Ayatollah Ali Khamenei\\nd. L\\'Unione Europea\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Testo: Il dottor Lee si \u00e8 detto preoccupato anche in merito ai rapporti che rivelano che i bambini in Turchia ora sono stati contagiati dal virus dell'influenza aviaria A(H5N1) senza ammalarsi. Ha sottolineato che secondo alcuni studi la malattia diventer\u00e0 meno mortale prima che possa causare un'epidemia globale. Si teme che se permangono sintomi influenzali di lieve entit\u00e0, i pazienti possano continuare a contagiare pi\u00f9 persone durante la loro routine quotidiana.\\nDomanda: Secondo il brano, cosa dovrebbe accadere alla malattia prima di causare un'epidemia globale?\\nOpzioni:\\na. Deve diventare meno letale\\nb. I sintomi devono rimanere lievi\\nc. Occorre che pi\u00f9 pazienti vengano infettati\\nd. I bambini devono manifestare i sintomi\",\n  \"label\": \"a\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Le seguenti sono domande a scelta multipla (con relative risposte).\n</code></pre></li> <li>Base prompt template:   <pre><code>Domanda: {text}\nOpzioni:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nRisposta: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Domanda: {text}\nOpzioni:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nRispondete alla domanda precedente con 'a', 'b', 'c' o 'd', e nient'altro.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset belebele-it\n</code></pre>"},{"location":"datasets/italian/#knowledge","title":"Knowledge","text":""},{"location":"datasets/italian/#mmlu-it","title":"MMLU-it","text":"<p>This dataset is a machine translated version of the English MMLU dataset and features questions within 57 different topics, such as elementary mathematics, US history and law. The translation to Italian was done by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 269 / 1,410 / 13,200 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). These splits are new and there can thus be some overlap between the original validation and test sets and our validation and test sets.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Quale delle seguenti situazioni \u00e8 meglio modellata dalla distribuzione binomiale?\\nScelte:\\na. Il numero di minuti in un'ora in cui la media Dow-Jones \u00e8 superiore alla sua media iniziale del giorno.\\nb. Il numero di citt\u00e0 tra le 10 pi\u00f9 grandi dello Stato di New York in cui il tempo \u00e8 nuvoloso per la maggior parte di un determinato giorno.\\nc. Il numero di conducenti che indossano le cinture di sicurezza se 10 conducenti consecutivi vengono fermati in un posto di blocco della polizia.\\nd. Nessuna delle precedenti.\",\n  \"label\": \"d\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Il 'nuovo razzismo' si riferisce a:\\nScelte:\\na. una forma pi\u00f9 sottile di pregiudizio, mascherata dall'orgoglio nazionale\\nb. una decostruzione post-moderna delle idee razziste per rivelarne la mancanza di profondit\u00e0\\nc. pratiche razziste riscontrabili in aree sociali di recente emergenza, come il cyberspazio\\nd. un movimento antifascista che sfida le politiche nazionaliste\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Tutti i seguenti possono agire come messaggeri intracellulari, TRANNE\\nScelte:\\na. ioni di calcio\\nb. cAMP\\nc. acetilcolina\\nd. inositolo 1,4,5-trifosfato\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Le seguenti sono domande a scelta multipla (con relative risposte).\n</code></pre></li> <li>Base prompt template:   <pre><code>Domanda: {text}\nScelte:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nR\u00e9ponse: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Domanda: {text}\nScelte:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nRispondete alla domanda precedente con 'a', 'b', 'c' o 'd' e nient'altro.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset mmlu-it\n</code></pre>"},{"location":"datasets/italian/#common-sense-reasoning","title":"Common-sense Reasoning","text":""},{"location":"datasets/italian/#hellaswag-it","title":"HellaSwag-it","text":"<p>This dataset is a machine translated version of the English HellaSwag dataset. The original dataset was based on both video descriptions from ActivityNet as well as how-to articles from WikiHow. The dataset was translated by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 9,193 samples. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n    \"text\": \"[header] Come pulire il tappeto stainmaster [title] Aspirare lo sporco. [step] In primo luogo, si pu\u00f2 rimuovere molta della sporcizia con un aspirapolvere. Poi, si possono affrontare le macchie rimanenti con sapone e acqua.\\nScelte:\\na. Quindi, iniziare a spolverare e aspirare. Non usare prodotti chimici aggressivi, come detergenti abrasivi, poich\u00e9 questi possono causare la formazione di muffe sul tappeto.\\nb. [substeps] Fai spolverare la superficie prima di aspirare. Puoi farlo con un panno in microfibra o una spazzola.\\nc. [title] Usare sapone e acqua sulla macchia. [step] Mescolare acqua e \u00bc di tazza (21 grammi) di sapone liquido in una bottiglia spray e poi spruzzare direttamente questa miscela sulla macchia.\\nd. Cerca fango o macchie nere che puoi pulire localmente. [substeps] Se il tuo tappeto stainmaster non \u00e8 pulito, potrebbe essere necessario pulirlo da un professionista.\",\n    \"label\": \"c\"\n}\n</code></pre> <pre><code>{\n    \"text\": \"[header] Come sapere perch\u00e9 un bambino (sotto i 2 anni) sta piangendo [title] Ascolta il pianto forte, quasi un lamento. [step] Questo di solito significa \\\"ho dolore\\\" o \\\"sono malato\\\". Il bambino far\u00e0 una pausa, poi urler\u00e0 di nuovo e ripeter\u00e0 il processo.\\nScelte:\\na. Questo tipo di pianto \u00e8 di solito solo un segnale di avvertimento della fame. Un bambino pianger\u00e0 anche leggermente di pi\u00f9 se ha fame.\\nb. Questo pu\u00f2 essere molto sconvolgente da guardare, quindi fai venire un genitore ad aiutare il bambino. [substeps] Solo un genitore pu\u00f2 giudicare l'et\u00e0 del loro bambino.\\nc. Questo di solito finir\u00e0 dopo circa tre minuti. [title] Fai attenzione agli occhi chiusi del bambino.\\nd. \u00c8 persistente, penetrante e inequivocabile. Se senti questo pianto, vai immediatamente dal bambino.\",\n    \"label\": \"d\"\n}\n</code></pre> <pre><code>{\n    \"text\": \"Una donna mostra come asciugare la superficie del bancone e il lavandino dall'acqua schizzata dal rubinetto con un asciugamano di carta. una donna\\nScelte:\\na. mostra il suo metodo preparatorio meticoloso per il bancone e il pavimento sui quali applicher\u00e0 un asciugamano.\\nb. sta in cucina accanto al lavandino e parla alla telecamera.\\nc. impugna un asciugamano di carta e inizia a pulire una bevanda appoggiata sulla superficie del bancone e del lavandino.\\nd. sta di fronte ad un set di utensili sul bancone, prende un asciugacapelli con le sue parti accessorie fissate e sicure con una barra sul lavandino asciutto.\",\n    \"label\": \"b\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Le seguenti sono domande a scelta multipla (con relative risposte).\n</code></pre></li> <li>Base prompt template:   <pre><code>Domanda: {text}\nScelte:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nR\u00e9ponse: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Domanda: {text}\nScelte:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nRispondete alla domanda precedente con 'a', 'b', 'c' o 'd' e nient'altro.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset hellaswag-it\n</code></pre>"},{"location":"datasets/italian/#summarization","title":"Summarization","text":""},{"location":"datasets/italian/#ilpost-sum","title":"IlPost-Sum","text":"<p>This dataset was published in this paper and consists of news articles from Il Post. The summaries were written by the journalists themselves (the \"target\" field in the original dataset).</p> <p>The original dataset consists of 35,201 / 4,400 / 4,400 samples for training, validation and testing, respectively. We use 1,024 / 256 / 2,048 samples for training, validation, and testing, respectively. All our splits are subsets of the original ones.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Mai come nel 2013 abbiamo riflettuto sulla quantit\u00e0 di dati e informazioni su ciascuno di noi che nel corso degli anni hanno immagazzinato le grandi societ\u00e0 di Internet. Ne eravamo consapevoli anche prima, ma soprattutto in seguito alle rivelazioni sui sistemi usati dalla National Security Agency statunitense per spiare le attivit\u00e0 di centinaia di milioni di persone in giro per il mondo abbiamo iniziato a farci qualche domanda in pi\u00f9 su che fine facciano le email, le foto e gli aggiornamenti sui social network quando li carichiamo online. Sappiamo meglio di prima che tutte queste cose vengono consegnate alla rete \u201cper sempre\u201d e che continueranno a esistere su qualche server, anche se faremo clic sull\u2019icona di un cestino o su un tasto rosso con scritto sopra \u201cCancella\u201d. E forse proprio per questo motivo, in molti iniziano a provare sollievo nell\u2019avere a disposizione servizi e applicazioni che fanno l\u2019esatto contrario: che rendono effimera e del tutto temporanea l\u2019esistenza di qualcosa di nostro online. Come spiega Farhad Manjoo sul Wall Street Journal, la cosa pi\u00f9 rilevante in campo tecnologico nel 2013 \u00e8 stata probabilmente Snapchat, un\u2019applicazione basata su comunicazioni temporanee. In pochi anni ha ottenuto un successo considerevole, soprattutto negli Stati Uniti, attirando l\u2019attenzione di alcune grandi societ\u00e0 come Facebook e Google che si dice abbiano offerto diversi miliardi di dollari per acquisirla. Le offerte sono state fin qui rifiutate da quelli di Snapchat, che per ora sembrano essere solo interessati a migliorare e rendere ancora pi\u00f9 diffusa la loro applicazione.\",\n  \"target_text\": \"Snapchat e l\u2019Internet \u201ctemporanea\u201d. Come funziona \u2013 e cosa implica, per gli utenti \u2013 la popolare applicazione per mandarsi messaggi e foto che spariscono dopo pochi secondi, contesa a colpi di offerte miliardarie.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Con trovata da entertainer, nel suo discorso da sconfitto al ballottaggio delle primarie del centrosinistra, Matteo Renzi ha citato Bersani, \u201cma non Pierluigi, Samuele\u201d. \u00e8 sempre bellissima la cicatrice che mi ricorder\u00e0 di esser stato felice\",\n  \"target_text\": \"Pesce d\u2019aprile, Samuele Bersani. La canzone citata da Matteo Renzi nel suo \\\"concession speech\\\".\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Questa mattina i carabinieri hanno arrestato pi\u00f9 di 50 persone accusate di essere a capo o affiliate al clan mafioso D\u2019Abramo-Sforza. Gli arresti sono avvenuti a Bari, Altamura (Bari), Foggia, Cerignola (Foggia), Matera, Lecce e Roma. Le accuse contro gli arrestati sono di associazione armata di tipo mafioso, detenzione e porto d\u2019armi anche da guerra, traffico di sostanze stupefacenti, omicidio, tentato omicidio, estorsione, turbativa d\u2019asta. L\u2019operazione \u00e8 stata disposta dal gip di Bari su richiesta della Direzione distrettuale antimafia; le indagini sono state condotte dal nucleo investigativo del Comando provinciale Carabinieri di Bari.\",\n  \"target_text\": \"Sono state arrestate pi\u00f9 di 50 persone accusate di far parte del clan mafioso D\u2019Abramo-Sforza.\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>Di seguito sono riportati gli articoli con i relativi riassunti.\n</code></pre></li> <li>Base prompt template:   <pre><code>Articolo di cronaca: {text}\nSintesi: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Articolo di cronaca: {text}\n\nScrivete un riassunto dell'articolo sopra citato.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset ilpost-sum\n</code></pre>"},{"location":"datasets/norwegian/","title":"\ud83c\uddf3\ud83c\uddf4 Norwegian","text":"<p>This is an overview of all the datasets used in the Norwegian part of EuroEval. The datasets are grouped by their task - see the task overview for more information about what these constitute.</p>"},{"location":"datasets/norwegian/#sentiment-classification","title":"Sentiment Classification","text":""},{"location":"datasets/norwegian/#norec","title":"NoReC","text":"<p>This dataset was published in this paper and is based on reviews from three different media organisations: Schibsted Media Group, Aller Media and NRK.</p> <p>The original full dataset consists of 680,792 / 101,106 / 101,594 samples for training, validation and test, respectively. We use a split of 1,024 / 256 / 2,048 samples for training, validation and test, respectively. All the new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Den som ikke blir rystende ber\u00f8rt av \u00ab De utvalgte \u00bb , m\u00e5 v\u00e6re forherdet til det immune .\",\n  \"label\": \"positive\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Under er noen av funksjonene som er dels unike for LG G3 :\",\n  \"label\": \"neutral\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Tilsvarende f\u00e5r vi ogs\u00e5 lavere score i 3DMark enn hva tilfellet er for f.eks . Xperia Z2 og Galaxy S5 .\",\n  \"label\": \"negative\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er anmeldelser og deres sentiment, som kan v\u00e6re 'positiv', 'n\u00f8ytral' eller 'negativ'.\n</code></pre></li> <li>Base prompt template:   <pre><code>Anmeldelse: {text}\nSentiment: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Anmeldelse: {text}\n\nKlassifiser sentimentet i anmeldelsen. Svar med 'positiv', 'n\u00f8ytral' eller 'negativ'.\n</code></pre></li> <li>Label mapping:<ul> <li><code>positive</code> \u27a1\ufe0f <code>positiv</code></li> <li><code>neutral</code> \u27a1\ufe0f <code>n\u00f8ytral</code></li> <li><code>negative</code> \u27a1\ufe0f <code>negativ</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset norec\n</code></pre>"},{"location":"datasets/norwegian/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"datasets/norwegian/#norne-nb","title":"NorNE-nb","text":"<p>This dataset was published in this paper and is a manually NER annotated version of the Bokm\u00e5l Universal Dependencies treebank. The NER labels almost follow the CoNLL-2003 standard, but with some additional labels.</p> <p>The original full dataset consists of 15,696 / 2,410 / 1,939 samples for training, validation and test, respectively. We use a split of 1,024 / 256 / 2,048 samples for training, validation and test, respectively. The splits we use are new, so there might be some samples from the training split in the validation or test splits.</p> <p>We have mapped the labels into the CoNLL-2003 standard as follows:</p> <ul> <li><code>LOC</code> \u27a1\ufe0f <code>LOC</code></li> <li><code>PER</code> \u27a1\ufe0f <code>PER</code></li> <li><code>ORG</code> \u27a1\ufe0f <code>ORG</code></li> <li><code>MISC</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>GPE_LOC</code> \u27a1\ufe0f <code>LOC</code></li> <li><code>GPE_ORG</code> \u27a1\ufe0f <code>ORG</code></li> <li><code>PROD</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>DRV</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>EVT</code> \u27a1\ufe0f <code>MISC</code></li> </ul> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"tokens\": array(['Det', 'fremkommer', 'av', '\u00e5rsmeldingene', 'fra', 'Bergen', 'helser\u00e5d', 'i', '\u00e5rene', '1952', '-', '66', '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['Viktig', 'var', 'det', 'ogs\u00e5', 'at', 'Kina', 'allerede', 'var', 'blitt', 's\u00e5', 'avhengig', 'av', 'det', 'amerikanske', 'markedet', 'og', 'av', 'dollaren', ',', 'at', 'en', 'nedgang', 'i', 'USA', 'ogs\u00e5', 'ville', 'ramme', 'Kina', 'hardt', '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'B-ORG', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['Han', 'tok', 'fram', 'pistolen', 'og', 'dro', 'tilbake', 'til', 'Skaregata', '2', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O'], dtype=object)\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er fraser og JSON-ordb\u00f8ker med de navngitte enhetene som forekommer i den gitte frasen.\n</code></pre></li> <li>Base prompt template:   <pre><code>Frase: {text}\nNavngitte enheter: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Frase: {text}\n\nIdentifiser de navngitte enhetene i frasen. Du b\u00f8r outputte dette som en JSON-ordbok med n\u00f8klene 'person', 'sted', 'organisasjon' og 'diverse'. Verdiene skal v\u00e6re lister over de navngitte enhetene av den typen, akkurat som de vises i frasen.\n</code></pre></li> <li>Label mapping:<ul> <li><code>B-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>sted</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>sted</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>organisasjon</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>organisasjon</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>diverse</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>diverse</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset norne-nb\n</code></pre>"},{"location":"datasets/norwegian/#norne-nn","title":"NorNE-nn","text":"<p>This dataset was published in this paper and is a manually NER annotated version of the Nynorsk Universal Dependencies treebank. The NER labels almost follow the CoNLL-2003 standard, but with some additional labels.</p> <p>The original full dataset consists of 14,174 / 1,890 / 1,511 samples for training, validation and test, respectively. We use a split of 1,024 / 256 / 2,048 samples for training, validation and test, respectively. The splits we use are new, so there might be some samples from the training split in the validation or test splits.</p> <p>We have mapped the labels into the CoNLL-2003 standard as follows:</p> <ul> <li><code>LOC</code> \u27a1\ufe0f <code>LOC</code></li> <li><code>PER</code> \u27a1\ufe0f <code>PER</code></li> <li><code>ORG</code> \u27a1\ufe0f <code>ORG</code></li> <li><code>MISC</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>GPE_LOC</code> \u27a1\ufe0f <code>LOC</code></li> <li><code>GPE_ORG</code> \u27a1\ufe0f <code>ORG</code></li> <li><code>PROD</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>DRV</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>EVT</code> \u27a1\ufe0f <code>MISC</code></li> </ul> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"tokens\": array(['-', 'Ulfr', 'provoserer', 'kjapt', 'fram', 'eit', 'slagsm\u00e5l', ',', 'og', 'han', 'drep', 'hovdingen', '.'], dtype=object),\n  \"labels\": array(['O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['I', 'haust', 'blei', 'det', 'avsl\u00f8rt', 'at', 'minst', 'to', 'tolv\u00e5ringar', 'p\u00e5', 'mellomtrinnet', 'ved', 'Gimle', 'skule', 'hadde', 'med', 'seg', 'alkohol', 'p\u00e5', 'ein', 'skuletur', '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['Krigen', 'mot', 'Irak', 'skulle', 'aldri', 'ha', 'vore', 'gjennomf\u00f8rd', '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er fraser og JSON-ordb\u00f8ker med de navngitte enhetene som forekommer i den gitte frasen.\n</code></pre></li> <li>Base prompt template:   <pre><code>Frase: {text}\nNavngitte enheter: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Frase: {text}\n\nIdentifiser de navngitte enhetene i frasen. Du b\u00f8r outputte dette som en JSON-ordbok med n\u00f8klene 'person', 'sted', 'organisasjon' og 'diverse'. Verdiene skal v\u00e6re lister over de navngitte enhetene av den typen, akkurat som de vises i frasen.\n</code></pre></li> <li>Label mapping:<ul> <li><code>B-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>sted</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>sted</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>organisasjon</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>organisasjon</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>diverse</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>diverse</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset norne-nn\n</code></pre>"},{"location":"datasets/norwegian/#linguistic-acceptability","title":"Linguistic Acceptability","text":""},{"location":"datasets/norwegian/#scala-nb","title":"ScaLA-nb","text":"<p>This dataset was published in this paper and was automatically created from the Bokm\u00e5l Universal Dependencies treebank by assuming that the documents in the treebank are correct, and corrupting the samples to create grammatically incorrect samples. The corruptions were done by either removing a word from a sentence, or by swapping two neighbouring words in a sentence. To ensure that this does indeed break the grammaticality of the sentence, a set of rules were used on the part-of-speech tags of the words in the sentence.</p> <p>The original dataset consists of 20,044 samples, from which we use 1,024 / 256 / 2,048 samples for training, validation and testing, respectively (so 3,328 samples used in total). These splits are used as-is in the framework.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"En vellykket gjennomf\u00f8ring av denne reformen vil bli en avgj\u00f8rende pr\u00f8ve p\u00e5 Regjeringens handlekraft.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Lunde var ikke blant, mener Andreassen.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"72 kjoler g\u00e5r hver med sesong.\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er setninger og hvorvidt de er grammatisk korrekte.\n</code></pre></li> <li>Base prompt template:   <pre><code>Setning: {text}\nGrammatisk korrekt: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Setning: {text}\n\nBestem om setningen er grammatisk korrekt eller ikke. Svar med 'ja' hvis setningen er korrekt og 'nei' hvis den ikke er.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>ja</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>nei</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scala-nb\n</code></pre>"},{"location":"datasets/norwegian/#scala-nn","title":"ScaLA-nn","text":"<p>This dataset was published in this paper and was automatically created from the Nynorsk Universal Dependencies treebank by assuming that the documents in the treebank are correct, and corrupting the samples to create grammatically incorrect samples. The corruptions were done by either removing a word from a sentence, or by swapping two neighbouring words in a sentence. To ensure that this does indeed break the grammaticality of the sentence, a set of rules were used on the part-of-speech tags of the words in the sentence.</p> <p>The original dataset consists of 17,575 samples, from which we use 1,024 / 256 / 2,048 samples for training, validation and testing, respectively (so 3,328 samples used in total). These splits are used as-is in the framework.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Dersom Noreg snart g\u00e5r forbi Danmark i folketal, slik framskrivingane tilseier, kan ogs\u00e5 dette langt p\u00e5 veg forklarast med naturressursar.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Eg kan ikkje sj\u00e5 at det er grunn til \u00e5 ha ei slik grense i lova, det kan vurderast i, seier ho.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"SV har elles levert og i dag framsett ei gode forslag som kan bidra til \u00e5 gjera noko med straumprisproblematikken og straumforbruket, om viljen v\u00e5r er der.\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er setninger og hvorvidt de er grammatisk korrekte.\n</code></pre></li> <li>Base prompt template:   <pre><code>Setning: {text}\nGrammatisk korrekt: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Setning: {text}\n\nBestem om setningen er grammatisk korrekt eller ikke. Svar med 'ja' hvis setningen er korrekt og 'nei' hvis den ikke er.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>ja</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>nei</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scala-nn\n</code></pre>"},{"location":"datasets/norwegian/#unofficial-nocola","title":"Unofficial: NoCoLA","text":"<p>This dataset was published in this paper and is based on the annotated language learner corpus ASK. Notably, the individual types of errors are also annotated in this dataset. We use the error types to ensure that there is an equal representation of each error type, but then collapse the error types into <code>correct</code> and <code>incorrect</code>.</p> <p>The original dataset consists of 116,199 / 14,293 / 14,387 samples for training, validation and test, respectively. We use 1,024 / 256 / 2,048 samples for training, validation and test, respectively, where we sample each error type equally. All splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Vi har hatt krig i nesten ti \u00e5r. Jeg f\u00f8ler meg noen ganger trist fordi jeg har mistet flere venner og min far p\u00e5 grunn av krigen.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hvis jeg ikke sier in n genting, kan han spille hele dagen.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"De f\u00f8ler at samfunnet trenger ikke dem.\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er setninger og hvorvidt de er grammatisk korrekte.\n</code></pre></li> <li>Base prompt template:   <pre><code>Setning: {text}\nGrammatisk korrekt: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Setning: {text}\n\nBestem om setningen er grammatisk korrekt eller ikke. Svar med 'ja' hvis setningen er korrekt og 'nei' hvis den ikke er.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>ja</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>nei</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset no-cola-binary\n</code></pre>"},{"location":"datasets/norwegian/#unofficial-jentoft","title":"Unofficial: Jentoft","text":"<p>This dataset was published in this Master's thesis by Matias Jentoft.</p> <p>The original dataset consists of 85,771 / 10,827 / 10487 samples for training, validation and test, respectively. We use a split of 1,024 / 256 / 2,048 samples for training, validation and test, respectively. In each split, the distribution of <code>correct</code> and <code>incorrect</code> is 50/50.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"For to uker siden var jeg p\u00e5 en fotoutstilling om Erytrea.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Det viser seg at folk ikke kan leve uten mobiltelefonen.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Mobiltelefoner dominerer mange av oss, og vi bruker dem over alt, p\u00e5 gatene 'hvert hj\u00f8rne', i gatene, holdeplasser, kaffeteriaene og i parken, der folk burde tilbringe koselig tid sammen i naturen.\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er setninger og hvorvidt de er grammatisk korrekte.\n</code></pre></li> <li>Base prompt template:   <pre><code>Setning: {text}\nGrammatisk korrekt: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Setning: {text}\n\nBestem om setningen er grammatisk korrekt eller ikke. Svar med 'ja' hvis setningen er korrekt og 'nei' hvis den ikke er.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>ja</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>nei</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset jentoft\n</code></pre>"},{"location":"datasets/norwegian/#reading-comprehension","title":"Reading Comprehension","text":""},{"location":"datasets/norwegian/#norquad","title":"NorQuAD","text":"<p>This dataset was published in this paper and is a manually annotated dataset based on data from the Bokm\u00e5l Wikipedia.</p> <p>The original full dataset consists of 3,810 / 472 / 472 samples for training, validation and test, respectively. We use a split of 1,024 / 256 / 2,048 samples for training, validation and test, respectively. When creating the splits, we only select samples that contain an answer in the associated context. The splits we use are new, so there might be some samples from the training split in the validation or test splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"context\": 'Sprekpodden: Denne treningen gj\u00f8r deg smartere og lykkeligere\\nHJERNEFORSKER: \u2013 Hjernen er i utgangspunktet programmert for latskap. Derfor m\u00e5 vi i st\u00f8rre grad tvinge oss selv til \u00e5 v\u00e6re mer aktive, sier forsker Ole Petter Hjelle. Foto: Tor Stenersen (arkiv)\\nSPREKPODDEN: Denne uken har programleder Daniel R\u00f8ed-Johansen og Malene Indreb\u00f8-Langlo bes\u00f8k av Ole Petter Hjelle. Foto: Morten Uglum\\n\u2013 Vi var rett og slett lei av \u00e5 sitte og fortelle pasientene v\u00e5re at de m\u00e5tte v\u00e6re i fysisk aktivitet, uten at noe skjedde.\\nFor noen \u00e5r siden startet hjerneforsker og fastlege Ole Petter Hjelle, og de andre legene p\u00e5 \u00c5sg\u00e5rdstrand legekontor, en treningsgruppe for pasientene sine. Det ble stor suksess.\\n\u2013 Folk vet at det er bra \u00e5 trene for den fysiske helsen, men at fysisk aktivitet ogs\u00e5 er bra for den mentale helse, er et underkommunisert tema, sier han.\\nBedre enn sudoku og kryssord\\n\u2013 Er fysisk aktivitet bedre hjernetrim enn sudoku og kryssord?\\n\u2013 L\u00f8ser du masse kryssord, s\u00e5 blir du veldig til \u00e5 l\u00f8se kryssord. Men det har ikke de store ringvirkningene p\u00e5 v\u00e5re kognitive funksjoner, som det \u00e5 huske, planlegge og gjennomf\u00f8re, sier Hjelle.\\nHan forklarer at n\u00e5r pulsen v\u00e5r \u00f8ker, skilles det ut vekstfaktorer i hjernen som beskytter hjernecellene v\u00e5re og gj\u00f8r at cellene kommuniserer bedre.\\nForskning viser ogs\u00e5 at det dannes nye hjerneceller i enkelte deler av hjernen, under aktivitet.\\n\u2013 Men skal man f\u00e5 denne effekten, m\u00e5 man rett og slett v\u00e6re i aktivitet.\\nF\u00e5 opp pulsen\\nForskning viser ogs\u00e5 at fysisk aktivitet reduserer risiko for depresjon og demens, \u00f8ker intelligensen, bedrer hukommelsen, gj\u00f8r deg mer kreativ og gir deg et lengre og bedre liv.\\nHjelle forteller at det viktigste for \u00e5 hente ut disse fordelene er \u00e5 f\u00e5 opp pulsen.\\n\u2013 Men dersom du skulle valgt en aktivitet \u2013 som i st\u00f8rst mulig grad stimulerte flest mulig hjerneomr\u00e5der \u2013 pleier jeg \u00e5 si ballspill. Da f\u00e5r du opp pulsen, du samarbeider, har taktikk, koordinasjon, balanse og strategi, sier Hjelle.\\nH\u00f8r mer fra \u00abtreningslegen\u00bb i ukens Sprekpodden her.',\n  \"question\": 'Hva jobber Daniel som?',\n  \"answers\": {\n    \"answer_start\": array([286]),\n    \"text\": array(['programleder'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  \"context\": 'Litauiske medier: En utvekslingsavtale skal v\u00e6re p\u00e5 plass for Frode Berg\\nFrode Berg ble d\u00f8mt til 14 \u00e5rs fengsel i Russland. Foto: Tore Meek / NTB scanpix\\nRussland og Litauen er enige om \u00e5 utveksle en spiond\u00f8mt russer mot to litauere og en nordmann, opplyser kilder i den litauiske sikkerhetstjenesten til den litauiske nyhetstjenesten Baltic News Service (BNS).\\n\u2013 Utvekslingsavtalen inkluderer ogs\u00e5 en norsk statsborger som er d\u00f8mt i Russland, sier en anonym tjenestemann i den litauiske sikkerhetstjenesten.\\nAvisen navngir ikke Frode Berg, men Berg er den eneste nordmannen som soner en slik dom i Russland.\\nAftenposten og en rekke norske medier omtalte saken onsdag ettermiddag. Flere russiske medier melder ogs\u00e5 om det samme, alle med BNS som kilde\\n\u2013 H\u00e5per en avtale foreligger\\nFrode Bergs norske advokat Brynjulf Risnes kan ikke bekrefte opplysningene.\\n\u2013 Jeg har ikke informasjon som verken bekrefter eller avkrefter en slik avtale. Vi h\u00e5per selvsagt at en slik avtale foreligger, sier Risnes til NTB.\\nUD vil ikke kommentere saken.\\n\u2013 Norske myndigheter \u00f8nsker \u00e5 f\u00e5 Frode Berg hjem. Vi h\u00e5ndterer saken p\u00e5 den m\u00e5ten som vi mener er best for \u00e5 ivareta hans interesser. Utover det kommenterer vi ikke saken, sier underdirekt\u00f8r Ane Haavardsdatter Lunde i Utenriksdepartementet til NTB.\\nBergs russiske forsvarer, advokat Ilja Novikov, ikke vil kommentere saken, if\u00f8lge NRK.\\nSt\u00f8ttegruppen for Frode Berg h\u00e5per opplysningene stemmer.\\n\u2013 Dersom det viser seg at dette er riktig, er det en ufattelig god nyhet som vi har ventet p\u00e5 skulle skje, sier st\u00f8ttegruppemedlem Thorbj\u00f8rn Brox Webber til NTB.\\n\u2013 En slik avtale m\u00e5 bety at Frode kan komme tilbake til Norge og Kirkenes, legger han til.\\nD\u00f8mt for spionasje\\nBerg er d\u00f8mt til 14 \u00e5rs fengsel for spionasje. Han ble p\u00e5grepet i Moskva i desember 2017 og har sittet fengslet siden.\\nNRK meldte i august at UD er i forhandlinger med Russland om \u00e5 f\u00e5 Berg hjem og har informert hans n\u00e6rmeste familie om dette.\\nMuligheten for en utvekslingsavtale har v\u00e6rt antydet, men et problem har v\u00e6rt hvem den i s\u00e5 fall skal omfatte.',\n  \"question\": 'Hvilken norske advokat representerer Frode Berg?',\n  \"answers\": {\n    \"answer_start\": array([808]),\n    \"text\": array(['Brynjulf Risnes'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  \"context\": 'Ny nedtur for Ruud\\nCasper Ruud r\u00f8k torsdag ut av challengerturneringen i Koblenz. Bildet er fra en tidligere turnering.\\nAv Ole Henrik Tveten\\nDet ble en frustrerende kamp mot nederlandske Tallpon Griekspoor torsdag. Casper Ruud vant f\u00f8rste sett 6-4, men etter det var det lite som stemte for nordmannen i Tyskland.\\nI andre sett ble Ruud utspilt og tapte 1-6, mens feilene fortsatte \u00e5 florere ogs\u00e5 i tredje sett og Ruud tapte settet 2-6.\\nDen norske 20-\u00e5ringen gikk rett inn i 2. runde i Koblenz-turneringen etter \u00e5 ha f\u00e5tt walkover i den f\u00f8rste. Der slet han seg til seier mot italienske Raul Brancaccio onsdag. Torsdagens motstander, Tallpon Griekspoor, er nummer 233 p\u00e5 verdensrankingen.\\nDet startet bra for Snar\u00f8ya-gutten da han i f\u00f8rste sett br\u00f8t nederlenderens serve og tok ledelsen 4-3. Servebruddet ble avgj\u00f8rende for settet som Ruud vant 6-4, etter blant annet \u00e5 ha reddet en breakball etter en lengre ballveksling.\\nI andre sett begynte problemene for Casper Ruud. Griekspoor br\u00f8t Ruuds serve ved f\u00f8rste anledning og gikk opp i 2-0-ledelse. Deretter vant han egen serve, br\u00f8t Ruuds serve p\u00e5 ny og vant s\u00e5 egen serve. Da ledet plutselig nederlenderen 5-0.\\nNordmannen servet inn til 5-1, men det var dessverre ikke starten p\u00e5 noen snuoperasjon. Nederlenderen vant settet 6-1.\\nNordmannen hadde ikke ristet av seg problemene i pausen, og ble feid av banen av Griekspoor. Ruud kom under 0-4 i tredje sett f\u00f8r han omsider reduserte til 1-4. Men da var det for sent.\\nNederlenderen servet inn 5-1, Ruud reduserte, f\u00f8r Griekspoor servet seieren i land. Dermed tapte Ruud tredje sett 6-2 og r\u00f8k ut av turneringen.\\n\u00c5 ryke ut i Tyskland hjelper ikke nordmannens jakt p\u00e5 rankingpoeng for \u00e5 komme seg inn i topp 100 i verden. Han risikerer \u00e5 falle flere plasser ettersom han mister de 70 rankingpoengene han skaffet seg da han tok seg til 2. runde i Australian Open i fjor. Ruud er akkurat n\u00e5 nummer 112 p\u00e5 verdensrankingen. (NTB)',\n  \"question\": 'Hvordan endte 1. sett mellom Ruud og Griekspoor?',\n  \"answers\": {\n    \"answer_start\": array([244]),\n    \"text\": array(['6-4'], dtype=object)\n  }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 2</li> <li>Prefix prompt:   <pre><code>Her f\u00f8lger tekster med tilh\u00f8rende sp\u00f8rsm\u00e5l og svar.\n</code></pre></li> <li>Base prompt template:   <pre><code>Tekst: {text}\nSp\u00f8rsm\u00e5l: {question}\nSvar p\u00e5 maks 3 ord: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Tekst: {text}\n\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l om teksten ovenfor med maks 3 ord.\n\nSp\u00f8rsm\u00e5l: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset norquad\n</code></pre>"},{"location":"datasets/norwegian/#unofficial-norglm-multi-qa","title":"Unofficial: NorGLM Multi QA","text":"<p>This dataset was released in this paper and features a manually annotated reading comprehension dataset based on Norwegian news articles. This dataset is an abstractive question answering dataset, meaning that the answers do not always feature in the context. To fix this, they were rephrased using this script, which utilised the <code>gpt-4o-2024-05-13</code> model.</p> <p>The original dataset contains 2,406 samples, which we split into 1,024 / 256 / 1,126 samples for training, validation and test, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"context\": ' Kommer det melding om at ansatte kj\u00f8per aksjer i eget selskap, kan det v\u00e6re gode grunner til at du ogs\u00e5 b\u00f8r gj\u00f8re det. \u2013 V\u00e6r p\u00e5 lag med innsiderne, er ekspertens r\u00e5d.Har du lyst til \u00e5 pr\u00f8ve deg som aksjeinvestor helt gratis og uten reell risiko? Meld deg p\u00e5 Aksje-NM her!Mange assosierer innsidehandel med kj\u00f8p og salg av aksjer basert p\u00e5 tilgang p\u00e5 selskapsnyheter f\u00f8r de blir offentliggjort i markedet. Slik handel kan gi stor \u00f8konomisk gevinst, og er ulovlig.Det finnes derimot ogs\u00e5 en lovlig form for innsidehandel, og denne kan det v\u00e6re lurt \u00e5 f\u00f8lge med p\u00e5, skal vi tro forskningssjef Geir Linl\u00f8kken i Investtech. Aksjeskolen er en del av E24s Aksje-NM. En tidligere versjon av denne artikkelserien ble publisert i 2020.N\u00e5r man snakker om \u00abinnsidehandel\u00bb i b\u00f8rssammenheng, siktes det som regel til handler som direkt\u00f8rer, styremedlemmer og andre n\u00f8kkelmedarbeidere gj\u00f8r. Disse handlene m\u00e5 rapporteres inn til Oslo B\u00f8rs, og kj\u00f8pet eller salget blir offentlig informasjon. Denne informasjonen kan v\u00e6re gull verdt, skal vi tro forskningen til Investtech.\u2013 N\u00f8kkelpersoner som direkt\u00f8rer og styremedlemmer sitter p\u00e5 veldig mye kunnskap om bedriften. N\u00e5r disse enten selger eller kj\u00f8per aksjer i eget selskap, kan det ses p\u00e5 som et signal til andre akt\u00f8rer, sier Linl\u00f8kken. Linl\u00f8kken har forsket p\u00e5 innsidehandel og tatt utgangspunkt i over 11.000 rapporterte innsidekj\u00f8p i norske og svenske selskaper. Han har sett n\u00e6rmere p\u00e5 hvordan kursen utviklet seg i tiden etter innsidekj\u00f8pet. \u2013 Vi fant at disse selskapene p\u00e5 \u00e5rlig basis steg med 7,1 prosentpoeng mer enn andre selskaper. Det kan alts\u00e5 v\u00e6re et godt tips \u00e5 f\u00f8lge med p\u00e5 innsidekj\u00f8p.Dersom det tikker inn meldinger om at innsidere selger aksjene sine, er det ogs\u00e5 lurt \u00e5 f\u00f8lge n\u00f8ye med. Investtech har tatt utgangspunkt i over 6.900 slike tilfeller i Norge og Sverige, og gjorde spennende funn. \u2013 I snitt gjorde disse aksjene det 3,0 prosentpoeng svakere enn b\u00f8rsen, sier han. Linl\u00f8kken forteller at noen av aksjene kan ha falt for eksempel 50 prosent etter innsidesalg, mens det kan ha g\u00e5tt ganske bra i andre selskaper med innsidesalg.\u2013 Men i gjennomsnitt har disse aksjene gjort det d\u00e5rlig, fastsl\u00e5r han.Linl\u00f8kken sier at Investtech anser innsidehandelanalyse som en forenklet fundamental analyse, alts\u00e5 en analyse av om aksjen er billig eller dyr i forhold til verdiene i selskapet. Har man ikke tid eller kunnskap til \u00e5 gj\u00f8re slik analyse selv, er det et godt alternativ \u00e5 se til innsiderne. \u2013 Historisk og statistisk sett, har det v\u00e6rt riktig \u00e5 f\u00f8lge innsiderne og v\u00e6re p\u00e5 lag med dem, svarer Linl\u00f8kken.',\n  \"question\": 'Hva kan man gj\u00f8re dersom man ikke har tid eller kunnskap til \u00e5 gj\u00f8re en analyse av aksjene til et selskap?',\n  \"answers\": {\n    \"answer_start\": 2434,\n    \"text\": array(['Se til innsiderne.'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  \"context\": ' Alt om pubertet, penis, psyken og livet sj\u00e6l. Nok en fullkommen bok fra duoen bak et par av de st\u00f8rste boksuksessene de siste \u00e5rene. \u00abDe har gjort det igjen\u00bb, skrev jeg i VG for ganske n\u00f8yaktig to \u00e5r siden, da jeg satt her og leste og anmeldte \u00abJenteboka\u00bb av legene Nina Brochmann og Ellen St\u00f8kken Dahl. Da hadde det g\u00e5tt to \u00e5r siden de brak-debuterte med \u00abGleden med skjeden\u00bb. Jeg gav \u00abJenteboka\u00bb terningkast 6. Vel, vel. Du har kanskje gjettet det n\u00e5, men n\u00e5 har de alts\u00e5 gjort det enda en gang: Laget en knallgod, fullkommen bok vi f\u00e5r h\u00e5pe mange leser.For jeg t\u00f8r p\u00e5st\u00e5 at guttene trenger sin Guttebok vel s\u00e5 mye som jentene trenger sin. For selv om det er jentene vi har snakket mest om, er det mange unge gutter som sliter. Unge gutter faller oftere ut av skolen, er mer deprimerte og har mindre fremtidsoptimisme enn f\u00f8r. Det finnes dyster statistikk, kort fortalt: De opplever ogs\u00e5 stress og press og uhelse. Og s\u00e5 er de ikke s\u00e5 flinke til \u00e5 snakke om det. I \u00abGutteboka\u00bb tar Brochmann og Dahl for seg alt man m\u00e5 vite og forst\u00e5 n\u00e5r man er p\u00e5 vei inn i eller st\u00e5r midt i puberteten. (Eller senere i livet, for den saks skyld, jeg plukket opp noen gode tips selv, jeg.) De skriver om kroppsh\u00e5r, kviser, stemmeskifte,  legning, penisst\u00f8rrelse, pung, kj\u00f8nn, s\u00e6d, k\u00e5thet, ereksjonsknipe (!) og svettelukt, for \u00e5 nevne noen av mange h\u00f8ydepunkter.  Legeduoen havnet p\u00e5 denne lista: De ti heteste norske forfatterne i utlandet! Foruten alle de rent kroppslige og fysiske forandringene man kan oppleve p\u00e5 veien fra gutt til mann, inneholder boka gode kapitler om de psykiske aspektene og livet sj\u00e6l. Grensesetting, samtykke, nettvett, om \u00e5 trenge en pornopause, om psykisk uhelse, stress og press. \u00abAlle har det vondt iblant, men ingen har det vondt for alltid. Du kommer til \u00e5 bli glad igjen!\u00bb Det er noe med tonen i boka, som er s\u00e5 fin. Lett, \u00e5pen, sympatisk, avv\u00e6pnende. Smart, kul og og med faglig tyngde. Men aldri formanende, ingen pekefinger. \u00abOnani er godt og sunt. Onani er ikke bare ufarlig \u2013 det er bra for deg.\u00bb \u00abKroppen din er laget for \u00e5 brukes og nytes.\u00bb  \u00abDet er synd at trening ender opp med \u00e5 handle om bare utseendet. \u00c5 trene er nemlig bra for deg. Det er ikke jakten p\u00e5 \u00abdr\u00f8mmekroppen\u00bb.\u00bb Selv de mer alvorlige og kliniske temaene er dessuten en forn\u00f8yelse \u00e5 bla om til, ogs\u00e5 takket v\u00e6re de fantastiske illustrasjonene til Magnhild Wisnes. De er fargerike og morsomme, og gj\u00f8r boka komplett. S\u00e5 mange peniser har jeg ikke sett siden vi fniste og lo av \u00abPenisatlaset\u00bb p\u00e5 et nachspiel i studietiden. S\u00e5 kan man jo stille seg sp\u00f8rsm\u00e5let, om denne boka n\u00e5r frem til dem som trenger \u00e5 lese den. Den burde egentlig v\u00e6rt pensum, tenker jeg, eller i alle fall utgangspunkt for et prosjekt p\u00e5 skolen. \u00c5 sette seg ned med en bok, som attp\u00e5til handler om puberteten, st\u00e5r vel ikke h\u00f8yest p\u00e5 lista over hva ten\u00e5ringsgutter flest vil bruke fritiden sin p\u00e5. Pr\u00f8v likevel.  Jeg vet ikke, kanskje betale gutten noen kroner for \u00e5 lese den, om det er det som skal til. Jeg f\u00f8ler meg sikker p\u00e5 at det vil v\u00e6re verdt det. For hvis de unge guttene v\u00e5re leser denne boka, er jeg sikker p\u00e5 at livet blir lettere \u00e5 leve og verden et morsommere sted. Anmeldt av: Trine Saugestad Hatlen',\n  \"question\": 'Hvem st\u00e5r for illustrasjonene i \u00abGutteboka\u00bb?',\n  \"answers\": {\n    \"answer_start\": 2321,\n    \"text\": array(['illustrasjonene til Magnhild Wisnes'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  \"context\": ' Regjeringen lanserer ny handlingsplan for \u00e5 beskytte den truede villaksen. \u2013 Altfor slapt, sier SV-politiker.Regjeringen lanserer n\u00e5 en handlingsplan for \u00e5 bevare den truede villaksen.\u2013 Villaksen kan n\u00e5 bli r\u00f8dlistet i Norge for f\u00f8rste gong. Det er helt klart at det trengs konkrete tiltak for \u00e5 snu denne utviklingen, sier Sveinung Rotevatn i pressemeldingen fra regjeringen.Handlingsplanen inneholder tiltak mot blant annet lakselus, r\u00f8mt oppdrettsfisk, lakseparasitten Gyro, vannkraftregulering, forsuring, overbeskatning og fremmende fiskearter som pukkellaks.Regjeringen viser til at lakselus utgj\u00f8r den st\u00f8rste risikoen for \u00e5 gj\u00f8re ytterligere skade p\u00e5 vill atlantisk laks, if\u00f8lge Vitenskapelig r\u00e5d for lakseforvaltning.\u2013 Lakselus utgj\u00f8r en stor risiko for villaksen. Regjeringen vil blant annet utrede krav om nullutslipp av lakselus fra oppdrettsanlegg fra og med 2030, sier Rotevatn.Det vil i s\u00e5 fall inneb\u00e6re krav om lukkede anlegg.Lakselus finnes naturlig i alle havomr\u00e5der p\u00e5 den nordlige halvkule, og er den vanligste parasitten p\u00e5 laksefisk.Blir forekomsten av lus h\u00f8y, kan det v\u00e6re en utfordring b\u00e5de for oppdrettsfisk og vill laksefisk.Havbruk medf\u00f8rer at antall fisk i sj\u00f8en \u00f8ker, og dermed \u00f8ker ogs\u00e5 antall verter for lakselus. Niv\u00e5ene med lakselus i anleggene m\u00e5 derfor holdes lavest mulig, slik at de samlede lusemengdene i sj\u00f8en ikke blir for store.Som f\u00f8lge av omfattende resistens hos lusen mot kjemiske behandlingsmidler, har n\u00e6ringen de siste \u00e5rene v\u00e6rt tvunget til \u00e5 ta i bruk mekaniske metoder for \u00e5 fjerne lusen, med negative konsekvenser for fiskens velferd.Kilde: Lusedata, MattilsynetDagens trafikklyssystem som regulerer veksten i n\u00e6ringen i forhold til luseutviklingen, skal ogs\u00e5 utvikles og forbedres.Planen inneholder ogs\u00e5 tiltak mot en rekke andre p\u00e5virkningsfaktorer. Utfisking av r\u00f8mt oppdrettslaks skal \u00f8kes, og det skal vurderes nye metoder for \u00e5 spore og merke oppdrettslaks og hindre at r\u00f8mt oppdrettslaks gyter.Hele 80 prosent av villaksbestandene i Norge n\u00e5r for tiden ikke minstem\u00e5let for god kvalitet. R\u00f8mt oppdrettslaks og lakselus er regnet som de to st\u00f8rste truslene, skriver regjeringen.Fremmende fiskearter utgj\u00f8r ogs\u00e5 en risiko for b\u00e5de biologisk mangfold, produktiviteten til lokal laksefisk og akvakultur.I \u00e5r har Norge hatt den st\u00f8rste invasjonen av pukkellaks noensinne, og regjeringen vil derfor opprette en nasjonal kompetansegruppe for \u00e5 koordinere arbeidet med dette.SVs nestleder Torgeir Knag Fylkesnes er ikke forn\u00f8yd med tiltakene.\u2013 Dette er altfor, altfor slapt. Regjeringen tar ikke tak i elefanten i rommet, nemlig den lite b\u00e6rekraftige forvaltningen av oppdrettsn\u00e6ringa. Vi m\u00e5 stille strengere milj\u00f8krav til alle nye oppdrettstillatelser, og fase inn disse kravene hos de med eksisterende tillatelser, skriver han i en kommentar til E24.Han p\u00e5peker at det i dag tildeles oppdrettstillatelser til den h\u00f8ystbydende, og ikke til de med den mest milj\u00f8vennlige teknologien. \u2013 Skal vi redde villaksen og sikre en b\u00e6rekraftig vekst for oppdrettsn\u00e6ringen, m\u00e5 vi legge om systemet slik at vi gjennom \u00e5 gi billigere tillatelser, men med krav om nullutslipp, null r\u00f8mming og null ressurser p\u00e5 avveie.Fylkesnes understreker videre at teknologien finnes, og at n\u00e6ringen har god r\u00e5d.\u2013 N\u00e5r man for eksempel ser p\u00e5 Salmars investeringsaktivitet de siste ukene, s\u00e5 ser vi at n\u00e6ringen b\u00e5de kan betale for ny teknologi og skatt p\u00e5 formue og grunnrente.Fylkesnes gikk tidligere denne uken hardt ut mot Salmar-eier Gustav Witz\u00f8e, etter at laksemilliard\u00e6ren uttalte seg kritisk mot \u00f8kning i formuesskatten tidligere i sommer.',\n  \"question\": 'Hva inneholder regjeringens nye handlingsplan for villaksen?',\n  \"answers\": {\n    \"answer_start\": 377,\n    \"text\": array(['Handlingsplanen inneholder tiltak mot blant annet'], dtype=object)\n  }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 2</li> <li>Prefix prompt:   <pre><code>Her f\u00f8lger tekster med tilh\u00f8rende sp\u00f8rsm\u00e5l og svar.\n</code></pre></li> <li>Base prompt template:   <pre><code>Tekst: {text}\nSp\u00f8rsm\u00e5l: {question}\nSvar p\u00e5 maks 3 ord: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Tekst: {text}\n\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l om teksten ovenfor med maks 3 ord.\n\nSp\u00f8rsm\u00e5l: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset norglm-multi-qa\n</code></pre>"},{"location":"datasets/norwegian/#unofficial-belebele-no","title":"Unofficial: BeleBele-no","text":"<p>This dataset was published in this paper and features multiple-choice reading comprehension questions across 122 languages.</p> <p>The original dataset contains 900 unique multiple-choice reading comprehension passages and questions. From these, we use a 256 / 64 / 580 split for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Tekst: Det kinesiske nyhetsbyr\u00e5et Xinhua meldte tidligere fra om at et fly var kapret. Det ble senere rapportert at flyet fikk en bombetrussel og ble veiledet mot retur til Afghanistan med landing i Kandahar. If\u00f8lge de f\u00f8rste rapportene ble flyet dirigert tilbake til Afghanistan etter \u00e5 ha blitt nektet n\u00f8dlanding i \u00dcr\u00fcmqi.\\nSp\u00f8rsm\u00e5l: Hva ble ikke sagt i den nyeste rapporten fra nyhetsbyr\u00e5et Xinhua?\\nSvaralternativer:\\na. Flyet fikk en bombetrussel\\nb. Flyet landet i \u00dcr\u00fcmqi\\nc. Flyet ble dirigert til Afghanistan\\nd. Flyet landet i Kandahar\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Tekst: Tyskland begynte \u00e5 gj\u00f8re seg klare til \u00e5 invadere Storbritannia da kampen om Frankrike var over. Tyskland gav angrepet kodenavnet \u00aboperasjon sj\u00f8l\u00f8ve\u00bb. Mesteparten av den britiske h\u00e6rens tunge v\u00e5pen og forsyninger hadde g\u00e5tt tapt da den flyktet fra Dunkirk, s\u00e5 de var sv\u00e6rt s\u00e5rbar. Den britiske marinen var imidlertid fremdeles mye kraftigere enn den tyske (\u00abKriegsmarine\u00bb) og kunne ha \u00f8delagt en eventuell invasjonsfl\u00e5te sendt over den engelske kanal. Det var likevel sv\u00e6rt f\u00e5 skip fra Royal Navy som ble stasjonert n\u00e6r de sannsynlige invasjonsrutene siden admiralene var engstelige for at de kom til \u00e5 bli senket av tyske luftangrep.\\nSp\u00f8rsm\u00e5l: Hva kalte Tyskland angrepet p\u00e5 Storbritannia?\\nSvaralternativer:\\na. Dunkirk\\nb. Operasjon sj\u00f8l\u00f8ve\\nc. Kriegsmarine\\nd. Royal Navy\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Tekst: Det italienske og det tyske landslaget er de nest beste lagene i verden og var FIFA World Cup-mestere i 2006. Fotball, basketball, volleyball, vannpolo, fekting, rugby, sykling, ishockey, rullehockey og Formel-1 bilsport er godt likte sportsgrener. Vintersport er mest popul\u00e6rt i nordlige omr\u00e5der, der italienere deltar i internasjonale konkurranser og OL-arrangementer.\\nSp\u00f8rsm\u00e5l: Hvilke av f\u00f8lgende sporter vant et verdensmesterskap for Italia, basert p\u00e5 informasjonen i avsnittet?\\nSvaralternativer:\\na. Fotball\\nb. Vannpolo\\nc. Basketball\\nd. Sykling\",\n  \"label\": \"a\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er flervalgssp\u00f8rsm\u00e5l (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Sp\u00f8rsm\u00e5l: {text}\nSvaralternativer:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Sp\u00f8rsm\u00e5l: {text}\nSvaralternativer:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', 'c' eller 'd', og ikke noe annet.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset belebele-no\n</code></pre>"},{"location":"datasets/norwegian/#knowledge","title":"Knowledge","text":""},{"location":"datasets/norwegian/#nrk-quiz-qa","title":"NRK Quiz QA","text":"<p>This dataset was published in this paper and is a multiple-choice question answering (QA) dataset designed for evaluation of the Norwegian language and culture, including both Bokm\u00e5l and Nynorsk. The dataset consists of quizzes from NRK, the national public broadcaster in Norway.</p> <p>The original dataset contains 4,930 samples, spread across 549 quizzes. We keep the top-256 quizzes, allowing us to create splits stratified across all the remaining quizzes. We 635 / 256 / 2048 samples for training, validation and test, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Gunnar har hatt plutselige og sterke smerteanfall siden han var liten gutt. Det var vondt \u00e5 tisse og det gjorde vondt i ryggen og magen. Det hjalp litt \u00e5 drikke vann. Reseptbelagte medisiner kan v\u00e6re n\u00f8dvendig under anfall.\\nSvaralternativer:\\na. Nyrestein, kronisk\\nb. Irritabel tarmsyndrom\\nc. Angst\\nd. Urinveisinfeksjon\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"80 \u00e5r gamle Harrison Ford er nok ein gong aktuell i rolla som Indiana Jones. Kva heiter filmen?\\nSvaralternativer:\\na. Indiana Jones and the Nasty Nazis\\nb. Indiana Jones and the Dial of Destiny\\nc. Indiana Jones and the Hunt for Power\\nd. Indiana Jones Forever\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"I 1980 m\u00e5tte denne bassisten overnatte ni netter i fengsel i Japan fordi han pr\u00f8vde \u00e5 f\u00e5 med seg ca. 200 gram marihuana inn i landet. Hvem var det?\\nSvaralternativer:\\na. Sting\\nb. Lemmy Kilmister\\nc. Paul McCartney\\nd. Bootsy Collins\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er flervalgssp\u00f8rsm\u00e5l (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Sp\u00f8rsm\u00e5l: {text}\nSvaralternativer:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Sp\u00f8rsm\u00e5l: {text}\nSvaralternativer:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', 'c', eller 'd', og ikke noe annet.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset nrk-quiz-qa\n</code></pre>"},{"location":"datasets/norwegian/#idioms-no","title":"Idioms-no","text":"<p>This dataset was published here and consists of 3,553 Norwegian idioms and phrases that appear more than 100 times in the online library of the National Library of Norway.</p> <p>We have reformulated the dataset as a multiple-choice question format with 4 options, where the alternative answers have been generated using GPT-4o. Based on 3,232 samples (3,144 Bokm\u00e5l, 88 Nynorsk) from the original dataset, we use a 928 (27 Nynorsk) / 256 (11 Nynorsk) / 2,048 (50 Nynorsk) split for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Complete the Nynorsk idiom:\\nalle gode ting er _____\\n\\nSvaralternativer::\\na. s\u00f8te\\nb. tre\\nc. fire\\nd. vennlege\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Complete the Bokm\u00e5l idiom:\\ndet er ikke bare , _____\\n\\nSvaralternativer::\\na. moro\\nb. bare\\nc. lett\\nd. enkelt\",\n  \"label\": \"b\",\n}\n</code></pre> <pre><code>{\n  \"text\": \"Complete the Bokm\u00e5l idiom:\\ndet f\u00e5r st\u00e5 sin _____\\n\\nSvaralternativer::\\na. pr\u00f8ve\\nb. vegg\\nc. sak\\nd. greie\",\n  \"label\": \"a\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er flervalgssp\u00f8rsm\u00e5l (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Sp\u00f8rsm\u00e5l: {text}\nSvaralternativer:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Sp\u00f8rsm\u00e5l: {text}\nSvaralternativer:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', 'c' eller 'd', og ikke noe annet.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset idioms-no\n</code></pre>"},{"location":"datasets/norwegian/#unofficial-mmlu-no","title":"Unofficial: MMLU-no","text":"<p>This dataset is a machine translated version of the English MMLU dataset and features questions within 57 different topics, such as elementary mathematics, US history and law. The translation to Norwegian was conducted using the DeepL translation API.</p> <p>The original full dataset consists of 269 / 1,410 / 13,200 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). These splits are new and there can thus be some overlap between the original validation and test sets and our validation and test sets.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Hvorfor er Mahavira en viktig person i jainatradisjonene?\\nSvaralternativer:\\na. Han er den siste av de asketiske profetene.\\nb. Han er den f\u00f8rste av de asketiske profetene\\nc. Han er den mest l\u00e6rde av de asketiske profetene\\nd. Han er den helligste av de asketiske profetene\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"En enfaset fullbroomformer kan drives i lastkommuteringsmodus hvis belastningen best\u00e5r av\\nSvaralternativer:\\na. RL.\\nb. RLC underdempet.\\nc. RLC overdempet.\\nd. RLC kritisk dempet.\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"En professor, som var eneeier av en boligblokk, skrev et skj\u00f8te med f\u00f8lgende ordlyd: \\\"Jeg overdrar herved min boligblokk til min s\u00f8nn og datter som leietakere i fellesskap.\\\" I skj\u00f8tet, som var korrekt utferdiget, forbeholdt professoren seg en livsvarig eiendomsrett. Professoren fortalte deretter barna sine om overdragelsen og la den i familiehvelvet i biblioteket for oppbevaring. Deretter giftet s\u00f8nnen seg med en lege. Professoren, som mislikte legen, utferdiget deretter et nytt skj\u00f8te som han kalte \\\"et korreksjonsskj\u00f8te\\\". I \\\"korreksjonsskj\u00f8tet\\\" overf\u00f8rte professoren byg\u00e5rden \\\"til min s\u00f8nn og datter som sameiere med overlevelsesrett.\\\" If\u00f8lge det nye skj\u00f8tet forbeholdt professoren seg igjen livsvarig eiendomsrett. Begge barna aksepterte overdragelsen av \\\"korreksjonsskj\u00f8tet.\\\" Et halvt \u00e5r senere d\u00f8de s\u00f8nnen, og etterlot seg legen som eneste arving. Eiendomsretten til boligblokken er i datterens og\\nSvaralternativer:\\na. datteren og legen som sameiere.\\nb. datteren med forbehold om professorens livstidsarv.\\nc. datteren og legen som sameiere, med forbehold om professorens livsarvinger.\\nd. datteren og legen som sameiere med overlevelsesrett, med forbehold for professorens livsarvinger.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er flervalgssp\u00f8rsm\u00e5l (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Sp\u00f8rsm\u00e5l: {text}\nSvaralternativer:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Sp\u00f8rsm\u00e5l: {text}\nSvaralternativer:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', 'c' eller 'd', og ikke noe annet.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset mmlu-no\n</code></pre>"},{"location":"datasets/norwegian/#unofficial-arc-no","title":"Unofficial: ARC-no","text":"<p>This dataset is a machine translated version of the English ARC dataset and features US grade-school science questions. The translation to Norwegian was conducted using the DeepL translation API.</p> <p>The original full dataset consists of 1,110 / 297 / 1,170 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively (so 2,304 samples used in total). All new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Hvorfor er det tryggere \u00e5 se p\u00e5 m\u00e5nen enn p\u00e5 solen?\\nSvaralternativer:\\na. M\u00e5nen er mindre lyssterk.\\nb. M\u00e5nen er n\u00e6rmere jorden.\\nc. M\u00e5nen skinner mest om natten.\\nd. M\u00e5nen er full bare \u00e9n gang i m\u00e5neden.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hvilket av f\u00f8lgende er et biprodukt av celle\u00e5nding hos dyr?\\nSvaralternativer:\\na. oksygen\\nb. varme\\nc. sukker\\nd. protein\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Big Bang-teorien sier at universet\\nSvaralternativer:\\na. trekker seg sammen.\\nb. ikke har noen begynnelse.\\nc. startet som \u00e9n enkelt masse.\\nd. hele tiden danner hydrogen.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er flervalgssp\u00f8rsm\u00e5l (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Sp\u00f8rsm\u00e5l: {text}\nSvaralternativer:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Sp\u00f8rsm\u00e5l: {text}\nSvaralternativer:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', 'c' eller 'd', og ikke noe annet.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset arc-no\n</code></pre>"},{"location":"datasets/norwegian/#common-sense-reasoning","title":"Common-sense Reasoning","text":""},{"location":"datasets/norwegian/#norcommonsenseqa","title":"NorCommonSenseQA","text":"<p>This dataset was published in this paper and is a manually translated and localised version of the English CommonSenseQA dataset. There are samples in both Bokm\u00e5l and Nynorsk, but with the vast majority being Bokm\u00e5l.</p> <p>The original dataset contains 1,093 samples. We use a 128 / 128 / 787 split for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Hvor er det sannsynlig at en fugl lager hjemmet sitt?\\nSvaralternativer:\\na. I skogen\\nb. I et rede\\nc. P\u00e5 taket\\nd. P\u00e5 blader\\ne. I himmelen\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hvis et hjem har et abonnoment, hva f\u00e5r de sannsyneligvis hver dag i posten?\\nSvaralternativer:\\na. Delestykker\\nb. En avis\\nc. En gate\\nd. En vaskemaskin\\ne. Jordas overflate\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"N\u00e5r du ikke klarer \u00e5 gj\u00f8re noe ferdig, hva feilet du i da?\\nSvaralternativer:\\na. \u00c5 vinne\\nb. \u00c5 best\u00e5\\nc. \u00c5 fullf\u00f8r\\nd. \u00c5 gj\u00f8re det bra\\ne. \u00c5 lykkes\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er flervalgssp\u00f8rsm\u00e5l (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Sp\u00f8rsm\u00e5l: {text}\nSvaralternativer:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\ne. {option_e}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Sp\u00f8rsm\u00e5l: {text}\nSvaralternativer:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\ne. {option_e}\n\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', 'c', 'd' eller 'e', og ikke noe annet.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset nor-common-sense-qa\n</code></pre>"},{"location":"datasets/norwegian/#unofficial-hellaswag-no","title":"Unofficial: HellaSwag-no","text":"<p>This dataset is a machine translated version of the English HellaSwag dataset. The original dataset was based on both video descriptions from ActivityNet as well as how-to articles from WikiHow. The dataset was translated to Norwegian using the DeepL translation API.</p> <p>The original full dataset consists of 9,310 samples. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"[header] Slik holder du deg kj\u00f8lig og f\u00f8ler deg frisk om sommeren [title] Dusj hver dag. [step] Bruk en eksfolierende dusjs\u00e5pe for \u00e5 fjerne smuss. Sett vannet p\u00e5 varmt i starten av dusjen (fordi det rengj\u00f8r deg mer effektivt), men mot slutten av dusjen setter du vannet p\u00e5 lunkent eller kj\u00f8lig.\\nSvaralternativer:\\na. Dette senker kroppstemperaturen slik at du f\u00f8ler deg kj\u00f8ligere (og v\u00e5kner opp om morgenen!). [Sm\u00f8r deg med fuktighetskrem rett etter at du har g\u00e5tt ut av dusjen.\\nb. P\u00e5f\u00f8r denne gelen p\u00e5 svetten under armene eller p\u00e5 kroppen. Tenk p\u00e5 det som \u00e5 spyle den ene armhulen med vann (du kan lage din egen dusjs\u00e5pe med armene eller bena, og du kan vaske av deg litt med en gang).\\nc. Alternativt kan du \u00e5pne d\u00f8ren og la kj\u00f8lig vann str\u00f8mme gjennom det \u00e5pne vinduet i minst en time. [Bruk en ansiktsmaske mens du dusjer.\\nd. Vannet skal v\u00e6re varmt nok til \u00e5 skylle ut smuss og d\u00f8d hud som henger over ansiktet. P\u00e5f\u00f8r kroppss\u00e5pe (eller la den v\u00e6re \u00e5pen for lufting) p\u00e5 hudoverflaten i korte riller.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"En l\u00f8per l\u00f8per p\u00e5 en bane foran en folkemengde. en mann\\nSvaralternativer:\\na. kaster en ball som hunden skal fange.\\nb. snakker til kameraet.\\nc. l\u00f8per ikke n\u00e5r han hopper ned i en sandkasse.\\nd. gir en kort introduksjon f\u00f8r han fortsetter og konkurrerer mot mannen i svart.\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"[header] Slik vet du om hunden din liker deg best [title] Legg merke til at hunden din f\u00f8lger mye etter deg. [En m\u00e5te \u00e5 bevise at en hund liker deg best, er n\u00e5r den er mye sammen med deg. S\u00e5 hold \u00f8ye med om hunden din liker \u00e5 v\u00e6re i n\u00e6rheten av deg.\\nSvaralternativer:\\na. [Hold \u00f8ye med eventuell fysisk atferd. [Et godt eksempel p\u00e5 denne atferden er hvis den presser rumpa opp mot l\u00e5ret ditt og sjekker hva du har p\u00e5 deg.\\nb. [Se etter tegn p\u00e5 at hunden din kan v\u00e6re fl\u00f8rtende. [Et godt tegn p\u00e5 at hunden din liker deg er at den klapper deg mye eller stirrer p\u00e5 deg i intime \u00f8yeblikk.\\nc. [Finn ut om hunden din liker \u00e5 leke med deg. [Hvis det er en hund som elsker leker, kan du leke med dem, og hvis den er veldig glad i \u00e5 leke, s\u00e5 liker den at du leker med den.\\nd. Legg merke til at hunden din f\u00f8lger deg rundt i huset hver dag n\u00e5r du er ute og g\u00e5r. Selv om du kanskje ikke har lyst til det, kan det \u00e5 tilbringe mye tid sammen med en hund f\u00e5 den til \u00e5 f\u00f8le seg komfortabel med deg.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er flervalgssp\u00f8rsm\u00e5l (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Sp\u00f8rsm\u00e5l: {text}\nSvaralternativer:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Sp\u00f8rsm\u00e5l: {text}\nSvaralternativer:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', 'c' eller 'd', og ikke noe annet.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset hellaswag-no\n</code></pre>"},{"location":"datasets/norwegian/#summarization","title":"Summarization","text":""},{"location":"datasets/norwegian/#nosammendrag","title":"NoSammendrag","text":"<p>This dataset is a combination of the SNL and VG summarisation datasets as well as a translated version of the English XSum dataset, based on British BBC news articles. The SNL dataset is based on the Norwegian encyclopedia Store Norske Leksikon, while the VG dataset is based on the Norwegian articles from the newspaper VG. The translation of the XSum dataset was done using the NLLB model.</p> <p>The original full dataset consists of 472,000 samples, and we use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"P\u00e5 Akvariet i Bergen har pingvinene f\u00e5tt et ekstra fristende sommertilbud denne uken. \u2013 Vi fikk en litt artig id\u00e9, og bestemte oss for \u00e5 gi pingvinene v\u00e5re en slags \u00abslush-is\u00bb i g\u00e5r. Det ble til en morsom aktivisering for pingvinene, og det falt virkelig i god smak hos dem, sier dyrepasser Jannicke Johannessen. Hun forteller at de eldre pingvinene f\u00f8rst var litt skeptiske, og at det var de yngste som ledet an i isleken. \u2013 Ett- og to\u00e5ringene var veldig interesserte da vi kom ut med isen, og hoppet opp p\u00e5 den og storkoste seg. En av pingvinene ble faktisk liggende opp\u00e5 isen helt til den smeltet, ler hun. Hun forteller at isen falt i s\u00e5 god smak, at de skal gjenta suksessen l\u00f8rdag, slik at flere gjester i parken ogs\u00e5 kan f\u00e5 med seg aktiviteten.Selv om sommeren har satt flere varmerekorder i hele landet, forteller Johannessen at dyrene i Akvariet slettes ikke har lidd noen n\u00f8d. \u2013 Vi har California-sj\u00f8l\u00f8ver, som overhodet ikke har hatt noen problemer med varmen. Tvert imot, de elsker \u00e5 ligge \u00e5 sole seg. Vi har ogs\u00e5 europeiske otere, som takler klimaet godt, da det er dyr man finner naturlig i s\u00f8rlige deler av Europa. Dessuten er vi ekstremt heldige her p\u00e5 Akvariet, og pumper opp nytt saltvann hele tiden, og dyrene har mange muligheter til \u00e5 kj\u00f8le seg ned p\u00e5. Hun gir imidlertid et viktig r\u00e5d til dyreeiere som vil kj\u00f8le ned dyrene sine: \u2013 Jeg har f\u00e5tt med meg at folk gir is som hundene kan spise for eksempel, og det er ikke akkurat et sjakktrekk. N\u00e5r man kj\u00f8ler ned dyrene fra innsiden samtidig som det er veldig varmt ute, tuller det med kroppstemperaturen. Kroppen jobber for \u00e5 varme opp innsiden samtidig som de f\u00e5r varme utenfra. Du gir dem egentlig et heteslag, sier hun. \u2013 Det beste er \u00e5 kj\u00f8le dem ned p\u00e5 utsiden. Dusj dem under \u00abarmhulene\u00bb, eller generelt der de har tynn hud.Ogs\u00e5 i Tyskland har det v\u00e6rt h\u00f8ye temperaturer i sommer, og dyrepassere har m\u00e5ttet ta grep for \u00e5 avkj\u00f8le dyrene i varmen. I Osnabr\u00fcck, nord i landet, ble det registrert rundt 35 varmegrader onsdag. For tapirene i dyrehagen ble maten strategisk servert i skyggen, slik at dyrene ikke blir solbrent. Dyrepasser Daniel Chirico bestemte seg dessuten for \u00e5 spyle tapirene med en hageslange, for \u00e5 kj\u00f8le dem ned ytterligere. \u2013 Spesielt de nordiske artene i dyreparken har merket heteb\u00f8lgen, og tilbringer mesteparten av dagen i skyggen, sier Tobias Klumpe, biolog i Osnabr\u00fcck Zoo til den tyske avisen Osnabr\u00fccker Zeitung . Svartbj\u00f8rnene tar mer enn gjerne en kald dukkert i sola, samtidig som de nyter kalde forfriskninger med frukt og b\u00e6r.I Finland har ogs\u00e5 sommervarmen sl\u00e5tt inn for fullt. I Korkeasaari Zoo i Helsinki ble det torsdag registrert 30 varmegrader. L\u00f8sningen har blant annet v\u00e6rt \u00e5 installere en \u00abregnskog\u00bb for kenguruene, mens papeg\u00f8yene har f\u00e5tt egne dusjer de kan bruke. Bj\u00f8rnene har f\u00e5tt iskald vannmelon, som de nyter i det kalde vannet, og tigerne f\u00e5r frosne kaniner \u2013 s\u00e5fremt de faktisk \u00f8nsker \u00e5 spise. \u2013 Appetitten deres blir mindre i varmen. For eksempel spiser hunnene i snitt bare annenhver dag, sier dyrepasser Jonne Stenroth til den finske avisen MTV . Ellers tilbringer tigrene mesteparten av dagen i skyggen mens de slapper av i bassenget, skriver avisen.\",\n  \"target_text\": \"Mens solen skinner og temperaturene er som h\u00f8yest, tar dyreparker rundt om i Europa i bruk kreative l\u00f8sninger for \u00e5 holde dyrene avkj\u00f8lte.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Nick Corsellis, advokat for Carl Wood, sa at en \\\"innend\u00f8rs mann\\\" m\u00e5 ha v\u00e6rt involvert i razzia, men hans klient manglet ekspertise til \u00e5 v\u00e6re den personen. Mr Wood og tre andre menn nekter \u00e5 ha deltatt i \u00a3 14m r\u00f8veriet. Fire andre har allerede erkl\u00e6rt seg skyldig for deres roller i r\u00f8veriet. \\\"Og dette er en av grunnene til at Mr. Wood ikke er skyldig. Hva tok han med seg til bordet?\\\" sa han. Mr. Corsellis sa at det ikke fulgte at hans klient var mannen som ble identifisert av anklagemyndigheten som \\\"Man F\\\" i CCTV-opptak av razzia. \\\"Male F var faktisk en spiller. En innsider, eller knyttet til innsiden, som var fullt kjent med det indre arbeidet i Hatton Garden Safe Deposit\\\". Mr. Wood manglet slik kunnskap og ville bare ha v\u00e6rt i stand til \u00e5 fungere som en \\\"generell hundekrop\\\", sa advokaten. Corsellis spurte juryen om profesjonelle kriminelle ville v\u00e6rt forberedt p\u00e5 \u00e5 gi opp en del av sine millioner til en person som bare ville ha v\u00e6rt et \\\"ekstrapar hender (EPH)\\\". Han kalte det \\\"ilogisk\\\" og \\\"utrolig\\\" at en slik person var involvert da \\\"kriminelle ikke er veldedig folk\\\". \\\"Men hvem ville spille Carl Wood - EPH? Tror du at Mr. Tom Hardy eller Mr. Vinnie Jones vil haste \u00e5 ta rollen som... EPH?\\\" spurte han.\",\n  \"target_text\": \"En av mennene som er anklaget for \u00e5 v\u00e6re en del av Hatton Garden-raiden, kunne ikke ha v\u00e6rt involvert fordi han manglet noen ferdigheter \u00e5 tilby gjengen, har en domstol h\u00f8rt.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Verdenshjelpen forlot klubben i fjor p\u00e5 grunn av arbeids- og studietilbud, pluss behovet for \u00e5 komme seg fra en ryggskade. Manager Jamie Sherwood sa til klubbens nettside: \\\"Jeg er virkelig glad for \u00e5 ha brakt Natalie tilbake til klubben. \\\"Hennes erfaring, lederskap og \u00e5penbare evne blir et utmerket tillegg til v\u00e5r tropp for 2017\\\". Haigh la til: \\\"Etter skaden jeg fikk p\u00e5 ryggen for nesten 15 m\u00e5neder siden, trodde jeg aldri at jeg ville spille igjen, enn si p\u00e5 dette niv\u00e5et. \\\"Det er flott \u00e5 v\u00e6re tilbake i og rundt klubben - det er en ekte buzz etter den suksessen de oppn\u00e5dde i fjor\\\".\",\n  \"target_text\": \"Yeovil Town Ladies har gjenforenet tidligere kaptein Natalie Haigh f\u00f8r damer Super League One klubbens f\u00f8rste sesong i toppklassen.\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>Her f\u00f8lger nyhetsartikler med tilh\u00f8rende sammendrag.\n</code></pre></li> <li>Base prompt template:   <pre><code>Nyhetsartikkel: {text}\nSammendrag: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Nyhetsartikkel: {text}\n\nSkriv et sammendrag av den ovennevnte artikkelen.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset no-sammendrag\n</code></pre>"},{"location":"datasets/norwegian/#unofficial-norglm-multi-sum","title":"Unofficial: NorGLM Multi Sum","text":"<p>This dataset was released in this paper and features a manually annotated summarisation dataset based on Norwegian news articles.</p> <p>The original dataset contains 467 samples, which we split into 147 / 64 / 256 samples for training, validation and test, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \" En sel i England ble fanget i plast. Det kunne g\u00e5tt galt. Hver dag blir ogs\u00e5 dyr i Norge fanget i plast. Et vondt syn m\u00f8tte nylig dyrevernere p\u00e5 en strand i England. Der l\u00e5 en sel som hadde tuklet seg inn i plast. Det kunne g\u00e5tt veldig galt.\u2013 Det var tydelig at selen hadde det vondt, forteller en kvinne som s\u00e5 selen p\u00e5 stranden, til kanalen BBC.Men dyrlegene fra den britiske dyrevernsorganisasjonen BDMLR kom heldigvis i tide. De klarte \u00e5 fri selen fra plasten. Selen ble sluppet tilbake i sj\u00f8en.Heldigvis ble ikke selen skadet denne gangen, forklarte dyrevernsorganisasjonen til BBC.Men mange dyr er ikke s\u00e5 heldige n\u00e5r de blir fanget i plast. Dyr setter seg fast i plast over hele verden. Norske sj\u00f8dyr setter seg fast i plast hver eneste dag, forteller Per-Erik Schulze. Han jobber i Naturvernforbundet og er ekspert p\u00e5 plast og forurensing i havet. \u2013 Mange av dyrene st\u00e5r fast i mange dager eller m\u00e5neder uten \u00e5 slippe l\u00f8s. Det er helt grusomt, sier Schulze.Han forteller at disse dyrene ofte setter seg fast i plast: Sj\u00f8fuglerFiskSelerSm\u00e5hvalerHummerSkilpadderDet er ogs\u00e5 dyr p\u00e5 land som setter seg fast i plast, for eksempel sauer og reinsdyr. Hvert \u00e5r havner over \u00e5tte millioner tonn plast i havet, if\u00f8lge Verdens naturfond (WWF). Det meste synker til havbunnen, resten skyller inn p\u00e5 strender eller flyter p\u00e5 havoverflaten.Det er farlig for dyr som lever i og rundt havet, fordi de kan sette seg fast i plasten eller f\u00e5 den i magen.Hva skjer med dyrene som setter seg fast i plast?\u2013 Det er det st\u00f8rste dyreplageriet i verden. Det er veldig vondt \u00e5 hekte seg fast. Mange d\u00f8r kanskje ikke av plasten, men av sult, fordi de ikke kommer seg l\u00f8s s\u00e5 de kan dra og spise, sier han.Derfor er det viktig ikke \u00e5 kaste plast som fors\u00f8pler naturen, mener Schulze.\u2013 En fin tanke er at hver plastbit vi rydder opp, kanskje kan redde et dyr. For det finnes ogs\u00e5 en god nyhet: De siste \u00e5rene har mange ryddet s\u00f8ppel i naturen og langs kysten i Norge. Har det hjulpet? \u2013 Ja, det har v\u00e6rt en kjempe-ryddedugnad i Norge de siste fem \u00e5rene. Noen steder er det s\u00e5 rent n\u00e5 at det er vanskelig \u00e5 finne noe plast. Det er et godt tegn, sier Schulze.\",\n  \"target_text\": \" En sel i England som var fanget i plast ble reddet av dyrevernere. Dette er en vanlig situasjon, b\u00e5de i Norge og andre steder i verden, da mange dyr setter seg fast og lider lenge fordi de ikke kan komme seg l\u00f8s. Per-Erik Schulze, en ekspert fra Naturvernforbundet, oppfordrer folk til \u00e5 fortsette ryddearbeidet for \u00e5 minimere risikoen for dyr \u00e5 komme til skade assosiert med plastfors\u00f8pling. Han bekrefter at ryddedugnadene i Norge har v\u00e6rt en suksess.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \" Det drar seg til mot sommer, ferietid, og ikke minst helg. Usikker p\u00e5 hva du skal vie den til? Her har du et lite knippe velmente tips.Denne guiden gjelder fra fredag 10. juni til s\u00f8ndag 12. juni.Fredag og l\u00f8rdag er det duket for folkefest og musikkbonanza p\u00e5 Viking stadion i J\u00e5tt\u00e5v\u00e5gen.Anledningen er to konserter fra det folkekj\u00e6re Stavangerbandet Mods, som er tilbake igjen p\u00e5 arenaen hvor de i 2012 og i 2017 spilte foran flere titalls tusen elleville fans. Ogs\u00e5 Kvelertak er med p\u00e5 \u00e5 innramme en meget sterk musikkhelg i regionen. P\u00e5 fredag g\u00e5r de nemlig opp p\u00e5 scenen p\u00e5 Folken i Stavanger, og skal by p\u00e5 de herligste toner med b\u00e5de hardrock og metall. Ogs\u00e5 i utelivets verden skjer det ting i helgen. Fredag kveld gj\u00f8r et nytt nattklubb- og cocktailbar-konsept sitt inntog i Stavanger n\u00e5r LouLou \u00e5pner d\u00f8rene i de gamle Hot-lokalene i Skagen. \u2013 Vi har sett at Stavanger manglet en annen og kanskje litt mer eksklusiv plass, hvor man kan feire bursdager og andre store begivenheter, sa daglig leder i Rekom, Frederik Mygind til Byas i forrige uke.Ogs\u00e5 p\u00e5 Show Bar, nysatsingen til duoen Dennis Poppe og \u00d8yvind S\u00f8rensen, blir det \u00e5pning til helgen. \u00abEin liden (ein) pre-opening i morgen (l\u00f8rdag) og s\u00f8ndag p\u00e5 Show Bar! Sees kl. 20:00\u00bb, skriver Poppe p\u00e5 sin Instagram-konto. Etter seieren borte mot Sverige sist s\u00f8ndag, er det en revansjelysten \u00abs\u00f6ta bror\u00bb som gjester Ullevaal kommende s\u00f8ndag. Flere rogalendinger figurerer i viktige roller p\u00e5 landslaget, med Erling Braut Haaland, Veton Berisha, Kristian Thorstvedt og Birger Meling som navnene. Kampen kan sees p\u00e5 flere utesteder i Stavanger, men kan ogs\u00e5 nytes fra sofaen fra klokken 20:45. I det Aftenbladet omtaler som \u00absuperdagene\u00bb, med en hel rekke arrangementer den kommende uken, finner flere av de sted denne helgen. Det 91 kilometer lange sykkell\u00f8pet, Nordsj\u00f8rittet, fra Egersund til Sandnes g\u00e5r av stabelen l\u00f8rdag, og kan la svettekjertlene f\u00e5 fri utfoldelse. Rittet s\u00e5 dagens lys tilbake i 1998 og er et samarbeid mellom flere lokale sykkelklubber. Og p\u00e5 Sola blir det moro for b\u00e5de store og sm\u00e5 n\u00e5r Sola Airshow 2022, flystevnet som har vist fram gamle og nye luftmaskiner i en \u00e5rrekke, holdes p\u00e5 l\u00f8rdagen og s\u00f8ndagen. Er du derimot mer opptatt av folkelivet, s\u00e5 kan enten Tanangerdagene, eller Solafestivalen v\u00e6re for deg. I Sola kulturhus er det p\u00e5 fredag og l\u00f8rdag duket for ungdomsfestival.Arrangementet er gratis, for de mellom 13 og 20 \u00e5r, og byr blant annet p\u00e5 musikk fra den norske rapperen Hkeem, samt Stavanger-bandet Kriminell Kunst. Og et lite stykke unna, fra onsdag denne uken og fram til og med s\u00f8ndag, blir det folkeliv i Tananger, n\u00e5r Tanagerdagene g\u00e5r av stabelen. Arrangementet holdes i regi av Lions Club Tananger, og lover fem dager fulle av aktiviteter for familier, barn, ungdom og voksne. \u2013 Her er noe for alle og mye for mange. Hjertelig velkommen, skriver arrang\u00f8ren p\u00e5 Facebook-arrangementet sitt. Fra 10. til 12. juni holder fem kunstnere pop up-utstilling i Pedersgata.Kunstnerne det er snakk om er ragnhild.kristine, pryl.art, hwks.art, corneliussen.art og Rosa Ottestad.Det hele finner sted i Pedersgata 43, og det er ventet flere bes\u00f8kende til arrangementet. Utstillingen \u00e5pner kl. 18 p\u00e5 fredag, og holder \u00e5pent gjennom helga. Vet du bedre enn oss hva skjer neste helg? Send en e-post til\u00a0helga@byas.no!\",\n  \"target_text\": \" Artikkelen handler om hvilke arrangementer som skal holdes i perioden fra 10. juni til 12. juni. Blant arrangementene er konserter med bandene Mods og Kvelertak, landskamp i fotball p\u00e5 Ullevaal, og flystevnet Sola Airshow 2022 p\u00e5 Sola der det skal vises fram gamle og nye luftmaskiner. I tillegg arrangeres Tanangerdagene og Solafestivalen.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \" Regjeringen foresl\u00e5r \u00e5 \u00e5pne nye omr\u00e5der for oppdrettsn\u00e6ringen, men med strenge milj\u00f8krav. \u2013 Gir betydelige muligheter for \u00e5 \u00f8ke produksjonen, sier fiskeriministeren.N\u00e6rings- og fiskeridepartementet foresl\u00e5r n\u00e5 en ny tillatelsesordning for oppdrett med milj\u00f8krav.Det f\u00f8rste \u00e5ret kan det tildeles tillatelser p\u00e5 maksimalt 15.000 tonn biomasse (fisk). Hver enkelt s\u00f8ker kan maksimalt f\u00e5 tildelt ti tillatelser, og det vil stilles strenge milj\u00f8krav til s\u00f8kerne, heter det i meldingen fra departementet.\u2013 Dagens produksjon i \u00e5pne merder vil fortsatt v\u00e6re grunnstammen i norsk oppdrett. I tillegg har vi lagt til rette for landbasert oppdrett og havbruk til havs. Med denne ordningen peker vi ut en ny retning som gir oppdrettsn\u00e6ringen mulighet til \u00e5 ta i bruk nye arealer langs kysten, sier fiskeri- og sj\u00f8matminister Odd Emil Ingebrigtsen (H).Til sammenligning ble det produsert rundt 1,4 millioner tonn laks i Norge i 2019, if\u00f8lge SSB.Tillatelsene i den nye milj\u00f8teknologiordningen kommer i tillegg til veksten som blir tilbudt p\u00e5 ordin\u00e6r m\u00e5te gjennom trafikklyssystemet.\u2013 Samlet sett gir dette norsk havbruksn\u00e6ring betydelige muligheter for \u00e5 \u00f8ke produksjonen fremover, sier ministeren.Forslaget inneb\u00e6rer f\u00f8lgende milj\u00f8krav: Null utslipp av egg og frittsv\u00f8mmende stadier av lakselus, minimum 60 prosent oppsamling av slam, samt krav til r\u00f8mningssikkerhet.Prisen for tillatelsene vil bli satt med utgangspunkt i auksjonsprisene som er oppn\u00e5dd i forbindelse med ordin\u00e6re kapasitetsjusteringer, men med et rimelig fradrag.\u2013 Havbruksn\u00e6ringen skaper store verdier for Norge. Men videre vekst m\u00e5 skje innenfor b\u00e6rekraftige rammer. Hensynet til natur generelt, og villaksen spesielt, er av avgj\u00f8rende betydning, sier klima- og milj\u00f8minister Sveinung Rotevatn (V).Til tross for bedring p\u00e5 viktige omr\u00e5der, er antallet norsk laks i havet mer enn halvert siden 1980-tallet, if\u00f8lge\u00a0Vitenskapelig r\u00e5d for lakseforvaltning.Det er flere grunner til det, ogs\u00e5 overfiske, men r\u00e5det sl\u00e5r fast at r\u00f8mt oppdrettslaks og lakselus n\u00e5 er de st\u00f8rste truslene mot villaks.Forslaget skal p\u00e5 kort tid ut p\u00e5 h\u00f8ring.E24 skrev tidligere at siste sitat i saken var fra Ingebrigtsen, mens det egentlig var fra Rotevatn. E24 beklager og har n\u00e5 rettet feilen.\",\n  \"target_text\": \" Regjeringen foresl\u00e5r en ny tillatelsesordning for oppdrett med strenge milj\u00f8krav for \u00e5 muliggj\u00f8re b\u00e6rekraftig vekst i havbruksn\u00e6ringen. Denne ordningen vil \u00e5pne nye omr\u00e5der for oppdrett, tillate hver s\u00f8ker \u00e5 f\u00e5 maksimalt ti tillatelser, og krever null utslipp av egg og frittsv\u00f8mmende stadier av lakselus, minimum 60 prosent oppsamling av slam, samt krav til r\u00f8mningssikkerhet. Dette skal gi n\u00e6ringen mulighet til \u00e5 \u00f8ke produksjonen p\u00e5 b\u00e6rekraftig m\u00e5te.\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>Her f\u00f8lger nyhetsartikler med tilh\u00f8rende sammendrag.\n</code></pre></li> <li>Base prompt template:   <pre><code>Nyhetsartikkel: {text}\nSammendrag: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Nyhetsartikkel: {text}\n\nSkriv et sammendrag av den ovennevnte artikkelen.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset norglm-multi-sum\n</code></pre>"},{"location":"datasets/norwegian/#unofficial-schibsted-no","title":"Unofficial: Schibsted-no","text":"<p>This dataset was released here and features summaries of news articles from Schibsted Medias Norwegian newsrooms.</p> <p>The original dataset contains 1,240 / 347 / 374 samples for training, validation and testing, respectively. We use these splits as-is.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Klubblegenden med innr\u00f8mmelse under VAR-debatten: \u2013 Vanskelig \u00e5 st\u00e5 her : VAR-oppr\u00f8ret tok en knusende seier i Trondheim. Til og med styremedlem Ola By Rise m\u00e5tte innr\u00f8mme at det var mange gode argumenter imot videod\u00f8mmingen.  Den gamle keeperhelten talte RBK-styrets sak for VAR sammen med medstyremedlem Tore Reginiussen:  \u2013 Det er en veldig vanskelig sak. Det er ikke to VAR-tilhengere som st\u00e5r her, sa en engasjert By Rise fra talerstolen.  VAR-debatten hadde kommet til Rosenborgs medlemmer torsdag, som skulle stemme for at Rosenborg aktivt skulle arbeide for \u00e5 fjerne VAR eller ikke.  489 stemte for \u00e5 avvikle VAR. 157 stemte for \u00e5 beholde VAR. Stemmene ble lest opp til enorm applaus fra salen.  Forslaget om at RBK-styret skulle f\u00e5 \u00abutrede ulike modeller for \u00e5 f\u00e5 kapital inn i klubben\u00bb ble ogs\u00e5 stemt ned med god margin. \u2013 Medlemmene har definitivt makta i Rosenborg og de bruker den. Dette er et gedigent nederlag for det sittende styret og leder Cecilie Gotaas Johnsen, sier Adresseavisens kommentator Birger L\u00f8faldli til VG.  \u2013 S\u00e6rlig investorsaken tror jeg er tung \u00e5 svelge, der det forel\u00f8pig kun var snakk om en utredning. Jeg er spent p\u00e5 hvordan Gotaas Johnsen vil reagere p\u00e5 dette og hvordan hun vurderer arbeidsbetingelsene det kommende \u00e5ret, sier L\u00f8faldli.  VAR-debatten var den som tok lengst tid:  \u2013 Jeg har forst\u00e5else for klubbens posisjon og forst\u00e5r at m\u00e5ten oppleves som uvanlig detaljstyrende. Men for mange er dette en ekstraordin\u00e6r sak. Det er viktig at styret forst\u00e5r: VAR m\u00e5 ikke forbedres, VAR m\u00e5 fjernes! sa forslagsstiller Ole Christian Gullv\u00e5g.  \u2013 Talelista begynner \u00e5 bli lang, var meldingen fra ordstyrer etter at et par stykker hadde snakket sin side i VAR-saken.  Styremedlem By Rise argumenterte med at det ville bli vanskelig \u00e5 \u00absette tannkremen tilbake p\u00e5 tuben\u00bb. Forslagsstiller Gullv\u00e5g svarte:  \u2013 For oss oppleves det som at noen har spr\u00f8ytet tannkrem p\u00e5 stua midt under fredagstacoen. Vi har ikke bedt om det, vil ikke ha det.  Ola By Rise har tidligere v\u00e6rt ute p\u00e5 Twitter og v\u00e6rt kritisk til VAR. Han innr\u00f8mmet ogs\u00e5 sin tvil rundt temaet.  \u2013 Det er vanskelig \u00e5 st\u00e5 her. Man m\u00e5 ikke st\u00e5 hver kamp p\u00e5 \u00d8vre \u00d8st for \u00e5 reagere p\u00e5 hvordan VAR praktiseres i dag. S\u00e5 er det ikke sikkert den blir god nok. Involveringen av supporterne burde definitivt blitt bedre. Men det er ikke sikkert det er verkt\u00f8yet som er problemet, men gjennomf\u00f8ringen, sa By Rise.  Han og Reginiussen listet opp b\u00e5de negative og positive sider ved VAR, og pekte som flere andre klubber p\u00e5 det potensielle \u00f8konomiske tapet ved \u00e5 fjerne VAR.  Styret argumenterte for at Rosenborg skulle v\u00e6re en kritisk meningsb\u00e6rer rundt videod\u00f8mming. Et titalls medlemmer tok ordet og sa seg sv\u00e6rt uenige, og til slutt var det forslaget fra medlemmene som vant frem.  RBK-medlem Emil Alm\u00e5s var forslagsstiller sammen med Gullv\u00e5rg. Han sier f\u00f8lgende til VG: \u2013 Det vi har f\u00e5tt til i norsk toppfotball de siste dagene er en seier for fotballen og en seier for medlemsdemokratiet. Ved \u00e5 takke nei til VAR, har norske supportere startet et jordskred, som kommer til \u00e5 rase gjennom fotballeuropa i \u00e5rene som kommer! Den dagen VAR er historie, skal jeg med stolthet si at jeg, og mange andre norske fotballsupportere var med p\u00e5 \u00e5 trille de f\u00f8rste steinene nedover dalsiden, sier Alm\u00e5s.  PS. En r\u00f8rt Rune Bratseth mottok tittelen som \u00e6resmedlem i Rosenborg, etter en lang karriere som spiller, sportssjef og styremedlem. - Det er veldig spesielt for meg, sa Bratseth. \",\n  \"target_text\": \"489 RBK-medlemmer stemte for \u00e5 avvikle VAR ved et m\u00f8te torsdag, med 157 mot Styremedlem Ola By Rise innr\u00f8mmet gode argumenter mot videod\u00f8mming, men argumenterte for at Rosenborg skulle v\u00e6re en kritisk stemme imot. RBK-medlem Emil Alm\u00e5s hevder \\\"norske supportere starter et jordskred\\\" mot VAR i Europa Medlemmene ga ogs\u00e5 sitt nei til at RBK-styret skulle f\u00e5 \u00abutrede ulike modeller for \u00e5 f\u00e5 kapital inn i klubben\u00bb.  \u2013 Et gedigent nederlag for det sittende styret, mener Adresseavisens kommentator Birger L\u00f8faldli \"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Gazas befolkning sultes med vilje, sier FN-ekspert: Krigen har \u00f8delagt matproduksjonen. Samtidig slippes det ikke inn nok n\u00f8dhjelp. Israel driver en aktiv politikk for \u00e5 sulte ut Gazas befolkning, mener FNs spesialrapport\u00f8r. Israel har som m\u00e5l \u00e5 begrense Gazas sivilbefolkning tilgang til mat. Det hevder FNs spesialrapport\u00f8r for retten til mat, Michael Fakhri, til The Guardian. \u2013 Det finnes ingen grunn til \u00e5 med vilje stoppe leveringen av humanit\u00e6r hjelp eller \u00f8delegger sm\u00e5 fiskeb\u00e5ter, drivhus og frukt\u00e5kere, bortsett fra \u00e5 nekte folk tilgang til mat, sier Fakhri til den britiske avisen. Han mener at Israel med dette gj\u00f8r seg skyldig i b\u00e5de krigsforbrytelser og folkemord. Jan Egeland: \u2013 Fullstendig galskap Sentrale israelske politikere er flere ganger blitt anklaget for \u00e5 ha brukt retorikk som oppfordrer til folkemord. Dette ble blant annet lagt til grunn da S\u00f8r-Afrika klaget Israel inn til ICJ. \u2013 Som en menneskerettighetsekspert ved FN mener jeg at dette n\u00e5 er en folkemord-situasjon, understreker Fakhri. Fakhri er ikke den eneste som har advart om konsekvensene av hungersn\u00f8den i Gaza. En FN-rapport konkluderte nylig: Flyktninghjelpens generalsekret\u00e6r, Jan Egeland, reiste tirsdag inn i Gaza. Han beskriver rystende scener med desperate mennesker som gj\u00f8r alt i sin makt for \u00e5 kare til seg mat. \u2013 Jeg er fullstendig sjokkert over forholdene her. Folk sl\u00e5ss som ville og gale over madrasser og sekker med mat, sier Egeland til VG. \u2013 Det er fullstendig galskap at verden har latt en befolkning best\u00e5ende av stort sett helt uskyldige kvinner og barn bli utsatt for bombardement og utsulting siden midten av oktober. Hevder Israel trosser FN-domstol Situasjonen er ikke blitt bedre de siste ukene. Det sier bistandsorganisasjoner. Det til tross for at Den internasjonale domstolen (ICJ), FNs viktigste domstol, for \u00e9n m\u00e5ned siden bestemte at Israel m\u00e5 gj\u00f8re alt i sin makt for \u00e5 s\u00f8rge for \u00e5 stoppe et folkemord og s\u00f8rge for at palestinere har tilgang til bistand. Human Rights Watch (HRW) og Amnesty International p\u00e5peker at det slippes inn 30 prosent f\u00e6rre lastebiler med n\u00f8dhjelp hver dag n\u00e5 sammenlignet med f\u00f8r ICJs p\u00e5legg 26. januar. I februar slapp det inn halvparten s\u00e5 mye n\u00f8dhjelp i Gaza som m\u00e5neden f\u00f8r, if\u00f8lge FNs organisasjon for palestinske flyktninger (Unrwa). \u2013 Den israelske regjeringen sulter 2,4 millioner palestinere i Gaza.  Det sier Omar Shakir, som er lederen for HRWs virksomhet i Israel og Palestina. \u2013 Den israelske regjeringen har ganske enkelt oversett domstolens p\u00e5legg, f\u00f8yer han til. Tirsdag redegjorde Ramesh Rajasingham ved FNs kontor for koordinering av humanit\u00e6r innsats (UNOCHA) om situasjonen for FNs sikkerhetsr\u00e5d. Han advarte om at jordbruket i Gaza vil kollapse innen mai hvis situasjonen ikke blir bedre, og hvis det ikke blir pause i krigshandlingene. \u2013 Vi understreker derfor nok en gang v\u00e5rt krav om en v\u00e5penhvile, sa han. USA blokkerte i februar enda en gang en resolusjon i Sikkerhetsr\u00e5det om v\u00e5penhvile. Begrunnelsen var at resolusjonen kunne \u00f8delegge forhandlinger om v\u00e5penhvile og fangeutveksling som p\u00e5g\u00e5r mellom Egypt, Israel og Qatar. \u2013 Hvis ingenting skjer, frykter vi at storskala sult i Gaza nesten er uunng\u00e5elig, og det vil f\u00f8re til mange flere ofre, sa Rajasingham til Sikkerhetsr\u00e5det.\",\n  \"target_text\": \"FN mener Israel pr\u00f8ver \u00e5 sulte ut befolkningen p\u00e5 Gazastripen. M\u00e5lrettede angrep hindrer matproduksjon og levering av n\u00f8dhjelp.  Akutt underern\u00e6ring truer hele befolkningen. Barn og kvinner i Nord-Gaza og Rafah er mest utsatt.  Israel overser FN-domstolens p\u00e5legg om \u00e5 gi palestinere tilgang til bistand. Hjelpeorganisasjoner ser mindre n\u00f8dhjelp komme inn.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Marokkanske og albanske mafianettverk dominerer. Svenskene blir en stadig st\u00f8rre trussel.: Flere er bygd p\u00e5 lojalitet til familie og klan, if\u00f8lge ny rapport fra Kripos. Om kort tid legger politiet frem sin trusselvurdering. Der vil Politi-Norge peke p\u00e5 de st\u00f8rste truslene mot det norske samfunnet. En av truslene som vil bli viet mye plass, er organiserte kriminelle nettverk. I Norge er det rundt hundre slike nettverk. Kripos mener politiet har kapasitet til \u00e5 f\u00f8lge med p\u00e5 40 av dem. Nettverkene smugler og selger enorme mengder narkotika. De st\u00e5r bak skyteepisoder, eksplosjoner, menneskesmugling og bedragerier. M\u00e5let er profitt. Midlene er vold og hard indre justis. Noen av de mektigste nettverkene er bygd p\u00e5 lojalitet til familie og klan. N\u00e5 letter Kripos p\u00e5 sl\u00f8ret. For f\u00f8rste gang g\u00e5r politiet ut med en egen rapport om nettverkene som dominerer i den kriminelle underverdenen: I rapporten trekker Kripos frem fem store trusler: 1. Marokkanske narkonettverk En av de aller st\u00f8rste truslene er marokkanske narkonettverk. \u2013 De er utrolig sentrale, ikke bare i Norge og Norden, sier Eivind Borge fra Kripos. Norskmarokkanere dukker ogs\u00e5 opp i etterforskninger i andre europeiske land. Aftenposten har tidligere omtalt Zakariya Rahali, som har v\u00e6rt p\u00e5 r\u00f8mmen siden 2017. Rahali er pekt ut som lederen av Norges st\u00f8rste narkonettverk. 2. Albanske narkonettverk Etter marokkanerne, er det albanske nettverk som utgj\u00f8r den st\u00f8rste trusselen. Disse regnes for \u00e5 v\u00e6re blant de st\u00f8rste nettverkene som driver med kokain i hele Europa.  3. Svenske narkonettverk Borges skrekkscenario er at Norge kommer dit Sverige er i dag. Der har gjengkrigen herjet og deler av samfunnet er i ferd med \u00e5 bli infiltrert av kriminelle. I Norge har samtlige politidistrikt st\u00f8tt p\u00e5 svenske kriminelle nettverk. Og trusselen er \u00f8kende, vurderer Kripos. 4. Litauiske kriminelle nettverk For \u00e5 frakte narkotika, trengs det logistikk. For \u00e5 gj\u00f8re dette, tar mange kriminelle i bruk litauiske nettverk.  5. Norge som transittland I fjor opplevde Europa en \u00abkokaintsunami\u00bb. Enorme mengder kokain ble tatt av politi og tollere, ogs\u00e5 i Norge. Men prisene gikk ikke opp. Et tegn p\u00e5 at store mengder kokain er i oml\u00f8p.  I flere \u00e5r har havnene i Rotterdam og Antwerpen v\u00e6rt stedet hvor kokain er blitt smuglet inn til Europa. Men der har myndighetene kastet seg rundt. Dermed m\u00e5 de kriminelle se seg om etter nye havner for \u00e5 f\u00e5 det hvite pulveret til kundene. De store beslagene i fjor, kan peke mot at Norge i st\u00f8rre grad er i ferd med \u00e5 bli et av disse stedene. Enn s\u00e5 lenge er det for tidlig \u00e5 konkludere om Norge er blitt en del av kokainruten til Europa, mener Borge og Ole J\u00f8rgen Arvesen, avdelingsleder med ansvar for etterretning i Kripos. G\u00e5r sammen med kartellene Hvordan kan Kripos v\u00e6re s\u00e5 sikre i sin sak? Mye kommer fra p\u00e5g\u00e5ende etterforskninger, men de siste \u00e5rene har de ogs\u00e5 f\u00e5tt et unikt innblikk i hvordan de kriminelle jobber og samarbeider. De har f\u00e5tt meldinger og bilder fra Encrochat, Sky ECC og Anom. Det har ledet til flere store saker, men likevel er trusselen fra de kriminelle nettverkene blitt st\u00f8rre. \u2013 Den er betydelig og \u00f8kende for hele Europa, ogs\u00e5 Norge, sier Arvesen. Nettverkene er blitt mer profesjonelle og samarbeider mer med kriminelle i andre land.  \u2013 Vi ser tydelig at norske nettverk har direkte kontakt med karteller i S\u00f8r-Amerika, sier Eivind Borge fra Kripos. Han sier bakmennene de jobber for \u00e5 ta, ikke lar seg stoppe med forebygging. Det krever mye etterforskning og samarbeid med politi i andre land.\",\n  \"target_text\": \"For f\u00f8rste gang g\u00e5r politiet ut med en egen rapport om kriminelle nettverk. Rapporten peker p\u00e5 fem store trusler: marokkanske og albanske narkonettverk, svenske narkonettverk, litauiske kriminelle nettverk og at Norge blir et transittland for kokain. Nettverkene i Norge er blitt mer profesjonelle, har direkte kontakt med karteller i S\u00f8r-Amerika. Dette krever mer etterforskning og internasjonalt samarbeid.\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>Her f\u00f8lger nyhetsartikler med tilh\u00f8rende sammendrag.\n</code></pre></li> <li>Base prompt template:   <pre><code>Nyhetsartikkel: {text}\nSammendrag: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Nyhetsartikkel: {text}\n\nSkriv et sammendrag av den ovennevnte artikkelen.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset schibsted-no\n</code></pre>"},{"location":"datasets/norwegian/#unofficial-personal-sum","title":"Unofficial: Personal Sum","text":"<p>This dataset was released here and contains human annotated summaries that reflect individual user preferences.</p> <p>The original dataset contains 1,099 summaries based on 441 unique articles. The dataset has been restructured into 441 samples, where each sample represents a unique article paired with all of its corresponding summaries (1 or more). The dataset has been split such that we have 121 / 64 / 256 samples for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n    \"text\": \"I en ny bok forteller Abid Rajas s\u00f8ster Abida Raja (49) at hun over lengre tid levde i et voldelig forhold. I en pressemelding avviser eksmannen anklagene. \u2013 Min klient \u00f8nsker \u00e5 p\u00e5peke at han nekter straffeskyld for partnervold og\\nvoldtektsanklager. Han vedkjenner at ekteskapet har hatt sine utfordringer, og at de derfor skilte seg i 2015, skriver eksmannens advokat Javeed H. Shah i en pressemelding. I boken \u00abFrihetens \u00d8yeblikk\u00bb, beskriver Raja at eksmannen hennes var voldelig, og at hun flere ganger fors\u00f8kte \u00e5 unnslippe mannen. I boken skriver forfatter H\u00e5kon F. H\u00f8ydal:\u00abDe siste tjue \u00e5rene hadde v\u00e6rt en kamp mot seg selv: Hun \u00f8nsket \u00e5 g\u00e5 fra mannen. Men hun m\u00e5tte bli. P\u00e5 grunn av barna, og p\u00e5 grunn av familien, p\u00e5 grunn av frykten for fattigdom og skam. N\u00e5 hadde hun verken barna, penger eller hus.\u00bbVG har tidligere v\u00e6rt i kontakt med Abida Rajas eksmann i forbindelse med bokutgivelsen, som tirsdag ikke hadde lest boken.\u2013 Jeg er i utlandet og har ikke lest boken, s\u00e5 kan ikke kommentere uten \u00e5 lese det, skriver han i en SMS til VG.I boka skriver forfatteren at Abida etter stort press fra familien, skal ha m\u00f8tt \u00e9n av ektemannkandidatene, en 23 \u00e5r gammel inngiftet onkel i Pakistan. Hun var 18 \u00e5r og skulle g\u00e5tt i andre klasse p\u00e5 videreg\u00e5ende hjemme i Norge.\u00abAbida husker ikke om hun sa ja. Men hun sa heller ikke nei. Hun ville bare bort\u00bb, heter det i boken.Onsdag svarer eksmannen via sin advokat, at han har levd i god tro om at Abida giftet seg av fri vilje slik hun selv uttrykte ovenfor han. \u2013 Derfor er opplysningene om tvangsekteskap noe han ble kjent med f\u00f8rst i 2020. Boken kommer ett \u00e5r etter at venstrepolitiker og tidligere statsr\u00e5d Abid Raja kom med sin bok\\xa0\u00abMin skyld\u00bb. Boken er skrevet av VG-journalist H\u00e5kon F. H\u00f8ydal og ble lansert tirsdag morgen\\xa0etter mye hemmelighold. VG har ikke hatt noe med utgivelsen \u00e5 gj\u00f8re.\",\n    \"target_text\": [\"I en ny bok forteller Abid Rajas s\u00f8ster Abida Raja om hennes erfaringer med et voldelig ekteskap, hvor hun beskriver flere fors\u00f8k p\u00e5 \u00e5 unnslippe. Eksmannen avviser anklagene og hevder at han levde i god tro om at ekteskapet var av fri vilje, noe han f\u00f8rst ble klar over i 2020.\",\n    \"Abida Raja beskriver i en ny bok et voldelig forhold med sin eksmann, som avviser anklagene om partnervold og voldtektsanklager. Boken avsl\u00f8rer ogs\u00e5 at Abida ble presset til \u00e5 m\u00f8te en ektemannkandidat i en tvangssituasjon, noe eksmannen hevder han ikke var klar over f\u00f8r i 2020.\",\n    \"I boken \u00abFrihetens \u00f8yeblikk\u00bb forteller forfatteren H\u00e5kon F. H\u00f8ydal at Rajas eksmann var voldelig og hun \u00f8nsket \u00e5 forlate ham. Hun ble v\u00e6rende fordi hun var redd for barnas lidelser, redd for fattigdom og hun skammet seg.\"]\n}\n</code></pre> <pre><code>{\n    \"text\": \"Flere lakseaksjer falt igjen tungt, dagen etter at skatteforslag ga b\u00f8rsras for sj\u00f8matselskaper. Samtidig steg Norwegian etter anbefaling fra storbank.Det Ble en noe vinglete dag p\u00e5 Oslo B\u00f8rs torsdag.Etter en positiv start vendte B\u00f8rsen snuten nedover i tidlig handel, f\u00f8r den hentet seg inn igjen til forsiktig oppgang omtrent halvveis ut i handelsdagen. Utover ettermiddagen snudde B\u00f8rsen s\u00e5 nedover igjen.Hovedindeksen endte til slutt dagen ned 1,58 prosent.Nedgangen tiltok den siste timen med handel, samtidig som Wall Street falt kraftig.Oljeprisen steg solid gjennom g\u00e5rsdagen, og handles rundt \u00e9n dollar h\u00f8yere enn da B\u00f8rsen stengte onsdag. Et fat Nordsj\u00f8olje (brent spot) koster ved stengetid torsdag 88,4 dollar, ned rundt 0,9 prosentsiden midnatt.Oljeselskapene Equinor og Aker BP falt i overkant av \u00e9n prosent, mens V\u00e5r Energi endte ned 3,82 prosent.Onsdag falt Hovedindeksen 2,76 prosent etter at lakseselskapene fikk gjennomg\u00e5 etter regjeringens foresl\u00e5tte grunnrenteskatt p\u00e5 havbruk. Verst gikk det for Salmar som stupte 30 prosent, samtidig som Ler\u00f8y Seafood falt 27,5 prosent. Torsdag fortsetter nedgangen for lakseaksjene. Sj\u00f8matindeksen endte ned 5,05 prosent.Slik s\u00e5 det ut for lakseaksjene ved stengetid (utvikling onsdag i parentes): Salmar falt 1,05 prosent (stupte 30,3 prosent)Grieg Seafood falt 2,75 prosent (falt 26,6 prosent)Mowi falt 3,15 prosent (falt 18,9 prosent) Ler\u00f8y Seafood falt 8,10 prosent (raste 27,5 prosent)Austevoll Seafood falt 6,28 prosent (falt 21,7 prosentNorway Royal Salmon falt 8,94 prosent (endte ned 22,9 prosent)Bakkafrost-aksjen falt samtidig 12,83 prosent.Selskapet har virksomhet p\u00e5 F\u00e6r\u00f8yene og understreket onsdag at de ikke p\u00e5virkes av det nye norske skatteforslaget. Samtidig understreket de at det arbeides med et forslag om justeringer av skattesatsen p\u00e5 F\u00e6r\u00f8yene.I USA peker pilene solid nedover p\u00e5 b\u00f8rsene torsdag ettermiddag.Det er kraftig nedgang p\u00e5 Wall Street, der den brede S&amp;P 500-indeksen faller godt over to prosent. Teknologiindeksen Nasdaq faller samtidig mer enn tre prosent.I Europa er det ogs\u00e5 bred, kraftig nedgang p\u00e5 de viktigste b\u00f8rsene. London-b\u00f8rsen, Frankfurt-b\u00f8rsen og Paris-b\u00f8rsen er alle ned i overkant av to prosent rundt stengetid i Oslo.Storbanken HSBC har gjenopptatt dekning p\u00e5 flyselskapet Norwegian, if\u00f8lge Bloomberg. Banken anbefaler kj\u00f8p og har satt et kursm\u00e5l p\u00e5 14,50 kroner. Dermed ser banken for seg en oppside p\u00e5 hele 119 prosent i aksjen, skriver nyhetsbyr\u00e5et. Norwegian-aksjen steg 6,81 prosent.\u2013 Nye Norwegian er en annen forretning enn den f\u00f8r pandemien, som har omstrukturert operasjonelt og \u00f8konomisk, skriver HSBC i analysen.\u2013 Den nye ledelsen har en solid strategi, en enkel og kostnadseffektiv\\nforretningsmodell med en enkelt type fly, et sterkt fokus p\u00e5 sine n\u00f8kkelmarkeder i Norden og en solid balanse og likviditet, alt innenfor et gunstig konkurranselandskap som b\u00f8r tillate ny NAS \u00e5 ta markedsandeler fra sine konkurrenter, heter det videre i analysen.Storbanken begrunner ogs\u00e5 sin nye dekning p\u00e5 flyselskapet ved at dets konkurrenter venter mye motvind og ny ettersp\u00f8rsel for Norwegian kan komme ut av det. I tillegg nevnes Norges sikkerhetsnett rundt h\u00f8ye energi- og str\u00f8mpriser.- Mens Europa st\u00e5r overfor h\u00f8y inflasjon og lav forbrukertillit, har Norge betydelig lysere utsikter med sine omfattende energiressurser, statlig finansiering og h\u00f8y inntekt per innbygger.HSBC viser ogs\u00e5 til h\u00f8y reiseettersp\u00f8rsel blant nordmenn.Fornybarselskapet Scatec er i fokus i forbindelse med at selskapet har kommet med nye m\u00e5lsetninger. Selskapet vil investere 10 milliarder kroner av egenkapitalen i nye kraftverk frem mot 2027. Investeringene har som m\u00e5l \u00e5 utvide kapasiteten med 1,5 gigawatt hvert \u00e5r i perioden. Scatec-aksjen endte dagen ned 2,93 prosentXXL er samtidig blant b\u00f8rstaperne torsdag. Aksjen til sportsbutikk-kjeden falt 11,66 prosent.\",\n    \"target_text\": [\"Lakseaksjer opplever fortsatt betydelig nedgang p\u00e5 Oslo B\u00f8rs etter regjeringens foresl\u00e5tte grunnrenteskatt p\u00e5 havbruk. Hovedindeksen endte ned 1,58 prosent, og sj\u00f8matindeksen falt ytterligere 5,05 prosent. Samtidig steg Norwegian-aksjen etter anbefaling fra HSBC, som gjenopptok dekning p\u00e5 selskapet og anbefalte kj\u00f8p med et kursm\u00e5l p\u00e5 14,50 kroner, med en forventet oppside p\u00e5 119 prosent.\"]\n}\n</code></pre> <pre><code>{\n    \"text\": \"(Minnesota Wild \u2013 St. Louis Blues 4\u20136) Mats Zuccarello (34) var sv\u00e6rt kritisk til seg selv og lagkameratene i Minnesota Wild etter nattens tap mot St. Louis Blues i 23 minusgrader foran 38.000 tilskuere.\u2013 Jeg har egentlig ikke ord. Det er pinlig n\u00e5r du har 40.000 mennesker som kommer og fryser r\u00e6va av seg, og s\u00e5 spiller vi s\u00e5nn, sa Zuccarello p\u00e5 pressekonferansen etter \u00abWinter Classic\u00bb-oppgj\u00f8ret p\u00e5 Target Field \u2013 et baseballstadion i Minneapolis. F\u00f8r siste periode ledet Blues 6\u20132, og Zuccarello beskriver de to f\u00f8rste periodene som at de ble \u00ablett utspilt\u00bb av Blues. Zuccarello hadde \u00e9n assist \u2013 da Ryan Hartman scoret lagets tredje m\u00e5l . Wild reduserte to ganger i siste periode og fastsatte sluttresultatet til 4\u20136. 34-\u00e5ringen mener det ikke nytter \u00e5 forklare tapet med kulden, vanskelige forhold og det faktum at de ikke har spilt kamp siden 20. desember: \u2013 Det er ingen unnskyldninger ... Det er kaldt for begge lag, isen er humpete for begge lag. Vi spilte ikke smart hockey som vi har gjort i store deler av sesongen. Det var Wilds femte strake tap i en sesong der Zuccarello og laget jevnt over har levert meget bra. \u2013 Dessverre skjedde det p\u00e5 en stor kveld som dette. Folk forlater hjemmene sine i kulden for \u00e5 st\u00f8tte oss, og s\u00e5 serverer vi dem dette. Vi har skuffet oss selv og alle andre. Det var p\u00e5 forh\u00e5nd varslet sprengkulde, og m\u00e5lingene viste 23 minusgrader. Zuccarello beskriver opplevelsen slik:\u2013 Jeg var skikkelig kald under oppvarmingen, men n\u00e5r kampen starter sl\u00e5r adrenalinet inn. Men jeg tror aldri jeg har v\u00e6rt s\u00e5 kald i hele mitt liv f\u00f8r sisteperioden da vi l\u00e5 under 6\u20132, eller hva det var. Det var ingen god f\u00f8lelse. \u2013 Det store bildet n\u00e5 er at vi har fem strake tap, og vi m\u00e5 finne tilbake til m\u00e5ten \u00e5 vinne p\u00e5 og hvordan vi skal spille som et lag, sier Zuccarello. Zuccarello har scoret \u00e5tte m\u00e5l og lagt 17 m\u00e5lgivende pasninger i l\u00f8pet av 25 kamper denne sesongen. Det vil si ett m\u00e5lpoeng per kamp i snitt. I sine beste m\u00e5lpoengsesonger for New York Rangers \u2013 2013/14, 2015/16 og 2016/17 \u2013 oppn\u00e5dde han henholdsvis 59 m\u00e5lpoeng p\u00e5 77 kamper, 61 m\u00e5lpoeng p\u00e5 81 kamper og 59 p\u00e5 80 kamper.PS! Natt til fredag spiller Minnesota Wild borte mot Boston Bruins. To dager senere er det hjemmekamp mot Washington Capitals.\",\n    \"target_text\": [\"Minnesota Wild led et nederlag mot St. Louis Blues under ekstreme v\u00e6rforhold p\u00e5 Target Field. Mats Zuccarello uttrykte sin skuffelse over lagets ytelse foran 38 000 tilskuere, og tilskrev tapet til d\u00e5rlig spill heller enn kulden. Til tross for Zuccarellos bidrag med en assist, endte Wild med sitt femte strake tap, noe som f\u00f8rte til et press for \u00e5 finne tilbake til seiersformen f\u00f8r kommende kamper mot Boston Bruins og Washington Capitals.\",\n    \"Det er ingen unnskyldninger for Wilds femte strake tap, til tross for at b\u00e5de Zuccarello og resten av laget generelt har spilt bra denne sesongen. Forholdene var like for begge lag, men laget spilte ikke smart hockey slik de har gjort tidligere i sesongen.\"]\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>Her f\u00f8lger nyhetsartikler med tilh\u00f8rende sammendrag.\n</code></pre></li> <li>Base prompt template:   <pre><code>Nyhetsartikkel: {text}\nSammendrag: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Nyhetsartikkel: {text}\n\nSkriv et sammendrag av den ovennevnte artikkelen.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset personal-sum\n</code></pre>"},{"location":"datasets/spanish/","title":"\ud83c\uddea\ud83c\uddf8 Spanish","text":"<p>This is an overview of all the datasets used in the Spanish part of EuroEval. The datasets are grouped by their task - see the task overview for more information about what these constitute.</p>"},{"location":"datasets/spanish/#sentiment-classification","title":"Sentiment Classification","text":""},{"location":"datasets/spanish/#sentimentheadlines-es","title":"SentimentHeadlines-es","text":"<p>This dataset was published in this paper and features political news headlines.</p> <p>The original full dataset consists of 1,371 /  609 / 459 samples for training, validation, and testing, respectively. We use 861 /  256 / 1,024 samples for training, validation, and testing, respectively. All our splits are subsets of the original ones. The label distribution for the splits are as follows:</p> Split positive negative neutral Total Train 368 248 245 861 Val 88 90 78 256 Test 417 293 314 1,024 Total 873 631 637 2,141 <p>Here are a few examples from the training split:</p> <p><pre><code>{\n    \"text\": \"Mauricio Macri, en el cierre de campa\u00f1a: \u201cEsta marcha no termina hoy ac\u00e1, sino en noviembre\u201d\",\n    \"label\": \"neutral\"\n}\n</code></pre> <pre><code>{\n    \"text\": \"Lavagna reforz\u00f3 su discurso econ\u00f3mico y pidi\u00f3 m\u00e1s consumo\",\n    \"label\": \"positive\"\n}\n</code></pre> <pre><code>{\n    \"text\": \"Sin la aprobaci\u00f3n del Fondo, Macri quema reservas para la fuga\",\n    \"label\": \"negative\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Lo siguiente son rese\u00f1as y su sentimiento, que puede ser 'positivo', 'neutral' o 'negativo'.\n</code></pre></li> <li>Base prompt template:   <pre><code>Texto: {text}\nSentimiento: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Texto: {text}\n\nClasifica el sentimiento de la rese\u00f1a. Responde con 'positivo', 'neutral' o 'negativo', y nada m\u00e1s.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset sentiment-headlines-es\n</code></pre>"},{"location":"datasets/spanish/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"datasets/spanish/#conll-es","title":"CoNLL-es","text":"<p>This dataset was published in this paper and contains 8,324 / 1,916 / 1,518 samples for training, validation, and testing, respectively. We use 1,024 / 256 / 1,024 samples for training, validation, and testing, respectively. All the new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n    \"tokens\": array([\"Todo\", \"estar\u00e1\", \"integrado\", \",\", \"la\", \"relaci\u00f3n\", \"entre\", \"los\", \"espacios\", \"y\", \"entre\", \"los\", \"m\u00fasicos\", \"y\", \"la\", \"audiencia\", \".\"], dtype=object),\n    \"labels\": array([\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"], dtype=object),\n}\n</code></pre> <pre><code>{\n  \"tokens\": array([\"(\", \"NA2428-NH4437\", \")\", \"PSOE\", \"PIDE\", \"QUE\", \"COMISION\", \"CONTROL\", \"DE\", \"RTVE\", \"CONOZCA\", \"PRESUPUESTO\", \"ENTE\", \"Madrid\", \"(\", \"EFE\", \")\", \".\"], dtype=object),\n  \"labels\": array([\"O\", \"O\", \"O\", \"B-ORG\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"O\", \"B-ORG\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\", \"B-ORG\", \"O\", \"O\"], dtype=object),\n}\n</code></pre> <pre><code>{\n  \"tokens\": array([\"(\", \"NA2428-NH4437\", \")\", \"PSOE\", \"PIDE\", \"QUE\", \"COMISION\", \"CONTROL\", \"DE\", \"RTVE\", \"CONOZCA\", \"PRESUPUESTO\", \"ENTE\", \"Madrid\", \"(\", \"EFE\", \")\", \".\"], dtype=object),\n  \"labels\": array([\"O\", \"O\", \"O\", \"B-ORG\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"O\", \"B-ORG\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\", \"B-ORG\", \"O\", \"O\"], dtype=object),\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>Lo siguiente son oraciones y diccionarios JSON con las entidades nombradas que aparecen en la oraci\u00f3n dada.\n</code></pre></li> <li>Base prompt template:   <pre><code>Oraci\u00f3n: {text}\nEntidades nombradas: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Oraci\u00f3n: {text}\n\nIdentifica las entidades nombradas en la oraci\u00f3n. Debes producir esto como un diccionario JSON con las claves 'persona', 'lugar', 'organizaci\u00f3n' y 'miscel\u00e1neo'. Los valores deben ser listas de las entidades nombradas de ese tipo, exactamente como aparecen en la oraci\u00f3n.\n</code></pre></li> <li>Label mapping:<ul> <li><code>B-PER</code> \u27a1\ufe0f <code>persona</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>persona</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>lugar</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>lugar</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>organizaci\u00f3n</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>organizaci\u00f3n</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>miscel\u00e1neo</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>miscel\u00e1neo</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset conll-es\n</code></pre>"},{"location":"datasets/spanish/#linguistic-acceptability","title":"Linguistic Acceptability","text":""},{"location":"datasets/spanish/#scala-es","title":"ScaLA-es","text":"<p>This dataset was published in this paper and was automatically created from the Spanish Universal Dependencies by assuming that the documents in the treebank are correct, and corrupting the samples to create grammatically incorrect samples. The corruptions were done by either removing a word from a sentence, or by swapping two neighbouring words in a sentence. To ensure that this does indeed break the grammaticality of the sentence, a set of rules were used on the part-of-speech tags of the words in the sentence.</p> <p>The original dataset consists of 17,662 samples, from which we use 1,024 / 256 / 2,048 samples for training, validation and testing, respectively (so 3,328 samples used in total). These splits are used as-is in the framework.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n    \"text\": \"El fuego oblig\u00f3 al a el desalojo preventivo de algunas casas y del de el observatorio del de el Roque de los Muchachos, del de el Instituto de Astrof\u00edsica de Canarias.\",\n    \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n    \"text\": \"El libro que leemos intenta explicarlo explicar, pero sin exagerar las posturas de tirios y troyanos.\",\n    \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n    \"text\": \"Por su parte, el Consejo de Ministros dio ayer otra vuelta de tuerca al a el control urban\u00edstico de las ciudades aut\u00f3nomas de Ceuta y de Melilla para evitar la urban\u00edstica por parte del de el Grupo Independiente Liberal (GIL), que gobierna en Ceuta.\",\n    \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Lo siguiente son textos y si son gramaticalmente correctos.\n</code></pre></li> <li>Base prompt template:   <pre><code>  Texto: {text}\n  Gramaticalmente correcto: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>  Texto: {text}\n\n  Determina si el texto es gramaticalmente correcto o no. Responde con 's\u00ed' si el texto es correcto, y 'no' si no lo es.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>s\u00ed</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>no</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scala-es\n</code></pre>"},{"location":"datasets/spanish/#reading-comprehension","title":"Reading Comprehension","text":""},{"location":"datasets/spanish/#mlqa-es","title":"MLQA-es","text":"<p>This dataset was published in this paper and contains 0 / 500 / 5,253 samples for training, validation, and testing, respectively. We have made a 1,024 / 256 / 2,048 split, where we use the 500 validation samples + 524 test samples for training. Then we split the remaining test set into validation (256 samples) and test (2048 samples).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n    \"context\": \"En 1978, el Banco Estatal de Vietnam introdujo los primeros billetes de 5 hao, 1, 5, 10, 20 y 50 \u0111\u1ed3ng fechados en 1976. En 1980 se a\u00f1adieron los billetes de 2 y 10 \u0111\u1ed3ng, seguidos de los de 30 y 100 \u0111\u1ed3ng en 1981.\",\n    \"question\": \"\u00bfCu\u00e1ndo a\u00f1adi\u00f3 el Banco Estatal de Vietnam los billetes de 2 y 10 \u0111\u1ed3ng?\",\n    \"answers\": {\n      \"answer_start\": [120],\n      \"text\": [\"En 1980\"]\n    }\n}\n</code></pre> <pre><code>{\n    \"context\": \"Como otros ter\u00f3podos de la familia Dromaeosauridae, Saurornitholestes era un dinosaurio carn\u00edvoro b\u00edpedo, equipado con una garra retr\u00e1ctil con forma de oz en el segundo dedo de cada pie. Saurornitholestes era m\u00e1s ligero y ten\u00eda las patas m\u00e1s largas que otros dromaeos\u00e1uridos como Velociraptor o Dromaeosaurus. Se asemeja a Velociraptor en tener dientes grandes, parecidos a colmillos, en la parte frontal de las mand\u00edbulas.\",\n    \"question\": \"\u00bfD\u00f3nde se encuentra la garra de Saurornitholestes?\",\n    \"answers\": {\n        \"answer_start\": [161],\n        \"text\": [\"segundo dedo de cada pie\"]\n    }\n}\n</code></pre> <pre><code>{\n    \"context\": \"En cinco ediciones (en las tres primeras, 1896, 1900 y 1904, as\u00ed como en las de 1988 y 1992) fueron entregadas por prueba dos medallas de bronce (una a cada uno de los perdedores de las semifinales); en el resto de ediciones se ha disputado adicionalmente un partido por el tercer lugar para definir al ganador de la medalla de bronce.\",\n    \"question\": \"\u00bfDe qu\u00e9 material fueron las medallas entregadas a los semifinalistas en 1896?\",\n    \"answers\": {\n        \"answer_start\": [138], \"text\": [\"bronce\"]\n        }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>A continuaci\u00f3n se presentan textos con sus preguntas y respuestas correspondientes.\n</code></pre></li> <li>Base prompt template:   <pre><code>Texto: {text}\nPregunta: {question}\nRespuesta en m\u00e1ximo 3 palabras: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Texto: {text}\n\nResponda la siguiente pregunta sobre el texto anterior en m\u00e1ximo 3 palabras.\n\nPregunta: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset xquad-es\n</code></pre>"},{"location":"datasets/spanish/#unofficial-xquad-es","title":"Unofficial: XQuAD-es","text":"<p>This dataset was published in this paper and contains 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.</p> <p>The dataset is split intro 550 / 128 / 512 question-answer pairs for training, validation, and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n    \"context\": \"El Mercado del Grainger reemplaz\u00f3 a un mercado anterior construido originalmente en 1808 llamado el Mercado del Carnicero. El Mercado del Grainger en s\u00ed mismo, se abri\u00f3 en 1835 y fue el primer mercado interior de Newcastle. En el momento de su apertura en 1835 se dijo que era uno de los mercados m\u00e1s grandes y hermosos de Europa. La inauguraci\u00f3n se celebr\u00f3 con una gran cena a la que asistieron 2000 invitados, y la Galer\u00eda de Arte Laing tiene un cuadro de este evento. Con la excepci\u00f3n del techo de madera, que fue destruido por un incendio en 1901 y sustituido por arcos de celos\u00eda de acero, el mercado se encuentra en su mayor parte en su estado original. La arquitectura del Mercado del Grainger, como la mayor\u00eda de las de Grainger Town, que est\u00e1n clasificadas en el grado I o II, fue clasificada en el grado I en 1954 por Patrimonio Ingl\u00e9s.\",\n    \"question\": \"\u00bfCu\u00e1ntos invitados asistieron a la cena de inauguraci\u00f3n del Mercado del Grainger?\",\n    \"answer\": {\n      \"answer_start\": [396],\n      \"text\": [\"2000\"]\n    }\n}\n</code></pre> <pre><code>{\n    \"context\": \"Los avances realizados en Oriente Medio en bot\u00e1nica y qu\u00edmica llevaron a la medicina en el Islam medieval a desarrollar sustancialmente la farmacolog\u00eda. Muhammad ibn Zakar\u012bya R\u0101zi (Rhazes) (865-915), por ejemplo, actu\u00f3 para promover los usos m\u00e9dicos de los compuestos qu\u00edmicos. Abu al-Qasim al-Zahrawi (Abulcasis) (936-1013) fue pionero en la preparaci\u00f3n de medicamentos por sublimaci\u00f3n y destilaci\u00f3n. Su Liber servitoris es de particular inter\u00e9s, ya que proporciona al lector recetas y explica c\u00f3mo preparar los 'simples' a partir de los cuales se compon\u00edan los complejos medicamentos que se utilizaban entonces de forma generalizada. Sabur Ibn Sahl (d 869), fue, sin embargo, el primer m\u00e9dico en iniciar la farmacopedia, describiendo una gran variedad de medicamentos y remedios para las dolencias. Al-Biruni (973-1050) escribi\u00f3 una de las obras isl\u00e1micas m\u00e1s valiosas sobre farmacolog\u00eda, titulada Kitab al-Saydalah (El libro de los medicamentos), en la que detallaba las propiedades de los medicamentos y esbozaba el papel de la farmacia, as\u00ed como las atribuciones y los deberes de los farmac\u00e9uticos. Avicena tambi\u00e9n describi\u00f3 nada menos que 700 preparados, sus propiedades, modos de acci\u00f3n y sus indicaciones. De hecho, dedic\u00f3 todo un volumen a los medicamentos simples en El canon de la medicina. De gran impacto fueron tambi\u00e9n las obras de al-Maridini de Bagdad y El Cairo, y de Ibn al-Wafid (1008-1074), ambas impresas en lat\u00edn m\u00e1s de cincuenta veces, apareciendo como De Medicinis universalibus et particularibus de 'Mesue' el m\u00e1s joven, y el Medicamentis simplicibus de 'Abenguefit'. Pedro de Abano (1250-1316) tradujo y a\u00f1adi\u00f3 un suplemento a la obra de al-Maridini bajo el t\u00edtulo De Veneris. Las contribuciones de Al-Muwaffaq en este campo tambi\u00e9n son pioneras. En su vida en el siglo X, escribi\u00f3 Los fundamentos de las verdaderas propiedades de los remedios, describiendo, entre otras cosas, el \u00f3xido arsenioso, y conociendo el \u00e1cido sil\u00edcico. Hizo una clara distinci\u00f3n entre carbonato de sodio y carbonato de potasio y llam\u00f3 la atenci\u00f3n sobre la naturaleza venenosa de los compuestos de cobre, especialmente el vitriolo de cobre, y tambi\u00e9n los compuestos de plomo. Tambi\u00e9n describe la destilaci\u00f3n de agua de mar para beber [se requiere verificaci\u00f3n].\",\n    \"question\": \"\u00bfCu\u00e1les fueron los desarrollos en los que los cient\u00edficos influyeron en la creaci\u00f3n de la farmacolog\u00eda en el Islam medieval?\",\n    \"answer\": {\n      \"answer_start\": [43],\n      \"text\": [\"bot\u00e1nica y qu\u00edmica\"]\n    }\n}\n</code></pre> <pre><code>{\n    \"id\": \"5725c91e38643c19005acced\",\n    \"context\": \"A pesar de sus cuerpos blandos y gelatinosos, los f\u00f3siles que se cree que representan a los cten\u00f3foros, aparentemente sin tent\u00e1culos pero con muchas m\u00e1s filas de p\u00faas que las formas modernas, han sido encontrados en lagerst\u00e4tten en los primeros tiempos de la \u00e9poca de la era C\u00e1mbrica, hace alrededor de 515 millones de a\u00f1os. La posici\u00f3n de los cten\u00f3foros en el \u00e1rbol geneal\u00f3gico evolutivo de los animales se ha discutido durante mucho tiempo, y la opini\u00f3n mayoritaria en la actualidad, basada en la filogen\u00e9tica molecular, es que los cnidarios y los bilaterianos est\u00e1n m\u00e1s estrechamente relacionados entre s\u00ed que cualquiera de ellos con los cten\u00f3foros. Un an\u00e1lisis reciente de filogen\u00e9tica molecular concluy\u00f3 que el antepasado com\u00fan de todos los cten\u00f3foros modernos era similar a los cid\u00edpidos, y que todos los grupos modernos aparecieron relativamente recientemente, probablemente despu\u00e9s del evento de extinci\u00f3n del Cret\u00e1cico-Pale\u00f3geno hace 66 millones de a\u00f1os. Las pruebas acumuladas desde la d\u00e9cada de 1980 indican que los \"cid\u00edpidos\" no son monofil\u00e9ticos, es decir, no incluyen a todos y solo a los descendientes de un \u00fanico antepasado com\u00fan, ya que todos los dem\u00e1s grupos tradicionales de cten\u00f3foros son descendientes de varios cid\u00edpidos.\",\n    \"question\": \"\u00bfQu\u00e9 edad tienen los f\u00f3siles encontrados que representan los cten\u00f3foros?\",\n    \"answer\": {\n      \"answer_start\": [303],\n      \"text\": [\"515 millones de a\u00f1os\"]\n    }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>A continuaci\u00f3n se presentan textos con sus preguntas y respuestas correspondientes.\n</code></pre></li> <li>Base prompt template:   <pre><code>Texto: {text}\nPregunta: {question}\nRespuesta en m\u00e1ximo 3 palabras: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Texto: {text}\n\nResponda la siguiente pregunta sobre el texto anterior en m\u00e1ximo 3 palabras.\n\nPregunta: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset xquad-es\n</code></pre>"},{"location":"datasets/spanish/#unofficial-belebele-es","title":"Unofficial: BeleBele-es","text":"<p>This dataset was published in this paper and features multiple-choice reading comprehension questions across 122 languages.</p> <p>The original dataset contains 900 unique multiple-choice reading comprehension passages and questions. From these, we use a 256 / 64 / 580 split for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Texto: Beba alcohol con moderaci\u00f3n. Este afecta a cada persona de manera diferente y conocer sus propios l\u00edmites es sumamente importante. La ingesta excesiva de alcohol puede causar problemas de salud cr\u00f3nicos como da\u00f1o hep\u00e1tico e incluso ceguera y muerte. El peligro potencial se incrementa con el consumo de alcohol elaborado de forma ilegal. En las bebidas alcoh\u00f3licas ilegales puede haber varias impurezas amenazantes, como el metanol, capaz de provocar ceguera o incluso la muerte, aun cuando se ingiera poca cantidad.\\nPregunta: Seg\u00fan el fragmento, \u00bfcu\u00e1l de los siguientes sentidos puede verse afectado por el consumo excesivo de alcohol?\\nOpciones:\\na. Audici\u00f3n\\nb. Vista\\nc. Gusto\\nd. Olfato\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Texto: Leslie Aun, vocero de la Fundaci\u00f3n Komen, inform\u00f3 que rige una nueva normativa en la organizaci\u00f3n conforme la cual no proceder\u00e1 el otorgamiento de subvenciones o fondos en favor de entidades que sean objeto de investigaci\u00f3n oficial. La pol\u00edtica de Komen desacredit\u00f3 a Planned Parenthood a ra\u00edz de una investigaci\u00f3n en curso que dirige el representante Cliff Stearns sobre la forma en la que esta organizaci\u00f3n informa y utiliza sus fondos. En su rol de director del Subcomit\u00e9 de Supervisi\u00f3n e Investigaci\u00f3n, que se encuentra bajo el paraguas del Comit\u00e9 de Energ\u00eda y Comercio, Stearns conduce una investigaci\u00f3n para determinar si los impuestos se usan para financiar interrupciones de embarazos a trav\u00e9s de Paternidad Planificada.\\nPregunta: \u00bfQu\u00e9 comit\u00e9 preside Cliff Stearns?\\nOpciones:\\na. Comit\u00e9 de Energ\u00eda y Comercio de la C\u00e1mara de Representantes\\nb. La Fundaci\u00f3n Komen\\nc. Planned Parenthood\\nd. El Subcomit\u00e9 de Supervisi\u00f3n e Investigaci\u00f3n\",\n  \"label\": \"d\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Texto: El elemento del determinismo cultural se encontraba muy presente en el romanticismo, seg\u00fan estudiosos como Goether, Fichte y Schlegel. En el contexto del Romanticismo, la geograf\u00eda molde\u00f3 a las personas y, con el transcurso del tiempo, se desarrollaron costumbres y culturas relacionadas con esa geograf\u00eda que, al estar en armon\u00eda con la localizaci\u00f3n de esa sociedad, eran preferibles a leyes que se impusieran de forma arbitraria.\\nPregunta: De acuerdo con el texto, \u00bfqu\u00e9 molde\u00f3 a las personas durante el per\u00edodo del Romanticismo?\\nOpciones:\\na. Leyes\\nb. Geograf\u00eda\\nc. Costumbres\\nd. Cultura\",\n  \"label\": \"b\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Las siguientes son preguntas de opci\u00f3n m\u00faltiple (con respuestas).\n</code></pre></li> <li>Base prompt template:   <pre><code>Pregunta: {text}\nOpciones:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nRespuesta: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Pregunta: {text}\nOpciones:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nResponda la pregunta anterior usando solo 'a', 'b', 'c' o 'd', y nada m\u00e1s.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset belebele-es\n</code></pre>"},{"location":"datasets/spanish/#knowledge","title":"Knowledge","text":""},{"location":"datasets/spanish/#mmlu-es","title":"MMLU-es","text":"<p>This dataset is a machine translated version of the English MMLU dataset and features questions within 57 different topics, such as elementary mathematics, US history and law. The translation to French was done by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 272 / 1,465 / 13,334 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). These splits are new and there can thus be some overlap between the original validation and test sets and our validation and test sets.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n    \"text\": \"\u00bfQu\u00e9 m\u00e9todo de los siguientes utiliza el m\u00e9todo de loci como ayuda para la memoria?\\nOpciones:\\na. Codificaci\u00f3n sem\u00e1ntica\\nb. Imaginer\u00eda visual\\nc. Se\u00f1ales auditivas\\nd. Memoria ecoica\",\n    \"label\": \"b\",\n}\n</code></pre> <pre><code>{\n    \"text\": \"Cuando una medida realmente cuantifica lo que afirma medir, decimos que tiene buena\\nOpciones:\\na. precisi\u00f3n\\nb. validez\\nc. confiabilidad\\nd. valor asociativo\",\n    \"label\": \"b\",\n}\n</code></pre> <pre><code>{\n    \"text\": \"Un ranchero, siendo el propietario en un simple t\u00edtulo, transfiri\u00f3 la propiedad mediante una escritura de garant\u00eda a una mujer. La mujer opignor\u00f3 la finca a favor de su sobrina para asegurar un pr\u00e9stamo de la sobrina a la mujer por la cantidad de $500,000. La hipoteca fue inmediatamente registrada. Dos a\u00f1os despu\u00e9s, la mujer transfiri\u00f3 la finca a un granjero mediante una escritura de renuncia. La mujer, entonces, incumpli\u00f3 con la hipoteca, y la sobrina entabl\u00f3 una acci\u00f3n in personam contra el granjero para recuperar la cantidad adeudada por la hipoteca. Se presume que la escritura de renuncia de la mujer al granjero no hac\u00eda referencia a la hipoteca. Es probable que el acreedor hipotecario\\nOpciones:\\na. tenga \u00e9xito, porque la transferencia de la propiedad de la mujer al granjero result\u00f3 en una delegaci\u00f3n impl\u00edcita de responsabilidades.\\nb. tenga \u00e9xito, porque la sobrina era una beneficiaria de tercera parte en la transferencia entre la mujer y el granjero.\\nc. no tenga \u00e9xito, porque el granjero no prometi\u00f3 pagar la deuda hipotecaria.\\nd. no tenga \u00e9xito, a menos que el granjero tuviera conocimiento constructivo de la existencia de la hipoteca.\",\n    \"label\": \"c\",\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Las siguientes son preguntas de opci\u00f3n m\u00faltiple (con respuestas).\n</code></pre></li> <li>Base prompt template:   <pre><code>Pregunta: {text}\nOpciones:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nRespuesta: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Pregunta: {text}\nOpciones:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nResponda la pregunta anterior usando solo 'a', 'b', 'c' o 'd', y nada m\u00e1s.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset mmlu-es\n</code></pre>"},{"location":"datasets/spanish/#common-sense-reasoning","title":"Common-sense Reasoning","text":""},{"location":"datasets/spanish/#hellaswag-es","title":"HellaSwag-es","text":"<p>This dataset is a machine translated version of the English HellaSwag dataset. The original dataset was based on both video descriptions from ActivityNet as well as how-to articles from WikiHow. The dataset was translated by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 9,374 samples. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n    \"text\": \"[header] C\u00f3mo crear tinta de tatuaje de prisi\u00f3n [title] Encuentra una lata o un contenedor de metal. [step] Debe poder contener de 4 a 6 onzas de aceite para beb\u00e9s, junto con un poco de algod\u00f3n apretado. Prueba usando una lata de pulimento para botas vac\u00eda y limpia.\\nOpciones:\\na. [title] Usa alcohol isoprop\u00edlico como lubricante. [step] Mientras que algunos pulidores de escritorio tradicionalmente utilizados para tatuajes se manchan o amenazan, mezcla \u00bc taza de agua tibia y \u00bc taza de detergente regular para platos.\\nb. Si no tienes acceso a un contenedor pre-hecho: usa una herramienta afilada para cortar una lata de aluminio de 12 onzas por la mitad, y usa la mitad inferior como tu contenedor. [substeps] Puede que puedas comprar una lata de pulimento para botas en la tienda de la c\u00e1rcel.\\nc. No se recomienda el vidrio ya que se piensa que es muy fr\u00e1gil y es probable que reaccione mal al metal. [title] Quita cualquier objeto extranjero o bordado del contenedor.\\nd. [title] Vierte el pulimento blanco en un tubo de pl\u00e1stico como fluido sellante. [step] Un tubo ligero y bastante delgado funciona mejor como reservorio.\",\n    \"label\": \"b\",\n}\n</code></pre> <pre><code>{\n  \"text\": \"Entonces, la ni\u00f1a baja firmemente sus manos hacia su costado, junta sus pies y hace una reverencia, continuando con una rutina de varios movimientos de karate. la ni\u00f1a\\nOpciones:\\na. luego da una triunfante ola mientras levanta una mano derecha en el aire y contin\u00faa su rutina.\\nb. cae en un tatami alto en el aire y un hombre se acerca y le ayuda mientras desmonta.\\nc. finalmente desmonta y coloca su instrumento en su soporte, sin hacer una reverencia, su postura seria cambia a una de plena concentraci\u00f3n mientras levanta sus manos en el aire y eleva sus brazos.\\nd. termina su rutina un poco m\u00e1s lejos del punto donde comenz\u00f3, baja firmemente sus manos hacia su costado y hace una peque\u00f1a reverencia, luego abre sus piernas a la altura de los hombros y vuelve a la misma posici\u00f3n en la que estaba cuando empez\u00f3.\",\n  \"label\": \"d\",\n}\n</code></pre> <pre><code>{\n\"text\": \"[header] C\u00f3mo llevar tu peinado del d\u00eda a la noche [title] Humedece tu cabello. [step] Crear ondas a partir de un mo\u00f1o es una gran opci\u00f3n para cabello largo. Cuando quieras usar un mo\u00f1o para crear ondas en tu cabello, lo mejor es comenzar con el cabello al menos parcialmente h\u00famedo.\\nOpciones:\\na. As\u00ed que antes de comenzar, usa una toalla para secar en el lugar donde quieres poner el cabello. [substeps] Una buena regla es secar el cabello con una toalla antes de ponerlo en un mo\u00f1o.\\nb. Si te lavas el cabello por la ma\u00f1ana, s\u00e9calo con secadora o al aire hasta la mitad antes de hacer el mo\u00f1o. Si no planeas lavar tu cabello, roc\u00edalo ligeramente con una botella rociadora llena de agua.\\nc. [substeps] El cabello rizado se ver\u00e1 sin esfuerzo y m\u00e1s esponjado con la cabeza h\u00fameda porque es suave y brillante. Si tu cabello no est\u00e1 tan seco como quieres, no te vuelvas loca.\\nd. Si quieres dejarlo suelto durante la noche, usa una secadora. [substeps] Una secadora de cabello normalmente funciona mejor.\",\n\"label\": \"b\",\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Las siguientes son preguntas de opci\u00f3n m\u00faltiple (con respuestas).\n</code></pre></li> <li>Base prompt template:   <pre><code>Pregunta: {text}\nOpciones:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nRespuesta: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Pregunta: {text}\nOpciones:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nResponda la pregunta anterior usando solo 'a', 'b', 'c' o 'd', y nada m\u00e1s.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset hellaswag-es\n</code></pre>"},{"location":"datasets/spanish/#summarization","title":"Summarization","text":""},{"location":"datasets/spanish/#mlsum-es","title":"MLSum-es","text":"<p>The dataset was published in this paper and is obtained from online newspapers.</p> <p>The original full dataset consists of 266,367 / 10,358 / 13,920 samples for training, validation, and testing, respectively. We use 1,024 / 256 / 2,024 samples for training, validation, and testing, respectively. All our splits are subsets of the original ones.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n    \"text\": \"El todopoderoso secretario general de los populares bajo la presidencia de Jos\u00e9 Mar\u00eda Aznar, Francisco \u00c1lvarez-Cascos, ha desencadenado en su partido una tormenta en un vaso de agua. Ampar\u00e1ndose en una ret\u00f3rica de servicio a Asturias que apenas alcanza a disimular la frustraci\u00f3n de sus ambiciones personales, \u00c1lvarez-Cascos ha anunciado su baja en el partido de Mariano Rajoy y ha insinuado la creaci\u00f3n de una nueva fuerza pol\u00edtica para concurrir como candidato a la presidencia de Asturias en las elecciones auton\u00f3micas de mayo. Nada tiene de extra\u00f1o que quien fuera uno de los m\u00e1ximos adalides del 'todo vale' desde la oposici\u00f3n y tambi\u00e9n desde el Gobierno, aplique ahora esta m\u00e1xima a su propio partido. \u00c1lvarez-Cascos, durante sus a\u00f1os de protagonismo, tens\u00f3 la vida pol\u00edtica espa\u00f1ola hasta bordear los l\u00edmites de la estabilidad institucional, arremetiendo contra sus adversarios con instrumentos que despreciaban normas elementales del juego democr\u00e1tico. Su intento de regresar a la pol\u00edtica activa, rechazado por la direcci\u00f3n nacional de su partido, no responde al deseo de ofrecer un programa diferente a los asturianos, sino al de saciar su sed de poder tras a\u00f1os de obligada abstinencia. En la comparecencia para explicar las razones de su marcha dej\u00f3 entrever ajustes de cuentas y venganzas, pero ni una sola idea sobre la que articular el proyecto pol\u00edtico que defiende. Es cierto que la democracia interna que \u00c1lvarez-Cascos reclama ahora en el PP fue abolida mientras fue \u00e9l quien tuvo las riendas. Pero no porque sea \u00c1lvarez-Cascos su repentino y parad\u00f3jico abanderado deja de ser una reclamaci\u00f3n justa: el PP ha recurrido a la cooptaci\u00f3n para decidir la candidatura a la presidencia de Asturias, reafirm\u00e1ndose en un m\u00e9todo que aplica a todos los niveles, tanto municipal como auton\u00f3mico. E, incluso, nacional, como lo atestigua la presidencia de Mariano Rajoy por una decisi\u00f3n personal de su antecesor en el cargo. La aventura de \u00c1lvarez-Cascos no solo tendr\u00e1 dificultades para prosperar por las mezquinas razones que la impulsan, sino por el momento elegido para emprenderla. Un partido que se ve en la antesala del poder cierra filas con su direcci\u00f3n y no destruye sus expectativas desangr\u00e1ndose en luchas internas. Si el PP se encuentra en esta tesitura es por la forma de entender la pol\u00edtica de \u00c1lvarez-Cascos, pero tambi\u00e9n por la fragilidad del liderazgo de Rajoy. Dirigentes regionales como la presidenta de la Comunidad de Madrid no dudan en aprovechar cualquier circunstancia para desafiarlo. \u00c1lvarez-Cascos ha conseguido mostrar con un \u00fanico movimiento cu\u00e1l es la realidad interna de un partido que se considera en v\u00edsperas de alcanzar el Gobierno. El vaso de agua donde se desarrolla la ruidosa tormenta que ha desencadenado tiene el valor de un s\u00edntoma. Estas son las fuerzas que conviven en el PP y estas son las formas con las que los populares dirimen sus diferencias. * Este art\u00edculo apareci\u00f3 en la edici\u00f3n impresa del Martes, 4 de enero de 2011\",\n    \"target_text\": \"El hist\u00f3rico dirigente del PP se revuelve contra Rajoy al ver frustrada su ambici\u00f3n en Asturias\"\n}\n</code></pre> <pre><code>{\n    \"text\": \"Eladio Loizaga tiene un bigote fino y un hablar pausado. El Ministro de Relaciones Exteriores de Paraguay, de 66 a\u00f1os, ha estado en Madrid para preparar la visita del presidente de su pa\u00eds, Horacio Cartes, el pr\u00f3ximo junio. Despu\u00e9s de una charla en Casa de Am\u00e9rica, Loizaga reflexiona sobre las relaciones diplom\u00e1ticas en Am\u00e9rica Latina, la actualidad en Venezuela y Cuba, y los lazos de la regi\u00f3n con Estados Unidos, Europa y China. Pregunta. \u00bfQu\u00e9 tipo de relaci\u00f3n hay entre los pa\u00edses de Am\u00e9rica Latina? Respuesta. Las relaciones diplom\u00e1ticas, comerciales y pol\u00edticas son \u00f3ptimas. Se basan en respetar el principio de pluralidad y no injerencia en los asuntos internos de cada Estado, a menos que sea una decisi\u00f3n tan grosera que choque con los principios democr\u00e1ticos y las normas constitucionales. En Am\u00e9rica Latina hemos aprendido a convivir dentro de esa pluralidad, sin que esa pluralidad se uniforme. Cada uno tiene su filosof\u00eda y eso tiene que ser respetado. No hay conflictos que pongan en peligro las relaciones entre nosotros. Hemos entendido que podemos convivir con esas diferencias ideol\u00f3gicas. La no inferencia es una piedra angular. P. \u00bfIncluso en Venezuela con la situaci\u00f3n de los presos pol\u00edticos? R. Paraguay tiene una consolidaci\u00f3n democr\u00e1tica plena. En nuestro pa\u00eds ya no hay presos por expresar una idea pol\u00edtica distinta a la del Gobierno. Somos miembro del Consejo de Derechos Humanos de Naciones Unidas. En ese sentido, pensamos que acallar voces no contribuye a la libertad de la naci\u00f3n. P. \u00bfCondena pues las decisiones de Nicol\u00e1s Maduro? R. Tenemos una posici\u00f3n expresada a trav\u00e9s de Unasur. Constituy\u00f3 una decisi\u00f3n de tres cancilleres, Colombia, Brasil y Ecuador, para cooperar en el di\u00e1logo con todos los sectores pol\u00edticos democr\u00e1ticos de Venezuela. Queremos que Venezuela encuentre una salida conforme a sus propias reglas constitucionales. Hay una l\u00ednea muy fina en lo que es una injerencia interna, y nosotros somos muy celosos porque la hemos sufrido. Estados Unidos tuvo por mucho tiempo, no un abandono, sino una negligencia benigna hacia Am\u00e9rica Latina. Como Europa. P. \u00bfPor qu\u00e9 la mayor\u00eda de gobiernos latinoamericanos guardaron silencio? R. Varios gobiernos han mostrado su preocupaci\u00f3n y ratificado su posici\u00f3n de que las partes dialoguen, que el Gobierno y la oposici\u00f3n se sienten para encontrar una salida democr\u00e1tica. Tenemos que evitar una salida traum\u00e1tica. Queremos apoyar al pueblo venezolano, porque sabemos las necesidades que est\u00e1n pasando. Estamos en contacto con el Gobierno para ayudar y proveer alimentos y otros productos que se necesitan. P. \u00bfApoya la labor que pretende hacer Felipe Gonz\u00e1lez? R. No me puedo referir a eso. Hay situaciones en las que, sin desconocer los derechos fundamentales de la persona, hay que tener cierto respeto por el marco interno de cada pa\u00eds. P. \u00bfCu\u00e1l es la salud de los derechos humanos en Am\u00e9rica Latina? R. Los derechos humanos no se definen hoy solo como derechos pol\u00edticos. Am\u00e9rica Latina estaba gobernada por dictaduras, por posiciones extremas, de izquierda y de derecha. Hoy tenemos un adelanto pol\u00edtico en toda la regi\u00f3n y tambi\u00e9n la necesidad de ir dando respuesta a los derechos humanos de cuarta generaci\u00f3n, la vivienda, la salud, el agua potable... Avanzamos en la lucha contra la pobreza. Y en que los chicos vayan a la escuela. Sin educaci\u00f3n no vamos a desarrollarnos. P. \u00bfPuede Am\u00e9rica Latina tener una voz \u00fanica en cuanto a pol\u00edtica exterior? R. Hoy no va a ser posible. Sabemos muy bien las posiciones ideol\u00f3gicas de cada uno. En lo posible tratamos de consensuar en la educaci\u00f3n, el desarrollo social, pero tener una sola voz pol\u00edtica es dif\u00edcil. Tenemos visiones distintas de c\u00f3mo vemos el mundo y las relaciones con otros Estados. P. Colombia est\u00e1 en un proceso de paz. \u00bfQu\u00e9 es m\u00e1s importante, justicia o paz? R. No es f\u00e1cil. Hay muchas aristas que deben tenerse en cuenta en el campo penal. El Gobierno busca las medidas jur\u00eddicas que den garant\u00eda al proceso. P. En otra mesa se sientan Cuba y EE UU. \u00bfNormalizar\u00e1n plenamente sus relaciones? R. Era la \u00faltima r\u00e9mora de la guerra fr\u00eda. Obama ha tomado una decisi\u00f3n de mucho coraje, en un momento pol\u00edtico interno dif\u00edcil, y con un sentido pragm\u00e1tico. Se\u00f1al\u00f3 que las conductas hacia Cuba no daban resultado y que hab\u00eda que buscar otro camino. La Cumbre de las Am\u00e9ricas en Panam\u00e1 fue hist\u00f3rica. El presidente Castro se expres\u00f3 con mucha honestidad. Y Obama reconoci\u00f3 que no son perfectos, que tienen problemas. Ojal\u00e1 se restablezcan las embajadas y el pueblo cubano camine por la senda de la democracia. P. \u00bfCu\u00e1l es el papel del papa Francisco en la pol\u00edtica exterior en Latinoam\u00e9rica? R. El Papa ha tenido un rol muy activo en asuntos de inter\u00e9s general en el mundo, como los problemas de la mujer, el cambio clim\u00e1tico, Cuba y Estados Unidos... su presencia en el mundo social es importante. Nos recuerda que existe gente, gente marginada, necesitada. Los pa\u00edses m\u00e1s ricos tienen que contribuir a que tengamos un mundo m\u00e1s equilibrado. P. \u00bfQu\u00e9 tipo de relaci\u00f3n hay entre EE UU y Am\u00e9rica Latina? R. Estados Unidos tuvo por mucho tiempo, no un abandono, sino una negligencia benigna hacia Am\u00e9rica Latina. Como Europa. \u00bfQui\u00e9n ocup\u00f3 ese espacio? China. Con Europa tenemos valores compartidos, y la independencia paraguaya est\u00e1 inspirada en la revoluci\u00f3n francesa. De Espa\u00f1a, como puente, necesit\u00e1bamos m\u00e1s acompa\u00f1amiento. China ocup\u00f3 ese espacio. A Estados Unidos se le mira con diversos cristales. Para Paraguay es un pa\u00eds amigo. P. \u00bfLa relaci\u00f3n con Argentina? R. Es un socio comercial importante. Pero hay cuestiones del d\u00eda a d\u00eda que pueden enturbiar nuestras relaciones. Queremos hacer un Mercosur m\u00e1s abierto, sin trabas.\",\n    \"target_text\": \"El ministro paraguayo reflexiona sobre las relaciones diplom\u00e1ticas en Am\u00e9rica Latina y la actualidad en Venezuela y Cuba\"\n}\n</code></pre> <pre><code>{\n    \"text\": \"La Audiencia Nacional ha aprobado extraditar al empresario egipcio Husein Salem a Egipto, donde est\u00e1 siendo juzgado por su supuesta implicaci\u00f3n en el caso de corrupci\u00f3n que se sigue contra el expresidente Hosni Mubarak, seg\u00fan inform\u00f3 el Ministerio de Exteriores egipcio. El tribunal tambi\u00e9n aprob\u00f3 la entrega de Jaled, hijo de Salem, mientras se estudia si su hija Magda ser\u00e1 extraditada. La fiscal\u00eda acusa a Salem de haber obtenido favores pol\u00edticos a cambio de la donaci\u00f3n a la familia Mubarak de cinco mansiones, camuflada como una venta ficticia. Esos favores se tradujeron en la asignaci\u00f3n de terrenos a su favor y la adquisici\u00f3n fraudulenta de contratos p\u00fablicos de venta y exportaci\u00f3n de gas a Israel, en la localidad de Sharm El Sheik. Esta venta hizo perder al Estado egipcio 536 millones. El empresario, detenido en Espa\u00f1a el 16 junio de 2011, fue condenado el jueves a 15 a\u00f1os de c\u00e1rcel por otro caso de corrupci\u00f3n. Y en octubre ya fue sentenciado a siete a\u00f1os, al igual que sus hijos Jaled y Magda, por blanquear 1,7 millones.\",\n    \"target_text\": \"La fiscal\u00eda acusa a Salem de haber obtenido favores pol\u00edticos a cambio de la donaci\u00f3n al exdictador de cinco mansiones, como una venta ficticia\",\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>Los siguientes son art\u00edculos de noticias con sus res\u00famenes.\n</code></pre></li> <li>Base prompt template:   <pre><code>Art\u00edculo: {text}\nResumen: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Art\u00edculo: {text}\n\nEscribe un resumen del art\u00edculo anterior.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset mlsum-es\n</code></pre>"},{"location":"datasets/swedish/","title":"\ud83c\uddf8\ud83c\uddea Swedish","text":"<p>This is an overview of all the datasets used in the Swedish part of EuroEval. The datasets are grouped by their task - see the task overview for more information about what these constitute.</p>"},{"location":"datasets/swedish/#sentiment-classification","title":"Sentiment Classification","text":""},{"location":"datasets/swedish/#swerec","title":"SweReC","text":"<p>This dataset was published in this B.Sc. thesis and is a manually annotated dataset of Swedish reviews from both Trustpilot and Reco.se.</p> <p>The original dataset contains 10,757 reviews. We use a split of 1,024 / 256 / 2,048 samples for training, validation, and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"J\u00e4ttebra och rekommenderas till alla\",\n  \"label\": \"positive\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Lugnt och trevlig st\u00e4mning, inte f\u00f6r bullrigt. god mat, lite mer variation hade \u00f6nskats p\u00e5 de varma r\u00e4tterna. trevlig personal, dock missade de att ta dryckesbest\u00e4llningar fr\u00e5n oss vilket var ett litet minus. \u00f6verlag trevlig st\u00e4lle.\",\n  \"label\": \"neutral\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Extremt d\u00e5lig mottagning - b\u00e5de gsm och 3g? samtalen bryts hela tiden och s\u00e5 tar dom betalt f\u00f6r en ny uppkopplingsavgift varje g\u00e5ng.\",\n  \"label\": \"negative\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>F\u00f6ljande \u00e4r recensioner och deras sentiment, som kan vara 'positiv', 'neutral' eller 'negativ'.\n</code></pre></li> <li>Base prompt template:   <pre><code>Recension: {text}\nSentiment: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Recension: {text}\n\nKlassificera sentimentet i recensionen. Svara med 'positiv', 'neutral' eller 'negativ'.\n</code></pre></li> <li>Label mapping:<ul> <li><code>positive</code> \u27a1\ufe0f <code>positiv</code></li> <li><code>neutral</code> \u27a1\ufe0f <code>neutral</code></li> <li><code>negative</code> \u27a1\ufe0f <code>negativ</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset swerec\n</code></pre>"},{"location":"datasets/swedish/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"datasets/swedish/#suc-30","title":"SUC 3.0","text":"<p>This dataset, also known as the Stockholm-Ume\u00e5 Corpus 3.0, was published here and is a manually NER-annotated dataset, based on Swedish texts from the 1990s. The dataset does not follow the CONLL format, so we convert it into that format using the following mapping:</p> <ul> <li><code>animal</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>event</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>inst</code> \u27a1\ufe0f <code>ORG</code></li> <li><code>myth</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>other</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>person</code> \u27a1\ufe0f <code>PER</code></li> <li><code>place</code> \u27a1\ufe0f <code>LOC</code></li> <li><code>product</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>work</code> \u27a1\ufe0f <code>MISC</code></li> </ul> <p>The dataset consists of 74,245 samples, which we split into 1,024 / 256 / 2,048 samples for training, validation, and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"tokens\": array(['Det', 'l\u00e5ter', 'som', 'en', 'v\u00e4stanfl\u00e4kt', 'j\u00e4mf\u00f6rt', 'med', 'den', 'i', 'filmen', 'f\u00f6rk\u00e4ttrade', 'bilj\u00e4tten', 'General', 'Motors', ',', 'som', 'frist\u00e4llt', '35000', 'jobbare', 'i', 'staden', 'Flint', ',', 'Michigan', '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['En', 'liknande', 'kunskapsteoretisk', 'grundfr\u00e5ga', ',', 'fast', 'i', 'mer', 'modernt', 'sofistikerad', 'form', ',', 'n\u00e5r', 'oss', 'nu', 'fr\u00e5n', 'Paris', ':'], dtype=object),\n  \"labels\": array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['-', 'Dessv\u00e4rre', ',', 'sa', 'man', ',', 'vi', 'har', 'ingen', 'Bj\u00f6rn', 'Eriksson', 'p\u00e5', 'passagerarlistan', '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>F\u00f6ljande \u00e4r meningar och JSON-ordb\u00f6cker med de namngivna enheter som f\u00f6rekommer i den givna meningen.\n</code></pre></li> <li>Base prompt template:   <pre><code>Mening: {text}\nNamngivna entiteter: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Mening: {text}\n\nIdentifiera de namngivna enheterna i meningen. Du ska outputta detta som en JSON-ordbok med nycklarna 'person', 'plats', 'organisation' och 'diverse'. V\u00e4rdena ska vara listor \u00f6ver de namngivna enheter av den typen, precis som de f\u00f6rekommer i meningen.\n</code></pre></li> <li>Label mapping:<ul> <li><code>B-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>plats</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>plats</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>organisation</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>organisation</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>diverse</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>diverse</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset suc3\n</code></pre>"},{"location":"datasets/swedish/#linguistic-acceptability","title":"Linguistic Acceptability","text":""},{"location":"datasets/swedish/#scala-sv","title":"ScaLA-sv","text":"<p>This dataset was published in this paper and was automatically created from the Swedish Universal Dependencies treebank by assuming that the documents in the treebank are correct, and corrupting the samples to create grammatically incorrect samples. The corruptions were done by either removing a word from a sentence, or by swapping two neighbouring words in a sentence. To ensure that this does indeed break the grammaticality of the sentence, a set of rules were used on the part-of-speech tags of the words in the sentence.</p> <p>The original dataset consists of 6,026 samples, from which we use 1,024 / 256 / 2,048 samples for training, validation and testing, respectively (so 3,328 samples used in total). These splits are used as-is in the framework.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"U-l\u00e4nderna m\u00e5ste ta en genv\u00e4g f\u00f6r att komma i fatt.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Undra att vi blev lite undandragna.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Det \u00e4r ocks\u00e5 att viktigt ha tillr\u00e4ckligt korta dubbar.\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>F\u00f6ljande \u00e4r meningar och huruvida de \u00e4r grammatiskt korrekta.\n</code></pre></li> <li>Base prompt template:   <pre><code>Mening: {text}\nGrammatisk korrekt: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Mening: {text}\n\nBest\u00e4m om meningen \u00e4r grammatiskt korrekt eller inte. Svara med 'ja' om meningen \u00e4r korrekt och 'nej' om den inte \u00e4r.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>ja</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>nej</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scala-sv\n</code></pre>"},{"location":"datasets/swedish/#reading-comprehension","title":"Reading Comprehension","text":""},{"location":"datasets/swedish/#scandiqa-sv","title":"ScandiQA-sv","text":"<p>This dataset was published in this paper and was automatically created from the Swedish part of the MKQA dataset. The MKQA dataset is based on the English Natural Questions dataset, based on search queries from the Google search engine. The questions and answers were manually translated to Swedish (and other languages) as part of MKQA, and the contexts were in ScandiQA-sv machine translated using the DeepL translation API. A rule-based approach was used to ensure that the translated contexts still contained the answer to the question, potentially by changing the answers slightly.</p> <p>The original full dataset consists of 6,810 / 500 / 500 samples for training, validation and testing, respectively (so 3,328 samples used in total). We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively, where the splits are made by randomly sampling from the full dataset without considering the original train/validation/test splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"context\": \"I Freedom Cry f\u00e5r spelaren ta rollen som Ad\u00e9wal\u00e9, en frigiven slav fr\u00e5n Trinidad som blev Edward Kenways kvarterm\u00e4stare och senare medlem i Assassin Order. Ber\u00e4ttelsel\u00e4get utspelar sig 15 \u00e5r efter h\u00e4ndelserna i Assassin's Creed IV: Black Flag d\u00e4r Ad\u00e9wal\u00e9 har blivit en tr\u00e4nad l\u00f6nnm\u00f6rdare och finner sig sj\u00e4lv skeppsbruten i Saint-Domingue, d\u00e4r han st\u00e4lls \u00f6ga mot \u00f6ga med n\u00e5got av det mest brutala slaveriet i V\u00e4stindien. DLC:n \u00e4r skriven av Jill Murray, som skrev Liberation och Aveline-inneh\u00e5llet f\u00f6r Black Flag. I februari 2014 meddelades att Freedom Cry skulle sl\u00e4ppas som en frist\u00e5ende titel till PlayStation 4 och PlayStation 3 den 18 februari 2014 f\u00f6r Nordamerika och den 19 februari 2014 f\u00f6r Europa. Det sl\u00e4pptes f\u00f6r PC den 25 februari 2014.\",\n  \"question\": \"N\u00e4r sl\u00e4pptes assassin's creed freedom cry?\",\n  \"answers\": {\n    \"answer_start\": array([637]),\n    \"text\": array(['18 februari 2014'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  \"context\": 'Political history of the United Kingdom (1945\u2013present)\\n\u00c5r 1950 orsakade Koreakriget ett nytt tungt tryck p\u00e5 statskassan f\u00f6r milit\u00e4ra utgifter. Detta orsakade en bitter splittring inom Labourpartiet.  De konservativa gjorde \u00e5tstramningspolitiken till en viktig fr\u00e5ga i parlamentsvalet 1950. Labour f\u00f6rlorade det mesta av sin stora majoritet. Sv\u00e4ngningen var 3,6 % mot dem och de f\u00f6rlorade 78 platser, vilket gav Attlee en knapp majoritet i parlamentet. Ett \u00e5r senare f\u00f6rlorade Labour dock parlamentsvalet 1951 trots att det fick fler r\u00f6ster \u00e4n i valet 1945, och faktiskt fler r\u00f6ster \u00e4n det konservativa partiet.',\n  \"question\": 'Hur m\u00e5nga \u00e5r har det varit sen 1940?',\n  \"answers\": {\n    \"answer_start\": array([388]),\n    \"text\": array(['78'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  \"context\": 'Data link layer\\nOSI-modellen\\nper skikt\\n\\n\\n\\n\\n7.  Applikationslager[visa]\\n\\n\\nNNTP\\nSIP\\nSSI\\nDNS\\nFTP\\nGopher\\nHTTP\\nNFS\\nNTP\\nSMPP\\nSMTP\\nSNMP\\nTelnet\\nDHCP\\nNetconf\\nmer....\\n\\n\\n\\n\\n\\n\\n\\n\\n6.  Presentationslager[visa]\\n\\n\\nMIME\\nXDR\\n\\n\\n\\n\\n\\n\\n\\n\\n5.  Sessionsskikt[visa]\\n\\n\\nNamngiven pipe\\nNetBIOS\\nSAP\\nPPTP\\nRTP\\nSOCKS\\nSPDY\\n\\n\\n\\n\\n\\n\\n\\n\\n4.  Transportlager[visa]\\n\\n\\nTCP\\nUDP\\nSCTP\\nDCCP\\nSPX\\n\\n\\n\\n\\n\\n\\n\\n\\n3.  N\u00e4tverksskikt[visa]\\n\\n\\nIP\\n\\nIPv4\\nIPv6\\n\\n\\nICMP\\nIPsec\\nIGMP\\nIPX\\nAppleTalk\\nX.25 PLP\\n\\n\\n\\n\\n\\n\\n\\n\\n2.  Datal\u00e4nkskiktet[visa]\\n\\n\\nATM\\nARP\\nIS-IS\\nSDLC\\nHDLC\\nCSLIP\\nSLIP\\nGFP\\nPLIP\\nIEEE 802.2\\nLLC\\nMAC\\nL2TP\\nIEEE 802.3\\nFrame Relay\\nITU-T G.hn DLL\\nPPP\\nX.25 LAPB\\nQ.921 LAPD\\nQ.922 LAPF\\n\\n\\n\\n\\n\\n\\n\\n\\n1.  Fysiskt lager[visa]\\n\\n\\nEIA/TIA-232\\nEIA/TIA-449\\nITU-T V-serien\\nI.430\\nI.431\\nPDH\\nSONET/SDH\\nPON\\nOTN\\nDSL\\nIEEE 802.3\\nIEEE 802.11\\nIEEE 802.15\\nIEEE 802.16\\nIEEE 1394\\nITU-T G.hn PHY\\nUSB\\nBluetooth\\nRS-232\\nRS-449\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nv\\nt\\ne',\n  \"question\": 'Vilket lager av osi-modellen \u00e4r uppdelad i tv\u00e5 delskikt?',\n  \"answers\": {\n    \"answer_start\": array([0]),\n    \"text\": array(['Data link layer'], dtype=object)\n  }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>Nedan f\u00f6ljer texter med tillh\u00f6rande fr\u00e5gor och svar.\n</code></pre></li> <li>Base prompt template:   <pre><code>Text: {text}\nFr\u00e5ga: {question}\nSvar p\u00e5 max 3 ord: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Text: {text}\n\nBesvara f\u00f6ljande fr\u00e5ga om texten ovan med h\u00f6gst 3 ord.\n\nFr\u00e5ga: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scandiqa-sv\n</code></pre>"},{"location":"datasets/swedish/#unofficial-belebele-sv","title":"Unofficial: BeleBele-sv","text":"<p>This dataset was published in this paper and features multiple-choice reading comprehension questions across 122 languages.</p> <p>The original dataset contains 900 unique multiple-choice reading comprehension passages and questions. From these, we use a 256 / 64 / 580 split for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Sundarbans \u00e4r det st\u00f6rsta kustmangroveb\u00e4ltet i v\u00e4rlden och str\u00e4cker sig 80 km in i Bangladesh och det indiska inlandet fr\u00e5n kusten. Sundarbans har antagits p\u00e5 Unescos v\u00e4rldsarvslista. Den del av skogen som ligger p\u00e5 indiskt territorium kallas Sundarbans National Park. Skogarna \u00e4r dock inte bara mangrovetr\u00e4sk \u2014 de inneh\u00e5ller n\u00e5gra av de sista kvarvarande st\u00e5ndorterna av de stora djunglerna som en g\u00e5ng t\u00e4ckte Gangessl\u00e4tten. Sundarban t\u00e4cker ett omr\u00e5de p\u00e5 3 850 km\u00b2, varav ungef\u00e4r en tredjedel utg\u00f6rs av v\u00e5tmarker. Sedan 1966 har Sundarbans varit ett reservat f\u00f6r vilda djur, och det uppskattas att det nu finns 400 bengaliska tigrar och omkring 30 000 axishjortar i omr\u00e5det.\\nFr\u00e5ga: Vilken del av skogen ligger p\u00e5 indiskt territorium?\\nSvarsalternativ:\\na. Sundarbans National Park\\nb. Reservatet f\u00f6r vilda djur\\nc. V\u00e4rldsarvet\\nd. Gangessl\u00e4tten\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Italiens nationella fotboll, tillsammans med det tyska fotbollslaget, \u00e4r v\u00e4rldens n\u00e4st mest framg\u00e5ngsrika lag och var m\u00e4stare i FIFA-v\u00e4rldscupen \u00e5r 2006. Popul\u00e4ra sporter inkluderar fotboll, basket, volleyboll, vattenpolo, f\u00e4ktning, rugby, cykel, ishockey, rullskridskohockey och Formel 1. Vintersporter \u00e4r mest popul\u00e4ra i de norra regionerna, d\u00e4r italienare t\u00e4vlar i internationella t\u00e4vlingar och olympiska evenemang.\\nFr\u00e5ga: I vilken av f\u00f6ljande sporter vann Italien en v\u00e4rldscup enligt avsnittet?\\nSvarsalternativ:\\na. Fotboll\\nb. Vattenpolo\\nc. Basket\\nd. Cykel\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Bokning i f\u00f6rv\u00e4g ger resen\u00e4ren trygghet och en f\u00f6rs\u00e4kran om att de kommer att ha n\u00e5gonstans att sova n\u00e4r de anl\u00e4nder till sin destination. Resebyr\u00e5er har ofta avtal med s\u00e4rskilda hotell, men det kan vara m\u00f6jligt att boka andra typer av boenden, s\u00e5som campingplatser, genom en resebyr\u00e5. Resebyr\u00e5er erbjuder ofta paket som inkluderar frukost, transfer till och fr\u00e5n flygplatsen, och till och med paketresor som kombinerar flyg och hotell. De kan ocks\u00e5 reservera din bokning \u00e5t dig om du beh\u00f6ver tid att t\u00e4nka \u00f6ver erbjudandet eller skaffa fram ytterligare dokument som kr\u00e4vs f\u00f6r din destination (t.ex. visering). Alla \u00e4ndringar och f\u00f6rfr\u00e5gningar ska dock g\u00e5 genom resebyr\u00e5n f\u00f6rst, och inte direkt till hotellet.\\nFr\u00e5ga: Vilken typ av resen\u00e4r kommer sannolikt inte att dra nytta av att anv\u00e4nda sig av tj\u00e4nster fr\u00e5n en resebyr\u00e5, enligt det som st\u00e5r i texten?\\nSvarsalternativ:\\na. En obeslutsam resen\u00e4r\\nb. En resen\u00e4r som \u00e4r spontan\\nc. En resen\u00e4r som inte har skaffat visum \u00e4n\\nd. En resen\u00e4r som f\u00f6redra att boka paketerbjudanden\",\n  \"label\": \"b\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f6ljande \u00e4r flervalsfr\u00e5gor (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Fr\u00e5ga: {text}\nSvarsalternativ:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Fr\u00e5ga: {text}\nSvarsalternativ:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBesvara f\u00f6ljande fr\u00e5ga med 'a', 'b', 'c' eller 'd', och inget annat.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset belebele-sv\n</code></pre>"},{"location":"datasets/swedish/#knowledge","title":"Knowledge","text":""},{"location":"datasets/swedish/#mmlu-sv","title":"MMLU-sv","text":"<p>This dataset is a machine translated version of the English MMLU dataset and features questions within 57 different topics, such as elementary mathematics, US history and law. The translation to Swedish was done by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 269 / 1,410 / 13,200 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). These splits are new and there can thus be some overlap between the original validation and test sets and our validation and test sets.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Varf\u00f6r \u00e4r tidpunkten f\u00f6r monumental byggnation vid Ceibal signifikant?\\nSvarsalternativ:\\na. Det mots\u00e4ger hypotesen att den monumental byggnationen av Maya i huvudsak inspirerades av Olmekerna.\\nb. Det bekr\u00e4ftar att inv\u00e5narna i Ceibal inspirerades av Olmekerna f\u00f6r att bygga stora plattformar.\\nc. Det mots\u00e4ger hypotesen att utvecklingen av monumental byggnation bland Maya var en intern process.\\nd. Det bekr\u00e4ftar att Olmekerna, som byggde de flesta Maya-monumenten, inspirerades av egyptierna.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Vilken populationsstatistik visar f\u00f6delsetalet vid vilket en befolkning precis f\u00e5r tillr\u00e4ckligt med f\u00f6dslar f\u00f6r att ers\u00e4tta f\u00f6r\u00e4ldrarna och kompensera f\u00f6r tidiga d\u00f6dsfall?\\nSvarsalternativ:\\na. R\u00e5 f\u00f6delsetal\\nb. Ers\u00e4ttningstal\\nc. D\u00f6dlighetstal\\nd. Total fertilitetstal\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"En subenhet av DNA och protein som best\u00e5r av 134-baspar l\u00e5nga str\u00e4ckor av DNA som omger en proteinoktomer kallas (a)\\nSvarsalternativ:\\na. histon\\nb. kromatin\\nc. nukleosom\\nd. solenoid\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f6ljande \u00e4r flervalsfr\u00e5gor (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Fr\u00e5ga: {text}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Fr\u00e5ga: {text}\n\nBesvara f\u00f6ljande fr\u00e5ga med 'a', 'b', 'c' eller 'd', och inget annat.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset mmlu-sv\n</code></pre>"},{"location":"datasets/swedish/#unofficial-arc-sv","title":"Unofficial: ARC-sv","text":"<p>This dataset is a machine translated version of the English ARC dataset and features US grade-school science questions. The translation to Swedish was done by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 1,110 / 297 / 1,170 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively (so 2,304 samples used in total). All new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"En typ av f\u00e5gel i Afrika \u00e4ter blodsugande insekter fr\u00e5n stora d\u00e4ggdjur. Vilket ord beskriver b\u00e4st relationen mellan f\u00e5geln och d\u00e4ggdjuren?\\nSvarsalternativ:\\na. mutualism\\nb. parasitism\\nc. neutralism\\nd. kommensalism\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Mr. Pratt g\u00f6r en vetenskaplig demonstration. Han bl\u00e5ser upp en ballong, placerar den i en frys och tar sedan ut den efter 10 minuter. Vilket alternativ beskriver b\u00e4st ballongens volym n\u00e4r den \u00e4r i frysen och efter att den har tagits ut och \u00e5ter till\u00e5tits att v\u00e4rmas upp?\\nSvarsalternativ:\\na. expanderar i frysen och kontraherar sedan n\u00e4r den blir varmare igen\\nb. kontraherar i frysen och expanderar sedan n\u00e4r den blir varmare igen\\nc. expanderar i frysen och h\u00e5ller sedan den volymen n\u00e4r den v\u00e4rms upp\\nd. kontraherar i frysen och h\u00e5ller sedan den volymen n\u00e4r den v\u00e4rms upp\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"En elev tills\u00e4tter vatten och reng\u00f6ringsmedel till en kopp med jord. Blandningen skakas och till\u00e5ts s\u00e4tta sig. Eleven observerar att silt-partiklar f\u00f6rblir uppsuspenderade l\u00e5ngt efter att de andra partiklarna bildar lager p\u00e5 botten av beh\u00e5llaren. Den mest troliga f\u00f6rklaringen \u00e4r att silt-partiklarna \u00e4r\\nSvarsalternativ:\\na. organiska.\\nb. uppl\u00f6sta.\\nc. mindre t\u00e4tt packade.\\nd. r\u00f6r sig snabbare.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f6ljande \u00e4r flervalsfr\u00e5gor (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Fr\u00e5ga: {text}\nSvarsalternativ:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Fr\u00e5ga: {text}\nSvarsalternativ:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBesvara f\u00f6ljande fr\u00e5ga med 'a', 'b', 'c' eller 'd', och inget annat.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset arc-sv\n</code></pre>"},{"location":"datasets/swedish/#common-sense-reasoning","title":"Common-sense Reasoning","text":""},{"location":"datasets/swedish/#hellaswag-sv","title":"HellaSwag-sv","text":"<p>This dataset is a machine translated version of the English HellaSwag dataset. The original dataset was based on both video descriptions from ActivityNet as well as how-to articles from WikiHow. The dataset was translated by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 9,310 samples. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"[header] Hur man hittar de perfekta brudt\u00e4rnekl\u00e4nningarna [title] Internet \u00e4r en underbar resurs f\u00f6r att hitta brudt\u00e4rnekl\u00e4nningar. [step] Vi rekommenderar ocks\u00e5 att bl\u00e4ddra genom popul\u00e4ra br\u00f6llopstidningar, s\u00e5som brudens och moderna brudt\u00e4rnets tidningar. Rekommenderat \u00e4r att bruden g\u00e5r och handlar med en eller tv\u00e5 av sina brudt\u00e4rnor och ser vilka stilar de gillar.\\nSvarsalternativ:\\na. N\u00e4r du har begr\u00e4nsat urvalet kan du sedan f\u00e5 input fr\u00e5n dina andra brudt\u00e4rnor om du \u00f6nskar det. [title] Vilka \u00e4r de senaste trenderna i brudt\u00e4rnekl\u00e4nningar? [title] A-linje kl\u00e4nningar som ser bra ut p\u00e5 alla olika kroppsformer och storlekar \u00e4r mycket popul\u00e4ra.\\nb. Tyv\u00e4rr kan du inte handla lika ofta som om du letade efter matchade brudt\u00e4rnor. [title] N\u00e4r du v\u00e4ljer din brud, v\u00e4lj tre olika stilar: [step] Klipp l\u00e4ngd, klipp tjocklek och fr\u00e5n de flesta \\\"f\u00f6r-skjutna\\\" stilarna till de grundl\u00e4ggande.\\nc. Medan varje brud \u00e4r annorlunda, alla \u00e4r b\u00e5de olika och har olika smaker. [title] Se om bruden har en favoritlook f\u00f6r sin br\u00f6llopskl\u00e4nning.\\nd. [title] B\u00f6rja s\u00f6ka efter id\u00e9er eller allm\u00e4nna \u00e5sikter om s\u00e4rskilda br\u00f6llopskl\u00e4nningar. [step] F\u00f6rs\u00f6k att inte bli f\u00f6r stel och s\u00f6k bara efter n\u00e5gra kl\u00e4nningar som du tror kan fungera bra tillsammans.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"[header] Hur man g\u00f6r en pedikyr [title] Ta bort all befintlig f\u00e4rg med nagellacksborttagare. [step] T\u00e4ck toppen p\u00e5 din nagellacksborttagare med en bomullstuss, v\u00e4nd snabbt upp och ner den och omedelbart upp och ner igen f\u00f6r att applicera lite av produkten. Gnugga sedan nagellacksborttagaren \u00f6ver dina t\u00e5naglar f\u00f6r att ta bort f\u00e4rgen.\\nSvarsalternativ:\\na. [title] L\u00e5t dina t\u00e5naglar bl\u00f6tl\u00e4ggas i vatten i 10 till 20 minuter. [step] Vatten kan g\u00f6ra dina naglar vitare genom att l\u00f6sa upp andra f\u00f6reningar, s\u00e4rskilt syror.\\nb. [substeps] Flytta bomullstussen i sm\u00e5, cirkul\u00e4ra r\u00f6relser om du har sv\u00e5rt att ta bort f\u00e4rgen. [title] Fyll en fotspa eller en balja med varmt vatten.\\nc. [substeps] Om du inte har nagellacksborttagare kan du \u00f6verv\u00e4ga att anv\u00e4nda den vita nagellacksborttagaren fr\u00e5n f\u00f6reg\u00e5ende steg f\u00f6r en enklare applikation. [title] T\u00e4ck dina h\u00e4nder med bandage eller tejp med canvas-lining.\\nd. [title] Anv\u00e4nd aceton p\u00e5 dina t\u00e5naglar. [step] Aceton kan verkligen hj\u00e4lpa till att ta bort gammalt nagellack fr\u00e5n dina naglar.\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Han forts\u00e4tter att klippa gr\u00e4set. Kameran fokuserar p\u00e5 det rinnande vattnet igen. Den g\u00e5r tillbaka till mannen som klipper gr\u00e4set. sedan\\nSvarsalternativ:\\na. den g\u00e5r tillbaka till filmen av mannen som klipper jord.\\nb. \u00e5terv\u00e4nder till honom och dem som pratar igen.\\nc. v\u00e4xlar tillbaka till det rinnande vattnet.\\nd. m\u00f6rk himmel igen.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f6ljande \u00e4r flervalsfr\u00e5gor (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Fr\u00e5ga: {text}\nSvarsalternativ:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Fr\u00e5ga: {text}\nSvarsalternativ:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBesvara f\u00f6ljande fr\u00e5ga med 'a', 'b', 'c' eller 'd', och inget annat.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset hellaswag-sv\n</code></pre>"},{"location":"datasets/swedish/#summarization","title":"Summarization","text":""},{"location":"datasets/swedish/#swedn","title":"SweDN","text":"<p>This dataset was published in this paper and are based on news articles from the Swedish newspaper Dagens Nyheter, with the summaries being the first paragraph of the article (and that paragraph being removed from the article).</p> <p>The original dataset consists of 29,800 / 4,530 / 3,750 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). All the new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Ett \u00f6verraskande ras p\u00e5 den ryska lastbilsmarknaden har gjort att Scania blivit fr\u00e5nsprunget av konkurrenten Volvo som \u00f6kat sina leveranser, skriver Dagens Industri. Bakom Scanias tapp p\u00e5 24 procent ligger bland annat problem med tillst\u00e5nden f\u00f6r att producera Euro-3 lastbilar i fabriken i S:t Petersburg. Men det r\u00e4knar Scanias Rysslandschef Hans Tardell med att ta tillbaka under \u00e5ret. Konkurrenten Volvo, som \u00f6kat leveranserna med 40 procent och ordering\u00e5ngen med 68 procent j\u00e4mf\u00f6rt mot f\u00f6rsta kvartalet 2011, hoppas kunna v\u00e4xa ytterligare.  \",\n  \"target_text\": \"Ett \u00f6verraskande ras p\u00e5 den ryska lastbilsmarknaden har gjort att Scania blivit fr\u00e5nsprunget av konkurrenten Volvo som \u00f6kat sina leveranser, skriver Dagens Industri.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Scenen som beskrivs i \u00e5talet kunde vara h\u00e4mtad ur en skr\u00e4ckfilm. Den d\u00e5 tolv\u00e5riga flickan har ber\u00e4ttat hur hon f\u00f6rs\u00e5gs med handbojor och kedjades vid en krok i taket. Enligt \u00e5talet ska hon \u00e4ven ha f\u00e5tt ett koppel kring halsen och piskats. \u00c5klagaren menar att det handlar om ett utdraget f\u00f6rlopp. \u2013 En tolv\u00e5rig flicka ska inte sitta fastsatt i en krok i taket, s\u00e4ger \u00e5klagare Daniel Veivo Pettersson, som nu har \u00e5talat en 25-\u00e5rig man f\u00f6r grov v\u00e5ldt\u00e4kt mot barn. I veckan ber\u00e4ttade TT att sju m\u00e4n d\u00f6mts f\u00f6r att vid olika tillf\u00e4llen ha utsatt samma flicka f\u00f6r sexuella \u00f6vergrepp. M\u00e4nnen fick kontakt med flickan via forum p\u00e5 n\u00e4tet och tjatade sig till tr\u00e4ffar med henne. En av m\u00e4nnen band och v\u00e5ldtog henne i en skog. 25-\u00e5ringen blir nu den \u00e5ttonde mannen som \u00e5talas f\u00f6r \u00f6vergrepp. \u2013 Man h\u00e4pnar n\u00e4r man h\u00f6r hennes ber\u00e4ttelse. Hon \u00e4r mycket trov\u00e4rdig och vi har \u00e4ven kunnat styrka \u00e5talen mot m\u00e4nnen genom teknisk bevisning som chattkonversationer och i n\u00e5got fall fanns dna p\u00e5 en kondom och p\u00e5 en bh, s\u00e4ger Daniel Veivo Pettersson. Vid en husrannsakan i 25-\u00e5ringens hem i Stockholm, d\u00e4r v\u00e5ldt\u00e4kten ska ha beg\u00e5tts under h\u00f6sten 2013, hittades kedjor, handbojor, koppel och en piska. Enligt flickan hade delar av \u00f6vergreppen filmats. Polisen misst\u00e4nkte att filmerna kunde ha sparats i en s\u00e5 kallad molntj\u00e4nst, och \u00e5klagaren fick ta hj\u00e4lp av Microsoft i USA. \u2013 Det drog ut p\u00e5 tiden, men tyv\u00e4rr hittade vi inte det vi letade efter. Han har raderat en hel del information i sin dator, s\u00e4ger Daniel Veivo Pettersson. 25-\u00e5ringen \u00e5talas dessutom f\u00f6r ytterligare en v\u00e5ldt\u00e4kt p\u00e5 flickan, eftersom han misst\u00e4nks ha v\u00e5ldtagit henne p\u00e5 en toalett. Mannen \u00e4r tidigare d\u00f6md f\u00f6r \u00f6vergrepp p\u00e5 en annan minder\u00e5rig flicka, och \u00e5klagaren har nu beg\u00e4rt honom h\u00e4ktad i sin fr\u00e5nvaro. \u2013 Han kan vara hemma, men han kan \u00e4ven vara utomlands. Om han h\u00e4ktas i sin utevaro kommer han att efterlysas, s\u00e4ger Daniel Veivo Pettersson. 25-\u00e5ringen f\u00f6rsvaras av advokat Thomas Bodstr\u00f6m. Han vill inte ber\u00e4tta om 25-\u00e5ringen kommer n\u00e4rvara vid h\u00e4ktningsf\u00f6rhandlingen, men han s\u00e4ger: \u2013 Han nekar till samtliga brott, \u00e4r helt oskyldig och det finns ingen grund f\u00f6r h\u00e4ktning. Enligt \u00e5klagaren misst\u00e4nks flickan ha utsatts av ytterligare minst en man som polisen inte har lyckats identifiera. M\u00e4nnen i h\u00e4rvan 37-\u00e5ring, \u00d6sterg\u00f6tland: V\u00e5ldt\u00e4kt mot barn och barnpornografibrott \u2013 fem \u00e5rs f\u00e4ngelse. 26-\u00e5ring, Dalarna: Sexuellt ofredande \u2013 skyddstillsyn. 29-\u00e5ring, Stockholmstrakten: V\u00e5ldt\u00e4kt mot barn (tv\u00e5 tillf\u00e4llen) \u2013 tre \u00e5rs f\u00e4ngelse. 26-\u00e5ring, Stockholmstrakten: V\u00e5ldt\u00e4kt mot barn \u2013 tv\u00e5 och ett halvt \u00e5rs f\u00e4ngelse. 27-\u00e5ring, Stockholmstrakten: Grov v\u00e5ldt\u00e4kt mot barn och v\u00e5ldt\u00e4kt mot barn (fyra tillf\u00e4llen) \u2013 sju \u00e5rs f\u00e4ngelse. 55-\u00e5ring, \u00d6sterg\u00f6tland: Utnyttjande av barn f\u00f6r sexuell posering (elva tillf\u00e4llen) och sexuellt ofredande (tv\u00e5 tillf\u00e4llen) \u2013 \u00e5tta m\u00e5naders f\u00e4ngelse. 19-\u00e5ring, V\u00e4stra G\u00f6taland: V\u00e5ldt\u00e4kt mot barn \u2013 \u00e5tta m\u00e5naders f\u00e4ngelse (domen \u00e4r \u00f6verklagad). 25-\u00e5ring, Stockholmstrakten: \u00c5talad f\u00f6r grov v\u00e5ldt\u00e4kt mot barn och v\u00e5ldt\u00e4kt mot barn. \",\n  \"target_text\": \"Den tolv\u00e5riga flickan kedjades vid en krok i taket och v\u00e5ldtogs. En 25-\u00e5rig man har nu \u00e5talats f\u00f6r grov v\u00e5ldt\u00e4kt mot barn, men det \u00e4r oklart var han \u00e4r. Sju m\u00e4n d\u00f6mdes nyss f\u00f6r \u00f6vergrepp p\u00e5 samma flicka.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Det \u00e4r Gr\u00f6na partiets ledare Jill Stein som har uppmanat valkommissionen i delstaten Wisconsin att r\u00e4kna om r\u00f6sterna, det skriver Reuters och Wisconsins valkommission. Valkommissionen skriver att man \u201dr\u00e4knar med att omr\u00e4kningen b\u00f6rjar inom en vecka efter det att Steins kampanj har betalat avgiften omr\u00e4kningen, som vi fortfarande h\u00e5ller p\u00e5 att ber\u00e4kna\u201d. En omr\u00e4kning ska vara genomf\u00f6rd f\u00f6re den 13 december. Delstaten vanns av Donald Trump med 47,9 procent av r\u00f6sterna mot Hillary Clintons 46,9 procent och gav honom 10 elektorsr\u00f6ster. Skillnaden mellan de tv\u00e5 kandidaterna var 23.000 r\u00f6ster. Jill Stein har tidigare sagt att hon \u00e4r beredd att \u00e4ven f\u00f6rs\u00f6ka f\u00e5 r\u00f6sterna i Michigan och Pennsylvania omr\u00e4knade. Om hon ska beg\u00e4ra en omr\u00e4kning ocks\u00e5 i dessa tv\u00e5 delstater m\u00e5ste den beg\u00e4ran inkomma under n\u00e4sta vecka, skriver NBC News. Jill Stein. Foto: AP F\u00f6r att f\u00e5 till st\u00e5nd en omr\u00e4kning m\u00e5ste Gr\u00f6na partiet ha pengar nog att driva en s\u00e5dan. Enligt Washington Post har partiet lyckats samla in 4,5 miljoner dollar som ska t\u00e4cka juridiska omkostnader och annat som har med en eventuell omr\u00e4kning att g\u00f6ra i de tre delstaterna. Enligt tidningen kommer det sannolikt att beh\u00f6vas sammanlagt mellan 6 och 7 miljoner f\u00f6r att genomf\u00f6ra en omr\u00e4kning. Om Clinton skulle g\u00e5 segrande ur en omr\u00e4kning i Wisconsin skulle detta \u00e4nd\u00e5 inte inneb\u00e4ra n\u00e5gon skillnad n\u00e4r det g\u00e4ller utg\u00e5ngen av presidentvalet. Skulle Clinton vinna \u00e4ven i Michigan och Pennsylvania skulle det d\u00e4remot betyda en annan utg\u00e5ng av valet. \u00c4ven om f\u00e5 tror att en omr\u00e4kning skulle betyda n\u00e5got i praktiken, Hillary Clinton har redan erk\u00e4nt sig besegrad, s\u00e5 skulle en omr\u00e4kning i hennes fav\u00f6r i Wisconsin och Pennsylvania ge henne 30 elektorsr\u00f6ster medan Trump f\u00f6rlorar lika m\u00e5nga. Om s\u00e5, rent hypotetiskt, skulle bli fallet, skiljer bara 10 elektorsr\u00f6ster till Trumps f\u00f6rdel \u2013 och d\u00e5 \u00e5terst\u00e5r \u00e4nnu Michigans r\u00f6ster att slutr\u00e4knas. Skulle Clinton vinna \u00e4ven dem s\u00e5 har hon flest antal elektorsr\u00f6ster. Jill Stein har i en intervju sj\u00e4lv sagt att hon inte beg\u00e4r en omr\u00e4kning f\u00f6r att gynna n\u00e5gon av kandidaterna utan f\u00f6r att \u201damerikanerna inte blev s\u00e4rskilt glada \u00f6ver utg\u00e5ngen av valet\u201d. Sett till enbart r\u00f6sterna, och inte till elektorerna, leder just nu Hillary Clinton med 48,1 procent av r\u00f6sterna mot Donald Trumps 46,6 procent. I antal r\u00f6ster leder Clinton med 2.012.331 r\u00f6ster. \",\n  \"target_text\": \"Valkommissionen i Wisconsin i har f\u00e5tt en uppmaning om att r\u00f6sterna i presidentvalet ska r\u00e4knas om. Wisconsin har nu b\u00f6rjat f\u00f6rbereda en omr\u00e4kning. Och det kan bli fler.\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>Nedan f\u00f6ljer artiklar med tillh\u00f6rande sammanfattningar.\n</code></pre></li> <li>Base prompt template:   <pre><code>Artikel: {text}\nSammanfattning: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Artikel: {text}\n\nSkriv en sammanfattning av artikeln ovan.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset swedn\n</code></pre>"},{"location":"datasets/swedish/#unofficial-schibsted-sv","title":"Unofficial: Schibsted-sv","text":"<p>This dataset was published here and features summaries of news articles from Schibsted Medias Swedish newsroom, from Aftonbladet.</p> <p>The original dataset has 528 / 96 / 89 samples for training, validation and testing, respectively. We use these splits as-is.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Richard Jomshof blir uppr\u00f6rd och v\u00e4grar svara p\u00e5 fr\u00e5gor: SD-toppen Richard Jomshof v\u00e4grar kommentera kritiken efter p\u00e5hoppet p\u00e5 Daniel Riazat (V).  N\u00e4r Aftonbladet m\u00f6ter honom i riksdagen blir han uppr\u00f6rd och g\u00e5r iv\u00e4g. \u2013 Jag uppskattar inte skjutj\u00e4rnsjournalistik, det \u00e4r ett oseri\u00f6st s\u00e4tt att jobba, s\u00e4ger han.  Justitieutskottets ordf\u00f6rande Richard Jomshof (SD) f\u00e5r h\u00e5rd kritik f\u00f6r sitt uttalande att V-ledamoten Daniel Riazat borde flytta fr\u00e5n Sverige.  Flera i den politiska oppositionen d\u00f6mer ut det som rasistiskt. \u00c4ven i Tid\u00f6partierna h\u00f6rs protester.  \u201d\u00c4r man svensk medborgare s\u00e5 \u00e4r man. Skamligt var ordet!\u201d skriver L-politikern Jan J\u00f6nsson i ett uttalande p\u00e5 X.  \u201dTa det med pressavdelningen\u201d Aftonbladet var p\u00e5 plats utanf\u00f6r justitieutskottets m\u00f6te i riksdagen vid lunchtid p\u00e5 tisdagen. Jomshof anl\u00e4nde f\u00f6rst av alla ledam\u00f6ter, tio minuter innan m\u00f6tet inleddes, men ville inte svara p\u00e5 fr\u00e5gor.  \u2013 Du f\u00e5r ta det med pressavdelningen. Varf\u00f6r vill du inte svara, det \u00e4r ju du som har skrivit de h\u00e4r tweetsen? \u2013 Du f\u00e5r ta det med pressavdelningen. Du kan l\u00e4sa min senaste tweet f\u00f6rresten, s\u00e5 kan vi utg\u00e5 fr\u00e5n den. Varf\u00f6r tycker du att han borde l\u00e4mna Sverige? \u2013 B\u00f6rja med att l\u00e4sa min tweet, det framg\u00e5r v\u00e4ldigt tydligt d\u00e4r. \u201dUppskattar inte skjutj\u00e4rnsjournalistik\u201d Inl\u00e4gget som Jomshof syftar p\u00e5 lades upp kort innan justitieutskottets m\u00f6te. Jomshof g\u00e5r d\u00e4r till nytt angrepp mot Riazat. Han anklagar honom f\u00f6r att ha ett \u201dsunkigt\u201d beteende, att vara of\u00f6rsk\u00e4md och komma med aggressiva p\u00e5hopp p\u00e5 politiska motst\u00e5ndare.  M\u00f6tet med justitieutskottet varade en timme, n\u00e4r Richard Jomshof kom ut fr\u00e5n salen var uppr\u00f6rd \u00f6ver Aftonbladets n\u00e4rvaro. Detta trots att media brukar bevaka m\u00f6tena och att ledam\u00f6terna i utskottet ofta tar tillf\u00e4lle att ge intervjuer efter\u00e5t.  \u2013 F\u00f6r det f\u00f6rsta, vill ni prata med mig s\u00e5 g\u00e5r ni till pressavdelningen. Jag uppskattar inte skjutj\u00e4rnsjournalistik, det \u00e4r ett oseri\u00f6st s\u00e4tt att jobba. Tv\u00e5, jag har inget mer att till\u00e4gga \u00e4n det jag lagt ut p\u00e5 plattformen X. D\u00e4r framg\u00e5r det tydligt vad det h\u00e4r handlar om. Tre, ett tips i all v\u00e4nlighet, ni kan ju prata med Riazat sj\u00e4lv, om hans of\u00f6rsk\u00e4mdheter och aggressiva beteende, om varf\u00f6r han inte vill ta politiska motst\u00e5ndare och kvinnor i hand. Nu t\u00e4nker jag g\u00e5 och \u00e4ta lunch, s\u00e4ger Jomshof.  Busch: Jag \u00e4r ganska osugen Daniel Riazat kallade ig\u00e5r Richard Jomshofs uttalande f\u00f6r rasistiskt och uppmanar statsminister Ulf Kristersson (M) att ta avst\u00e5nd. Aftonbladet har s\u00f6kt Kristersson, hans pressekreterare ber att f\u00e5 \u00e5terkomma om statsministern har m\u00f6jlighet att uttala sig. Vice statsminister Ebba Busch (KD) var f\u00e5ordig n\u00e4r hon fick fr\u00e5gor om det p\u00e5 tisdagen.  \u2013 Jag \u00e4r ganska osugen p\u00e5 att bidra till det rubrikspelet, sa hon i samband med en utfr\u00e5gning i riksdagen.  Vice ordf\u00f6rande i justitieutskottet, Ardalan Shekarabi (S), har tidigare kr\u00e4vt Jomshofs avg\u00e5ng. Han uppmanar f\u00f6retr\u00e4dare f\u00f6r regeringen att sluta ge Jomshof st\u00f6d.  \u2013 Tyv\u00e4rr \u00e4r det ett konsekvent beteende han har. Han verkar f\u00f6r splittring, mots\u00e4ttningar och i vissa fall hat mot folkgrupper. Han anv\u00e4nder den plattform som ordf\u00f6rande i justitieutskottet medf\u00f6r till att bedriva den typen av agitation, s\u00e4ger han.  Aftonbladet har s\u00f6kt Sverigedemokraternas pressavdelning. De ber om att f\u00e5 fr\u00e5gorna till Richard Jomshof p\u00e5 mejl och att f\u00e5 \u00e5terkomma senare. Aftonbladet har s\u00f6kt Daniel Riazat. V\u00e4nsterpartiets pressavdelning ber att f\u00e5 \u00e5terkomma. \",\n  \"target_text\": \"SD-toppen Richard Jomshof v\u00e4grar kommentera kritiken f\u00f6r sitt p\u00e5st\u00e5ende att V\u00e4nsterpartiets riksdagsledamot Daniel Riazat borde l\u00e4mna Sverige. M\u00e5nga inom den politiska oppositionen kallar uttalandet rasistiskt N\u00e4r Jomshof konfronteras med fr\u00e5gor fr\u00e5n Aftonbladet vid ett utskottsm\u00f6te i riksdagen, blir han uppr\u00f6rd och g\u00e5r iv\u00e4g utan att svara p\u00e5 fr\u00e5gorna. Han h\u00e4nvisar till SD:s pressavdelning.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Fredrik Bolanders uttalande i \u201dRobinson\u201d f\u00e5r kritik: \u201dSkriver att jag \u00e4r en mansgris\u201d: Kvinnor \u00e4r bra p\u00e5 att st\u00e4da, laga mat och h\u00e5lla ordning.  Killar vill \u00e4ta mat, \u00e4r starkare och b\u00e4ttre. Fredrik Bolanders uttalande i \u201dRobinson\u201d har f\u00e5tt m\u00e5nga att reagera. \u2013 Jag vet att folk st\u00f6r sig p\u00e5 s\u00e5dana uttalanden, det \u00e4r ju ett s\u00e5dan samh\u00e4lle vi lever vi, s\u00e4ger han. \u2013 Om jag hade f\u00e5tt best\u00e4mma hade det varit en kvinna i laget f\u00f6r de \u00e4r ju bra p\u00e5 att laga mat, de \u00e4r bra p\u00e5 att h\u00e5lla ordning och st\u00e4da. D\u00e4r har vi det negativa med att inte ha en kvinna i laget. Vi m\u00e4n vill ju \u00e4ta s\u00e5klart. Uttalandet fr\u00e5n \u201dRobinson\u201d-deltagaren Fredrik Bolander, 40, har f\u00e5tt m\u00e5nga att reagera, bland annat p\u00e5 \u201dRobinsons\u201d sociala medier.  \u00c4ndringen i \u201dRobinson\u201d 2024 I \u00e5rets s\u00e4song delas kvinnor och m\u00e4n upp i olika lag.  N\u00e4r programledaren Anders Lundin, 65, fr\u00e5gar Bolander om han tror att det ger kvinnorna en st\u00f6rre chans att vinna i \u00e5r f\u00e5r han ett snabbt svar.  \u2013 Nej, det blir en kille som vinner i \u00e5r. Killar \u00e4r ofta lite starkare och b\u00e4ttre \u00e4n tjejer. Flera deltagare reagerar p\u00e5 uttalandet i programmet. Tjejerna protesterar h\u00f6gljutt och Gustav Jacobson, 27, g\u00f6r en f\u00f6rskr\u00e4ckt min.  Bolander s\u00e4ger \u00e4ven i programmet att han inte g\u00e5r s\u00e5 bra ihop med kvinnor och feminister. \u2013 Jag \u00e4r v\u00e4ldigt manlig i mig sj\u00e4lv, och jag har en v\u00e4ldigt manlig jargong, och tycker att det ska vara j\u00e4mlikt men man ska ocks\u00e5 f\u00f6rst\u00e5 vem som \u00e4r mannen i huset. \u201dSkriver att jag \u00e4r en mansgris\u201d N\u00e4r Aftonbladet pratar med Bolander samma dag som \u201dRobinson\u201d har premi\u00e4r ber\u00e4ttar han att han redan f\u00e5tt reaktioner och meddelanden fr\u00e5n tittare.  \u2013 De skriver att jag \u00e4r en mansgris och att jag har fel kvinnosyn. Samtidigt \u00e4r han medveten om att det han s\u00e4ger om kvinnor triggar folk.  \u2013 Jag \u00e4lskar att provocera. Det \u00e4r klart att jag gillar att se reaktioner, det vill jag ju, s\u00e4ger Bolander.  Han forts\u00e4tter:  \u2013 Jag vet att folk st\u00f6r sig p\u00e5 s\u00e5dana uttalanden, det \u00e4r ju ett s\u00e5dan samh\u00e4lle vi lever vi. S\u00e5 det var roligt att k\u00f6ra lite tv\u00e4rtom t\u00e4nkte jag. Fredrik Bolander om reaktionerna Just uttalandet om att det beh\u00f6vs en kvinna f\u00f6r att st\u00e4da och laga mat i killarnas lag \u00e4r det han f\u00e5tt mest reaktioner p\u00e5.  \u2013 M\u00e5nga som skrivit \u00e4r ju inte j\u00e4tteglada. Vad skriver folk? \u2013 Att vi lever i 2024 och man ska inte vara s\u00e5 och alla ska vara lika och allt det d\u00e4r. Men samtidigt s\u00e5, man g\u00f6r ju det man \u00e4r bra p\u00e5? Men m\u00e4n kan v\u00e4l ocks\u00e5 vara bra p\u00e5 att laga mat och st\u00e4da? \u2013 Jo men vi har ju mycket annat att g\u00f6ra? Som att tr\u00e4na med stenar? \u2013 Exakt. Pumpa muskler och tr\u00e4na, vi m\u00e5ste t\u00e4nka p\u00e5 hur vi ser ut, vi m\u00e5ste se solbr\u00e4nda ut och det tar tid. Det h\u00e4r \u00e4r ju ett uttalande som uppr\u00f6r m\u00e5nga. K\u00e4nner du att du kan st\u00e5 f\u00f6r det uttalandet? \u2013 Det d\u00e4r \u00e4r en sv\u00e5r fr\u00e5ga. Jag s\u00e4ger s\u00e5 h\u00e4r; man f\u00e5r se lite under programmets g\u00e5ng om det \u00e4r n\u00e5got jag st\u00e5r f\u00f6r eller inte. S\u00e5 kan jag s\u00e4ga. M\u00e5nga undrar ocks\u00e5 om du \u00e4r seri\u00f6s eller skojar? \u2013 Det \u00e4r det som \u00e4r fr\u00e5gan, skojar jag eller \u00e4r jag seri\u00f6s? Det svarar jag inte p\u00e5. Varf\u00f6r inte? \u2013 Antingen kanske jag st\u00e5r f\u00f6r det senare eller s\u00e5 g\u00f6r jag inte det. Det f\u00e5r ni se. \u201dRobinson\u201d s\u00e4nds s\u00f6ndagar klockan 21.00 samt m\u00e5ndag till torsdag klockan 19.30 p\u00e5 TV4 och p\u00e5 TV4 play. \",\n  \"target_text\": \"\\\"Robinson\\\"-deltagaren Fredrik Bolander har hamnat i bl\u00e5sv\u00e4der efter sina uttalanden om kvinnor och m\u00e4n, och f\u00e5r kritik p\u00e5 sociala medier. Han p\u00e5st\u00e5r att kvinnor \u00e4r bra p\u00e5 att laga mat och st\u00e4dning medan m\u00e4n \u00e4r starkare och b\u00e4ttre, och detta uppr\u00f6rde andra deltagare och tittare. Bolander s\u00e4ger att han \u00e4lskar att provocera, men v\u00e4grar svara p\u00e5 fr\u00e5gan om han sk\u00e4mtar eller \u00e4r seri\u00f6s.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Polisen om den \u00f6vergivna diplomatbilen: \u201dVi unders\u00f6ker immunitetsfr\u00e5gan\u201d: En diplomatbil l\u00e4mnades \u00f6vergiven p\u00e5 ett t\u00e5gsp\u00e5r i centrala Stockholm i helgen. Fordonet tillh\u00f6r Etiopiens ambassad som har bett om urs\u00e4kt f\u00f6r vansinnesf\u00e4rden. Men n\u00e4r Aftonbladet knackar p\u00e5 \u00e4r de f\u00e5ordiga.  \u2013 Vi \u00e5terkommer s\u00e5 fort det g\u00e5r, s\u00e4ger en anst\u00e4lld p\u00e5 ambassaden. Det var natten till s\u00f6ndag som minibussen krockade p\u00e5 tv\u00e4rbanans sp\u00e5r vid Alviks strand i Stockholm. \u201dV\u00e5r ambassad ber om urs\u00e4kt f\u00f6r olyckan och besv\u00e4ren den orsakat. Vi har startat en internutredning f\u00f6r att ta reda p\u00e5 hur olyckan ska ha skett\u201d, skriver Etiopiens ambassad i Stockholm i ett mail till Aftonbladet. I \u00f6vrigt har de inte kommenterat h\u00e4ndelsen och n\u00e4r Aftonbladet knackar p\u00e5 hos ambassaden \u00e4r svaret kort. \u2013 Vi h\u00e5ller p\u00e5 att jobba med det. Vi \u00e5terkommer s\u00e5 fort det g\u00e5r, s\u00e4ger en anst\u00e4lld p\u00e5 ambassaden. Men n\u00e4r vill de inte svara p\u00e5. 17 300 kronor i obetalda b\u00f6ter T\u00e5gtrafiken var tillf\u00e4lligt avst\u00e4ngd under s\u00f6ndagsmorgonen och bilen fick b\u00e4rgas med hj\u00e4lp av en sp\u00e5rtraktor. Den har troligtvis k\u00f6rt upp p\u00e5 sp\u00e5ret vid Gr\u00f6ndal, enligt SL. D\u00e4r k\u00f6r bilar och sp\u00e5rvagnar p\u00e5 gatan innan r\u00e4lsen viker av p\u00e5 en egen banvall. \u2013 D\u00e4refter ska den i s\u00e5 fall ha k\u00f6rt tv\u00e5 kilometer p\u00e5 kross och makadam innan den krockat med en stolpe, s\u00e4ger Claes Keisu, pressansvarig p\u00e5 SL. Minibussen har ocks\u00e5 obetalda b\u00f6ter p\u00e5 17\u00a0300 kronor, enligt Transportstyrelsen.  \u201dHar skett en g\u00e5ng tidigare\u201d Den h\u00e4r typen av felk\u00f6rning sker cirka tio g\u00e5nger om \u00e5ret. Under februari skedde det tv\u00e5 g\u00e5nger, just vid Gr\u00f6ndal. Vanligtvis uppt\u00e4cks misstaget tidigt och d\u00e5 brukar f\u00f6raren kunna backa tillbaka p\u00e5 v\u00e4gen. \u2013 Det h\u00e4r fordonet har lite h\u00f6gre markfrig\u00e5ng s\u00e5 det kan f\u00f6rklara att den kunnat ta sig l\u00e4ngre, s\u00e4ger Claes Keisu. Men att bilen lyckats ta sig s\u00e5 l\u00e5ngt \u00e4r v\u00e4ldigt ovanligt. \u2013 Vad vi vet har det bara skett en g\u00e5ng tidigare. 2012 var det en \u00c5l\u00e4nning med sin familj som kom upp p\u00e5 banan i Hammarby sj\u00f6stad och k\u00f6rde hela v\u00e4gen till Gullmarsplan, s\u00e4ger Keisu. F\u00f6raren ska d\u00e5 ha k\u00f6rt uppemot en kilometer p\u00e5 sp\u00e5ret. \u201dVi unders\u00f6ker immunitetsfr\u00e5gan\u201d Polisen har inlett en f\u00f6runders\u00f6kning om v\u00e5rdsl\u00f6shet i trafik. Det \u00e4r fortfarande oklart om n\u00e5gon kan \u00e5talas.  \u2013 Vi unders\u00f6ker immunitetsfr\u00e5gan, s\u00e4ger Nadya Norton, presstalesperson vid Stockholmspolisen. \u201dUtredningen f\u00e5r visa om personen som k\u00f6rde bilen hade immunitet eller inte. Om en person har immunitet kan denne inte lagf\u00f6ras i Sverige\u201d, skriver f\u00f6runders\u00f6kningsledaren, Timmy Malmgren, i ett mail till Aftonbladet. Diplomater f\u00e5r inte straffas i landet de arbetar i, enligt internationella \u00f6verrenskommelser. \u2013 Jag har inga uppgifter om n\u00e5gon \u00e4r misst\u00e4nkt i \u00e4rendet, s\u00e4ger Nadya Norton. Hade fest under kv\u00e4llen Kv\u00e4llen innan bilen hittades p\u00e5 t\u00e5gsp\u00e5ret ska Ambassaden anordnat en fest i sina lokaler. \u201dVi p\u00e5 Ambassaden f\u00f6r Demokratiska f\u00f6rbundsrepubliken Etiopien p\u00e5 v\u00e5ning 3 kommer att ha ett event p\u00e5 l\u00f6rdag den 2. Observera att vi kommer ha g\u00e4ster. Vi hoppas att vi inte st\u00f6r er, k\u00e4ra grannar. Tack f\u00f6r er f\u00f6rst\u00e5else\u201d, skriver de p\u00e5 en lapp som sitter i fastighetens hiss.\",\n  \"target_text\": \"En bil fr\u00e5n Etiopiens ambassad l\u00e4mnades \u00f6vergiven p\u00e5 ett t\u00e5gsp\u00e5r i centrala Stockholm under helgen, vilket ledde till tillf\u00e4lligt avst\u00e4ngd t\u00e5gtrafik. Ambassaden har bett om urs\u00e4kt och p\u00e5b\u00f6rjat en intern utredning f\u00f6r att ta reda p\u00e5 h\u00e4ndelsef\u00f6rloppet. En polisutredning \u00e4r ig\u00e5ng f\u00f6r v\u00e5rdsl\u00f6shet i trafik, men det \u00e4r oklart om n\u00e5gon kan \u00e5talas p\u00e5 grund av diplomatisk immunitet.\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>Nedan f\u00f6ljer artiklar med tillh\u00f6rande sammanfattningar.\n</code></pre></li> <li>Base prompt template:   <pre><code>Artikel: {text}\nSammanfattning: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Artikel: {text}\n\nSkriv en sammanfattning av artikeln ovan.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset schibsted-sv\n</code></pre>"},{"location":"extras/radial_plotter/","title":"Radial Plot","text":"<p>This tool can be used to compare individual models across each task in the benchmark, using a so-called radial plot (also known as a spider plot). You can access the underlying Hugging Face Space here.</p> <p> </p> <p></p>"},{"location":"leaderboards/","title":"Leaderboards","text":"<p>\ud83d\udc48 Choose a leaderboard on the left to see the results.</p>"},{"location":"leaderboards/#types-of-leaderboards","title":"\ud83c\udff7\ufe0f Types of Leaderboards","text":"<p>Each language has two leaderboards:</p> <ul> <li>Generative Leaderboard: This leaderboard shows the performance of models that can   generate text. These models have been evaluated on all tasks, both NLU and   NLG.</li> <li>NLU Leaderboard: This leaderboard shows the performance of models that can only   understand text, and not generate text themselves. These models have been evaluated on   the NLU tasks only.</li> </ul>"},{"location":"leaderboards/#how-to-read-the-leaderboards","title":"\ud83d\udcca How to Read the Leaderboards","text":"<p>The main score column is the <code>Rank</code>, showing the mean rank score of the model across all the tasks in the leaderboard. The lower the rank, the better the model.</p> <p>The columns that follow the rank columns are metadata about the model:</p> <ul> <li><code>Parameters</code>: The total number of parameters in the model, in millions.</li> <li><code>Vocabulary</code>: The size of the model's vocabulary, in thousands.</li> <li><code>Context</code>: The maximum number of tokens that the model can process at a time.</li> <li><code>Speed</code>: The inference time of the model - see more here.</li> <li><code>Type</code>: The type of model:<ul> <li>\ud83d\udd0d indicates that it is an encoder model (e.g., BERT)</li> <li>\ud83e\udde0 indicates that it is a base generative model (e.g., GPT-2)</li> <li>\ud83d\udcdd indicates that it is an instruction-tuned model (e.g., ChatGPT)</li> <li>\ud83e\udd14 indicates that it is a reasoning model (e.g., o1)</li> </ul> </li> <li><code>Commercial</code>: Whether the model can be used for commercial purposes. See here   for more information.</li> <li><code>Merge</code>: Whether the model is a merge of other models.</li> </ul> <p>After these metadata columns, the individual scores for each dataset is shown. Each dataset has a primary and secondary score - see what these are on the task page. Lastly, the final columns show the EuroEval version used to benchmark the given model on each of the datasets.</p> <p>To read more about the individual datasets, see the datasets page. Uf you're interested in the methodology behind the benchmark, see the methodology page.</p> Generative Scatter PlotNLU Scatter Plot"},{"location":"leaderboards/Monolingual/danish/","title":"\ud83c\udde9\ud83c\uddf0 Danish","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU LeaderboardGenerative Scatter PlotNLU Scatter Plot"},{"location":"leaderboards/Monolingual/dutch/","title":"\ud83c\uddf3\ud83c\uddf1 Dutch","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU LeaderboardGenerative Scatter PlotNLU Scatter Plot"},{"location":"leaderboards/Monolingual/english/","title":"\ud83c\uddec\ud83c\udde7 English","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU LeaderboardGenerative Scatter PlotNLU Scatter Plot"},{"location":"leaderboards/Monolingual/faroese/","title":"\ud83c\uddeb\ud83c\uddf4 Faroese","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU LeaderboardNLU Scatter Plot <p>   A generative leaderboard for Faroese is not available yet, as the necessary datasets   are not yet available. </p>"},{"location":"leaderboards/Monolingual/finnish/","title":"\ud83c\uddeb\ud83c\uddee Finnish","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU LeaderboardGenerative Scatter PlotNLU Scatter Plot"},{"location":"leaderboards/Monolingual/french/","title":"\ud83c\uddeb\ud83c\uddf7 French","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU LeaderboardGenerative Scatter PlotNLU Scatter Plot"},{"location":"leaderboards/Monolingual/german/","title":"\ud83c\udde9\ud83c\uddea German","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU LeaderboardGenerative Scatter PlotNLU Scatter Plot"},{"location":"leaderboards/Monolingual/icelandic/","title":"\ud83c\uddee\ud83c\uddf8 Icelandic","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU LeaderboardGenerative Scatter PlotNLU Scatter Plot"},{"location":"leaderboards/Monolingual/italian/","title":"\ud83c\uddee\ud83c\uddf9 Italian","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU LeaderboardGenerative Scatter PlotNLU Scatter Plot"},{"location":"leaderboards/Monolingual/norwegian/","title":"\ud83c\uddf3\ud83c\uddf4 Norwegian","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU LeaderboardGenerative Scatter PlotNLU Scatter Plot"},{"location":"leaderboards/Monolingual/spanish/","title":"\ud83c\uddea\ud83c\uddf8 Spanish","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU LeaderboardGenerative Scatter PlotNLU Scatter Plot"},{"location":"leaderboards/Monolingual/swedish/","title":"\ud83c\uddf8\ud83c\uddea Swedish","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU LeaderboardGenerative Scatter PlotNLU Scatter Plot"},{"location":"leaderboards/Multilingual/european/","title":"\ud83c\uddea\ud83c\uddfa European","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU LeaderboardGenerative Scatter PlotNLU Scatter Plot"},{"location":"leaderboards/Multilingual/germanic/","title":"\ud83c\udde9\ud83c\uddf0\ud83c\uddf3\ud83c\uddf1\ud83c\uddec\ud83c\udde7\ud83c\uddeb\ud83c\uddf4\ud83c\udde9\ud83c\uddea\ud83c\uddee\ud83c\uddf8\ud83c\uddf3\ud83c\uddf4\ud83c\uddf8\ud83c\uddea Germanic","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU LeaderboardGenerative Scatter PlotNLU Scatter Plot"},{"location":"leaderboards/Multilingual/mainland-scandinavian/","title":"\ud83c\udde9\ud83c\uddf0\ud83c\uddf3\ud83c\uddf4\ud83c\uddf8\ud83c\uddea Mainland Scandinavian","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU LeaderboardGenerative Scatter PlotNLU Scatter Plot"},{"location":"leaderboards/Multilingual/romance/","title":"\ud83c\uddeb\ud83c\uddf7\ud83c\uddee\ud83c\uddf9\ud83c\uddea\ud83c\uddf8 Romance","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU LeaderboardGenerative Scatter PlotNLU Scatter Plot"},{"location":"tasks/","title":"Tasks","text":"<p>\ud83d\udc48 Choose a task on the left to see detailed information about that task.</p>"},{"location":"tasks/#overview","title":"\ud83d\udcda Overview","text":"<p>This page covers all the evaluation tasks used in EuroEval. These tasks fall under two categories, corresponding to whether the models should merely understand the input documents (NLU), or rather they are also required to generate new text (NLG).</p>"},{"location":"tasks/#nlu-tasks","title":"NLU Tasks","text":"<p>NLU tasks are tasks where the model is required to understand the natural language input and provide an output based on this understanding. The outputs are typically very short, often just a single label or a couple of words. The performance on these tasks is thus relevant to you if you primarily aim to use the language models for processing documents rather than generating entirely new documents. Both encoder and decoder models can be evaluated on these tasks, enabling you to compare the performance across all language models out there. The tasks in this category are:</p> <ol> <li>Sentiment Classification</li> <li>Named Entity Recognition</li> <li>Linguistic Acceptability</li> <li>Reading Comprehension</li> </ol>"},{"location":"tasks/#nlg-tasks","title":"NLG Tasks","text":"<p>NLG tasks are tasks where the model is required to generate natural language output based on some input. The outputs are typically longer than in NLU tasks, often multiple paragraphs. The performance on these tasks is thus relevant to you if you aim to use the language models for generating new documents. Only decoder models can be evaluated on these tasks, as encoder models do not have the capability to generate text. The tasks in this category are:</p> <ol> <li>Summarization</li> <li>Knowledge \uff0a</li> <li>Common-sense Reasoning \uff0a</li> </ol> <p>\uff0a These tasks should be considered as NLU tasks, but currently encoder models have not been set up to be evaluated on them. This will be added in a future version of EuroEval - see the progress in this issue.</p>"},{"location":"tasks/common-sense-reasoning/","title":"Common-sense Reasoning","text":""},{"location":"tasks/common-sense-reasoning/#overview","title":"\ud83d\udcda Overview","text":"<p>Common-sense reasoning is testing whether a model is able to understand basic deduction about the world. For instance, if the model is given the statement \"It is raining outside, and Peter is in his garden without an umbrella\", it should be able to deduce that Peter is getting wet. The task is set up as a multiple-choice question answering task, where the model is given a question and a set of possible answers, and it has to choose the correct answer.</p> <p>When evaluating generative models, we allow the model to generate 5 tokens on this task.</p>"},{"location":"tasks/common-sense-reasoning/#metrics","title":"\ud83d\udcca Metrics","text":"<p>The primary metric we use when evaluating the performance of a model on the common-sense reasoning task, we use Matthews correlation coefficient (MCC), which has a value between -100% and +100%, where 0% reflects a random guess. The primary benefit of MCC is that it is balanced even if the classes are imbalanced.</p> <p>We also report the accuracy score, as this is the most common metric used for this task, enabling comparisons with other benchmarks.</p>"},{"location":"tasks/common-sense-reasoning/#how-to-run","title":"\ud83d\udee0\ufe0f How to run","text":"<p>In the command line interface of the EuroEval Python package, you can benchmark your favorite model on the common-sense reasoning task like so:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --task common-sense-reasoning\n</code></pre>"},{"location":"tasks/knowledge/","title":"Knowledge","text":""},{"location":"tasks/knowledge/#overview","title":"\ud83d\udcda Overview","text":"<p>The knowledge task is testing how much factual knowledge a model has. The task is set up as a multiple-choice question answering task, where the model is given a question and a set of possible answers, and it has to choose the correct answer. Crucially, it is not given any context in which the answer appears, so it has to answer purely based on its knowledge of the world.</p> <p>When evaluating generative models, we allow the model to generate 5 tokens on this task.</p>"},{"location":"tasks/knowledge/#metrics","title":"\ud83d\udcca Metrics","text":"<p>The primary metric we use when evaluating the performance of a model on the knowledge task, we use Matthews correlation coefficient (MCC), which has a value between -100% and +100%, where 0% reflects a random guess. The primary benefit of MCC is that it is balanced even if the classes are imbalanced.</p> <p>We also report the accuracy score, as this is the most common metric used for this task, enabling comparisons with other benchmarks.</p>"},{"location":"tasks/knowledge/#how-to-run","title":"\ud83d\udee0\ufe0f How to run","text":"<p>In the command line interface of the EuroEval Python package, you can benchmark your favorite model on the knowledge task like so:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --task knowledge\n</code></pre>"},{"location":"tasks/linguistic-acceptability/","title":"Linguistic Acceptability","text":""},{"location":"tasks/linguistic-acceptability/#overview","title":"\ud83d\udcda Overview","text":"<p>Linguistic acceptability is a task of determining whether a given text is grammatically correct or not. It thus tests whether the model is able to understand the detailed syntax of a given document, and not just understand the overall gist of it. It roughly corresponds to when a native speaker would say \"this sentence sounds weird\".</p> <p>When evaluating generative models, we allow the model to generate 5 tokens on this task.</p>"},{"location":"tasks/linguistic-acceptability/#metrics","title":"\ud83d\udcca Metrics","text":"<p>The primary metric we use when evaluating the performance of a model on the linguistic acceptability task, we use Matthews correlation coefficient (MCC), which has a value between -100% and +100%, where 0% reflects a random guess. The primary benefit of MCC is that it is balanced even if the classes are imbalanced.</p> <p>We also report the macro-average F1-score, being the average of the F1-score for each class, thus again weighing each class equally.</p>"},{"location":"tasks/linguistic-acceptability/#how-to-run","title":"\ud83d\udee0\ufe0f How to run","text":"<p>In the command line interface of the EuroEval Python package, you can benchmark your favorite model on the linguistic acceptability task like so:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --task linguistic-acceptability\n</code></pre>"},{"location":"tasks/named-entity-recognition/","title":"Named Entity Recognition","text":""},{"location":"tasks/named-entity-recognition/#overview","title":"\ud83d\udcda Overview","text":"<p>Named entity recognition is a task of determining the named entities in a given text, such as named of persons, organisations, or locations. It thus both tests the knowledge the model has about these things, as well as being able to extract multiple pieces of information from a document at once.</p> <p>When evaluating generative models, we allow the model to generate 128 tokens on this task.</p>"},{"location":"tasks/named-entity-recognition/#metrics","title":"\ud83d\udcca Metrics","text":"<p>The primary metric we use when evaluating the performance of a model on the named entity recognition task, we use the micro-average F1-score without MISC, computed as the total number of true positives for all (non-trivial) entities except <code>MISC</code>, divided by the total number of predicted positives for all entities except <code>MISC</code>.</p> <p>We also report the micro-average F1-score, computed the same way, but where we include the <code>MISC</code> entity as well. This is useful for comparing with other benchmarks, as it is the most common metric used for this task. We find that excluding <code>MISC</code> gives a more accurate picture of the model's performance, however, as the the <code>MISC</code> entity is not well-defined and varies across datasets.</p>"},{"location":"tasks/named-entity-recognition/#how-to-run","title":"\ud83d\udee0\ufe0f How to run","text":"<p>In the command line interface of the EuroEval Python package, you can benchmark your favorite model on the named entity recognition task like so:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --task named-entity-recognition\n</code></pre>"},{"location":"tasks/reading-comprehension/","title":"Reading Comprenhension","text":""},{"location":"tasks/reading-comprehension/#overview","title":"\ud83d\udcda Overview","text":"<p>Reading comprehension is a task of determining whether a model is able to understand a given text and answer questions about it. The model receives a text passage and a question about the text, and it has to provide the answer as it is stated in the text. This is very related to Retrieval-augmented Generation (RAG) applications, where a generative model is used to answer a question based on one or more retrieved documents.</p> <p>When evaluating generative models, we allow the model to generate 32 tokens on this task.</p>"},{"location":"tasks/reading-comprehension/#metrics","title":"\ud83d\udcca Metrics","text":"<p>The primary metric we use when evaluating the performance of a model on the reading comprehension task is the exact match (EM) score, which is the percentage of questions for which the model provides the exact answer.</p> <p>We also report the F1-score on a character-basis, which is more lenient than the EM score, as it allows for small differences in the answer.</p>"},{"location":"tasks/reading-comprehension/#how-to-run","title":"\ud83d\udee0\ufe0f How to run","text":"<p>In the command line interface of the EuroEval Python package, you can benchmark your favorite model on the reading comprehension task like so:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --task reading-comprehension\n</code></pre>"},{"location":"tasks/sentiment-classification/","title":"Sentiment Classification","text":""},{"location":"tasks/sentiment-classification/#overview","title":"\ud83d\udcda Overview","text":"<p>Sentiment classification is a classical task of determining the sentiment of a given text, which can be positive, negative, or neutral. It thus tests whether the model is able to understand the overall semantics of a given document.</p> <p>When evaluating generative models, we allow the model to generate 5 tokens on this task.</p>"},{"location":"tasks/sentiment-classification/#metrics","title":"\ud83d\udcca Metrics","text":"<p>The primary metric we use when evaluating the performance of a model on the sentiment classification task, we use Matthews correlation coefficient (MCC), which has a value between -100% and +100%, where 0% reflects a random guess. The primary benefit of MCC is that it is balanced even if the classes are imbalanced.</p> <p>We also report the macro-average F1-score, being the average of the F1-score for each class, thus again weighing each class equally.</p>"},{"location":"tasks/sentiment-classification/#how-to-run","title":"\ud83d\udee0\ufe0f How to run","text":"<p>In the command line interface of the EuroEval Python package, you can benchmark your favorite model on the sentiment classification task like so:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --task sentiment-classification\n</code></pre>"},{"location":"tasks/speed/","title":"Speed","text":""},{"location":"tasks/speed/#overview","title":"\ud83d\udcda Overview","text":"<p>Speed is a task of measuring how quickly a model can process a given input. The model receives text passages of varying lengths, and it has to process the documents as quickly as possible, which includes tokenisation of the input. We let the model process the input repeatedly for 3 seconds, and then we measure how quick it was. We use the <code>pyinfer</code> package to perform the speed measurement.</p> <p>The speed is of course very dependent on available hardware, and for APIs it also fluctuates depending on the number of requests in the queue, so the speed benchmark should be taken as only a rough estimate of the model's speed, rather than an exact measurement.</p>"},{"location":"tasks/speed/#metrics","title":"\ud83d\udcca Metrics","text":"<p>The primary metric used to evaluate the performance of a model on the speed task is the average number of GPT-2 tokens processed per second on GPUs, when the model is processing documents with roughly 100, 200, ..., 1,000 tokens. If the model is only accessible through an API then the speed is measured on the API. The GPUs used here vary, depending on the size of the model - we preferably use an NVIDIA RTX 3090 Ti GPU, if the model has less than ~8B parameters, and one or more NVIDIA A100 GPUs is larger.</p> <p>The secondary metric is the same, but where the documents are shorter, with roughly 12.5, 15, ..., 125 tokens.</p>"},{"location":"tasks/speed/#how-to-run","title":"\ud83d\udee0\ufe0f How to run","text":"<p>In the command line interface of the EuroEval Python package, you can benchmark your favorite model on the speed task like so:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --task speed\n</code></pre>"},{"location":"tasks/summarization/","title":"Summarization","text":""},{"location":"tasks/summarization/#overview","title":"\ud83d\udcda Overview","text":"<p>Summarization is a task of generating a shorter version of a given text, while preserving the main points of the original text. The model receives a long text and has to generate a shorter version of it, typically a handful of sentences long. This is abstractive summarization, meaning that the summary typically do not appear verbatim in the original text, but that the model has to generate new text based on the input.</p> <p>When evaluating generative models, we allow the model to generate 256 tokens on this task.</p>"},{"location":"tasks/summarization/#metrics","title":"\ud83d\udcca Metrics","text":"<p>The primary metric used to evaluate the performance of a model on the summarization task is the BERTScore, which uses a pretrained encoder model to encode each token in both the reference summary and the generated summary, and then uses cosine similarity to measure how the tokens match up. Using an encoder model allows for the model to phrase a summary differently than the reference, while still being rewarded for capturing the same meaning. We use the <code>microsoft/mdeberta-v3-base</code> encoder model for all languages, as it is the best performing encoder model consistently across all languages in the framework.</p> <p>We also report the ROUGE-L score, which measures the longest sequence of words that the generated summary and the reference summary have in common. This is a more traditional metric for summarization, which is why we report it as well, but it correlates less well with human judgments than BERTScore.</p>"},{"location":"tasks/summarization/#how-to-run","title":"\ud83d\udee0\ufe0f How to run","text":"<p>In the command line interface of the EuroEval Python package, you can benchmark your favorite model on the summarization task like so:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --task summarization\n</code></pre>"},{"location":"api/euroeval/","title":"euroeval","text":"euroeval<p> source package euroeval </p> <p>EuroEval - A benchmarking framework for language models.</p> <p> Classes </p> <ul> <li> <p>Benchmarker \u2014 Benchmarking all the language models.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>block_terminal_output \u2014 Blocks libraries from writing output to the terminal.</p> </li> </ul> <p> source class Benchmarker(progress_bar: bool = True, save_results: bool = True, task: str | list[str] | None = None, dataset: list[str] | str | None = None, language: str | list[str] = 'all', model_language: str | list[str] | None = None, dataset_language: str | list[str] | None = None, device: Device | None = None, batch_size: int = 32, raise_errors: bool = False, cache_dir: str = '.euroeval_cache', api_key: str | None = None, force: bool = False, verbose: bool = False, trust_remote_code: bool = False, clear_model_cache: bool = False, evaluate_test_split: bool = False, few_shot: bool = True, num_iterations: int = 10, api_base: str | None = None, api_version: str | None = None, debug: bool = False, run_with_cli: bool = False, only_allow_safetensors: bool = False) </p> <p>Benchmarking all the language models.</p> <p>Initialise the benchmarker.</p> <p> Attributes </p> <ul> <li> <p>benchmark_config_default_params \u2014 The default parameters for the benchmark configuration.</p> </li> <li> <p>benchmark_config \u2014 The benchmark configuration.</p> </li> <li> <p>force \u2014 Whether to force evaluations of models, even if they have been benchmarked already.</p> </li> <li> <p>results_path \u2014 The path to the results file.</p> </li> <li> <p>benchmark_results :  list[BenchmarkResult] \u2014 The benchmark results.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>progress_bar :  bool \u2014 Whether progress bars should be shown. Defaults to True.</p> </li> <li> <p>save_results :  bool \u2014 Whether to save the benchmark results to 'euroeval_benchmark_results.jsonl'. Defaults to True.</p> </li> <li> <p>task :  str | list[str] | None \u2014 The tasks benchmark the model(s) on. Mutually exclusive with <code>dataset</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked.</p> </li> <li> <p>dataset :  list[str] | str | None \u2014 The datasets to benchmark on. Mutually exclusive with <code>task</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked.</p> </li> <li> <p>language :  str | list[str] \u2014 The language codes of the languages to include, both for models and datasets. Set this to 'all' if all languages should be considered. Defaults to \"all\".</p> </li> <li> <p>model_language :  str | list[str] | None \u2014 The language codes of the languages to include for models. If specified then this overrides the <code>language</code> parameter for model languages. Defaults to None.</p> </li> <li> <p>dataset_language :  str | list[str] | None \u2014 The language codes of the languages to include for datasets. If specified then this overrides the <code>language</code> parameter for dataset languages. Defaults to None.</p> </li> <li> <p>device :  Device | None \u2014 The device to use for benchmarking. Defaults to None.</p> </li> <li> <p>batch_size :  int \u2014 The batch size to use. Defaults to 32.</p> </li> <li> <p>raise_errors :  bool \u2014 Whether to raise errors instead of skipping the model evaluation. Defaults to False.</p> </li> <li> <p>cache_dir :  str \u2014 Directory to store cached models. Defaults to '.euroeval_cache'.</p> </li> <li> <p>api_key :  str | None \u2014 The API key to use for a given inference API.</p> </li> <li> <p>force :  bool \u2014 Whether to force evaluations of models, even if they have been benchmarked already. Defaults to False.</p> </li> <li> <p>verbose :  bool \u2014 Whether to output additional output. This is automatically set if <code>debug</code> is True. Defaults to False.</p> </li> <li> <p>trust_remote_code :  bool \u2014 Whether to trust remote code when loading models. Defaults to False.</p> </li> <li> <p>clear_model_cache :  bool \u2014 Whether to clear the model cache after benchmarking each model. Defaults to False.</p> </li> <li> <p>evaluate_test_split :  bool \u2014 Whether to evaluate the test split of the datasets. Defaults to False.</p> </li> <li> <p>few_shot :  bool \u2014 Whether to only evaluate the model using few-shot evaluation. Only relevant if the model is generative. Defaults to True.</p> </li> <li> <p>num_iterations :  int \u2014 The number of times each model should be evaluated. This is only meant to be used for power users, and scores will not be allowed on the leaderboards if this is changed. Defaults to 10.</p> </li> <li> <p>api_base :  str | None \u2014 The base URL for a given inference API. Only relevant if <code>model</code> refers to a model on an inference API. Defaults to None.</p> </li> <li> <p>api_version :  str | None \u2014 The version of the API to use. Defaults to None.</p> </li> <li> <p>debug :  bool \u2014 Whether to output debug information. Defaults to False.</p> </li> <li> <p>run_with_cli :  bool \u2014 Whether the benchmarker is being run from the command-line interface. Defaults to False.</p> </li> <li> <p>only_allow_safetensors :  bool \u2014 Whether to only allow models that use the safetensors format. Defaults to False.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014 If both <code>task</code> and <code>dataset</code> are specified.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>benchmark \u2014 Benchmarks models on datasets.</p> </li> </ul> <p> source property Benchmarker.benchmark_results: list[BenchmarkResult] </p> <p>The benchmark results.</p> <p> source method Benchmarker.benchmark(model: list[str] | str, task: str | list[str] | None = None, dataset: list[str] | str | None = None, progress_bar: bool | None = None, save_results: bool | None = None, language: str | list[str] | None = None, model_language: str | list[str] | None = None, dataset_language: str | list[str] | None = None, device: Device | None = None, batch_size: int | None = None, raise_errors: bool | None = None, cache_dir: str | None = None, api_key: str | None = None, force: bool | None = None, verbose: bool | None = None, trust_remote_code: bool | None = None, clear_model_cache: bool | None = None, evaluate_test_split: bool | None = None, few_shot: bool | None = None, num_iterations: int | None = None, only_allow_safetensors: bool | None = None) \u2192 list[BenchmarkResult] </p> <p>Benchmarks models on datasets.</p> <p> Parameters </p> <ul> <li> <p>model :  list[str] | str \u2014 The full Hugging Face Hub path(s) to the pretrained transformer model. The specific model version to use can be added after the suffix '@': \"model@v1.0.0\". It can be a branch name, a tag name, or a commit id, and defaults to the latest version if not specified.</p> </li> <li> <p>task :  str | list[str] | None \u2014 The tasks benchmark the model(s) on. Mutually exclusive with <code>dataset</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked. Defaults to None.</p> </li> <li> <p>dataset :  list[str] | str | None \u2014 The datasets to benchmark on. Mutually exclusive with <code>task</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked. Defaults to None.</p> </li> <li> <p>progress_bar :  bool | None \u2014 Whether progress bars should be shown. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>save_results :  bool | None \u2014 Whether to save the benchmark results to 'euroeval_benchmark_results.jsonl'. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>language :  str | list[str] | None \u2014 The language codes of the languages to include, both for models and datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this to 'all' if all languages should be considered. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>model_language :  str | list[str] | None \u2014 The language codes of the languages to include for models. If specified then this overrides the <code>language</code> parameter for model languages. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>dataset_language :  str | list[str] | None \u2014 The language codes of the languages to include for datasets. If specified then this overrides the <code>language</code> parameter for dataset languages. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>device :  Device | None \u2014 The device to use for benchmarking. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>batch_size :  int | None \u2014 The batch size to use. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>raise_errors :  bool | None \u2014 Whether to raise errors instead of skipping the model evaluation.</p> </li> <li> <p>cache_dir :  str | None \u2014 Directory to store cached models. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>api_key :  str | None \u2014 The API key to use for a given inference server. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>force :  bool | None \u2014 Whether to force evaluations of models, even if they have been benchmarked already. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>verbose :  bool | None \u2014 Whether to output additional output. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>trust_remote_code :  bool | None \u2014 Whether to trust remote code when loading models. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>clear_model_cache :  bool | None \u2014 Whether to clear the model cache after benchmarking each model. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>evaluate_test_split :  bool | None \u2014 Whether to evaluate the test split of the datasets. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>few_shot :  bool | None \u2014 Whether to only evaluate the model using few-shot evaluation. Only relevant if the model is generative. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>num_iterations :  int | None \u2014 The number of times each model should be evaluated. This is only meant to be used for power users, and scores will not be allowed on the leaderboards if this is changed. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>only_allow_safetensors :  bool | None \u2014 Whether to only allow models that use the safetensors format. Defaults to the value specified when initialising the benchmarker.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[BenchmarkResult] \u2014 A list of benchmark results.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014 If both <code>task</code> and <code>dataset</code> are specified.</p> </li> <li> <p>benchmark_output_or_err</p> </li> <li> <p>e</p> </li> </ul> <p> source block_terminal_output() \u2192 None </p> <p>Blocks libraries from writing output to the terminal.</p> <p>This filters warnings from some libraries, sets the logging level to ERROR for some libraries, disabled tokeniser progress bars when using Hugging Face tokenisers, and disables most of the logging from the <code>transformers</code> library.</p>"},{"location":"api/euroeval/benchmark_modules/","title":"euroeval.benchmark_modules","text":"euroeval.benchmark_modules<p> source package euroeval.benchmark_modules </p> <p>The different types of modules that can be benchmarked.</p> <p> Classes </p> <ul> <li> <p>BenchmarkModule \u2014 Abstract class for a benchmark module.</p> </li> <li> <p>FreshEncoderModel \u2014 A freshly initialised encoder model.</p> </li> <li> <p>HuggingFaceEncoderModel \u2014 An encoder model from the Hugging Face Hub.</p> </li> <li> <p>LiteLLMModel \u2014 A generative model from LiteLLM.</p> </li> <li> <p>VLLMModel \u2014 A generative model using the vLLM inference framework.</p> </li> </ul> <p> source class BenchmarkModule(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) </p> <p>Bases : ABC</p> <p>Abstract class for a benchmark module.</p> <p>Initialise the benchmark module.</p> <p> Attributes </p> <ul> <li> <p>model_config \u2014 The model configuration.</p> </li> <li> <p>dataset_config \u2014 The dataset configuration.</p> </li> <li> <p>benchmark_config \u2014 The benchmark configuration.</p> </li> <li> <p>buffer :  dict[str, t.Any] \u2014 A buffer to store temporary data.</p> </li> <li> <p>num_params :  int \u2014 The number of parameters in the model.</p> </li> <li> <p>generative_type :  GenerativeType | None \u2014 Get the generative type of the model.</p> </li> <li> <p>vocab_size :  int \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length :  int \u2014 The maximum length of the model.</p> </li> <li> <p>data_collator :  c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>compute_metrics :  ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> <li> <p>extract_labels_from_generation :  ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class :  t.Type['Trainer'] \u2014 The Trainer class to use for finetuning.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014 The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>get_pytorch_module \u2014 Get the underlying PyTorch module.</p> </li> <li> <p>get_tokenizer \u2014 Get the underlying tokenizer.</p> </li> <li> <p>prepare_datasets \u2014 Prepare the datasets for the model.</p> </li> <li> <p>prepare_dataset \u2014 Prepare the dataset for the model.</p> </li> <li> <p>generate \u2014 Generate outputs from the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> </ul> <p> source method BenchmarkModule.get_pytorch_module() \u2192 nn.Module </p> <p>Get the underlying PyTorch module.</p> <p> Returns </p> <ul> <li> <p>nn.Module \u2014 The PyTorch module.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method BenchmarkModule.get_tokenizer() \u2192 PreTrainedTokenizer </p> <p>Get the underlying tokenizer.</p> <p> Returns </p> <ul> <li> <p>PreTrainedTokenizer \u2014 The tokenizer.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source property BenchmarkModule.num_params: int </p> <p>The number of parameters in the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The number of parameters in the model.</p> </li> </ul> <p> source property BenchmarkModule.generative_type: GenerativeType | None </p> <p>Get the generative type of the model.</p> <p> Returns </p> <ul> <li> <p>GenerativeType | None \u2014 The generative type of the model, or None if the model is not generative.</p> </li> </ul> <p> source property BenchmarkModule.vocab_size: int </p> <p>The vocabulary size of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The vocabulary size of the model.</p> </li> </ul> <p> source property BenchmarkModule.model_max_length: int </p> <p>The maximum length of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum length of the model.</p> </li> </ul> <p> source property BenchmarkModule.data_collator: c.Callable[[list[t.Any]], dict[str, t.Any]] </p> <p>The data collator used to prepare samples during finetuning.</p> <p> Returns </p> <ul> <li> <p>c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator.</p> </li> </ul> <p> source property BenchmarkModule.compute_metrics: ComputeMetricsFunction </p> <p>The function used to compute the metrics.</p> <p> Returns </p> <ul> <li> <p>ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> </ul> <p> source property BenchmarkModule.extract_labels_from_generation: ExtractLabelsFunction </p> <p>The function used to extract the labels from the generated output.</p> <p> Returns </p> <ul> <li> <p>ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> </ul> <p> source property BenchmarkModule.trainer_class: t.Type['Trainer'] </p> <p>The Trainer class to use for finetuning.</p> <p> Returns </p> <ul> <li> <p>t.Type['Trainer'] \u2014 The Trainer class.</p> </li> </ul> <p> source method BenchmarkModule.prepare_datasets(datasets: list[DatasetDict], task: Task) \u2192 list[DatasetDict] </p> <p>Prepare the datasets for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>datasets :  list[DatasetDict] \u2014 The datasets to prepare.</p> </li> <li> <p>task :  Task \u2014 The task to prepare the datasets for.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[DatasetDict] \u2014 The prepared datasets.</p> </li> </ul> <p> source method BenchmarkModule.prepare_dataset(dataset: DatasetDict, task: Task, itr_idx: int) \u2192 DatasetDict </p> <p>Prepare the dataset for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  DatasetDict \u2014 The dataset to prepare.</p> </li> <li> <p>task :  Task \u2014 The task to prepare the dataset for.</p> </li> <li> <p>itr_idx :  int \u2014 The index of the dataset in the iterator.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetDict \u2014 The prepared dataset.</p> </li> </ul> <p> source method BenchmarkModule.generate(inputs: dict) \u2192 GenerativeModelOutput </p> <p>Generate outputs from the model.</p> <p> Parameters </p> <ul> <li> <p>inputs :  dict \u2014 A batch of inputs to pass through the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>GenerativeModelOutput \u2014 The generated model outputs.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source classmethod BenchmarkModule.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> source classmethod BenchmarkModule.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> source class FreshEncoderModel(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) </p> <p>Bases : HuggingFaceEncoderModel</p> <p>A freshly initialised encoder model.</p> <p>Initialise the model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014 The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Attributes </p> <ul> <li> <p>num_params :  int \u2014 The number of parameters in the model.</p> </li> <li> <p>generative_type :  GenerativeType | None \u2014 Get the generative type of the model.</p> </li> <li> <p>vocab_size :  int \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length :  int \u2014 The maximum context length of the model.</p> </li> <li> <p>data_collator :  c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>compute_metrics :  ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> <li> <p>extract_labels_from_generation :  ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class :  t.Type['Trainer'] \u2014 The Trainer class to use for finetuning.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> </ul> <p> source property FreshEncoderModel.num_params: int </p> <p>The number of parameters in the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The number of parameters in the model.</p> </li> </ul> <p> source property FreshEncoderModel.vocab_size: int </p> <p>The vocabulary size of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The vocabulary size of the model.</p> </li> </ul> <p> source property FreshEncoderModel.model_max_length: int </p> <p>The maximum context length of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum context length of the model.</p> </li> </ul> <p> source classmethod FreshEncoderModel.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> source classmethod FreshEncoderModel.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> source class HuggingFaceEncoderModel(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) </p> <p>Bases : BenchmarkModule</p> <p>An encoder model from the Hugging Face Hub.</p> <p>Initialise the model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014 The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Attributes </p> <ul> <li> <p>num_params :  int \u2014 The number of parameters in the model.</p> </li> <li> <p>generative_type :  GenerativeType | None \u2014 Get the generative type of the model.</p> </li> <li> <p>vocab_size :  int \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length :  int \u2014 The maximum context length of the model.</p> </li> <li> <p>data_collator :  c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>compute_metrics :  ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> <li> <p>extract_labels_from_generation :  ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class :  t.Type['Trainer'] \u2014 The Trainer class to use for finetuning.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>prepare_dataset \u2014 Prepare the dataset for the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> </ul> <p> source property HuggingFaceEncoderModel.num_params: int </p> <p>The number of parameters in the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The number of parameters in the model.</p> </li> </ul> <p> source property HuggingFaceEncoderModel.vocab_size: int </p> <p>The vocabulary size of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The vocabulary size of the model.</p> </li> </ul> <p> source property HuggingFaceEncoderModel.model_max_length: int </p> <p>The maximum context length of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum context length of the model.</p> </li> </ul> <p> source property HuggingFaceEncoderModel.data_collator: c.Callable[[list[t.Any]], dict[str, t.Any]] </p> <p>The data collator used to prepare samples during finetuning.</p> <p> Returns </p> <ul> <li> <p>c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator.</p> </li> </ul> <p> source property HuggingFaceEncoderModel.generative_type: GenerativeType | None </p> <p>Get the generative type of the model.</p> <p> Returns </p> <ul> <li> <p>GenerativeType | None \u2014 The generative type of the model, or None if it has not been set yet.</p> </li> </ul> <p> source property HuggingFaceEncoderModel.extract_labels_from_generation: ExtractLabelsFunction </p> <p>The function used to extract the labels from the generated output.</p> <p> Returns </p> <ul> <li> <p>ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> </ul> <p> source property HuggingFaceEncoderModel.trainer_class: t.Type['Trainer'] </p> <p>The Trainer class to use for finetuning.</p> <p> Returns </p> <ul> <li> <p>t.Type['Trainer'] \u2014 The Trainer class.</p> </li> </ul> <p> source method HuggingFaceEncoderModel.prepare_dataset(dataset: DatasetDict, task: Task, itr_idx: int) \u2192 DatasetDict </p> <p>Prepare the dataset for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  DatasetDict \u2014 The dataset to prepare.</p> </li> <li> <p>task :  Task \u2014 The task to prepare the dataset for.</p> </li> <li> <p>itr_idx :  int \u2014 The index of the dataset in the iterator.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetDict \u2014 The prepared dataset.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source classmethod HuggingFaceEncoderModel.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> source classmethod HuggingFaceEncoderModel.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source class LiteLLMModel(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) </p> <p>Bases : BenchmarkModule</p> <p>A generative model from LiteLLM.</p> <p>Initialise the model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014 The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Attributes </p> <ul> <li> <p>num_params :  int \u2014 The number of parameters in the model.</p> </li> <li> <p>generative_type :  GenerativeType | None \u2014 Get the generative type of the model.</p> </li> <li> <p>vocab_size :  int \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length :  int \u2014 The maximum length of the model.</p> </li> <li> <p>data_collator :  c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>compute_metrics :  ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> <li> <p>extract_labels_from_generation :  ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class :  t.Type['Trainer'] \u2014 The Trainer class to use for finetuning.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>generate \u2014 Generate outputs from the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> <li> <p>prepare_dataset \u2014 Prepare the dataset for the model.</p> </li> </ul> <p> source property LiteLLMModel.generative_type: GenerativeType | None </p> <p>Get the generative type of the model.</p> <p> Returns </p> <ul> <li> <p>GenerativeType | None \u2014 The generative type of the model, or None if it has not been set yet.</p> </li> </ul> <p> source method LiteLLMModel.generate(inputs: dict) \u2192 GenerativeModelOutput </p> <p>Generate outputs from the model.</p> <p> Parameters </p> <ul> <li> <p>inputs :  dict \u2014 A batch of inputs to pass through the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>GenerativeModelOutput \u2014 The generated model outputs.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source property LiteLLMModel.num_params: int </p> <p>The number of parameters in the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The number of parameters in the model.</p> </li> </ul> <p> source property LiteLLMModel.vocab_size: int </p> <p>The vocabulary size of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The vocabulary size of the model.</p> </li> </ul> <p> source property LiteLLMModel.model_max_length: int </p> <p>The maximum length of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum length of the model.</p> </li> </ul> <p> source property LiteLLMModel.data_collator: c.Callable[[list[t.Any]], dict[str, t.Any]] </p> <p>The data collator used to prepare samples during finetuning.</p> <p> Returns </p> <ul> <li> <p>c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator.</p> </li> </ul> <p> source property LiteLLMModel.extract_labels_from_generation: ExtractLabelsFunction </p> <p>The function used to extract the labels from the generated output.</p> <p> Returns </p> <ul> <li> <p>ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> </ul> <p> source property LiteLLMModel.trainer_class: t.Type['Trainer'] </p> <p>The Trainer class to use for finetuning.</p> <p> Returns </p> <ul> <li> <p>t.Type['Trainer'] \u2014 The Trainer class.</p> </li> </ul> <p> source classmethod LiteLLMModel.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>e</p> </li> </ul> <p> source classmethod LiteLLMModel.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> source method LiteLLMModel.prepare_dataset(dataset: DatasetDict, task: Task, itr_idx: int) \u2192 DatasetDict </p> <p>Prepare the dataset for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  DatasetDict \u2014 The dataset to prepare.</p> </li> <li> <p>task :  Task \u2014 The task to prepare the dataset for.</p> </li> <li> <p>itr_idx :  int \u2014 The index of the dataset in the iterator.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetDict \u2014 The prepared dataset.</p> </li> </ul> <p> source class VLLMModel(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) </p> <p>Bases : HuggingFaceEncoderModel</p> <p>A generative model using the vLLM inference framework.</p> <p>Initialise the vLLM model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014 The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Attributes </p> <ul> <li> <p>num_params :  int \u2014 The number of parameters in the model.</p> </li> <li> <p>generative_type :  GenerativeType | None \u2014 Get the generative type of the model.</p> </li> <li> <p>vocab_size :  int \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length :  int \u2014 The maximum context length of the model.</p> </li> <li> <p>data_collator :  c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>compute_metrics :  ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> <li> <p>extract_labels_from_generation :  ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class :  t.Type['Trainer'] \u2014 The Trainer class to use for finetuning.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>prepare_dataset \u2014 Prepare the dataset for the model.</p> </li> <li> <p>generate \u2014 Generate outputs from the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> </ul> <p> source property VLLMModel.generative_type: GenerativeType | None </p> <p>Get the generative type of the model.</p> <p> Returns </p> <ul> <li> <p>GenerativeType | None \u2014 The generative type of the model, or None if it has not been set yet.</p> </li> </ul> <p> source property VLLMModel.extract_labels_from_generation: ExtractLabelsFunction </p> <p>The function used to extract the labels from the generated output.</p> <p> Returns </p> <ul> <li> <p>ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> </ul> <p> source method VLLMModel.prepare_dataset(dataset: DatasetDict, task: Task, itr_idx: int) \u2192 DatasetDict </p> <p>Prepare the dataset for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  DatasetDict \u2014 The dataset to prepare.</p> </li> <li> <p>task :  Task \u2014 The task to prepare the dataset for.</p> </li> <li> <p>itr_idx :  int \u2014 The index of the dataset in the iterator.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetDict \u2014 The prepared dataset.</p> </li> </ul> <p> source method VLLMModel.generate(inputs: dict) \u2192 GenerativeModelOutput </p> <p>Generate outputs from the model.</p> <p> Parameters </p> <ul> <li> <p>inputs :  dict \u2014 A batch of inputs to pass through the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>GenerativeModelOutput \u2014 The generated model outputs.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source classmethod VLLMModel.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> source classmethod VLLMModel.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source property VLLMModel.data_collator: c.Callable[[list[t.Any]], dict[str, t.Any]] </p> <p>The data collator used to prepare samples during finetuning.</p> <p> Returns </p> <ul> <li> <p>c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator.</p> </li> </ul> <p> source property VLLMModel.trainer_class: t.Type['Trainer'] </p> <p>The Trainer class to use for finetuning.</p> <p> Returns </p> <ul> <li> <p>t.Type['Trainer'] \u2014 The Trainer class.</p> </li> </ul>"},{"location":"api/euroeval/benchmark_modules/base/","title":"euroeval.benchmark_modules.base","text":"euroeval.benchmark_modules.base<p> source module euroeval.benchmark_modules.base </p> <p>Abstract benchmark module class that the model classes inherit from.</p> <p> Classes </p> <ul> <li> <p>BenchmarkModule \u2014 Abstract class for a benchmark module.</p> </li> </ul> <p> source class BenchmarkModule(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) </p> <p>Bases : ABC</p> <p>Abstract class for a benchmark module.</p> <p>Initialise the benchmark module.</p> <p> Attributes </p> <ul> <li> <p>model_config \u2014 The model configuration.</p> </li> <li> <p>dataset_config \u2014 The dataset configuration.</p> </li> <li> <p>benchmark_config \u2014 The benchmark configuration.</p> </li> <li> <p>buffer :  dict[str, t.Any] \u2014 A buffer to store temporary data.</p> </li> <li> <p>num_params :  int \u2014 The number of parameters in the model.</p> </li> <li> <p>generative_type :  GenerativeType | None \u2014 Get the generative type of the model.</p> </li> <li> <p>vocab_size :  int \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length :  int \u2014 The maximum length of the model.</p> </li> <li> <p>data_collator :  c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>compute_metrics :  ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> <li> <p>extract_labels_from_generation :  ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class :  t.Type['Trainer'] \u2014 The Trainer class to use for finetuning.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014 The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>get_pytorch_module \u2014 Get the underlying PyTorch module.</p> </li> <li> <p>get_tokenizer \u2014 Get the underlying tokenizer.</p> </li> <li> <p>prepare_datasets \u2014 Prepare the datasets for the model.</p> </li> <li> <p>prepare_dataset \u2014 Prepare the dataset for the model.</p> </li> <li> <p>generate \u2014 Generate outputs from the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> </ul> <p> source method BenchmarkModule.get_pytorch_module() \u2192 nn.Module </p> <p>Get the underlying PyTorch module.</p> <p> Returns </p> <ul> <li> <p>nn.Module \u2014 The PyTorch module.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method BenchmarkModule.get_tokenizer() \u2192 PreTrainedTokenizer </p> <p>Get the underlying tokenizer.</p> <p> Returns </p> <ul> <li> <p>PreTrainedTokenizer \u2014 The tokenizer.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source property BenchmarkModule.num_params: int </p> <p>The number of parameters in the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The number of parameters in the model.</p> </li> </ul> <p> source property BenchmarkModule.generative_type: GenerativeType | None </p> <p>Get the generative type of the model.</p> <p> Returns </p> <ul> <li> <p>GenerativeType | None \u2014 The generative type of the model, or None if the model is not generative.</p> </li> </ul> <p> source property BenchmarkModule.vocab_size: int </p> <p>The vocabulary size of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The vocabulary size of the model.</p> </li> </ul> <p> source property BenchmarkModule.model_max_length: int </p> <p>The maximum length of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum length of the model.</p> </li> </ul> <p> source property BenchmarkModule.data_collator: c.Callable[[list[t.Any]], dict[str, t.Any]] </p> <p>The data collator used to prepare samples during finetuning.</p> <p> Returns </p> <ul> <li> <p>c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator.</p> </li> </ul> <p> source property BenchmarkModule.compute_metrics: ComputeMetricsFunction </p> <p>The function used to compute the metrics.</p> <p> Returns </p> <ul> <li> <p>ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> </ul> <p> source property BenchmarkModule.extract_labels_from_generation: ExtractLabelsFunction </p> <p>The function used to extract the labels from the generated output.</p> <p> Returns </p> <ul> <li> <p>ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> </ul> <p> source property BenchmarkModule.trainer_class: t.Type['Trainer'] </p> <p>The Trainer class to use for finetuning.</p> <p> Returns </p> <ul> <li> <p>t.Type['Trainer'] \u2014 The Trainer class.</p> </li> </ul> <p> source method BenchmarkModule.prepare_datasets(datasets: list[DatasetDict], task: Task) \u2192 list[DatasetDict] </p> <p>Prepare the datasets for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>datasets :  list[DatasetDict] \u2014 The datasets to prepare.</p> </li> <li> <p>task :  Task \u2014 The task to prepare the datasets for.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[DatasetDict] \u2014 The prepared datasets.</p> </li> </ul> <p> source method BenchmarkModule.prepare_dataset(dataset: DatasetDict, task: Task, itr_idx: int) \u2192 DatasetDict </p> <p>Prepare the dataset for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  DatasetDict \u2014 The dataset to prepare.</p> </li> <li> <p>task :  Task \u2014 The task to prepare the dataset for.</p> </li> <li> <p>itr_idx :  int \u2014 The index of the dataset in the iterator.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetDict \u2014 The prepared dataset.</p> </li> </ul> <p> source method BenchmarkModule.generate(inputs: dict) \u2192 GenerativeModelOutput </p> <p>Generate outputs from the model.</p> <p> Parameters </p> <ul> <li> <p>inputs :  dict \u2014 A batch of inputs to pass through the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>GenerativeModelOutput \u2014 The generated model outputs.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source classmethod BenchmarkModule.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> source classmethod BenchmarkModule.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul>"},{"location":"api/euroeval/benchmark_modules/fresh/","title":"euroeval.benchmark_modules.fresh","text":"euroeval.benchmark_modules.fresh<p> source module euroeval.benchmark_modules.fresh </p> <p>Freshly initialised encoder models.</p> <p> Classes </p> <ul> <li> <p>FreshEncoderModel \u2014 A freshly initialised encoder model.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>load_model_and_tokenizer \u2014 Load the model and tokenizer.</p> </li> </ul> <p> source class FreshEncoderModel(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) </p> <p>Bases : HuggingFaceEncoderModel</p> <p>A freshly initialised encoder model.</p> <p>Initialise the model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014 The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Attributes </p> <ul> <li> <p>num_params :  int \u2014 The number of parameters in the model.</p> </li> <li> <p>generative_type :  GenerativeType | None \u2014 Get the generative type of the model.</p> </li> <li> <p>vocab_size :  int \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length :  int \u2014 The maximum context length of the model.</p> </li> <li> <p>data_collator :  c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>compute_metrics :  ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> <li> <p>extract_labels_from_generation :  ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class :  t.Type['Trainer'] \u2014 The Trainer class to use for finetuning.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> </ul> <p> source property FreshEncoderModel.num_params: int </p> <p>The number of parameters in the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The number of parameters in the model.</p> </li> </ul> <p> source property FreshEncoderModel.vocab_size: int </p> <p>The vocabulary size of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The vocabulary size of the model.</p> </li> </ul> <p> source property FreshEncoderModel.model_max_length: int </p> <p>The maximum context length of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum context length of the model.</p> </li> </ul> <p> source classmethod FreshEncoderModel.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> source classmethod FreshEncoderModel.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> source load_model_and_tokenizer(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig, model_max_length: int) \u2192 tuple[PreTrainedModel, PreTrainedTokenizer] </p> <p>Load the model and tokenizer.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014 The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> <li> <p>model_max_length :  int \u2014 The maximum context length of the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[PreTrainedModel, PreTrainedTokenizer] \u2014 The loaded model and tokenizer.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> <li> <p>InvalidModel</p> </li> </ul>"},{"location":"api/euroeval/benchmark_modules/hf/","title":"euroeval.benchmark_modules.hf","text":"euroeval.benchmark_modules.hf<p> source module euroeval.benchmark_modules.hf </p> <p>Encoder models from the Hugging Face Hub.</p> <p> Classes </p> <ul> <li> <p>HuggingFaceEncoderModel \u2014 An encoder model from the Hugging Face Hub.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>load_model_and_tokenizer \u2014 Load the model and tokenizer.</p> </li> <li> <p>get_model_repo_info \u2014 Get the information about the model from the HF Hub or a local directory.</p> </li> <li> <p>load_tokenizer \u2014 Load the tokenizer.</p> </li> <li> <p>get_torch_dtype \u2014 Get the torch dtype, used for loading the model.</p> </li> <li> <p>load_hf_model_config \u2014 Load the Hugging Face model configuration.</p> </li> <li> <p>setup_model_for_question_answering \u2014 Setup a model for question answering.</p> </li> <li> <p>get_children_of_module \u2014 Get the children of a module.</p> </li> <li> <p>align_model_and_tokenizer \u2014 Aligns the model and the tokenizer.</p> </li> <li> <p>task_group_to_class_name \u2014 Convert a task group to a class name.</p> </li> </ul> <p> source class HuggingFaceEncoderModel(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) </p> <p>Bases : BenchmarkModule</p> <p>An encoder model from the Hugging Face Hub.</p> <p>Initialise the model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014 The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Attributes </p> <ul> <li> <p>num_params :  int \u2014 The number of parameters in the model.</p> </li> <li> <p>generative_type :  GenerativeType | None \u2014 Get the generative type of the model.</p> </li> <li> <p>vocab_size :  int \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length :  int \u2014 The maximum context length of the model.</p> </li> <li> <p>data_collator :  c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>compute_metrics :  ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> <li> <p>extract_labels_from_generation :  ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class :  t.Type['Trainer'] \u2014 The Trainer class to use for finetuning.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>prepare_dataset \u2014 Prepare the dataset for the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> </ul> <p> source property HuggingFaceEncoderModel.num_params: int </p> <p>The number of parameters in the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The number of parameters in the model.</p> </li> </ul> <p> source property HuggingFaceEncoderModel.vocab_size: int </p> <p>The vocabulary size of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The vocabulary size of the model.</p> </li> </ul> <p> source property HuggingFaceEncoderModel.model_max_length: int </p> <p>The maximum context length of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum context length of the model.</p> </li> </ul> <p> source property HuggingFaceEncoderModel.data_collator: c.Callable[[list[t.Any]], dict[str, t.Any]] </p> <p>The data collator used to prepare samples during finetuning.</p> <p> Returns </p> <ul> <li> <p>c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator.</p> </li> </ul> <p> source property HuggingFaceEncoderModel.generative_type: GenerativeType | None </p> <p>Get the generative type of the model.</p> <p> Returns </p> <ul> <li> <p>GenerativeType | None \u2014 The generative type of the model, or None if it has not been set yet.</p> </li> </ul> <p> source property HuggingFaceEncoderModel.extract_labels_from_generation: ExtractLabelsFunction </p> <p>The function used to extract the labels from the generated output.</p> <p> Returns </p> <ul> <li> <p>ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> </ul> <p> source property HuggingFaceEncoderModel.trainer_class: t.Type['Trainer'] </p> <p>The Trainer class to use for finetuning.</p> <p> Returns </p> <ul> <li> <p>t.Type['Trainer'] \u2014 The Trainer class.</p> </li> </ul> <p> source method HuggingFaceEncoderModel.prepare_dataset(dataset: DatasetDict, task: Task, itr_idx: int) \u2192 DatasetDict </p> <p>Prepare the dataset for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  DatasetDict \u2014 The dataset to prepare.</p> </li> <li> <p>task :  Task \u2014 The task to prepare the dataset for.</p> </li> <li> <p>itr_idx :  int \u2014 The index of the dataset in the iterator.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetDict \u2014 The prepared dataset.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source classmethod HuggingFaceEncoderModel.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> source classmethod HuggingFaceEncoderModel.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source load_model_and_tokenizer(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 tuple['PreTrainedModel', 'PreTrainedTokenizer'] </p> <p>Load the model and tokenizer.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014 The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple['PreTrainedModel', 'PreTrainedTokenizer'] \u2014 The loaded model and tokenizer.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> <li> <p>InvalidModel</p> </li> </ul> <p> source get_model_repo_info(model_id: str, revision: str, benchmark_config: BenchmarkConfig) \u2192 HFModelInfo | None </p> <p>Get the information about the model from the HF Hub or a local directory.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The model ID.</p> </li> <li> <p>revision :  str \u2014 The revision of the model.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>HFModelInfo | None \u2014 The information about the model, or None if the model could not be found.</p> </li> </ul> <p> source load_tokenizer(model: PreTrainedModel | None, model_id: str, trust_remote_code: bool) \u2192 PreTrainedTokenizer </p> <p>Load the tokenizer.</p> <p> Parameters </p> <ul> <li> <p>model :  PreTrainedModel | None \u2014 The model, which is used to determine whether to add a prefix space to the tokens. Can be None.</p> </li> <li> <p>model_id :  str \u2014 The model identifier. Used for logging.</p> </li> <li> <p>trust_remote_code :  bool \u2014 Whether to trust remote code.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>PreTrainedTokenizer \u2014 The loaded tokenizer.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source get_torch_dtype(device: torch.device, torch_dtype_is_set: bool, bf16_available: bool) \u2192 str | torch.dtype </p> <p>Get the torch dtype, used for loading the model.</p> <p> Parameters </p> <ul> <li> <p>device :  torch.device \u2014 The device to use.</p> </li> <li> <p>torch_dtype_is_set :  bool \u2014 Whether the torch data type is set in the model configuration.</p> </li> <li> <p>bf16_available :  bool \u2014 Whether bfloat16 is available.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str | torch.dtype \u2014 The torch dtype.</p> </li> </ul> <p> source load_hf_model_config(model_id: str, num_labels: int, id2label: dict[int, str], label2id: dict[str, int], revision: str, model_cache_dir: str | None, api_key: str | None, trust_remote_code: bool, run_with_cli: bool) \u2192 PretrainedConfig </p> <p>Load the Hugging Face model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The Hugging Face model ID.</p> </li> <li> <p>num_labels :  int \u2014 The number of labels in the dataset.</p> </li> <li> <p>id2label :  dict[int, str] \u2014 The mapping from label IDs to labels.</p> </li> <li> <p>label2id :  dict[str, int] \u2014 The mapping from labels to label IDs.</p> </li> <li> <p>revision :  str \u2014 The revision of the model.</p> </li> <li> <p>model_cache_dir :  str | None \u2014 The directory to cache the model in.</p> </li> <li> <p>api_key :  str | None \u2014 The Hugging Face API key.</p> </li> <li> <p>trust_remote_code :  bool \u2014 Whether to trust remote code.</p> </li> <li> <p>run_with_cli :  bool \u2014 Whether the script is being run with the CLI.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>PretrainedConfig \u2014 The Hugging Face model configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> <li> <p>NeedsAdditionalArgument</p> </li> </ul> <p> source setup_model_for_question_answering(model: PreTrainedModel) \u2192 PreTrainedModel </p> <p>Setup a model for question answering.</p> <p> Parameters </p> <ul> <li> <p>model :  PreTrainedModel \u2014 The model to setup.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>PreTrainedModel \u2014 The setup model.</p> </li> </ul> <p> source get_children_of_module(name: str, module: nn.Module) \u2192 nn.Module | dict[str, t.Any] | None </p> <p>Get the children of a module.</p> <p> Parameters </p> <ul> <li> <p>name :  str \u2014 The name of the module.</p> </li> <li> <p>module :  nn.Module \u2014 The module to get the children of.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>nn.Module | dict[str, t.Any] | None \u2014 The children of the module, or None if the module has no children.</p> </li> </ul> <p> source align_model_and_tokenizer(model: PreTrainedModel, tokenizer: PreTrainedTokenizer, model_max_length: int, raise_errors: bool = False) \u2192 tuple['PreTrainedModel', 'PreTrainedTokenizer'] </p> <p>Aligns the model and the tokenizer.</p> <p> Parameters </p> <ul> <li> <p>model :  PreTrainedModel \u2014 The model to fix.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer \u2014 The tokenizer to fix.</p> </li> <li> <p>model_max_length :  int \u2014 The maximum length of the model.</p> </li> <li> <p>raise_errors :  bool \u2014 Whether to raise errors instead of trying to fix them silently.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple['PreTrainedModel', 'PreTrainedTokenizer'] \u2014 The fixed model and tokenizer.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> <li> <p>e</p> </li> </ul> <p> source task_group_to_class_name(task_group: TaskGroup) \u2192 str </p> <p>Convert a task group to a class name.</p> <p> Parameters </p> <ul> <li> <p>task_group :  TaskGroup \u2014 The task group.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The class name.</p> </li> </ul>"},{"location":"api/euroeval/benchmark_modules/litellm/","title":"euroeval.benchmark_modules.litellm","text":"euroeval.benchmark_modules.litellm<p> source module euroeval.benchmark_modules.litellm </p> <p>Generative models from an inference API, using the LiteLLM framework.</p> <p> Classes </p> <ul> <li> <p>LiteLLMModel \u2014 A generative model from LiteLLM.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>raise_if_wrong_params \u2014 Raise an error if the model configuration has invalid parameters.</p> </li> <li> <p>try_download_ollama_model \u2014 Try to download an Ollama model.</p> </li> </ul> <p> source class LiteLLMModel(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) </p> <p>Bases : BenchmarkModule</p> <p>A generative model from LiteLLM.</p> <p>Initialise the model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014 The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Attributes </p> <ul> <li> <p>num_params :  int \u2014 The number of parameters in the model.</p> </li> <li> <p>generative_type :  GenerativeType | None \u2014 Get the generative type of the model.</p> </li> <li> <p>vocab_size :  int \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length :  int \u2014 The maximum length of the model.</p> </li> <li> <p>data_collator :  c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>compute_metrics :  ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> <li> <p>extract_labels_from_generation :  ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class :  t.Type['Trainer'] \u2014 The Trainer class to use for finetuning.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>generate \u2014 Generate outputs from the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> <li> <p>prepare_dataset \u2014 Prepare the dataset for the model.</p> </li> </ul> <p> source property LiteLLMModel.generative_type: GenerativeType | None </p> <p>Get the generative type of the model.</p> <p> Returns </p> <ul> <li> <p>GenerativeType | None \u2014 The generative type of the model, or None if it has not been set yet.</p> </li> </ul> <p> source method LiteLLMModel.generate(inputs: dict) \u2192 GenerativeModelOutput </p> <p>Generate outputs from the model.</p> <p> Parameters </p> <ul> <li> <p>inputs :  dict \u2014 A batch of inputs to pass through the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>GenerativeModelOutput \u2014 The generated model outputs.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source property LiteLLMModel.num_params: int </p> <p>The number of parameters in the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The number of parameters in the model.</p> </li> </ul> <p> source property LiteLLMModel.vocab_size: int </p> <p>The vocabulary size of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The vocabulary size of the model.</p> </li> </ul> <p> source property LiteLLMModel.model_max_length: int </p> <p>The maximum length of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum length of the model.</p> </li> </ul> <p> source property LiteLLMModel.data_collator: c.Callable[[list[t.Any]], dict[str, t.Any]] </p> <p>The data collator used to prepare samples during finetuning.</p> <p> Returns </p> <ul> <li> <p>c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator.</p> </li> </ul> <p> source property LiteLLMModel.extract_labels_from_generation: ExtractLabelsFunction </p> <p>The function used to extract the labels from the generated output.</p> <p> Returns </p> <ul> <li> <p>ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> </ul> <p> source property LiteLLMModel.trainer_class: t.Type['Trainer'] </p> <p>The Trainer class to use for finetuning.</p> <p> Returns </p> <ul> <li> <p>t.Type['Trainer'] \u2014 The Trainer class.</p> </li> </ul> <p> source classmethod LiteLLMModel.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>e</p> </li> </ul> <p> source classmethod LiteLLMModel.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> source method LiteLLMModel.prepare_dataset(dataset: DatasetDict, task: Task, itr_idx: int) \u2192 DatasetDict </p> <p>Prepare the dataset for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  DatasetDict \u2014 The dataset to prepare.</p> </li> <li> <p>task :  Task \u2014 The task to prepare the dataset for.</p> </li> <li> <p>itr_idx :  int \u2014 The index of the dataset in the iterator.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetDict \u2014 The prepared dataset.</p> </li> </ul> <p> source raise_if_wrong_params(model_config: ModelConfig, allowed_params: dict[str, list[str]]) \u2192 None </p> <p>Raise an error if the model configuration has invalid parameters.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014 The model configuration.</p> </li> <li> <p>allowed_params :  dict[str, list[str]] \u2014 The allowed parameters for the model.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel \u2014 If the model configuration has invalid parameters.</p> </li> </ul> <p> source try_download_ollama_model(model_id: str) \u2192 bool </p> <p>Try to download an Ollama model.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The model ID. If the model does not start with \"ollama/\" or \"ollama_chat/\" then this function will return False.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool \u2014 Whether the model was downloaded successfully.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel \u2014 If Ollama is not running or the model cannot be downloaded.</p> </li> </ul>"},{"location":"api/euroeval/benchmark_modules/vllm/","title":"euroeval.benchmark_modules.vllm","text":"euroeval.benchmark_modules.vllm<p> source module euroeval.benchmark_modules.vllm </p> <p>Generative models using the vLLM inference framework.</p> <p> Classes </p> <ul> <li> <p>VLLMModel \u2014 A generative model using the vLLM inference framework.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>load_model_and_tokenizer \u2014 Load the model and tokenizer.</p> </li> <li> <p>load_tokenizer \u2014 Load the tokenizer.</p> </li> <li> <p>clear_vllm \u2014 Clear the GPU memory used by the vLLM model, enabling re-initialisation.</p> </li> <li> <p>get_end_of_reasoning_token \u2014 Get the end-of-reasoning token for a generative model.</p> </li> <li> <p>get_custom_stop_tokens \u2014 Get the stop tokens for a generative model.</p> </li> <li> <p>get_pbar_without_leave \u2014 Get a progress bar for vLLM which disappears after completion.</p> </li> </ul> <p> source class VLLMModel(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) </p> <p>Bases : HuggingFaceEncoderModel</p> <p>A generative model using the vLLM inference framework.</p> <p>Initialise the vLLM model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014 The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Attributes </p> <ul> <li> <p>num_params :  int \u2014 The number of parameters in the model.</p> </li> <li> <p>generative_type :  GenerativeType | None \u2014 Get the generative type of the model.</p> </li> <li> <p>vocab_size :  int \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length :  int \u2014 The maximum context length of the model.</p> </li> <li> <p>data_collator :  c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>compute_metrics :  ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> <li> <p>extract_labels_from_generation :  ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class :  t.Type['Trainer'] \u2014 The Trainer class to use for finetuning.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>prepare_dataset \u2014 Prepare the dataset for the model.</p> </li> <li> <p>generate \u2014 Generate outputs from the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> </ul> <p> source property VLLMModel.generative_type: GenerativeType | None </p> <p>Get the generative type of the model.</p> <p> Returns </p> <ul> <li> <p>GenerativeType | None \u2014 The generative type of the model, or None if it has not been set yet.</p> </li> </ul> <p> source property VLLMModel.extract_labels_from_generation: ExtractLabelsFunction </p> <p>The function used to extract the labels from the generated output.</p> <p> Returns </p> <ul> <li> <p>ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> </ul> <p> source method VLLMModel.prepare_dataset(dataset: DatasetDict, task: Task, itr_idx: int) \u2192 DatasetDict </p> <p>Prepare the dataset for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  DatasetDict \u2014 The dataset to prepare.</p> </li> <li> <p>task :  Task \u2014 The task to prepare the dataset for.</p> </li> <li> <p>itr_idx :  int \u2014 The index of the dataset in the iterator.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetDict \u2014 The prepared dataset.</p> </li> </ul> <p> source method VLLMModel.generate(inputs: dict) \u2192 GenerativeModelOutput </p> <p>Generate outputs from the model.</p> <p> Parameters </p> <ul> <li> <p>inputs :  dict \u2014 A batch of inputs to pass through the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>GenerativeModelOutput \u2014 The generated model outputs.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source classmethod VLLMModel.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> source classmethod VLLMModel.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source property VLLMModel.data_collator: c.Callable[[list[t.Any]], dict[str, t.Any]] </p> <p>The data collator used to prepare samples during finetuning.</p> <p> Returns </p> <ul> <li> <p>c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator.</p> </li> </ul> <p> source property VLLMModel.trainer_class: t.Type['Trainer'] </p> <p>The Trainer class to use for finetuning.</p> <p> Returns </p> <ul> <li> <p>t.Type['Trainer'] \u2014 The Trainer class.</p> </li> </ul> <p> source load_model_and_tokenizer(model_config: ModelConfig, benchmark_config: BenchmarkConfig) \u2192 tuple['LLM', 'PreTrainedTokenizer'] </p> <p>Load the model and tokenizer.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014 The model configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple['LLM', 'PreTrainedTokenizer'] \u2014 A pair (model, tokenizer), with the loaded model and tokenizer</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NeedsExtraInstalled</p> </li> <li> <p>InvalidModel</p> </li> </ul> <p> source load_tokenizer(model_id: str, revision: str, adapter_base_model_id: str | None, trust_remote_code: bool, model_max_length: int, model_cache_dir: str, token: str | bool) \u2192 PreTrainedTokenizer </p> <p>Load the tokenizer.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The model identifier.</p> </li> <li> <p>revision :  str \u2014 The revision of the model.</p> </li> <li> <p>adapter_base_model_id :  str | None \u2014 The base model ID for the adapter model. Can be None if the model is not an adapter model.</p> </li> <li> <p>trust_remote_code :  bool \u2014 Whether to trust remote code.</p> </li> <li> <p>model_max_length :  int \u2014 The maximum length of the model.</p> </li> <li> <p>model_cache_dir :  str \u2014 The cache directory for the model.</p> </li> <li> <p>token :  str | bool \u2014 The Hugging Face API token.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>PreTrainedTokenizer \u2014 The loaded tokenizer.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source clear_vllm() \u2192 None </p> <p>Clear the GPU memory used by the vLLM model, enabling re-initialisation.</p> <p> source get_end_of_reasoning_token(model: LLM, tokenizer: PreTrainedTokenizer, model_id: str) \u2192 str | None </p> <p>Get the end-of-reasoning token for a generative model.</p> <p> Parameters </p> <ul> <li> <p>model :  LLM \u2014 The vLLM model.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer \u2014 The tokenizer.</p> </li> <li> <p>model_id :  str \u2014 The model ID.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str | None \u2014 The end of reasoning token, or None if it could not be found.</p> </li> </ul> <p> source get_custom_stop_tokens(model: LLM, tokenizer: PreTrainedTokenizer, model_id: str, is_reasoning_model: bool) \u2192 list[str] </p> <p>Get the stop tokens for a generative model.</p> <p> Parameters </p> <ul> <li> <p>model :  LLM \u2014 The vLLM model.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer \u2014 The tokenizer.</p> </li> <li> <p>model_id :  str \u2014 The model ID.</p> </li> <li> <p>is_reasoning_model :  bool \u2014 Whether the model is a reasoning model. This is used to determine the number of generated tokens to allow before stopping the generation.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 A list of stop tokens.</p> </li> </ul> <p> source get_pbar_without_leave(*tqdm_args, **tqdm_kwargs) \u2192 tqdm </p> <p>Get a progress bar for vLLM which disappears after completion.</p> <p> Parameters </p> <ul> <li> <p>*tqdm_args \u2014 Positional arguments to pass to tqdm.</p> </li> <li> <p>**tqdm_kwargs \u2014 Additional keyword arguments to pass to tqdm.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tqdm \u2014 A tqdm progress bar.</p> </li> </ul>"},{"location":"api/euroeval/dataset_configs/","title":"euroeval.dataset_configs","text":"euroeval.dataset_configs<p> source package euroeval.dataset_configs </p> <p>All dataset configurations used in EuroEval.</p> <p> Functions </p> <ul> <li> <p>get_all_dataset_configs \u2014 Get a mapping of all the dataset configurations.</p> </li> <li> <p>get_dataset_config \u2014 Get the dataset configuration for a dataset.</p> </li> </ul> <p> source get_all_dataset_configs() \u2192 dict[str, DatasetConfig] </p> <p>Get a mapping of all the dataset configurations.</p> <p> Returns </p> <ul> <li> <p>dict[str, DatasetConfig] \u2014 A mapping between names of datasets and their configurations.</p> </li> </ul> <p> source get_dataset_config(dataset_name: str) \u2192 DatasetConfig </p> <p>Get the dataset configuration for a dataset.</p> <p> Parameters </p> <ul> <li> <p>dataset_name :  str \u2014 The name of the dataset.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetConfig \u2014 The dataset configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014 If the dataset is not found.</p> </li> </ul>"},{"location":"api/euroeval/dataset_configs/danish/","title":"euroeval.dataset_configs.danish","text":"euroeval.dataset_configs.danish<p> source module euroeval.dataset_configs.danish </p> <p>All Danish dataset configurations used in EuroEval.</p>"},{"location":"api/euroeval/dataset_configs/dutch/","title":"euroeval.dataset_configs.dutch","text":"euroeval.dataset_configs.dutch<p> source module euroeval.dataset_configs.dutch </p> <p>All Dutch dataset configurations used in EuroEval.</p>"},{"location":"api/euroeval/dataset_configs/english/","title":"euroeval.dataset_configs.english","text":"euroeval.dataset_configs.english<p> source module euroeval.dataset_configs.english </p> <p>All English dataset configurations used in EuroEval.</p>"},{"location":"api/euroeval/dataset_configs/faroese/","title":"euroeval.dataset_configs.faroese","text":"euroeval.dataset_configs.faroese<p> source module euroeval.dataset_configs.faroese </p> <p>All Faroese dataset configurations used in EuroEval.</p>"},{"location":"api/euroeval/dataset_configs/finnish/","title":"euroeval.dataset_configs.finnish","text":"euroeval.dataset_configs.finnish<p> source module euroeval.dataset_configs.finnish </p> <p>All Finnish dataset configurations used in EuroEval.</p>"},{"location":"api/euroeval/dataset_configs/french/","title":"euroeval.dataset_configs.french","text":"euroeval.dataset_configs.french<p> source module euroeval.dataset_configs.french </p> <p>All French dataset configurations used in EuroEval.</p>"},{"location":"api/euroeval/dataset_configs/german/","title":"euroeval.dataset_configs.german","text":"euroeval.dataset_configs.german<p> source module euroeval.dataset_configs.german </p> <p>All German dataset configurations used in EuroEval.</p>"},{"location":"api/euroeval/dataset_configs/icelandic/","title":"euroeval.dataset_configs.icelandic","text":"euroeval.dataset_configs.icelandic<p> source module euroeval.dataset_configs.icelandic </p> <p>All Icelandic dataset configurations used in EuroEval.</p>"},{"location":"api/euroeval/dataset_configs/italian/","title":"euroeval.dataset_configs.italian","text":"euroeval.dataset_configs.italian<p> source module euroeval.dataset_configs.italian </p> <p>All Italian dataset configurations used in EuroEval.</p>"},{"location":"api/euroeval/dataset_configs/norwegian/","title":"euroeval.dataset_configs.norwegian","text":"euroeval.dataset_configs.norwegian<p> source module euroeval.dataset_configs.norwegian </p> <p>All Norwegian dataset configurations used in EuroEval.</p>"},{"location":"api/euroeval/dataset_configs/spanish/","title":"euroeval.dataset_configs.spanish","text":"euroeval.dataset_configs.spanish<p> source module euroeval.dataset_configs.spanish </p> <p>All Spanish dataset configurations used in EuroEval.</p>"},{"location":"api/euroeval/dataset_configs/swedish/","title":"euroeval.dataset_configs.swedish","text":"euroeval.dataset_configs.swedish<p> source module euroeval.dataset_configs.swedish </p> <p>All Swedish dataset configurations used in EuroEval.</p>"},{"location":"api/euroeval/prompt_templates/","title":"euroeval.prompt_templates","text":"euroeval.prompt_templates<p> source package euroeval.prompt_templates </p> <p>The different prompt templates used in EuroEval.</p> <p> Modules </p> <ul> <li> <p>euroeval.prompt_templates.linguistic_acceptability \u2014 Templates for the Linguistic Acceptability task.</p> </li> <li> <p>euroeval.prompt_templates.multiple_choice \u2014 Templates for all multiple choice tasks.</p> </li> <li> <p>euroeval.prompt_templates.named_entity_recognition \u2014 Templates for the Named Entity Recognition task.</p> </li> <li> <p>euroeval.prompt_templates.reading_comprehension \u2014 Templates for the Reading Comprehension task.</p> </li> <li> <p>euroeval.prompt_templates.sentiment_classification \u2014 Templates for the Sentiment Analysis task.</p> </li> <li> <p>euroeval.prompt_templates.summarization \u2014 Templates for the Summarization task.</p> </li> </ul>"},{"location":"api/euroeval/prompt_templates/linguistic_acceptability/","title":"euroeval.prompt_templates.linguistic_acceptability","text":"euroeval.prompt_templates.linguistic_acceptability<p> source module euroeval.prompt_templates.linguistic_acceptability </p> <p>Templates for the Linguistic Acceptability task.</p>"},{"location":"api/euroeval/prompt_templates/multiple_choice/","title":"euroeval.prompt_templates.multiple_choice","text":"euroeval.prompt_templates.multiple_choice<p> source module euroeval.prompt_templates.multiple_choice </p> <p>Templates for all multiple choice tasks.</p>"},{"location":"api/euroeval/prompt_templates/named_entity_recognition/","title":"euroeval.prompt_templates.named_entity_recognition","text":"euroeval.prompt_templates.named_entity_recognition<p> source module euroeval.prompt_templates.named_entity_recognition </p> <p>Templates for the Named Entity Recognition task.</p>"},{"location":"api/euroeval/prompt_templates/reading_comprehension/","title":"euroeval.prompt_templates.reading_comprehension","text":"euroeval.prompt_templates.reading_comprehension<p> source module euroeval.prompt_templates.reading_comprehension </p> <p>Templates for the Reading Comprehension task.</p>"},{"location":"api/euroeval/prompt_templates/sentiment_classification/","title":"euroeval.prompt_templates.sentiment_classification","text":"euroeval.prompt_templates.sentiment_classification<p> source module euroeval.prompt_templates.sentiment_classification </p> <p>Templates for the Sentiment Analysis task.</p>"},{"location":"api/euroeval/prompt_templates/summarization/","title":"euroeval.prompt_templates.summarization","text":"euroeval.prompt_templates.summarization<p> source module euroeval.prompt_templates.summarization </p> <p>Templates for the Summarization task.</p>"},{"location":"api/euroeval/task_group_utils/","title":"euroeval.task_group_utils","text":"euroeval.task_group_utils<p> source package euroeval.task_group_utils </p> <p>Utility functions related to the different tasks and task groups.</p> <p> Modules </p> <ul> <li> <p>euroeval.task_group_utils.multiple_choice_classification \u2014 Utility functions related to the multiple-choice classification task group.</p> </li> <li> <p>euroeval.task_group_utils.question_answering \u2014 Utility functions related to the question-answering task group.</p> </li> <li> <p>euroeval.task_group_utils.sequence_classification \u2014 Utility functions related to the sequence-classification task group.</p> </li> <li> <p>euroeval.task_group_utils.text_to_text \u2014 Utility functions related to the text-to-text task group.</p> </li> <li> <p>euroeval.task_group_utils.token_classification \u2014 Utility functions related to the token-classification task group.</p> </li> </ul>"},{"location":"api/euroeval/task_group_utils/multiple_choice_classification/","title":"euroeval.task_group_utils.multiple_choice_classification","text":"euroeval.task_group_utils.multiple_choice_classification<p> source module euroeval.task_group_utils.multiple_choice_classification </p> <p>Utility functions related to the multiple-choice classification task group.</p> <p> Classes </p> <ul> <li> <p>MultipleChoiceClassificationTrainer \u2014 Trainer subclass for multiple-choice classification tasks.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>prepare_examples \u2014 Prepare the features.</p> </li> <li> <p>postprocess_predictions_and_labels \u2014 Postprocess the predictions and labels.</p> </li> </ul> <p> source class MultipleChoiceClassificationTrainer(model: Union[PreTrainedModel, nn.Module, None] = None, args: TrainingArguments = None, data_collator: Optional[DataCollator] = None, train_dataset: Optional[Union[Dataset, IterableDataset, 'datasets.Dataset']] = None, eval_dataset: Optional[Union[Dataset, dict[str, Dataset], 'datasets.Dataset']] = None, processing_class: Optional[Union[PreTrainedTokenizerBase, BaseImageProcessor, FeatureExtractionMixin, ProcessorMixin]] = None, model_init: Optional[Callable[[], PreTrainedModel]] = None, compute_loss_func: Optional[Callable] = None, compute_metrics: Optional[Callable[[EvalPrediction], dict]] = None, callbacks: Optional[list[TrainerCallback]] = None, optimizers: tuple[Optional[torch.optim.Optimizer], Optional[torch.optim.lr_scheduler.LambdaLR]] = (None, None), optimizer_cls_and_kwargs: Optional[tuple[type[torch.optim.Optimizer], dict[str, Any]]] = None, preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None) </p> <p>Bases : Trainer</p> <p>Trainer subclass for multiple-choice classification tasks.</p> <p> Methods </p> <ul> <li> <p>evaluate \u2014 Evaluate the model on the given dataset.</p> </li> </ul> <p> source method MultipleChoiceClassificationTrainer.evaluate(eval_dataset: Dataset | None = None, ignore_keys: list[str] | None = None, metric_key_prefix: str = 'eval') \u2192 dict[str, float] </p> <p>Evaluate the model on the given dataset.</p> <p> Parameters </p> <ul> <li> <p>eval_dataset :  Dataset | None \u2014 The dataset to evaluate on. If None, then use the stored evaluation dataset.</p> </li> <li> <p>ignore_keys :  list[str] | None \u2014 The keys to ignore when computing the metrics.</p> </li> <li> <p>metric_key_prefix :  str \u2014 The prefix to use for the metric keys.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] \u2014 The metrics computed on the evaluation dataset.</p> </li> </ul> <p> source prepare_examples(examples: BatchEncoding, tokenizer: PreTrainedTokenizer) \u2192 BatchEncoding </p> <p>Prepare the features.</p> <p> Parameters </p> <ul> <li> <p>examples :  BatchEncoding \u2014 The examples to prepare.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer \u2014 The tokenizer to use to prepare the examples.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BatchEncoding \u2014 The prepared examples.</p> </li> </ul> <p> source postprocess_predictions_and_labels(predictions: np.ndarray, dataset: Dataset) \u2192 tuple['Predictions', 'Labels'] </p> <p>Postprocess the predictions and labels.</p> <p> Parameters </p> <ul> <li> <p>predictions :  np.ndarray \u2014 The model predictions, of shape (num_examples, 2), corresponding to the False/True probabilities for each example.</p> </li> <li> <p>dataset :  Dataset \u2014 The dataset containing the examples.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple['Predictions', 'Labels'] \u2014 The postprocessed predictions and labels.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul>"},{"location":"api/euroeval/task_group_utils/question_answering/","title":"euroeval.task_group_utils.question_answering","text":"euroeval.task_group_utils.question_answering<p> source module euroeval.task_group_utils.question_answering </p> <p>Utility functions related to the question-answering task group.</p> <p> Classes </p> <ul> <li> <p>QuestionAnsweringTrainer \u2014 Trainer subclass for question answering tasks.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>compute_metrics \u2014 Compute the metrics needed for evaluation.</p> </li> <li> <p>extract_labels_from_generation \u2014 Extract the predicted labels from the generated output.</p> </li> <li> <p>prepare_train_examples \u2014 Prepare the features for training.</p> </li> <li> <p>prepare_test_examples \u2014 Prepare test examples.</p> </li> <li> <p>postprocess_predictions_and_labels \u2014 Postprocess the predictions and labels, to allow easier metric computation.</p> </li> <li> <p>find_best_answer \u2014 Find the best answer for a given example.</p> </li> <li> <p>find_valid_answers \u2014 Find the valid answers from the start and end indexes.</p> </li> </ul> <p> source class QuestionAnsweringTrainer(model: PreTrainedModel | nn.Module, processing_class: PreTrainedTokenizerBase, args: TrainingArguments, train_dataset: Dataset, eval_dataset: Dataset, compute_metrics: c.Callable[[EvalPrediction], dict[str, float]], callbacks: list[TrainerCallback], data_collator: c.Callable, **kwargs) </p> <p>Bases : Trainer</p> <p>Trainer subclass for question answering tasks.</p> <p>Initialise the trainer.</p> <p> Methods </p> <ul> <li> <p>evaluate \u2014 Evaluate the model on the given dataset.</p> </li> </ul> <p> source method QuestionAnsweringTrainer.evaluate(eval_dataset: Dataset | None = None, orig_eval_dataset: Dataset | None = None, ignore_keys: list[str] | None = None, metric_key_prefix: str = 'eval') \u2192 dict[str, float] </p> <p>Evaluate the model on the given dataset.</p> <p> Parameters </p> <ul> <li> <p>eval_dataset :  Dataset | None \u2014 The dataset to evaluate on. If None, then use the stored evaluation dataset.</p> </li> <li> <p>orig_eval_dataset :  Dataset | None \u2014 The original evaluation dataset, before any postprocessing. If None, then use the stored original evaluation dataset.</p> </li> <li> <p>ignore_keys :  list[str] | None \u2014 The keys to ignore when computing the metrics.</p> </li> <li> <p>metric_key_prefix :  str \u2014 The prefix to use for the metric keys.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] \u2014 The metrics computed on the evaluation dataset.</p> </li> </ul> <p> source compute_metrics(model_outputs_and_labels: tuple[Predictions, Labels] | EvalPrediction, dataset_config: DatasetConfig) \u2192 dict[str, float] </p> <p>Compute the metrics needed for evaluation.</p> <p> Parameters </p> <ul> <li> <p>model_outputs_and_labels :  tuple[Predictions, Labels] | EvalPrediction \u2014 The first sequence contains the model outputs and the second sequence contains the true labels.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The configuration of the dataset.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] \u2014 A dictionary with the names of the metrics as keys and the metric values as values.</p> </li> </ul> <p> source extract_labels_from_generation(input_batch: dict[str, list], model_output: GenerativeModelOutput) \u2192 list[t.Any] </p> <p>Extract the predicted labels from the generated output.</p> <p> Parameters </p> <ul> <li> <p>input_batch :  dict[str, list] \u2014 The input batch, where the keys are the feature names and the values are lists with the feature values.</p> </li> <li> <p>model_output :  GenerativeModelOutput \u2014 The raw generated output of the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[t.Any] \u2014 The predicted labels.</p> </li> </ul> <p> source prepare_train_examples(examples: BatchEncoding, tokenizer: PreTrainedTokenizer) \u2192 BatchEncoding </p> <p>Prepare the features for training.</p> <p> Parameters </p> <ul> <li> <p>examples :  BatchEncoding \u2014 The examples to prepare.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer \u2014 The tokenizer to use to prepare the examples.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BatchEncoding \u2014 The prepared examples.</p> </li> </ul> <p> source prepare_test_examples(examples: BatchEncoding, tokenizer: PreTrainedTokenizer) \u2192 BatchEncoding </p> <p>Prepare test examples.</p> <p> Parameters </p> <ul> <li> <p>examples :  BatchEncoding \u2014 Dictionary of test examples.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer \u2014 The tokenizer used to preprocess the examples.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BatchEncoding \u2014 The prepared test examples.</p> </li> </ul> <p> source postprocess_predictions_and_labels(predictions: tuple[np.ndarray, ...], dataset: Dataset, prepared_dataset: Dataset, cls_token_index: int) \u2192 tuple[list[dict], list[dict]] </p> <p>Postprocess the predictions and labels, to allow easier metric computation.</p> <p> Parameters </p> <ul> <li> <p>predictions :  tuple[np.ndarray, ...] \u2014 A tuple whose first two elements are (start_logits, end_logits).</p> </li> <li> <p>dataset :  Dataset \u2014 The dataset containing the examples.</p> </li> <li> <p>prepared_dataset :  Dataset \u2014 The dataset containing the prepared examples.</p> </li> <li> <p>cls_token_index :  int \u2014 The index of the CLS token.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[list[dict], list[dict]] \u2014 The postprocessed predictions and labels.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source find_best_answer(all_start_logits: np.ndarray, all_end_logits: np.ndarray, prepared_dataset: Dataset, feature_indices: list[int], context: str, max_answer_length: int, num_best_logits: int, min_null_score: float, cls_token_index: int) \u2192 str </p> <p>Find the best answer for a given example.</p> <p> Parameters </p> <ul> <li> <p>all_start_logits :  np.ndarray \u2014 The start logits for all the features.</p> </li> <li> <p>all_end_logits :  np.ndarray \u2014 The end logits for all the features.</p> </li> <li> <p>prepared_dataset :  Dataset \u2014 The dataset containing the prepared examples.</p> </li> <li> <p>feature_indices :  list[int] \u2014 The indices of the features associated with the current example.</p> </li> <li> <p>context :  str \u2014 The context of the example.</p> </li> <li> <p>max_answer_length :  int \u2014 The maximum length of the answer.</p> </li> <li> <p>num_best_logits :  int \u2014 The number of best logits to consider.</p> </li> <li> <p>min_null_score :  float \u2014 The minimum score an answer can have.</p> </li> <li> <p>cls_token_index :  int \u2014 The index of the CLS token.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The best answer for the example.</p> </li> </ul> <p> source find_valid_answers(start_logits: np.ndarray, end_logits: np.ndarray, offset_mapping: list[tuple[int, int]], context: str, max_answer_length: int, num_best_logits: int, min_null_score: float) \u2192 list[dict] </p> <p>Find the valid answers from the start and end indexes.</p> <p> Parameters </p> <ul> <li> <p>start_logits :  np.ndarray \u2014 The logits for the start of the answer.</p> </li> <li> <p>end_logits :  np.ndarray \u2014 The logits for the end of the answer.</p> </li> <li> <p>offset_mapping :  list[tuple[int, int]] \u2014 The offset mapping, being a list of pairs of integers for each token index, containing the start and end character index in the original context.</p> </li> <li> <p>context :  str \u2014 The context of the example.</p> </li> <li> <p>max_answer_length :  int \u2014 The maximum length of the answer.</p> </li> <li> <p>num_best_logits :  int \u2014 The number of best logits to consider. Note that this function will run in O(<code>num_best_logits</code> ^ 2) time.</p> </li> <li> <p>min_null_score :  float \u2014 The minimum score an answer can have.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[dict] \u2014 A list of the valid answers, each being a dictionary with keys \"text\" and \"score\", the score being the sum of the start and end logits.</p> </li> </ul>"},{"location":"api/euroeval/task_group_utils/sequence_classification/","title":"euroeval.task_group_utils.sequence_classification","text":"euroeval.task_group_utils.sequence_classification<p> source module euroeval.task_group_utils.sequence_classification </p> <p>Utility functions related to the sequence-classification task group.</p> <p> Functions </p> <ul> <li> <p>compute_metrics \u2014 Compute the metrics needed for evaluation.</p> </li> <li> <p>extract_labels_from_generation \u2014 Extract the predicted labels from the generated output.</p> </li> <li> <p>get_closest_logprobs_labels \u2014 Get the labels with the highest predicted logprob value.</p> </li> </ul> <p> source compute_metrics(model_outputs_and_labels: tuple[Predictions, Labels] | EvalPrediction, dataset_config: DatasetConfig) \u2192 dict[str, float] </p> <p>Compute the metrics needed for evaluation.</p> <p> Parameters </p> <ul> <li> <p>model_outputs_and_labels :  tuple[Predictions, Labels] | EvalPrediction \u2014 The first sequence contains the model outputs and the second sequence contains the true labels.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The configuration of the dataset.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] \u2014 A dictionary with the names of the metrics as keys and the metric values as values.</p> </li> </ul> <p> source extract_labels_from_generation(input_batch: dict[str, list], model_output: GenerativeModelOutput, dataset_config: DatasetConfig, first_label_token_mapping: dict[str, str] | bool) \u2192 list[str] </p> <p>Extract the predicted labels from the generated output.</p> <p> Parameters </p> <ul> <li> <p>input_batch :  dict[str, list] \u2014 The input batch, where the keys are the feature names and the values are lists with the feature values.</p> </li> <li> <p>model_output :  GenerativeModelOutput \u2014 The raw generated output of the model.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The configuration of the dataset.</p> </li> <li> <p>first_label_token_mapping :  dict[str, str] | bool \u2014 A mapping from labels to the first token in each label, or alternatively a Boolean value indicating whether the model should output scores (if the mapping is outputted then the model will always output scores).</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 The predicted labels.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source get_closest_logprobs_labels(generation_logprobs: list[list[list[tuple[str, float]]]], dataset_config: DatasetConfig, first_label_token_mapping: dict[str, str] | t.Literal[True]) \u2192 list[str] | None </p> <p>Get the labels with the highest predicted logprob value.</p> <p>In case a candidate label is split into multiple tokens, we only use the first token to compute the logprob value. E.g., if the candidate label \"positive\" is tokenised as [\"pos\", \"itive\"], we only use the logprob value of \"pos\" to represent the logprob value of the entire label.</p> <p> Parameters </p> <ul> <li> <p>generation_logprobs :  list[list[list[tuple[str, float]]]] \u2014 The logprobs of the generated tokens, for all samples in the batch. Of shape (batch_size, num_tokens, num_logprobs).</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The configuration of the dataset.</p> </li> <li> <p>first_label_token_mapping :  dict[str, str] | t.Literal[True] \u2014 A mapping from labels to the first token in each label, or alternatively a <code>True</code> value indicating that the model should output logprobs.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] | None \u2014 The predicted labels, or None if labels could not be extracted.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark \u2014 If no candidate label can be found for any of the generated labels.</p> </li> </ul>"},{"location":"api/euroeval/task_group_utils/text_to_text/","title":"euroeval.task_group_utils.text_to_text","text":"euroeval.task_group_utils.text_to_text<p> source module euroeval.task_group_utils.text_to_text </p> <p>Utility functions related to the text-to-text task group.</p> <p> Functions </p> <ul> <li> <p>compute_metrics \u2014 Compute the metrics needed for evaluation.</p> </li> <li> <p>extract_labels_from_generation \u2014 Extract the predicted labels from the generated output.</p> </li> </ul> <p> source compute_metrics(model_outputs_and_labels: tuple[Predictions, Labels] | EvalPrediction, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 dict[str, float] </p> <p>Compute the metrics needed for evaluation.</p> <p> Parameters </p> <ul> <li> <p>model_outputs_and_labels :  tuple[Predictions, Labels] | EvalPrediction \u2014 The first sequence contains the model outputs and the second sequence contains the true labels.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The configuration of the dataset.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The configuration of the benchmark.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] \u2014 A dictionary with the names of the metrics as keys and the metric values as values.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source extract_labels_from_generation(input_batch: dict[str, list], model_output: GenerativeModelOutput) \u2192 list[t.Any] </p> <p>Extract the predicted labels from the generated output.</p> <p> Parameters </p> <ul> <li> <p>input_batch :  dict[str, list] \u2014 The input batch, where the keys are the feature names and the values are lists with the feature values.</p> </li> <li> <p>model_output :  GenerativeModelOutput \u2014 The raw generated output of the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[t.Any] \u2014 The predicted labels.</p> </li> </ul>"},{"location":"api/euroeval/task_group_utils/token_classification/","title":"euroeval.task_group_utils.token_classification","text":"euroeval.task_group_utils.token_classification<p> source module euroeval.task_group_utils.token_classification </p> <p>Utility functions related to the token-classification task group.</p> <p> Functions </p> <ul> <li> <p>compute_metrics \u2014 Compute the metrics needed for evaluation.</p> </li> <li> <p>extract_labels_from_generation \u2014 Extract the predicted labels from the generated output.</p> </li> <li> <p>tokenize_and_align_labels \u2014 Tokenise all texts and align the labels with them.</p> </li> <li> <p>handle_unk_tokens \u2014 Replace unknown tokens in the tokens with the corresponding word.</p> </li> </ul> <p> source compute_metrics(model_outputs_and_labels: tuple[Predictions, Labels] | EvalPrediction, has_misc_tags: bool, dataset_config: DatasetConfig) \u2192 dict[str, float] </p> <p>Compute the metrics needed for evaluation.</p> <p> Parameters </p> <ul> <li> <p>model_outputs_and_labels :  tuple[Predictions, Labels] | EvalPrediction \u2014 The first array contains the probability predictions and the second array contains the true labels.</p> </li> <li> <p>has_misc_tags :  bool \u2014 Whether the dataset has MISC tags.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The configuration of the dataset.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] \u2014 A dictionary with the names of the metrics as keys and the metric values as values.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source extract_labels_from_generation(input_batch: dict[str, list], model_output: GenerativeModelOutput, dataset_config: DatasetConfig) \u2192 list[t.Any] </p> <p>Extract the predicted labels from the generated output.</p> <p> Parameters </p> <ul> <li> <p>input_batch :  dict[str, list] \u2014 The input batch, where the keys are the feature names and the values are lists with the feature values.</p> </li> <li> <p>model_output :  GenerativeModelOutput \u2014 The raw generated output of the model.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The configuration of the dataset.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[t.Any] \u2014 The predicted labels.</p> </li> </ul> <p> source tokenize_and_align_labels(examples: dict, tokenizer: PreTrainedTokenizer, label2id: dict[str, int]) \u2192 BatchEncoding </p> <p>Tokenise all texts and align the labels with them.</p> <p> Parameters </p> <ul> <li> <p>examples :  dict \u2014 The examples to be tokenised.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer \u2014 A pretrained tokenizer.</p> </li> <li> <p>label2id :  dict[str, int] \u2014 A dictionary that converts NER tags to IDs.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BatchEncoding \u2014 A dictionary containing the tokenized data as well as labels.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source handle_unk_tokens(tokenizer: PreTrainedTokenizer, tokens: list[str], words: list[str]) \u2192 list[str] </p> <p>Replace unknown tokens in the tokens with the corresponding word.</p> <p> Parameters </p> <ul> <li> <p>tokenizer :  PreTrainedTokenizer \u2014 The tokenizer used to tokenize the words.</p> </li> <li> <p>tokens :  list[str] \u2014 The list of tokens.</p> </li> <li> <p>words :  list[str] \u2014 The list of words.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 The list of tokens with unknown tokens replaced by the corresponding word.</p> </li> </ul>"},{"location":"api/euroeval/benchmarker/","title":"euroeval.benchmarker","text":"euroeval.benchmarker<p> source module euroeval.benchmarker </p> <p>Class that benchmarks language models.</p> <p> Classes </p> <ul> <li> <p>Benchmarker \u2014 Benchmarking all the language models.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>model_has_been_benchmarked \u2014 Checks whether a model has already been benchmarked on a dataset.</p> </li> <li> <p>adjust_logging_level \u2014 Adjust the logging level based on verbosity.</p> </li> <li> <p>clear_model_cache_fn \u2014 Clear the model cache.</p> </li> <li> <p>prepare_dataset_configs \u2014 Prepare the dataset configuration(s) to be benchmarked.</p> </li> <li> <p>initial_logging \u2014 Initial logging at the start of the benchmarking process.</p> </li> </ul> <p> source class Benchmarker(progress_bar: bool = True, save_results: bool = True, task: str | list[str] | None = None, dataset: list[str] | str | None = None, language: str | list[str] = 'all', model_language: str | list[str] | None = None, dataset_language: str | list[str] | None = None, device: Device | None = None, batch_size: int = 32, raise_errors: bool = False, cache_dir: str = '.euroeval_cache', api_key: str | None = None, force: bool = False, verbose: bool = False, trust_remote_code: bool = False, clear_model_cache: bool = False, evaluate_test_split: bool = False, few_shot: bool = True, num_iterations: int = 10, api_base: str | None = None, api_version: str | None = None, debug: bool = False, run_with_cli: bool = False, only_allow_safetensors: bool = False) </p> <p>Benchmarking all the language models.</p> <p>Initialise the benchmarker.</p> <p> Attributes </p> <ul> <li> <p>benchmark_config_default_params \u2014 The default parameters for the benchmark configuration.</p> </li> <li> <p>benchmark_config \u2014 The benchmark configuration.</p> </li> <li> <p>force \u2014 Whether to force evaluations of models, even if they have been benchmarked already.</p> </li> <li> <p>results_path \u2014 The path to the results file.</p> </li> <li> <p>benchmark_results :  list[BenchmarkResult] \u2014 The benchmark results.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>progress_bar :  bool \u2014 Whether progress bars should be shown. Defaults to True.</p> </li> <li> <p>save_results :  bool \u2014 Whether to save the benchmark results to 'euroeval_benchmark_results.jsonl'. Defaults to True.</p> </li> <li> <p>task :  str | list[str] | None \u2014 The tasks benchmark the model(s) on. Mutually exclusive with <code>dataset</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked.</p> </li> <li> <p>dataset :  list[str] | str | None \u2014 The datasets to benchmark on. Mutually exclusive with <code>task</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked.</p> </li> <li> <p>language :  str | list[str] \u2014 The language codes of the languages to include, both for models and datasets. Set this to 'all' if all languages should be considered. Defaults to \"all\".</p> </li> <li> <p>model_language :  str | list[str] | None \u2014 The language codes of the languages to include for models. If specified then this overrides the <code>language</code> parameter for model languages. Defaults to None.</p> </li> <li> <p>dataset_language :  str | list[str] | None \u2014 The language codes of the languages to include for datasets. If specified then this overrides the <code>language</code> parameter for dataset languages. Defaults to None.</p> </li> <li> <p>device :  Device | None \u2014 The device to use for benchmarking. Defaults to None.</p> </li> <li> <p>batch_size :  int \u2014 The batch size to use. Defaults to 32.</p> </li> <li> <p>raise_errors :  bool \u2014 Whether to raise errors instead of skipping the model evaluation. Defaults to False.</p> </li> <li> <p>cache_dir :  str \u2014 Directory to store cached models. Defaults to '.euroeval_cache'.</p> </li> <li> <p>api_key :  str | None \u2014 The API key to use for a given inference API.</p> </li> <li> <p>force :  bool \u2014 Whether to force evaluations of models, even if they have been benchmarked already. Defaults to False.</p> </li> <li> <p>verbose :  bool \u2014 Whether to output additional output. This is automatically set if <code>debug</code> is True. Defaults to False.</p> </li> <li> <p>trust_remote_code :  bool \u2014 Whether to trust remote code when loading models. Defaults to False.</p> </li> <li> <p>clear_model_cache :  bool \u2014 Whether to clear the model cache after benchmarking each model. Defaults to False.</p> </li> <li> <p>evaluate_test_split :  bool \u2014 Whether to evaluate the test split of the datasets. Defaults to False.</p> </li> <li> <p>few_shot :  bool \u2014 Whether to only evaluate the model using few-shot evaluation. Only relevant if the model is generative. Defaults to True.</p> </li> <li> <p>num_iterations :  int \u2014 The number of times each model should be evaluated. This is only meant to be used for power users, and scores will not be allowed on the leaderboards if this is changed. Defaults to 10.</p> </li> <li> <p>api_base :  str | None \u2014 The base URL for a given inference API. Only relevant if <code>model</code> refers to a model on an inference API. Defaults to None.</p> </li> <li> <p>api_version :  str | None \u2014 The version of the API to use. Defaults to None.</p> </li> <li> <p>debug :  bool \u2014 Whether to output debug information. Defaults to False.</p> </li> <li> <p>run_with_cli :  bool \u2014 Whether the benchmarker is being run from the command-line interface. Defaults to False.</p> </li> <li> <p>only_allow_safetensors :  bool \u2014 Whether to only allow models that use the safetensors format. Defaults to False.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014 If both <code>task</code> and <code>dataset</code> are specified.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>benchmark \u2014 Benchmarks models on datasets.</p> </li> </ul> <p> source property Benchmarker.benchmark_results: list[BenchmarkResult] </p> <p>The benchmark results.</p> <p> source method Benchmarker.benchmark(model: list[str] | str, task: str | list[str] | None = None, dataset: list[str] | str | None = None, progress_bar: bool | None = None, save_results: bool | None = None, language: str | list[str] | None = None, model_language: str | list[str] | None = None, dataset_language: str | list[str] | None = None, device: Device | None = None, batch_size: int | None = None, raise_errors: bool | None = None, cache_dir: str | None = None, api_key: str | None = None, force: bool | None = None, verbose: bool | None = None, trust_remote_code: bool | None = None, clear_model_cache: bool | None = None, evaluate_test_split: bool | None = None, few_shot: bool | None = None, num_iterations: int | None = None, only_allow_safetensors: bool | None = None) \u2192 list[BenchmarkResult] </p> <p>Benchmarks models on datasets.</p> <p> Parameters </p> <ul> <li> <p>model :  list[str] | str \u2014 The full Hugging Face Hub path(s) to the pretrained transformer model. The specific model version to use can be added after the suffix '@': \"model@v1.0.0\". It can be a branch name, a tag name, or a commit id, and defaults to the latest version if not specified.</p> </li> <li> <p>task :  str | list[str] | None \u2014 The tasks benchmark the model(s) on. Mutually exclusive with <code>dataset</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked. Defaults to None.</p> </li> <li> <p>dataset :  list[str] | str | None \u2014 The datasets to benchmark on. Mutually exclusive with <code>task</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked. Defaults to None.</p> </li> <li> <p>progress_bar :  bool | None \u2014 Whether progress bars should be shown. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>save_results :  bool | None \u2014 Whether to save the benchmark results to 'euroeval_benchmark_results.jsonl'. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>language :  str | list[str] | None \u2014 The language codes of the languages to include, both for models and datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this to 'all' if all languages should be considered. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>model_language :  str | list[str] | None \u2014 The language codes of the languages to include for models. If specified then this overrides the <code>language</code> parameter for model languages. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>dataset_language :  str | list[str] | None \u2014 The language codes of the languages to include for datasets. If specified then this overrides the <code>language</code> parameter for dataset languages. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>device :  Device | None \u2014 The device to use for benchmarking. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>batch_size :  int | None \u2014 The batch size to use. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>raise_errors :  bool | None \u2014 Whether to raise errors instead of skipping the model evaluation.</p> </li> <li> <p>cache_dir :  str | None \u2014 Directory to store cached models. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>api_key :  str | None \u2014 The API key to use for a given inference server. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>force :  bool | None \u2014 Whether to force evaluations of models, even if they have been benchmarked already. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>verbose :  bool | None \u2014 Whether to output additional output. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>trust_remote_code :  bool | None \u2014 Whether to trust remote code when loading models. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>clear_model_cache :  bool | None \u2014 Whether to clear the model cache after benchmarking each model. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>evaluate_test_split :  bool | None \u2014 Whether to evaluate the test split of the datasets. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>few_shot :  bool | None \u2014 Whether to only evaluate the model using few-shot evaluation. Only relevant if the model is generative. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>num_iterations :  int | None \u2014 The number of times each model should be evaluated. This is only meant to be used for power users, and scores will not be allowed on the leaderboards if this is changed. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>only_allow_safetensors :  bool | None \u2014 Whether to only allow models that use the safetensors format. Defaults to the value specified when initialising the benchmarker.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[BenchmarkResult] \u2014 A list of benchmark results.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014 If both <code>task</code> and <code>dataset</code> are specified.</p> </li> <li> <p>benchmark_output_or_err</p> </li> <li> <p>e</p> </li> </ul> <p> source model_has_been_benchmarked(model_id: str, dataset: str, few_shot: bool, validation_split: bool, benchmark_results: list[BenchmarkResult]) \u2192 bool </p> <p>Checks whether a model has already been benchmarked on a dataset.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The model ID.</p> </li> <li> <p>dataset :  str \u2014 The dataset.</p> </li> <li> <p>few_shot :  bool \u2014 Whether the model was evaluated using few-shot evaluation.</p> </li> <li> <p>validation_split :  bool \u2014 Whether the model was evaluated on the validation split.</p> </li> <li> <p>benchmark_results :  list[BenchmarkResult] \u2014 The benchmark results.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool \u2014 Whether the model has already been evaluated on the dataset.</p> </li> </ul> <p> source adjust_logging_level(verbose: bool, ignore_testing: bool = False) \u2192 int </p> <p>Adjust the logging level based on verbosity.</p> <p> Parameters </p> <ul> <li> <p>verbose :  bool \u2014 Whether to output additional output.</p> </li> <li> <p>ignore_testing :  bool \u2014 Whether to ignore the testing flag.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>int \u2014 The logging level that was set.</p> </li> </ul> <p> source clear_model_cache_fn(cache_dir: str) \u2192 None </p> <p>Clear the model cache.</p> <p>Note that this will not remove the stored completions.</p> <p> Parameters </p> <ul> <li> <p>cache_dir :  str \u2014 The path to the cache directory.</p> </li> </ul> <p> source prepare_dataset_configs(dataset_names: list[str]) \u2192 list['DatasetConfig'] </p> <p>Prepare the dataset configuration(s) to be benchmarked.</p> <p> Parameters </p> <ul> <li> <p>dataset_names :  list[str] \u2014 The dataset names to benchmark.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list['DatasetConfig'] \u2014 The prepared list of model IDs.</p> </li> </ul> <p> source initial_logging(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 None </p> <p>Initial logging at the start of the benchmarking process.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014 The configuration of the model we are evaluating.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The configuration of the dataset we are evaluating on.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The general benchmark configuration.</p> </li> </ul>"},{"location":"api/euroeval/benchmark_config_factory/","title":"euroeval.benchmark_config_factory","text":"euroeval.benchmark_config_factory<p> source module euroeval.benchmark_config_factory </p> <p>Factory class for creating dataset configurations.</p> <p> Functions </p> <ul> <li> <p>build_benchmark_config \u2014 Create a benchmark configuration.</p> </li> <li> <p>get_correct_language_codes \u2014 Get correct language code(s).</p> </li> <li> <p>prepare_languages \u2014 Prepare language(s) for benchmarking.</p> </li> <li> <p>prepare_tasks_and_datasets \u2014 Prepare task(s) and dataset(s) for benchmarking.</p> </li> <li> <p>prepare_device \u2014 Prepare device for benchmarking.</p> </li> </ul> <p> source build_benchmark_config(progress_bar: bool, save_results: bool, task: str | list[str] | None, dataset: str | list[str] | None, language: str | list[str], model_language: str | list[str] | None, dataset_language: str | list[str] | None, device: Device | None, batch_size: int, raise_errors: bool, cache_dir: str, api_key: str | None, force: bool, verbose: bool, trust_remote_code: bool, clear_model_cache: bool, evaluate_test_split: bool, few_shot: bool, num_iterations: int, api_base: str | None, api_version: str | None, debug: bool, run_with_cli: bool, only_allow_safetensors: bool, first_time: bool = False) \u2192 BenchmarkConfig </p> <p>Create a benchmark configuration.</p> <p> Parameters </p> <ul> <li> <p>progress_bar :  bool \u2014 Whether to show a progress bar when running the benchmark.</p> </li> <li> <p>save_results :  bool \u2014 Whether to save the benchmark results to a file.</p> </li> <li> <p>task :  str | list[str] | None \u2014 The tasks to include for dataset. If None then datasets will not be filtered based on their task.</p> </li> <li> <p>dataset :  str | list[str] | None \u2014 The datasets to include for task. If None then all datasets will be included, limited by the <code>task</code> parameter.</p> </li> <li> <p>language :  str | list[str] \u2014 The language codes of the languages to include, both for models and datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this to 'all' if all languages should be considered.</p> </li> <li> <p>model_language :  str | list[str] | None \u2014 The language codes of the languages to include for models. If None then the <code>language</code> parameter will be used.</p> </li> <li> <p>dataset_language :  str | list[str] | None \u2014 The language codes of the languages to include for datasets. If None then the <code>language</code> parameter will be used.</p> </li> <li> <p>device :  Device | None \u2014 The device to use for running the models. If None then the device will be set automatically.</p> </li> <li> <p>batch_size :  int \u2014 The batch size to use for running the models.</p> </li> <li> <p>raise_errors :  bool \u2014 Whether to raise errors when running the benchmark.</p> </li> <li> <p>cache_dir :  str \u2014 The directory to use for caching the models.</p> </li> <li> <p>api_key :  str | None \u2014 The API key to use for a given inference server.</p> </li> <li> <p>force :  bool \u2014 Whether to force the benchmark to run even if the results are already cached.</p> </li> <li> <p>verbose :  bool \u2014 Whether to print verbose output when running the benchmark. This is automatically set if <code>debug</code> is True.</p> </li> <li> <p>trust_remote_code :  bool \u2014 Whether to trust remote code when running the benchmark.</p> </li> <li> <p>clear_model_cache :  bool \u2014 Whether to clear the model cache before running the benchmark.</p> </li> <li> <p>evaluate_test_split :  bool \u2014 Whether to use the test split for the datasets.</p> </li> <li> <p>few_shot :  bool \u2014 Whether to use few-shot learning for the models.</p> </li> <li> <p>num_iterations :  int \u2014 The number of iterations each model should be evaluated for.</p> </li> <li> <p>api_base :  str | None \u2014 The base URL for a given inference API. Only relevant if <code>model</code> refers to a model on an inference API.</p> </li> <li> <p>api_version :  str | None \u2014 The version of the API to use for a given inference API.</p> </li> <li> <p>debug :  bool \u2014 Whether to run the benchmark in debug mode.</p> </li> <li> <p>run_with_cli :  bool \u2014 Whether the benchmark is being run with the CLI.</p> </li> <li> <p>only_allow_safetensors :  bool \u2014 Whether to only allow evaluations of models stored as safetensors.</p> </li> <li> <p>first_time :  bool \u2014 Whether this is the first time the benchmark configuration is being created. Defaults to False.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> source get_correct_language_codes(language_codes: str | list[str]) \u2192 list[str] </p> <p>Get correct language code(s).</p> <p> Parameters </p> <ul> <li> <p>language_codes :  str | list[str] \u2014 The language codes of the languages to include, both for models and datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this to 'all' if all languages should be considered.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 The correct language codes.</p> </li> </ul> <p> source prepare_languages(language_codes: str | list[str] | None, default_language_codes: list[str]) \u2192 list['Language'] </p> <p>Prepare language(s) for benchmarking.</p> <p> Parameters </p> <ul> <li> <p>language_codes :  str | list[str] | None \u2014 The language codes of the languages to include for models or datasets. If specified then this overrides the <code>language</code> parameter for model or dataset languages.</p> </li> <li> <p>default_language_codes :  list[str] \u2014 The default language codes of the languages to include.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list['Language'] \u2014 The prepared dataset languages.</p> </li> </ul> <p> source prepare_tasks_and_datasets(task: str | list[str] | None, dataset_languages: list['Language'], dataset: str | list[str] | None) \u2192 tuple[list['Task'], list[str]] </p> <p>Prepare task(s) and dataset(s) for benchmarking.</p> <p> Parameters </p> <ul> <li> <p>task :  str | list[str] | None \u2014 The tasks to include for dataset. If None then datasets will not be filtered based on their task.</p> </li> <li> <p>dataset_languages :  list['Language'] \u2014 The languages of the datasets in the benchmark.</p> </li> <li> <p>dataset :  str | list[str] | None \u2014 The datasets to include for task. If None then all datasets will be included, limited by the <code>task</code> and <code>dataset_languages</code> parameters.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[list['Task'], list[str]] \u2014 The prepared tasks and datasets.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark \u2014 If the task or dataset is not found in the benchmark tasks or datasets.</p> </li> </ul> <p> source prepare_device(device: Device | None) \u2192 torch.device </p> <p>Prepare device for benchmarking.</p> <p> Parameters </p> <ul> <li> <p>device :  Device | None \u2014 The device to use for running the models. If None then the device will be set automatically.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>torch.device \u2014 The prepared device.</p> </li> </ul>"},{"location":"api/euroeval/callbacks/","title":"euroeval.callbacks","text":"euroeval.callbacks<p> source module euroeval.callbacks </p> <p>Callbacks for the Hugging Face Trainer.</p> <p> Classes </p> <ul> <li> <p>NeverLeaveProgressCallback \u2014 Progress callback which never leaves the progress bar.</p> </li> </ul> <p> source class NeverLeaveProgressCallback(max_str_len: int = 100) </p> <p>Bases : ProgressCallback</p> <p>Progress callback which never leaves the progress bar.</p> <p>Initialise the callback.</p> <p> Methods </p> <ul> <li> <p>on_train_begin \u2014 Callback actions when training begins.</p> </li> <li> <p>on_step_end \u2014 Callback actions when a training step ends.</p> </li> <li> <p>on_prediction_step \u2014 Callback actions when a prediction step ends.</p> </li> </ul> <p> source method NeverLeaveProgressCallback.on_train_begin(args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs: str) \u2192 None </p> <p>Callback actions when training begins.</p> <p> source method NeverLeaveProgressCallback.on_step_end(args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs: str) \u2192 None </p> <p>Callback actions when a training step ends.</p> <p> source method NeverLeaveProgressCallback.on_prediction_step(args: TrainingArguments, state: TrainerState, control: TrainerControl, eval_dataloader: DataLoader | None = None, **kwargs: str) \u2192 None </p> <p>Callback actions when a prediction step ends.</p>"},{"location":"api/euroeval/cli/","title":"euroeval.cli","text":"euroeval.cli<p> source module euroeval.cli </p> <p>Command-line interface for benchmarking.</p> <p> Functions </p> <ul> <li> <p>benchmark \u2014 Benchmark pretrained language models on language tasks.</p> </li> </ul> <p> source benchmark(model: tuple[str], dataset: tuple[str], language: tuple[str], model_language: tuple[str], dataset_language: tuple[str], raise_errors: bool, task: tuple[str], batch_size: str, progress_bar: bool, save_results: bool, cache_dir: str, api_key: str | None, force: bool, verbose: bool, device: str | None, trust_remote_code: bool, clear_model_cache: bool, evaluate_test_split: bool, few_shot: bool, num_iterations: int, api_base: str | None, api_version: str | None, debug: bool, only_allow_safetensors: bool) \u2192 None </p> <p>Benchmark pretrained language models on language tasks.</p>"},{"location":"api/euroeval/constants/","title":"euroeval.constants","text":"euroeval.constants<p> source module euroeval.constants </p> <p>Constants used throughout the project.</p>"},{"location":"api/euroeval/data_loading/","title":"euroeval.data_loading","text":"euroeval.data_loading<p> source module euroeval.data_loading </p> <p>Functions related to the loading of the data.</p> <p> Functions </p> <ul> <li> <p>load_data \u2014 Load the raw bootstrapped datasets.</p> </li> <li> <p>load_raw_data \u2014 Load the raw dataset.</p> </li> </ul> <p> source load_data(rng: Generator, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 list['DatasetDict'] </p> <p>Load the raw bootstrapped datasets.</p> <p> Parameters </p> <ul> <li> <p>rng :  Generator \u2014 The random number generator to use.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The configuration for the dataset.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The configuration for the benchmark.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list['DatasetDict'] \u2014 A list of bootstrapped datasets, one for each iteration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark \u2014 If the dataset cannot be loaded.</p> </li> <li> <p>HuggingFaceHubDown \u2014 If the Hugging Face Hub is down.</p> </li> </ul> <p> source load_raw_data(dataset_config: DatasetConfig, cache_dir: str) \u2192 DatasetDict </p> <p>Load the raw dataset.</p> <p> Parameters </p> <ul> <li> <p>dataset_config :  DatasetConfig \u2014 The configuration for the dataset.</p> </li> <li> <p>cache_dir :  str \u2014 The directory to cache the dataset.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetDict \u2014 The dataset.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> <li> <p>HuggingFaceHubDown</p> </li> </ul>"},{"location":"api/euroeval/data_models/","title":"euroeval.data_models","text":"euroeval.data_models<p> source module euroeval.data_models </p> <p>Data models used in EuroEval.</p> <p> Classes </p> <ul> <li> <p>Language \u2014 A benchmarkable language.</p> </li> <li> <p>Task \u2014 A dataset task.</p> </li> <li> <p>BenchmarkConfig \u2014 General benchmarking configuration, across datasets and models.</p> </li> <li> <p>BenchmarkConfigParams \u2014 The parameters for the benchmark configuration.</p> </li> <li> <p>BenchmarkResult \u2014 A benchmark result.</p> </li> <li> <p>DatasetConfig \u2014 Configuration for a dataset.</p> </li> <li> <p>ModelConfig \u2014 Configuration for a model.</p> </li> <li> <p>PreparedModelInputs \u2014 The inputs to a model.</p> </li> <li> <p>GenerativeModelOutput \u2014 The output of a generative model.</p> </li> <li> <p>SingleGenerativeModelOutput \u2014 A single output of a generative model.</p> </li> <li> <p>HFModelInfo \u2014 Information about a Hugging Face model.</p> </li> <li> <p>PromptConfig \u2014 Configuration for task-specific prompting across languages.</p> </li> </ul> <p> source dataclass Language(code: str, name: str, _and_separator: str | None = field(repr=False, default=None), _or_separator: str | None = field(repr=False, default=None)) </p> <p>A benchmarkable language.</p> <p> Attributes </p> <ul> <li> <p>code :  str \u2014 The ISO 639-1 language code of the language.</p> </li> <li> <p>name :  str \u2014 The name of the language.</p> </li> <li> <p>and_separator :  optional \u2014 The word 'and' in the language.</p> </li> <li> <p>or_separator :  optional \u2014 The word 'or' in the language.</p> </li> </ul> <p> source property Language.and_separator: str </p> <p>Get the word 'and' in the language.</p> <p> Returns </p> <ul> <li> <p>str \u2014 The word 'and' in the language.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError \u2014 If <code>and_separator</code> is <code>None</code>.</p> </li> </ul> <p> source property Language.or_separator: str </p> <p>Get the word 'or' in the language.</p> <p> Returns </p> <ul> <li> <p>str \u2014 The word 'or' in the language.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError \u2014 If <code>or_separator</code> is <code>None</code>.</p> </li> </ul> <p> source dataclass Task(name: str, task_group: TaskGroup, template_dict: dict['Language', 'PromptConfig'], metrics: list[Metric], default_num_few_shot_examples: int, default_max_generated_tokens: int, default_labels: list[str]) </p> <p>A dataset task.</p> <p> Attributes </p> <ul> <li> <p>name :  str \u2014 The name of the task.</p> </li> <li> <p>task_group :  TaskGroup \u2014 The task group of the task.</p> </li> <li> <p>template_dict :  dict['Language', 'PromptConfig'] \u2014 The template dictionary for the task, from language to prompt template.</p> </li> <li> <p>metrics :  list[Metric] \u2014 The metrics used to evaluate the task.</p> </li> <li> <p>default_num_few_shot_examples :  int \u2014 The default number of examples to use when benchmarking the task using few-shot evaluation. For a classification task, these will be drawn evenly from each label.</p> </li> <li> <p>default_max_generated_tokens :  int \u2014 The default maximum number of tokens to generate when benchmarking the task using few-shot evaluation.</p> </li> <li> <p>default_labels :  list[str] \u2014 The default labels for datasets using this task.</p> </li> </ul> <p> source dataclass BenchmarkConfig(model_languages: list[Language], dataset_languages: list[Language], tasks: list[Task], datasets: list[str], batch_size: int, raise_errors: bool, cache_dir: str, api_key: str | None, force: bool, progress_bar: bool, save_results: bool, device: torch.device, verbose: bool, trust_remote_code: bool, clear_model_cache: bool, evaluate_test_split: bool, few_shot: bool, num_iterations: int, api_base: str | None, api_version: str | None, debug: bool, run_with_cli: bool, only_allow_safetensors: bool) </p> <p>General benchmarking configuration, across datasets and models.</p> <p> Attributes </p> <ul> <li> <p>model_languages :  list[Language] \u2014 The languages of the models to benchmark.</p> </li> <li> <p>dataset_languages :  list[Language] \u2014 The languages of the datasets in the benchmark.</p> </li> <li> <p>tasks :  list[Task] \u2014 The tasks benchmark the model(s) on.</p> </li> <li> <p>datasets :  list[str] \u2014 The datasets to benchmark on.</p> </li> <li> <p>batch_size :  int \u2014 The batch size to use.</p> </li> <li> <p>raise_errors :  bool \u2014 Whether to raise errors instead of skipping them.</p> </li> <li> <p>cache_dir :  str \u2014 Directory to store cached models and datasets.</p> </li> <li> <p>api_key :  str | None \u2014 The API key to use for a given inference API.</p> </li> <li> <p>force :  bool \u2014 Whether to force the benchmark to run even if the results are already cached.</p> </li> <li> <p>progress_bar :  bool \u2014 Whether to show a progress bar.</p> </li> <li> <p>save_results :  bool \u2014 Whether to save the benchmark results to 'euroeval_benchmark_results.json'.</p> </li> <li> <p>device :  torch.device \u2014 The device to use for benchmarking.</p> </li> <li> <p>verbose :  bool \u2014 Whether to print verbose output.</p> </li> <li> <p>trust_remote_code :  bool \u2014 Whether to trust remote code when loading models from the Hugging Face Hub.</p> </li> <li> <p>clear_model_cache :  bool \u2014 Whether to clear the model cache after benchmarking each model.</p> </li> <li> <p>evaluate_test_split :  bool \u2014 Whether to evaluate on the test split.</p> </li> <li> <p>few_shot :  bool \u2014 Whether to only evaluate the model using few-shot evaluation. Only relevant if the model is generative.</p> </li> <li> <p>num_iterations :  int \u2014 The number of iterations each model should be evaluated for.</p> </li> <li> <p>api_base :  str | None \u2014 The base URL for a given inference API. Only relevant if <code>model</code> refers to a model on an inference API.</p> </li> <li> <p>api_version :  str | None \u2014 The version of the API to use. Only relevant if <code>model</code> refers to a model on an inference API.</p> </li> <li> <p>debug :  bool \u2014 Whether to run the benchmark in debug mode.</p> </li> <li> <p>run_with_cli :  bool \u2014 Whether the benchmark is being run with the CLI.</p> </li> <li> <p>only_allow_safetensors :  bool \u2014 Whether to only allow models that use the safetensors format.</p> </li> </ul> <p> source class BenchmarkConfigParams(**data: Any) </p> <p>Bases : pydantic.BaseModel</p> <p>The parameters for the benchmark configuration.</p> <p>Create a new model by parsing and validating input data from keyword arguments.</p> <p>Raises [<code>ValidationError</code>][pydantic_core.ValidationError] if the input data cannot be validated to form a valid model.</p> <p><code>self</code> is explicitly positional-only to allow <code>self</code> as a field name.</p> <p> Attributes </p> <ul> <li> <p>model_extra :  dict[str, Any] | None \u2014 Get extra fields set during validation.</p> </li> <li> <p>model_fields_set :  set[str] \u2014 Returns the set of fields that have been explicitly set on this model instance.</p> </li> </ul> <p> source class BenchmarkResult(**data: Any) </p> <p>Bases : pydantic.BaseModel</p> <p>A benchmark result.</p> <p>Create a new model by parsing and validating input data from keyword arguments.</p> <p>Raises [<code>ValidationError</code>][pydantic_core.ValidationError] if the input data cannot be validated to form a valid model.</p> <p><code>self</code> is explicitly positional-only to allow <code>self</code> as a field name.</p> <p> Attributes </p> <ul> <li> <p>model_config :  ClassVar[ConfigDict] \u2014 Configuration for the model, should be a dictionary conforming to [<code>ConfigDict</code>][pydantic.config.ConfigDict].</p> </li> <li> <p>model_extra :  dict[str, Any] | None \u2014 Get extra fields set during validation.</p> </li> <li> <p>model_fields_set :  set[str] \u2014 Returns the set of fields that have been explicitly set on this model instance.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>from_dict \u2014 Create a benchmark result from a dictionary.</p> </li> <li> <p>append_to_results \u2014 Append the benchmark result to the results file.</p> </li> </ul> <p> source classmethod BenchmarkResult.from_dict(config: dict) \u2192 BenchmarkResult </p> <p>Create a benchmark result from a dictionary.</p> <p> Parameters </p> <ul> <li> <p>config :  dict \u2014 The configuration dictionary.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BenchmarkResult \u2014 The benchmark result.</p> </li> </ul> <p> source method BenchmarkResult.append_to_results(results_path: pathlib.Path) \u2192 None </p> <p>Append the benchmark result to the results file.</p> <p> Parameters </p> <ul> <li> <p>results_path :  pathlib.Path \u2014 The path to the results file.</p> </li> </ul> <p> source dataclass DatasetConfig(name: str, pretty_name: str, huggingface_id: str, task: Task, languages: list[Language], _prompt_prefix: str | None = None, _prompt_template: str | None = None, _instruction_prompt: str | None = None, _num_few_shot_examples: int | None = None, _max_generated_tokens: int | None = None, _labels: list[str] | None = None, _prompt_label_mapping: dict[str, str] | t.Literal['auto'] | None = None, unofficial: bool = False) </p> <p>Configuration for a dataset.</p> <p> Attributes </p> <ul> <li> <p>name :  str \u2014 The name of the dataset. Must be lower case with no spaces.</p> </li> <li> <p>pretty_name :  str \u2014 A longer prettier name for the dataset, which allows cases and spaces. Used for logging.</p> </li> <li> <p>huggingface_id :  str \u2014 The Hugging Face ID of the dataset.</p> </li> <li> <p>task :  Task \u2014 The task of the dataset.</p> </li> <li> <p>languages :  list[Language] \u2014 The ISO 639-1 language codes of the entries in the dataset.</p> </li> <li> <p>id2label :  dict[int, str] \u2014 The mapping from ID to label.</p> </li> <li> <p>label2id :  dict[str, int] \u2014 The mapping from label to ID.</p> </li> <li> <p>num_labels :  int \u2014 The number of labels in the dataset.</p> </li> <li> <p>_prompt_prefix :  optional \u2014 The prefix to use in the few-shot prompt. Defaults to the template for the task and language.</p> </li> <li> <p>_prompt_template :  optional \u2014 The template for the prompt to use when benchmarking the dataset using few-shot evaluation. Defaults to the template for the task and language.</p> </li> <li> <p>_instruction_prompt :  optional \u2014 The prompt to use when benchmarking the dataset using instruction-based evaluation. Defaults to the template for the task and language.</p> </li> <li> <p>_num_few_shot_examples :  optional \u2014 The number of examples to use when benchmarking the dataset using few-shot evaluation. For a classification task, these will be drawn evenly from each label. Defaults to the template for the task and language.</p> </li> <li> <p>_max_generated_tokens :  optional \u2014 The maximum number of tokens to generate when benchmarking the dataset using few-shot evaluation. Defaults to the template for the task and language.</p> </li> <li> <p>_labels :  optional \u2014 The labels in the dataset. Defaults to the template for the task and language.</p> </li> <li> <p>_prompt_label_mapping :  optional \u2014 A mapping from the labels to another phrase which is used as a substitute for the label in few-shot evaluation. If \"auto\" then the mapping will be set to a 1:1 mapping between the labels and themselves. If None then the mapping will be set to the default mapping for the task and language. Defaults to None.</p> </li> <li> <p>unofficial :  optional \u2014 Whether the dataset is unofficial. Defaults to False.</p> </li> <li> <p>prompt_prefix :  str \u2014 The prefix to use in the few-shot prompt.</p> </li> <li> <p>prompt_template :  str \u2014 The template used during few-shot evaluation.</p> </li> <li> <p>instruction_prompt :  str \u2014 The prompt to use when evaluating instruction-tuned models.</p> </li> <li> <p>num_few_shot_examples :  int \u2014 The number of few-shot examples to use.</p> </li> <li> <p>max_generated_tokens :  int \u2014 The maximum number of tokens to generate when evaluating a model.</p> </li> <li> <p>labels :  list[str] \u2014 The labels in the dataset.</p> </li> <li> <p>prompt_label_mapping :  dict[str, str] \u2014 Mapping from English labels to localised labels.</p> </li> </ul> <p> source property DatasetConfig.prompt_prefix: str </p> <p>The prefix to use in the few-shot prompt.</p> <p> source property DatasetConfig.prompt_template: str </p> <p>The template used during few-shot evaluation.</p> <p> source property DatasetConfig.instruction_prompt: str </p> <p>The prompt to use when evaluating instruction-tuned models.</p> <p> source property DatasetConfig.num_few_shot_examples: int </p> <p>The number of few-shot examples to use.</p> <p> source property DatasetConfig.max_generated_tokens: int </p> <p>The maximum number of tokens to generate when evaluating a model.</p> <p> source property DatasetConfig.labels: list[str] </p> <p>The labels in the dataset.</p> <p> source property DatasetConfig.prompt_label_mapping: dict[str, str] </p> <p>Mapping from English labels to localised labels.</p> <p> source property DatasetConfig.id2label: dict[int, str] </p> <p>The mapping from ID to label.</p> <p> source property DatasetConfig.label2id: dict[str, int] </p> <p>The mapping from label to ID.</p> <p> source property DatasetConfig.num_labels: int </p> <p>The number of labels in the dataset.</p> <p> source dataclass ModelConfig(model_id: str, revision: str, task: str, languages: list[Language], inference_backend: InferenceBackend, merge: bool, model_type: ModelType, fresh: bool, model_cache_dir: str, adapter_base_model_id: str | None) </p> <p>Configuration for a model.</p> <p> Attributes </p> <ul> <li> <p>model_id :  str \u2014 The ID of the model.</p> </li> <li> <p>revision :  str \u2014 The revision of the model.</p> </li> <li> <p>task :  str \u2014 The task that the model was trained on.</p> </li> <li> <p>languages :  list[Language] \u2014 The languages of the model.</p> </li> <li> <p>inference_backend :  InferenceBackend \u2014 The backend used to perform inference with the model.</p> </li> <li> <p>merge :  bool \u2014 Whether the model is a merged model.</p> </li> <li> <p>model_type :  ModelType \u2014 The type of the model (e.g., encoder, base decoder, instruction tuned).</p> </li> <li> <p>fresh :  bool \u2014 Whether the model is freshly initialised.</p> </li> <li> <p>model_cache_dir :  str \u2014 The directory to cache the model in.</p> </li> <li> <p>adapter_base_model_id :  str | None \u2014 The model ID of the base model if the model is an adapter model. Can be None if the model is not an adapter model.</p> </li> </ul> <p> source dataclass PreparedModelInputs(texts: list[str] | None = None, input_ids: torch.Tensor | None = None, attention_mask: torch.Tensor | None = None) </p> <p>The inputs to a model.</p> <p> Attributes </p> <ul> <li> <p>texts :  list[str] | None \u2014 The texts to input to the model. Can be None if the input IDs and attention mask are provided instead.</p> </li> <li> <p>input_ids :  torch.Tensor | None \u2014 The input IDs of the texts. Can be None if the texts are provided instead.</p> </li> <li> <p>attention_mask :  torch.Tensor | None \u2014 The attention mask of the texts. Can be None if the texts are provided instead.</p> </li> </ul> <p> source dataclass GenerativeModelOutput(sequences: list[str], scores: list[list[list[tuple[str, float]]]] | None = None) </p> <p>The output of a generative model.</p> <p> Attributes </p> <ul> <li> <p>sequences :  list[str] \u2014 The generated sequences.</p> </li> <li> <p>scores :  list[list[list[tuple[str, float]]]] | None \u2014 The scores of the sequences. This is an array of shape (batch_size, num_tokens, num_logprobs, 2), where the last dimension contains the token and its logprob. Can be None if the scores are not available.</p> </li> </ul> <p> source dataclass SingleGenerativeModelOutput(sequence: str, scores: list[list[tuple[str, float]]] | None = None) </p> <p>A single output of a generative model.</p> <p> Attributes </p> <ul> <li> <p>sequence :  str \u2014 The generated sequence.</p> </li> <li> <p>scores :  list[list[tuple[str, float]]] | None \u2014 The scores of the sequence. This is an array of shape (num_tokens, num_logprobs, 2), where the last dimension contains the token and its logprob. Can be None if the scores are not available.</p> </li> </ul> <p> source dataclass HFModelInfo(pipeline_tag: str, tags: list[str], adapter_base_model_id: str | None) </p> <p>Information about a Hugging Face model.</p> <p> Attributes </p> <ul> <li> <p>pipeline_tag :  str \u2014 The pipeline tag of the model.</p> </li> <li> <p>tags :  list[str] \u2014 The other tags of the model.</p> </li> <li> <p>adapter_base_model_id :  str | None \u2014 The model ID of the base model if the model is an adapter model. Can be None if the model is not an adapter model.</p> </li> </ul> <p> source dataclass PromptConfig(default_prompt_prefix: str, default_prompt_template: str, default_instruction_prompt: str, default_prompt_label_mapping: dict[str, str] | t.Literal['auto']) </p> <p>Configuration for task-specific prompting across languages.</p> <p>Defines the prompt templates needed for evaluating a specific task in a given language.</p> <p> Attributes </p> <ul> <li> <p>default_prompt_prefix :  str \u2014 The default prefix to use in the few-shot prompt.</p> </li> <li> <p>default_prompt_template :  str \u2014 The default template for the prompt to use when benchmarking the dataset using few-shot evaluation.</p> </li> <li> <p>default_instruction_prompt :  str \u2014 The default prompt to use when benchmarking the dataset using instruction-based evaluation.</p> </li> <li> <p>default_prompt_label_mapping :  dict[str, str] | t.Literal['auto'] \u2014 The default mapping from the labels to another phrase which is used as a substitute for the label in few-shot evaluation. If set to \"auto\", the mapping will be set to a 1:1 mapping between the labels and themselves.</p> </li> </ul>"},{"location":"api/euroeval/enums/","title":"euroeval.enums","text":"euroeval.enums<p> source module euroeval.enums </p> <p>Enums used in the project.</p> <p> Classes </p> <ul> <li> <p>AutoStrEnum \u2014 StrEnum where auto() returns the field name in lower case.</p> </li> <li> <p>Device \u2014 The compute device to use for the evaluation.</p> </li> <li> <p>InferenceBackend \u2014 The backend used for model inference.</p> </li> <li> <p>ModelType \u2014 The type of a model.</p> </li> <li> <p>GenerativeType \u2014 The type of a generative model.</p> </li> <li> <p>DataType \u2014 The data type of the model weights.</p> </li> <li> <p>BatchingPreference \u2014 The preference for batching.</p> </li> <li> <p>TaskGroup \u2014 The overall task group of a task.</p> </li> </ul> <p> source enum AutoStrEnum(*args, **kwds) </p> <p>Bases : str, Enum</p> <p>StrEnum where auto() returns the field name in lower case.</p> <p> source enum Device(*args, **kwds) </p> <p>Bases : AutoStrEnum</p> <p>The compute device to use for the evaluation.</p> <p> Attributes </p> <ul> <li> <p>CPU \u2014 CPU device.</p> </li> <li> <p>MPS \u2014 MPS GPU, used in M-series MacBooks.</p> </li> <li> <p>CUDA \u2014 CUDA GPU, used with NVIDIA GPUs.</p> </li> </ul> <p> source enum InferenceBackend(*args, **kwds) </p> <p>Bases : AutoStrEnum</p> <p>The backend used for model inference.</p> <p> Attributes </p> <ul> <li> <p>TRANSFORMERS \u2014 Hugging Face <code>transformers</code> library.</p> </li> <li> <p>VLLM \u2014 VLLM library.</p> </li> <li> <p>LITELLM \u2014 LiteLLM library.</p> </li> <li> <p>NONE \u2014 No inference backend used (e.g., for human evaluation).</p> </li> </ul> <p> source enum ModelType(*args, **kwds) </p> <p>Bases : AutoStrEnum</p> <p>The type of a model.</p> <p> Attributes </p> <ul> <li> <p>ENCODER \u2014 An encoder (i.e., BERT-style) model.</p> </li> <li> <p>GENERATIVE \u2014 A generative model. Can be either decoder or encoder-decoder (aka seq2seq).</p> </li> <li> <p>HUMAN \u2014 Human evaluator.</p> </li> </ul> <p> source enum GenerativeType(*args, **kwds) </p> <p>Bases : AutoStrEnum</p> <p>The type of a generative model.</p> <p> Attributes </p> <ul> <li> <p>BASE \u2014 A base (i.e., pretrained) generative model.</p> </li> <li> <p>INSTRUCTION_TUNED \u2014 An instruction-tuned generative model.</p> </li> <li> <p>REASONING \u2014 A generative reasoning model.</p> </li> </ul> <p> source enum DataType(*args, **kwds) </p> <p>Bases : AutoStrEnum</p> <p>The data type of the model weights.</p> <p> Attributes </p> <ul> <li> <p>FP32 \u2014 32-bit floating point.</p> </li> <li> <p>FP16 \u2014 16-bit floating point.</p> </li> <li> <p>BF16 \u2014 16-bit bfloat.</p> </li> </ul> <p> source enum BatchingPreference(*args, **kwds) </p> <p>Bases : AutoStrEnum</p> <p>The preference for batching.</p> <p> Attributes </p> <ul> <li> <p>NO_PREFERENCE \u2014 No preference for batching.</p> </li> <li> <p>SINGLE_SAMPLE \u2014 Single sample batching.</p> </li> <li> <p>ALL_AT_ONCE \u2014 All samples at once batching.</p> </li> </ul> <p> source enum TaskGroup(*args, **kwds) </p> <p>Bases : AutoStrEnum</p> <p>The overall task group of a task.</p> <p> Attributes </p> <ul> <li> <p>SEQUENCE_CLASSIFICATION \u2014 Classification of documents.</p> </li> <li> <p>MULTIPLE_CHOICE_CLASSIFICATION \u2014 Classification of documents with multiple-choice options.</p> </li> <li> <p>TOKEN_CLASSIFICATION \u2014 Token-level classification.</p> </li> <li> <p>QUESTION_ANSWERING \u2014 Extractive question answering.</p> </li> <li> <p>TEXT_TO_TEXT \u2014 Text-to-text generation.</p> </li> <li> <p>SPEED \u2014 Speed benchmark.</p> </li> </ul>"},{"location":"api/euroeval/exceptions/","title":"euroeval.exceptions","text":"euroeval.exceptions<p> source module euroeval.exceptions </p> <p>Exceptions to used by other functions.</p> <p> Classes </p> <ul> <li> <p>InvalidBenchmark \u2014 The (model, dataset) combination cannot be benchmarked.</p> </li> <li> <p>InvalidModel \u2014 The model cannot be benchmarked on any datasets.</p> </li> <li> <p>HuggingFaceHubDown \u2014 The Hugging Face Hub seems to be down.</p> </li> <li> <p>NoInternetConnection \u2014 There seems to be no internet connection.</p> </li> <li> <p>NaNValueInModelOutput \u2014 There is a NaN value in the model output.</p> </li> <li> <p>NeedsExtraInstalled \u2014 The evaluation requires extra to be installed.</p> </li> <li> <p>NeedsManualDependency \u2014 The evaluation requires a dependency to be manually installed.</p> </li> <li> <p>NeedsAdditionalArgument \u2014 The evaluation requires additional arguments to the <code>euroeval</code> command.</p> </li> <li> <p>NeedsEnvironmentVariable \u2014 The evaluation requires an environment variable to be set.</p> </li> </ul> <p> source class InvalidBenchmark(message: str = 'This model cannot be benchmarked on the given dataset.') </p> <p>Bases : Exception</p> <p>The (model, dataset) combination cannot be benchmarked.</p> <p>Initialise the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014 The message to display.</p> </li> </ul> <p> source class InvalidModel(message: str = 'The model cannot be benchmarked on any datasets.') </p> <p>Bases : Exception</p> <p>The model cannot be benchmarked on any datasets.</p> <p>Initialise the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014 The message to display.</p> </li> </ul> <p> source class HuggingFaceHubDown(message: str = 'The Hugging Face Hub is currently down.') </p> <p>Bases : Exception</p> <p>The Hugging Face Hub seems to be down.</p> <p>Initialise the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014 The message to display.</p> </li> </ul> <p> source class NoInternetConnection(message: str = 'There is currently no internet connection.') </p> <p>Bases : Exception</p> <p>There seems to be no internet connection.</p> <p>Initialise the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014 The message to display.</p> </li> </ul> <p> source class NaNValueInModelOutput(message: str = 'There is a NaN value in the model output.') </p> <p>Bases : Exception</p> <p>There is a NaN value in the model output.</p> <p>Initialise the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014 The message to display.</p> </li> </ul> <p> source class NeedsExtraInstalled(extra: str) </p> <p>Bases : InvalidModel</p> <p>The evaluation requires extra to be installed.</p> <p>Initialise the exception.</p> <p> Parameters </p> <ul> <li> <p>extra :  str \u2014 The extra that needs to be installed.</p> </li> </ul> <p> source class NeedsManualDependency(package: str) </p> <p>Bases : InvalidModel</p> <p>The evaluation requires a dependency to be manually installed.</p> <p>Initialise the exception.</p> <p> Parameters </p> <ul> <li> <p>package :  str \u2014 The package that needs to be manually installed.</p> </li> </ul> <p> source class NeedsAdditionalArgument(cli_argument: str, script_argument: str, run_with_cli: bool) </p> <p>Bases : InvalidModel</p> <p>The evaluation requires additional arguments to the <code>euroeval</code> command.</p> <p>Initialise the exception.</p> <p> Parameters </p> <ul> <li> <p>cli_argument :  str \u2014 The argument that needs to be passed to the <code>euroeval</code> command.</p> </li> <li> <p>script_argument :  str \u2014 The argument that needs to be passed to the <code>Benchmarker</code> class.</p> </li> <li> <p>run_with_cli :  bool \u2014 Whether the benchmark is being run with the CLI.</p> </li> </ul> <p> source class NeedsEnvironmentVariable(env_var: str) </p> <p>Bases : InvalidModel</p> <p>The evaluation requires an environment variable to be set.</p> <p>Initialise the exception.</p> <p> Parameters </p> <ul> <li> <p>env_var :  str \u2014 The environment variable that needs to be set.</p> </li> </ul>"},{"location":"api/euroeval/finetuning/","title":"euroeval.finetuning","text":"euroeval.finetuning<p> source module euroeval.finetuning </p> <p>Functions related to the finetuning of models.</p> <p> Functions </p> <ul> <li> <p>finetune \u2014 Evaluate a model on a dataset through finetuning.</p> </li> <li> <p>finetune_single_iteration \u2014 Run a single iteration of a benchmark.</p> </li> <li> <p>get_training_args \u2014 Get the training arguments for the current iteration.</p> </li> <li> <p>remove_extra_tensors_from_logits \u2014 If the logits are a tuple, return only the first element.</p> </li> </ul> <p> source finetune(model: BenchmarkModule, datasets: list['DatasetDict'], model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 list[dict[str, float]] </p> <p>Evaluate a model on a dataset through finetuning.</p> <p> Parameters </p> <ul> <li> <p>model :  BenchmarkModule \u2014 The model to evaluate.</p> </li> <li> <p>datasets :  list['DatasetDict'] \u2014 The datasets to use for training and evaluation.</p> </li> <li> <p>model_config :  ModelConfig \u2014 The configuration of the model.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[dict[str, float]] \u2014 A list of dicts containing the scores for each metric for each iteration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source finetune_single_iteration(model: BenchmarkModule | None, dataset: DatasetDict, training_args: TrainingArguments, model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 dict[str, float] </p> <p>Run a single iteration of a benchmark.</p> <p> Parameters </p> <ul> <li> <p>model :  BenchmarkModule | None \u2014 The model to use in the benchmark. If None then a new model will be loaded.</p> </li> <li> <p>dataset :  DatasetDict \u2014 The dataset to use for training and evaluation.</p> </li> <li> <p>training_args :  TrainingArguments \u2014 The training arguments.</p> </li> <li> <p>model_config :  ModelConfig \u2014 The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] \u2014 The scores for the test dataset.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>e</p> </li> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source get_training_args(benchmark_config: BenchmarkConfig, model_config: ModelConfig, iteration_idx: int, dtype: DataType, batch_size: int | None = None) \u2192 TrainingArguments </p> <p>Get the training arguments for the current iteration.</p> <p> Parameters </p> <ul> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> <li> <p>model_config :  ModelConfig \u2014 The model configuration.</p> </li> <li> <p>iteration_idx :  int \u2014 The index of the current iteration. This is only used to generate a unique random seed for the current iteration.</p> </li> <li> <p>dtype :  DataType \u2014 The data type to use for the model weights.</p> </li> <li> <p>batch_size :  int | None \u2014 The batch size to use for the current iteration, or None if the batch size in the benchmark config should be used.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>TrainingArguments \u2014 The training arguments for the current iteration.</p> </li> </ul> <p> source remove_extra_tensors_from_logits(logits: torch.Tensor | tuple[torch.Tensor, ...], labels: torch.Tensor) \u2192 torch.Tensor | tuple[torch.Tensor, ...] </p> <p>If the logits are a tuple, return only the first element.</p> <p> Parameters </p> <ul> <li> <p>logits :  torch.Tensor | tuple[torch.Tensor, ...] \u2014 The logits to process.</p> </li> <li> <p>labels :  torch.Tensor \u2014 The labels to use for the processing.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>torch.Tensor | tuple[torch.Tensor, ...] \u2014 The processed logits.</p> </li> </ul>"},{"location":"api/euroeval/generation/","title":"euroeval.generation","text":"euroeval.generation<p> source module euroeval.generation </p> <p>Functions related to text generation of models.</p> <p> Functions </p> <ul> <li> <p>generate \u2014 Evaluate a model on a dataset through generation.</p> </li> <li> <p>generate_single_iteration \u2014 Evaluate a model on a dataset in a single iteration through generation.</p> </li> <li> <p>debug_log \u2014 Log inputs and outputs for debugging purposes.</p> </li> </ul> <p> source generate(model: BenchmarkModule, datasets: list['DatasetDict'], model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 list[dict[str, float]] </p> <p>Evaluate a model on a dataset through generation.</p> <p> Parameters </p> <ul> <li> <p>model :  BenchmarkModule \u2014 The model to evaluate.</p> </li> <li> <p>datasets :  list['DatasetDict'] \u2014 The datasets to evaluate on.</p> </li> <li> <p>model_config :  ModelConfig \u2014 The configuration of the model.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The configuration of the benchmark.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The configuration of the dataset.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[dict[str, float]] \u2014 A list of dictionaries containing the test scores.</p> </li> </ul> <p> source generate_single_iteration(dataset: Dataset, model: BenchmarkModule, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig, cache: ModelCache) \u2192 dict[str, float] </p> <p>Evaluate a model on a dataset in a single iteration through generation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  Dataset \u2014 The dataset to evaluate on.</p> </li> <li> <p>model :  BenchmarkModule \u2014 The model to evaluate.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The configuration of the dataset.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The configuration of the benchmark.</p> </li> <li> <p>cache :  ModelCache \u2014 The model output cache.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] \u2014 A list of dictionaries containing the scores for each metric.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError</p> </li> </ul> <p> source debug_log(batch: dict[str, t.Any], model_output: GenerativeModelOutput, extracted_labels: list[dict | str | list[str]], dataset_config: DatasetConfig) \u2192 None </p> <p>Log inputs and outputs for debugging purposes.</p> <p> Parameters </p> <ul> <li> <p>batch :  dict[str, t.Any] \u2014 The batch of examples to evaluate on.</p> </li> <li> <p>model_output :  GenerativeModelOutput \u2014 The output of the model.</p> </li> <li> <p>extracted_labels :  list[dict | str | list[str]] \u2014 The extracted labels from the model output.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The configuration of the dataset.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul>"},{"location":"api/euroeval/generation_utils/","title":"euroeval.generation_utils","text":"euroeval.generation_utils<p> source module euroeval.generation_utils </p> <p>Utility functions related to generative models.</p> <p> Functions </p> <ul> <li> <p>extract_few_shot_examples \u2014 Extract few-shot examples from a dataset.</p> </li> <li> <p>apply_prompt \u2014 Apply prompt template to an example, potentially with few-shot examples.</p> </li> </ul> <p> source extract_few_shot_examples(dataset: DatasetDict, dataset_config: DatasetConfig, itr_idx: int) \u2192 list[dict[str, t.Any]] </p> <p>Extract few-shot examples from a dataset.</p> <p>This will always extract the examples from the training split.</p> <p>We ensure that the few-shot examples are unique by picking them one at a time.</p> <p> Parameters </p> <ul> <li> <p>dataset :  DatasetDict \u2014 The dataset to extract the few-shot examples from.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The dataset configuration.</p> </li> <li> <p>itr_idx :  int \u2014 The index of the dataset in the iterator.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[dict[str, t.Any]] \u2014 The few-shot examples.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source apply_prompt(examples: dict[str, t.Any], few_shot_examples: list[dict[str, t.Any]], model_config: ModelConfig, dataset_config: DatasetConfig, instruction_model: bool, always_populate_text_field: bool, tokenizer: PreTrainedTokenizer | None) \u2192 dict[str, t.Any] </p> <p>Apply prompt template to an example, potentially with few-shot examples.</p> <p> Parameters </p> <ul> <li> <p>examples :  dict[str, t.Any] \u2014 The examples to apply the few-shot examples to.</p> </li> <li> <p>few_shot_examples :  list[dict[str, t.Any]] \u2014 The few-shot examples to apply.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The dataset configuration.</p> </li> <li> <p>instruction_model :  bool \u2014 Whether the model is instruction-tuned.</p> </li> <li> <p>always_populate_text_field :  bool \u2014 Whether to always populate the 'text' field in the examples, as opposed to the 'messages' field.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer | None \u2014 The tokenizer to use for the model. If None, the tokenizer is not used.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, t.Any] \u2014 The example with the few-shot examples applied.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError</p> </li> <li> <p>NotImplementedError</p> </li> </ul>"},{"location":"api/euroeval/human_evaluation/","title":"euroeval.human_evaluation","text":"euroeval.human_evaluation<p> source module euroeval.human_evaluation </p> <p>Gradio app for conducting human evaluation of the tasks.</p> <p> Classes </p> <ul> <li> <p>HumanEvaluator \u2014 An app for evaluating human performance on the EuroEval benchmark.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>main \u2014 Start the Gradio app for human evaluation.</p> </li> </ul> <p> source class HumanEvaluator(annotator_id: int, title: str, description: str, dummy_model_id: str = 'mistralai/Mistral-7B-v0.1') </p> <p>An app for evaluating human performance on the EuroEval benchmark.</p> <p>Initialise the HumanEvaluator.</p> <p> Parameters </p> <ul> <li> <p>annotator_id :  int \u2014 The annotator ID for the evaluation.</p> </li> <li> <p>title :  str \u2014 The title of the app.</p> </li> <li> <p>description :  str \u2014 The description of the app.</p> </li> <li> <p>dummy_model_id :  str \u2014 The model ID to use for generating prompts.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>create_app \u2014 Create the Gradio app for human evaluation.</p> </li> <li> <p>update_dataset_choices \u2014 Update the dataset choices based on the selected language and task.</p> </li> <li> <p>update_dataset \u2014 Update the dataset based on a selected dataset name.</p> </li> <li> <p>add_entity_to_answer \u2014 Add an entity to the answer.</p> </li> <li> <p>reset_entities \u2014 Reset the entities in the answer.</p> </li> <li> <p>submit_answer \u2014 Submit an answer to the dataset.</p> </li> <li> <p>example_to_markdown \u2014 Convert an example to a Markdown string.</p> </li> <li> <p>compute_and_log_scores \u2014 Computes and logs the scores for the dataset.</p> </li> </ul> <p> source method HumanEvaluator.create_app() \u2192 gr.Blocks </p> <p>Create the Gradio app for human evaluation.</p> <p> Returns </p> <ul> <li> <p>gr.Blocks \u2014 The Gradio app for human evaluation.</p> </li> </ul> <p> source method HumanEvaluator.update_dataset_choices(language: str | None, task: str | None) \u2192 Dropdown </p> <p>Update the dataset choices based on the selected language and task.</p> <p> Parameters </p> <ul> <li> <p>language :  str | None \u2014 The language selected by the user.</p> </li> <li> <p>task :  str | None \u2014 The task selected by the user.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Dropdown \u2014 A list of dataset names that match the selected language and task.</p> </li> </ul> <p> source method HumanEvaluator.update_dataset(dataset_name: str, iteration: int) \u2192 tuple[Markdown, Markdown, Dropdown, Textbox, Button, Button, Textbox, Button] </p> <p>Update the dataset based on a selected dataset name.</p> <p> Parameters </p> <ul> <li> <p>dataset_name :  str \u2014 The dataset name selected by the user.</p> </li> <li> <p>iteration :  int \u2014 The iteration index of the datasets to evaluate.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[Markdown, Markdown, Dropdown, Textbox, Button, Button, Textbox, Button] \u2014 A tuple (task_examples, question, entity_type, entity, entity_add_button, entity_reset_button, answer, submit_button) for the selected dataset.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method HumanEvaluator.add_entity_to_answer(question: str, entity_type: str, entity: str, answer: str) \u2192 tuple[Textbox, Textbox] </p> <p>Add an entity to the answer.</p> <p> Parameters </p> <ul> <li> <p>question :  str \u2014 The current question.</p> </li> <li> <p>entity_type :  str \u2014 The entity type selected by the user.</p> </li> <li> <p>entity :  str \u2014 The entity provided by the user.</p> </li> <li> <p>answer :  str \u2014 The current answer.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[Textbox, Textbox] \u2014 A tuple (entity, answer) with a (blank) entity and answer.</p> </li> </ul> <p> source method HumanEvaluator.reset_entities() \u2192 Textbox </p> <p>Reset the entities in the answer.</p> <p> Returns </p> <ul> <li> <p>Textbox \u2014 A blank answer.</p> </li> </ul> <p> source method HumanEvaluator.submit_answer(dataset_name: str, question: str, answer: str, annotator_id: int) \u2192 tuple[str, str] </p> <p>Submit an answer to the dataset.</p> <p> Parameters </p> <ul> <li> <p>dataset_name :  str \u2014 The name of the dataset.</p> </li> <li> <p>question :  str \u2014 The question for the dataset.</p> </li> <li> <p>answer :  str \u2014 The answer to the question.</p> </li> <li> <p>annotator_id :  int \u2014 The annotator ID for the evaluation.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[str, str] \u2014 A tuple (question, answer), with <code>question</code> being the next question, and <code>answer</code> being an empty string.</p> </li> </ul> <p> source method HumanEvaluator.example_to_markdown(example: dict) \u2192 tuple[str, str] </p> <p>Convert an example to a Markdown string.</p> <p> Parameters </p> <ul> <li> <p>example :  dict \u2014 The example to convert.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[str, str] \u2014 A tuple (task_examples, question) for the example.</p> </li> </ul> <p> source method HumanEvaluator.compute_and_log_scores() \u2192 None </p> <p>Computes and logs the scores for the dataset.</p> <p> source main(annotator_id: int) \u2192 None </p> <p>Start the Gradio app for human evaluation.</p> <p> Raises </p> <ul> <li> <p>NeedsExtraInstalled</p> </li> </ul>"},{"location":"api/euroeval/languages/","title":"euroeval.languages","text":"euroeval.languages<p> source module euroeval.languages </p> <p>List of languages and their ISO 639-1 codes.</p> <p>Taken from https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes.</p> <p>Last updated 19 June 2022.</p> <p> Functions </p> <ul> <li> <p>get_all_languages \u2014 Get a list of all the languages.</p> </li> </ul> <p> source get_all_languages() \u2192 dict[str, Language] </p> <p>Get a list of all the languages.</p> <p> Returns </p> <ul> <li> <p>dict[str, Language] \u2014 A mapping between language codes and their configurations.</p> </li> </ul>"},{"location":"api/euroeval/metrics/","title":"euroeval.metrics","text":"euroeval.metrics<p> source module euroeval.metrics </p> <p>All the metrics used in EuroEval.</p> <p> Classes </p> <ul> <li> <p>Metric \u2014 Abstract base class for all metrics.</p> </li> <li> <p>HuggingFaceMetric \u2014 A metric which is implemented in the <code>evaluate</code> package.</p> </li> <li> <p>LLMAsAJudgeMetric \u2014 Use an LLM to judge the quality of the predictions.</p> </li> <li> <p>SpeedMetric \u2014 Speed metric.</p> </li> <li> <p>Fluency \u2014 Response format for the fluency metric.</p> </li> </ul> <p> source class Metric(name: str, pretty_name: str, postprocessing_fn: t.Callable[[float], tuple[float, str]] | None = None) </p> <p>Bases : abc.ABC</p> <p>Abstract base class for all metrics.</p> <p>Initialise the metric.</p> <p> Parameters </p> <ul> <li> <p>name :  str \u2014 The name of the metric in snake_case.</p> </li> <li> <p>pretty_name :  str \u2014 The pretty name of the metric, used for display purposes.</p> </li> <li> <p>postprocessing_fn :  t.Callable[[float], tuple[float, str]] | None \u2014 A function to apply to the metric scores after they are computed, taking the score to the postprocessed score along with its string representation. Defaults to x -&gt; (100 * x, f\"{x:.2%}\").</p> </li> </ul> <p> source class HuggingFaceMetric(name: str, pretty_name: str, huggingface_id: str, results_key: str, compute_kwargs: dict[str, t.Any] | None = None, postprocessing_fn: t.Callable[[float], tuple[float, str]] | None = None) </p> <p>Bases : Metric</p> <p>A metric which is implemented in the <code>evaluate</code> package.</p> <p>Initialise the Hugging Face metric.</p> <p> Attributes </p> <ul> <li> <p>name \u2014 The name of the metric in snake_case.</p> </li> <li> <p>pretty_name \u2014 The pretty name of the metric, used for display purposes.</p> </li> <li> <p>huggingface_id \u2014 The Hugging Face ID of the metric.</p> </li> <li> <p>results_key \u2014 The name of the key used to extract the metric scores from the results dictionary.</p> </li> <li> <p>compute_kwargs :  dict[str, t.Any] \u2014 Keyword arguments to pass to the metric's compute function. Defaults to an empty dictionary.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>name :  str \u2014 The name of the metric in snake_case.</p> </li> <li> <p>pretty_name :  str \u2014 The pretty name of the metric, used for display purposes.</p> </li> <li> <p>huggingface_id :  str \u2014 The Hugging Face ID of the metric.</p> </li> <li> <p>results_key :  str \u2014 The name of the key used to extract the metric scores from the results dictionary.</p> </li> <li> <p>compute_kwargs :  dict[str, t.Any] | None \u2014 Keyword arguments to pass to the metric's compute function. Defaults to an empty dictionary.</p> </li> <li> <p>postprocessing_fn :  t.Callable[[float], tuple[float, str]] | None \u2014 A function to apply to the metric scores after they are computed, taking the score to the postprocessed score along with its string representation. Defaults to x -&gt; (100 * x, f\"{x:.2%}\").</p> </li> </ul> <p> source class LLMAsAJudgeMetric(name: str, pretty_name: str, judge_id: str, judge_kwargs: dict[str, t.Any], user_prompt: str, response_format: t.Type[BaseModel], scoring_fn: t.Callable[[BaseModel], float], condition_formatting_fn: t.Callable[[str], str] = lambda x: x, system_prompt: str | None = None) </p> <p>Bases : Metric</p> <p>Use an LLM to judge the quality of the predictions.</p> <p>Initialise the LLM as a judge metric.</p> <p> Parameters </p> <ul> <li> <p>name :  str \u2014 The name of the metric in snake_case.</p> </li> <li> <p>pretty_name :  str \u2014 The pretty name of the metric, used for display purposes.</p> </li> <li> <p>judge_id :  str \u2014 The model ID of the LLM to use as a judge.</p> </li> <li> <p>judge_kwargs :  dict[str, t.Any] \u2014 Generation parameters for the judge model, such as temperature.</p> </li> <li> <p>user_prompt :  str \u2014 The user prompt to use for the judge model. The prompt should be formatted with the variables <code>prediction</code> and <code>condition</code>, to include the model predictions and a description of what the prediction should be judged on, respectively. If the condition is not needed, it can be omitted from the prompt, but the <code>prediction</code> variable must still be present.</p> </li> <li> <p>response_format :  t.Type[BaseModel] \u2014 The response format to use for the judge model. This should be a Pydantic model that defines the expected structure of the judge's response.</p> </li> <li> <p>scoring_fn :  t.Callable[[BaseModel], float] \u2014 A function that takes the judge's response and returns a score.</p> </li> <li> <p>condition_formatting_fn :  optional \u2014 A function to format the condition string before it is included in the user prompt. Defaults to a no-op function that returns the input unchanged.</p> </li> <li> <p>system_prompt :  optional \u2014 The system prompt to use for the judge model. If not provided, no system prompt will be used.</p> </li> </ul> <p> source class SpeedMetric(name: str, pretty_name: str) </p> <p>Bases : Metric</p> <p>Speed metric.</p> <p>Initialise the speed metric.</p> <p> Parameters </p> <ul> <li> <p>name :  str \u2014 The name of the metric in snake_case.</p> </li> <li> <p>pretty_name :  str \u2014 The pretty name of the metric, used for display purposes.</p> </li> </ul> <p> source class Fluency(**data: Any) </p> <p>Bases : BaseModel</p> <p>Response format for the fluency metric.</p> <p>Create a new model by parsing and validating input data from keyword arguments.</p> <p>Raises [<code>ValidationError</code>][pydantic_core.ValidationError] if the input data cannot be validated to form a valid model.</p> <p><code>self</code> is explicitly positional-only to allow <code>self</code> as a field name.</p> <p> Attributes </p> <ul> <li> <p>fluency :  t.Annotated[int, Field(ge=1, le=5)] \u2014 The fluency rating, an integer between 1 and 5.</p> </li> <li> <p>model_config :  ClassVar[ConfigDict] \u2014 Configuration for the model, should be a dictionary conforming to [<code>ConfigDict</code>][pydantic.config.ConfigDict].</p> </li> <li> <p>model_extra :  dict[str, Any] | None \u2014 Get extra fields set during validation.</p> </li> <li> <p>model_fields_set :  set[str] \u2014 Returns the set of fields that have been explicitly set on this model instance.</p> </li> </ul>"},{"location":"api/euroeval/model_cache/","title":"euroeval.model_cache","text":"euroeval.model_cache<p> source module euroeval.model_cache </p> <p>ModelCache class for caching model outputs.</p> <p> Classes </p> <ul> <li> <p>ModelCache \u2014 A cache for model outputs.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>split_dataset_into_cached_and_non_cached \u2014 Split a dataset into a cached and non-cached part.</p> </li> <li> <p>load_cached_model_outputs \u2014 Load the cached model outputs.</p> </li> </ul> <p> source class ModelCache(model_cache_dir: Path, cache_name: str, max_generated_tokens: int) </p> <p>A cache for model outputs.</p> <p>Initialise the model output cache.</p> <p> Attributes </p> <ul> <li> <p>model_cache_dir \u2014 The directory to store the cache in.</p> </li> <li> <p>cache_path \u2014 The path to the cache file.</p> </li> <li> <p>cache \u2014 The model output cache.</p> </li> <li> <p>max_generated_tokens \u2014 The maximum number of tokens to generate for each example.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>model_cache_dir :  Path \u2014 The directory to store the cache in.</p> </li> <li> <p>cache_name :  str \u2014 The name of the cache file.</p> </li> <li> <p>max_generated_tokens :  int \u2014 The maximum number of tokens to generate for each example.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>load \u2014 Load the model output cache.</p> </li> <li> <p>save \u2014 Save the model output cache to disk.</p> </li> <li> <p>remove \u2014 Remove the cache from memory and delete it from disk.</p> </li> <li> <p>add_to_cache \u2014 Add the model input/output to the cache.</p> </li> </ul> <p> source method ModelCache.load() \u2192 None </p> <p>Load the model output cache.</p> <p> source method ModelCache.save() \u2192 None </p> <p>Save the model output cache to disk.</p> <p> source method ModelCache.remove() \u2192 None </p> <p>Remove the cache from memory and delete it from disk.</p> <p> source method ModelCache.add_to_cache(model_inputs: dict, model_output: GenerativeModelOutput) \u2192 None </p> <p>Add the model input/output to the cache.</p> <p> Parameters </p> <ul> <li> <p>model_inputs :  dict \u2014 The model inputs.</p> </li> <li> <p>model_output :  GenerativeModelOutput \u2014 The model output.</p> </li> </ul> <p> source split_dataset_into_cached_and_non_cached(dataset: Dataset, cache: ModelCache) \u2192 tuple['Dataset', 'Dataset'] </p> <p>Split a dataset into a cached and non-cached part.</p> <p> Parameters </p> <ul> <li> <p>dataset :  Dataset \u2014 The dataset to split.</p> </li> <li> <p>cache :  ModelCache \u2014 The model output cache.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple['Dataset', 'Dataset'] \u2014 The cached and non-cached parts of the dataset.</p> </li> </ul> <p> source load_cached_model_outputs(cached_dataset: Dataset, cache: ModelCache) \u2192 GenerativeModelOutput </p> <p>Load the cached model outputs.</p> <p> Parameters </p> <ul> <li> <p>cached_dataset :  Dataset \u2014 The dataset containing the cached examples.</p> </li> <li> <p>cache :  ModelCache \u2014 The model output cache.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>GenerativeModelOutput \u2014 The model output containing the cached sequences.</p> </li> </ul>"},{"location":"api/euroeval/model_config/","title":"euroeval.model_config","text":"euroeval.model_config<p> source module euroeval.model_config </p> <p>Functions related to getting the model configuration.</p> <p> Functions </p> <ul> <li> <p>get_model_config \u2014 Fetches configuration for a model.</p> </li> </ul> <p> source get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetches configuration for a model.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014 The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The configuration of the benchmark.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel \u2014 If all model setups can handle the model, but the model does not exist.</p> </li> </ul>"},{"location":"api/euroeval/model_loading/","title":"euroeval.model_loading","text":"euroeval.model_loading<p> source module euroeval.model_loading </p> <p>Functions related to the loading of models.</p> <p> Functions </p> <ul> <li> <p>load_model \u2014 Load a model.</p> </li> </ul> <p> source load_model(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 BenchmarkModule </p> <p>Load a model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014 The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014 The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BenchmarkModule \u2014 The model.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul>"},{"location":"api/euroeval/scores/","title":"euroeval.scores","text":"euroeval.scores<p> source module euroeval.scores </p> <p>Aggregation of raw scores into the mean and a confidence interval.</p> <p> Functions </p> <ul> <li> <p>log_scores \u2014 Log the scores.</p> </li> <li> <p>aggregate_scores \u2014 Helper function to compute the mean with confidence intervals.</p> </li> </ul> <p> source log_scores(dataset_name: str, metrics: list['Metric'], scores: list[dict[str, float]], model_id: str, model_revision: str) \u2192 ScoreDict </p> <p>Log the scores.</p> <p> Parameters </p> <ul> <li> <p>dataset_name :  str \u2014 Name of the dataset.</p> </li> <li> <p>metrics :  list['Metric'] \u2014 List of metrics to log.</p> </li> <li> <p>scores :  list[dict[str, float]] \u2014 The scores that are to be logged. This is a list of dictionaries full of scores.</p> </li> <li> <p>model_id :  str \u2014 The model ID of the model that was evaluated.</p> </li> <li> <p>model_revision :  str \u2014 The revision of the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ScoreDict \u2014 A dictionary with keys 'raw_scores' and 'total', with 'raw_scores' being identical to <code>scores</code> and 'total' being a dictionary with the aggregated scores (means and standard errors).</p> </li> </ul> <p> source aggregate_scores(scores: list[dict[str, float]], metric: Metric) \u2192 tuple[float, float] </p> <p>Helper function to compute the mean with confidence intervals.</p> <p> Parameters </p> <ul> <li> <p>scores :  list[dict[str, float]] \u2014 Dictionary with the names of the metrics as keys, of the form \"_\", such as \"val_f1\", and values the metric values. <li> <p>metric :  Metric \u2014 The metric, which is used to collect the correct metric from <code>scores</code>.</p> </li> <p> Returns </p> <ul> <li> <p>tuple[float, float] \u2014 A pair of floats, containing the score and the radius of its 95% confidence interval.</p> </li> </ul>"},{"location":"api/euroeval/speed_benchmark/","title":"euroeval.speed_benchmark","text":"euroeval.speed_benchmark<p> source module euroeval.speed_benchmark </p> <p>Benchmarking model inference speed.</p> <p> Functions </p> <ul> <li> <p>benchmark_speed \u2014 Benchmark model inference speed.</p> </li> <li> <p>benchmark_speed_single_iteration \u2014 Run a single iteration of the speed benchmark.</p> </li> </ul> <p> source benchmark_speed(model: BenchmarkModule, benchmark_config: BenchmarkConfig) \u2192 list[dict[str, float]] </p> <p>Benchmark model inference speed.</p> <p> Parameters </p> <ul> <li> <p>model :  BenchmarkModule \u2014 Model to use.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 Configuration for the benchmark.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[dict[str, float]] \u2014 Dictionary of scores.</p> </li> </ul> <p> source benchmark_speed_single_iteration(model: BenchmarkModule, itr_idx: int) \u2192 dict[str, float] </p> <p>Run a single iteration of the speed benchmark.</p> <p> Parameters </p> <ul> <li> <p>model :  BenchmarkModule \u2014 The model to use in the benchmark.</p> </li> <li> <p>itr_idx :  int \u2014 The index of the iteration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] \u2014 A dictionary containing the scores for the current iteration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> <li> <p>ValueError</p> </li> </ul>"},{"location":"api/euroeval/tasks/","title":"euroeval.tasks","text":"euroeval.tasks<p> source module euroeval.tasks </p> <p>All benchmarks tasks used in EuroEval.</p> <p> Functions </p> <ul> <li> <p>get_all_tasks \u2014 Get a list of all the dataset tasks.</p> </li> </ul> <p> source get_all_tasks() \u2192 dict[str, Task] </p> <p>Get a list of all the dataset tasks.</p> <p> Returns </p> <ul> <li> <p>dict[str, Task] \u2014 A mapping between names of dataset tasks and their configurations.</p> </li> </ul>"},{"location":"api/euroeval/tokenization_utils/","title":"euroeval.tokenization_utils","text":"euroeval.tokenization_utils<p> source module euroeval.tokenization_utils </p> <p>Utility functions related to tokenization.</p> <p> Functions </p> <ul> <li> <p>get_special_token_metadata \u2014 Get the special token metadata for a tokenizer.</p> </li> <li> <p>should_prompts_be_stripped \u2014 Determine if we should strip the prompts for few-shot evaluation.</p> </li> <li> <p>should_prefix_space_be_added_to_labels \u2014 Determine if we should add a prefix space to the labels.</p> </li> <li> <p>get_bos_token \u2014 Get the beginning-of-sequence token from a tokenizer.</p> </li> <li> <p>get_eos_token \u2014 Get the end-of-sequence token from a tokenizer.</p> </li> <li> <p>get_pad_token \u2014 Get the padding token from a tokenizer.</p> </li> <li> <p>get_end_of_chat_token_ids \u2014 Get the end token ID for chat models.</p> </li> <li> <p>get_first_label_token_mapping \u2014 Check if the model should output scores.</p> </li> </ul> <p> source get_special_token_metadata(tokenizer: PreTrainedTokenizerBase) \u2192 dict </p> <p>Get the special token metadata for a tokenizer.</p> <p> Parameters </p> <ul> <li> <p>tokenizer :  PreTrainedTokenizerBase \u2014 The tokenizer.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict \u2014 The special token metadata.</p> </li> </ul> <p> source should_prompts_be_stripped(labels_to_be_generated: list[str], tokenizer: PreTrainedTokenizer) \u2192 bool </p> <p>Determine if we should strip the prompts for few-shot evaluation.</p> <p>This is the case if the tokenizer needs to include the space as part of the label token. The strategy is thus to tokenize a label with a preceeding colon (as in the prompts), i.e., \": positive\", and check if the tokenization starts with the tokens of \": \". If this is the case, then we should not strip the prompts, since the tokenizer produces the whitespace token separately.</p> <p> Parameters </p> <ul> <li> <p>labels_to_be_generated :  list[str] \u2014 The labels that are to be generated.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer \u2014 The tokenizer used to tokenize the labels.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool \u2014 Whether we should strip the prompts.</p> </li> </ul> <p> source should_prefix_space_be_added_to_labels(labels_to_be_generated: list[str], tokenizer: PreTrainedTokenizer) \u2192 bool </p> <p>Determine if we should add a prefix space to the labels.</p> <p>This is the case if the prompts are stripped and the tokenizer doesn't automatically add prefix whitespaces to the labels.</p> <p> Parameters </p> <ul> <li> <p>labels_to_be_generated :  list[str] \u2014 The labels that are to be generated.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer \u2014 The tokenizer used to tokenize the labels.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool \u2014 Whether we should add a prefix space to the labels.</p> </li> </ul> <p> source get_bos_token(tokenizer: PreTrainedTokenizer) \u2192 tuple[str, int] | tuple[None, None] </p> <p>Get the beginning-of-sequence token from a tokenizer.</p> <p> Parameters </p> <ul> <li> <p>tokenizer :  PreTrainedTokenizer \u2014 The tokenizer.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[str, int] | tuple[None, None] \u2014 A pair (token, token_id) representing the beginning-of-sequence token and its token ID, or (None, None) if no BOS token is found.</p> </li> </ul> <p> source get_eos_token(tokenizer: PreTrainedTokenizer) \u2192 tuple[str, int] | tuple[None, None] </p> <p>Get the end-of-sequence token from a tokenizer.</p> <p> Parameters </p> <ul> <li> <p>tokenizer :  PreTrainedTokenizer \u2014 The tokenizer.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[str, int] | tuple[None, None] \u2014 A pair (token, token_id) representing the end-of-sequence token and its token ID, or (None, None) if no EOS token is found.</p> </li> </ul> <p> source get_pad_token(tokenizer: PreTrainedTokenizer) \u2192 tuple[str, int] | tuple[None, None] </p> <p>Get the padding token from a tokenizer.</p> <p> Parameters </p> <ul> <li> <p>tokenizer :  PreTrainedTokenizer \u2014 The tokenizer.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[str, int] | tuple[None, None] \u2014 A pair (token, token_id) representing the padding token and its token ID, or (None, None) if no padding token is found.</p> </li> </ul> <p> source get_end_of_chat_token_ids(tokenizer: PreTrainedTokenizer) \u2192 list[int] | None </p> <p>Get the end token ID for chat models.</p> <p>This is only relevant for tokenizers with a chat template.</p> <p> Parameters </p> <ul> <li> <p>tokenizer :  PreTrainedTokenizer \u2014 The tokenizer.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[int] | None \u2014 The token IDs used to end chats, or None if the tokenizer does not have a chat template.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014 If the end-of-chat token could not be located.</p> </li> </ul> <p> source get_first_label_token_mapping(dataset_config: DatasetConfig, model_config: ModelConfig, tokenizer: PreTrainedTokenizer | None, generative_type: GenerativeType | None) \u2192 dict[str, str] | bool </p> <p>Check if the model should output scores.</p> <p> Parameters </p> <ul> <li> <p>dataset_config :  DatasetConfig \u2014 The dataset configuration.</p> </li> <li> <p>model_config :  ModelConfig \u2014 The model configuration.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer | None \u2014 The tokenizer, or None if not available.</p> </li> <li> <p>generative_type :  GenerativeType | None \u2014 The generative type, or None if not available.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, str] | bool \u2014 A mapping from labels to the first token in each label, or alternatively a Boolean value indicating whether the model should output scores (if the mapping is outputted then the model will always output scores).</p> </li> </ul>"},{"location":"api/euroeval/types/","title":"euroeval.types","text":"euroeval.types<p> source module euroeval.types </p> <p>Types used throughout the project.</p> <p> Classes </p> <ul> <li> <p>ComputeMetricsFunction \u2014 A function used to compute the metrics.</p> </li> <li> <p>ExtractLabelsFunction \u2014 A function used to extract the labels from the generated output.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>is_list_of_int \u2014 Check if an object is a list of integers.</p> </li> <li> <p>is_list_of_list_of_int \u2014 Check if an object is a list of list of integers.</p> </li> <li> <p>is_list_of_str \u2014 Check if an object is a list of integers.</p> </li> </ul> <p> source class ComputeMetricsFunction() </p> <p>Bases : t.Protocol</p> <p>A function used to compute the metrics.</p> <p> source class ExtractLabelsFunction() </p> <p>Bases : t.Protocol</p> <p>A function used to extract the labels from the generated output.</p> <p> source is_list_of_int(x: object) \u2192 t.TypeGuard[list[int]] </p> <p>Check if an object is a list of integers.</p> <p> Parameters </p> <ul> <li> <p>x :  object \u2014 The object to check.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>t.TypeGuard[list[int]] \u2014 Whether the object is a list of integers.</p> </li> </ul> <p> source is_list_of_list_of_int(x: object) \u2192 t.TypeGuard[list[list[int]]] </p> <p>Check if an object is a list of list of integers.</p> <p> Parameters </p> <ul> <li> <p>x :  object \u2014 The object to check.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>t.TypeGuard[list[list[int]]] \u2014 Whether the object is a list of list of integers.</p> </li> </ul> <p> source is_list_of_str(x: object) \u2192 t.TypeGuard[list[str]] </p> <p>Check if an object is a list of integers.</p> <p> Parameters </p> <ul> <li> <p>x :  object \u2014 The object to check.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>t.TypeGuard[list[str]] \u2014 Whether the object is a list of strings.</p> </li> </ul>"},{"location":"api/euroeval/utils/","title":"euroeval.utils","text":"euroeval.utils<p> source module euroeval.utils </p> <p>Utility functions to be used in other scripts.</p> <p> Classes </p> <ul> <li> <p>HiddenPrints \u2014 Context manager which removes all terminal output.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>create_model_cache_dir \u2014 Create cache directory for a model.</p> </li> <li> <p>clear_memory \u2014 Clears the memory of unused items.</p> </li> <li> <p>enforce_reproducibility \u2014 Ensures reproducibility of experiments.</p> </li> <li> <p>block_terminal_output \u2014 Blocks libraries from writing output to the terminal.</p> </li> <li> <p>get_class_by_name \u2014 Get a class by its name.</p> </li> <li> <p>get_min_cuda_compute_capability \u2014 Gets the lowest cuda capability.</p> </li> <li> <p>internet_connection_available \u2014 Checks if internet connection is available by pinging google.com.</p> </li> <li> <p>raise_if_model_output_contains_nan_values \u2014 Raise an exception if the model output contains NaN values.</p> </li> <li> <p>scramble \u2014 Scramble a string in a bijective manner.</p> </li> <li> <p>unscramble \u2014 Unscramble a string in a bijective manner.</p> </li> <li> <p>log_once \u2014 Log a message once.</p> </li> <li> <p>get_package_version \u2014 Get the version of a package.</p> </li> <li> <p>safe_run \u2014 Run a coroutine, ensuring that the event loop is always closed when we're done.</p> </li> <li> <p>add_semaphore_and_catch_exception \u2014 Run a coroutine with a semaphore.</p> </li> </ul> <p> source create_model_cache_dir(cache_dir: str, model_id: str) \u2192 str </p> <p>Create cache directory for a model.</p> <p> Parameters </p> <ul> <li> <p>cache_dir :  str \u2014 The cache directory.</p> </li> <li> <p>model_id :  str \u2014 The model ID.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The path to the cache directory.</p> </li> </ul> <p> source clear_memory() \u2192 None </p> <p>Clears the memory of unused items.</p> <p> source enforce_reproducibility(seed: int = 4242) \u2192 np.random.Generator </p> <p>Ensures reproducibility of experiments.</p> <p> Parameters </p> <ul> <li> <p>seed :  int \u2014 Seed for the random number generator.</p> </li> </ul> <p> source block_terminal_output() \u2192 None </p> <p>Blocks libraries from writing output to the terminal.</p> <p>This filters warnings from some libraries, sets the logging level to ERROR for some libraries, disabled tokeniser progress bars when using Hugging Face tokenisers, and disables most of the logging from the <code>transformers</code> library.</p> <p> source get_class_by_name(class_name: str | list[str], module_name: str) \u2192 t.Type | None </p> <p>Get a class by its name.</p> <p> Parameters </p> <ul> <li> <p>class_name :  str | list[str] \u2014 The name of the class, written in kebab-case. The corresponding class name must be the same, but written in PascalCase, and lying in a module with the same name, but written in snake_case. If a list of strings is passed, the first class that is found is returned.</p> </li> <li> <p>module_name :  str \u2014 The name of the module where the class is located.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>t.Type | None \u2014 The class. If the class is not found, None is returned.</p> </li> </ul> <p> source get_min_cuda_compute_capability() \u2192 float | None </p> <p>Gets the lowest cuda capability.</p> <p> Returns </p> <ul> <li> <p>float | None \u2014 Device capability as float, or None if CUDA is not available.</p> </li> </ul> <p> source internet_connection_available() \u2192 bool </p> <p>Checks if internet connection is available by pinging google.com.</p> <p> Returns </p> <ul> <li> <p>bool \u2014 Whether or not internet connection is available.</p> </li> </ul> <p> source class HiddenPrints() </p> <p>Context manager which removes all terminal output.</p> <p> source raise_if_model_output_contains_nan_values(model_output: Predictions) \u2192 None </p> <p>Raise an exception if the model output contains NaN values.</p> <p> Parameters </p> <ul> <li> <p>model_output :  Predictions \u2014 The model output to check.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>If the model output contains NaN values.</p> </li> <li> <p>NaNValueInModelOutput</p> </li> </ul> <p> source scramble(text: str) \u2192 str </p> <p>Scramble a string in a bijective manner.</p> <p> Parameters </p> <ul> <li> <p>text :  str \u2014 The string to scramble.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The scrambled string.</p> </li> </ul> <p> source unscramble(scrambled_text: str) \u2192 str </p> <p>Unscramble a string in a bijective manner.</p> <p> Parameters </p> <ul> <li> <p>scrambled_text :  str \u2014 The scrambled string to unscramble.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The unscrambled string.</p> </li> </ul> <p> source log_once(message: str, level: int = logging.INFO) \u2192 None </p> <p>Log a message once.</p> <p>This is ensured by caching the input/output pairs of this function, using the <code>functools.cache</code> decorator.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014 The message to log.</p> </li> <li> <p>level :  int \u2014 The logging level. Defaults to logging.INFO.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError</p> </li> </ul> <p> source get_package_version(package_name: str) \u2192 str | None </p> <p>Get the version of a package.</p> <p> Parameters </p> <ul> <li> <p>package_name :  str \u2014 The name of the package.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str | None \u2014 The version of the package, or None if the package is not installed.</p> </li> </ul> <p> source safe_run(coroutine: t.Coroutine[t.Any, t.Any, T]) \u2192 T </p> <p>Run a coroutine, ensuring that the event loop is always closed when we're done.</p> <p> Parameters </p> <ul> <li> <p>coroutine :  t.Coroutine[t.Any, t.Any, T] \u2014 The coroutine to run.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>T \u2014 The result of the coroutine.</p> </li> </ul> <p> source async add_semaphore_and_catch_exception(coroutine: t.Coroutine[t.Any, t.Any, T], semaphore: asyncio.Semaphore) \u2192 T | Exception </p> <p>Run a coroutine with a semaphore.</p> <p> Parameters </p> <ul> <li> <p>coroutine :  t.Coroutine[t.Any, t.Any, T] \u2014 The coroutine to run.</p> </li> <li> <p>semaphore :  asyncio.Semaphore \u2014 The semaphore to use.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>T | Exception \u2014 The result of the coroutine.</p> </li> </ul>"}]}
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#evaluation-of-pretrained-language-models-on-mono-or-multilingual-language-tasks","title":"Evaluation of pretrained language models on mono- or multilingual language tasks.","text":""},{"location":"#maintainers","title":"Maintainers","text":"<ul> <li>Dan Saattrup Nielsen (@saattrupdan,   dan.nielsen@alexandra.dk)</li> <li>Kenneth Enevoldsen (@KennethEnevoldsen,   kenneth.enevoldsen@cas.au.dk)</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To install the package simply write the following command in your favorite terminal: <pre><code>$ pip install scandeval[all]\n</code></pre></p> <p>This will install the ScandEval package with all extras. You can also install the minimal version by leaving out the <code>[all]</code>, in which case the package will let you know when an evaluation requires a certain extra dependency, and how you install it.</p>"},{"location":"#quickstart","title":"Quickstart","text":""},{"location":"#benchmarking-from-the-command-line","title":"Benchmarking from the Command Line","text":"<p>The easiest way to benchmark pretrained models is via the command line interface. After having installed the package, you can benchmark your favorite model like so: <pre><code>$ scandeval --model &lt;model-id&gt;\n</code></pre></p> <p>Here <code>model</code> is the HuggingFace model ID, which can be found on the HuggingFace Hub. By default this will benchmark the model on all the tasks available. If you want to benchmark on a particular task, then use the <code>--task</code> argument: <pre><code>$ scandeval --model &lt;model-id&gt; --task sentiment-classification\n</code></pre></p> <p>We can also narrow down which languages we would like to benchmark on. This can be done by setting the <code>--language</code> argument. Here we thus benchmark the model on the Danish sentiment classification task: <pre><code>$ scandeval --model &lt;model-id&gt; --task sentiment-classification --language da\n</code></pre></p> <p>Multiple models, datasets and/or languages can be specified by just attaching multiple arguments. Here is an example with two models: <pre><code>$ scandeval --model &lt;model-id1&gt; --model &lt;model-id2&gt;\n</code></pre></p> <p>The specific model version/revision to use can also be added after the suffix '@': <pre><code>$ scandeval --model &lt;model-id&gt;@&lt;commit&gt;\n</code></pre></p> <p>This can be a branch name, a tag name, or a commit id. It defaults to 'main' for latest.</p> <p>See all the arguments and options available for the <code>scandeval</code> command by typing <pre><code>$ scandeval --help\n</code></pre></p>"},{"location":"#benchmarking-from-a-script","title":"Benchmarking from a Script","text":"<p>In a script, the syntax is similar to the command line interface. You simply initialise an object of the <code>Benchmarker</code> class, and call this benchmark object with your favorite model: <pre><code>&gt;&gt;&gt; from scandeval import Benchmarker\n&gt;&gt;&gt; benchmark = Benchmarker()\n&gt;&gt;&gt; benchmark(model=\"&lt;model&gt;\")\n</code></pre></p> <p>To benchmark on a specific task and/or language, you simply specify the <code>task</code> or <code>language</code> arguments, shown here with same example as above: <pre><code>&gt;&gt;&gt; benchmark(model=\"&lt;model&gt;\", task=\"sentiment-classification\", language=\"da\")\n</code></pre></p> <p>If you want to benchmark a subset of all the models on the Hugging Face Hub, you can simply leave out the <code>model</code> argument. In this example, we're benchmarking all Danish models on the Danish sentiment classification task: <pre><code>&gt;&gt;&gt; benchmark(task=\"sentiment-classification\", language=\"da\")\n</code></pre></p>"},{"location":"#benchmarking-from-docker","title":"Benchmarking from Docker","text":"<p>A Dockerfile is provided in the repo, which can be downloaded and run, without needing to clone the repo and installing from source. This can be fetched programmatically by running the following: <pre><code>$ wget https://raw.githubusercontent.com/ScandEval/ScandEval/main/Dockerfile.cuda\n</code></pre></p> <p>Next, to be able to build the Docker image, first ensure that the NVIDIA Container Toolkit is installed and configured. Ensure that the the CUDA version stated at the top of the Dockerfile matches the CUDA version installed (which you can check using <code>nvidia-smi</code>). After that, we build the image as follows: <pre><code>$ docker build --pull -t scandeval -f Dockerfile.cuda .\n</code></pre></p> <p>With the Docker image built, we can now evaluate any model as follows: <pre><code>$ docker run -e args=\"&lt;scandeval-arguments&gt;\" --gpus 1 --name scandeval --rm scandeval\n</code></pre></p> <p>Here <code>&lt;scandeval-arguments&gt;</code> consists of the arguments added to the <code>scandeval</code> CLI argument. This could for instance be <code>--model &lt;model-id&gt; --task sentiment-classification</code>.</p>"},{"location":"#special-thanks-pray","title":"Special Thanks :pray:","text":"<ul> <li>Thanks to OpenAI for sponsoring OpenAI credits as part of their   Researcher Access Program.</li> <li>Thanks to UWV and KU   Leuven for sponsoring the Azure OpenAI   credits used to evaluate GPT-4-turbo in Dutch.</li> <li>Thanks to Mi\u00f0eind for sponsoring the OpenAI   credits used to evaluate GPT-4-turbo in Icelandic and Faroese.</li> <li>Thanks to CHC for sponsoring the OpenAI credits used to   evaluate GPT-4-turbo in German.</li> </ul>"},{"location":"#citing-scandeval","title":"Citing ScandEval","text":"<p>If you want to cite the framework then feel free to use this:</p> <pre><code>@article{nielsen2024encoder,\n  title={Encoder vs Decoder: Comparative Analysis of Encoder and Decoder Language Models on Multilingual NLU Tasks},\n  author={Nielsen, Dan Saattrup and Enevoldsen, Kenneth and Schneider-Kamp, Peter},\n  journal={arXiv preprint arXiv:2406.13469},\n  year={2024}\n}\n@inproceedings{nielsen2023scandeval,\n  author = {Nielsen, Dan Saattrup},\n  booktitle = {Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  month = may,\n  pages = {185--201},\n  title = {{ScandEval: A Benchmark for Scandinavian Natural Language Processing}},\n  year = {2023}\n}\n</code></pre>"},{"location":"#remarks","title":"Remarks","text":"<p>The image used in the logo has been created by the amazing Scandinavia and the World team. Go check them out!</p>"},{"location":"api/scandeval/","title":"scandeval","text":"scandeval<p> source package scandeval </p> <p>ScandEval - A benchmarking framework for language models.</p> <p> Classes </p> <ul> <li> <p>Benchmarker \u2014 Benchmarking all the Scandinavian language models.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>block_terminal_output \u2014 Blocks libraries from writing output to the terminal.</p> </li> </ul> <p> source class Benchmarker(save_results: bool = True, task: str | list[str] | None = None, dataset: list[str] | str | None = None, language: str | list[str] = 'all', model_language: str | list[str] | None = None, dataset_language: str | list[str] | None = None, framework: Framework | str | None = None, device: Device | None = None, batch_size: int = 32, raise_errors: bool = False, cache_dir: str = '.scandeval_cache', api_key: str | None = None, force: bool = False, verbose: bool = False, trust_remote_code: bool = False, load_in_4bit: bool | None = None, use_flash_attention: bool | None = None, clear_model_cache: bool = False, evaluate_test_split: bool = False, few_shot: bool = True, num_iterations: int = 10, api_base: str | None = None, api_version: str | None = None, debug: bool = False, run_with_cli: bool = False) </p> <p>Benchmarking all the Scandinavian language models.</p> <p>Initialise the benchmarker.</p> <p> Attributes </p> <ul> <li> <p>benchmark_config_default_params \u2014</p> <p>The default parameters for the benchmark configuration.</p> </li> <li> <p>benchmark_config \u2014</p> <p>The benchmark configuration.</p> </li> <li> <p>force \u2014</p> <p>Whether to force evaluations of models, even if they have been benchmarked already.</p> </li> <li> <p>results_path \u2014</p> <p>The path to the results file.</p> </li> <li> <p>benchmark_results :  list[BenchmarkResult] \u2014</p> <p>The benchmark results.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>progress_bar :  bool \u2014</p> <p>Whether progress bars should be shown. Defaults to True.</p> </li> <li> <p>save_results :  bool \u2014</p> <p>Whether to save the benchmark results to 'scandeval_benchmark_results.jsonl'. Defaults to True.</p> </li> <li> <p>task :  str | list[str] | None \u2014</p> <p>The tasks benchmark the model(s) on. Mutually exclusive with <code>dataset</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked.</p> </li> <li> <p>dataset :  list[str] | str | None \u2014</p> <p>The datasets to benchmark on. Mutually exclusive with <code>task</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked.</p> </li> <li> <p>language :  str | list[str] \u2014</p> <p>The language codes of the languages to include, both for models and datasets. Set this to 'all' if all languages should be considered. Defaults to \"all\".</p> </li> <li> <p>model_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for models. If specified then this overrides the <code>language</code> parameter for model languages. Defaults to None.</p> </li> <li> <p>dataset_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for datasets. If specified then this overrides the <code>language</code> parameter for dataset languages. Defaults to None.</p> </li> <li> <p>framework :  Framework | str | None \u2014</p> <p>The model framework to use. Only relevant if <code>model-id</code> refers to a local path. Otherwise, the framework will be set automatically. Defaults to None.</p> </li> <li> <p>device :  Device | None \u2014</p> <p>The device to use for benchmarking. Defaults to None.</p> </li> <li> <p>batch_size :  int \u2014</p> <p>The batch size to use. Defaults to 32.</p> </li> <li> <p>raise_errors :  bool \u2014</p> <p>Whether to raise errors instead of skipping the model evaluation. Defaults to False.</p> </li> <li> <p>cache_dir :  str \u2014</p> <p>Directory to store cached models. Defaults to '.scandeval_cache'.</p> </li> <li> <p>api_key :  str | None \u2014</p> <p>The API key to use for a given inference API.</p> </li> <li> <p>force :  bool \u2014</p> <p>Whether to force evaluations of models, even if they have been benchmarked already. Defaults to False.</p> </li> <li> <p>verbose :  bool \u2014</p> <p>Whether to output additional output. This is automatically set if <code>debug</code> is True. Defaults to False.</p> </li> <li> <p>trust_remote_code :  bool \u2014</p> <p>Whether to trust remote code when loading models. Defaults to False.</p> </li> <li> <p>load_in_4bit :  bool | None \u2014</p> <p>Whether to load models in 4-bit precision. If None then this will be done if CUDA is available and the model is a decoder model. Defaults to None.</p> </li> <li> <p>use_flash_attention :  bool | None \u2014</p> <p>Whether to use Flash Attention. If None then it will be used if it is installed and the model is a decoder model. Defaults to None.</p> </li> <li> <p>clear_model_cache :  bool \u2014</p> <p>Whether to clear the model cache after benchmarking each model. Defaults to False.</p> </li> <li> <p>evaluate_test_split :  bool \u2014</p> <p>Whether to evaluate the test split of the datasets. Defaults to False.</p> </li> <li> <p>few_shot :  bool \u2014</p> <p>Whether to only evaluate the model using few-shot evaluation. Only relevant if the model is generative. Defaults to True.</p> </li> <li> <p>num_iterations :  int \u2014</p> <p>The number of times each model should be evaluated. This is only meant to be used for power users, and scores will not be allowed on the leaderboards if this is changed. Defaults to 10.</p> </li> <li> <p>api_base :  str | None \u2014</p> <p>The base URL for a given inference API. Only relevant if <code>model</code> refers to a model on an inference API. Defaults to None.</p> </li> <li> <p>api_version :  str | None \u2014</p> <p>The version of the API to use. Defaults to None.</p> </li> <li> <p>debug :  bool \u2014</p> <p>Whether to output debug information. Defaults to False.</p> </li> <li> <p>run_with_cli :  bool \u2014</p> <p>Whether the benchmarker is being run from the command-line interface. Defaults to False.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014</p> <p>If both <code>task</code> and <code>dataset</code> are specified.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>benchmark \u2014 Benchmarks models on datasets.</p> </li> </ul> <p> source property Benchmarker.benchmark_results: list[BenchmarkResult] </p> <p>The benchmark results.</p> <p> source method Benchmarker.benchmark(model: list[str] | str, task: str | list[str] | None = None, dataset: list[str] | str | None = None, progress_bar: bool | None = None, save_results: bool | None = None, language: str | list[str] | None = None, model_language: str | list[str] | None = None, dataset_language: str | list[str] | None = None, framework: Framework | str | None = None, device: Device | None = None, batch_size: int | None = None, raise_errors: bool | None = None, cache_dir: str | None = None, api_key: str | None = None, force: bool | None = None, verbose: bool | None = None, trust_remote_code: bool | None = None, load_in_4bit: bool | None = None, use_flash_attention: bool | None = None, clear_model_cache: bool | None = None, evaluate_test_split: bool | None = None, few_shot: bool | None = None, num_iterations: int | None = None) \u2192 list[BenchmarkResult] </p> <p>Benchmarks models on datasets.</p> <p> Parameters </p> <ul> <li> <p>model :  list[str] | str \u2014</p> <p>The full Hugging Face Hub path(s) to the pretrained transformer model. The specific model version to use can be added after the suffix '@': \"model@v1.0.0\". It can be a branch name, a tag name, or a commit id, and defaults to the latest version if not specified.</p> </li> <li> <p>task :  str | list[str] | None \u2014</p> <p>The tasks benchmark the model(s) on. Mutually exclusive with <code>dataset</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked. Defaults to None.</p> </li> <li> <p>dataset :  list[str] | str | None \u2014</p> <p>The datasets to benchmark on. Mutually exclusive with <code>task</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked. Defaults to None.</p> </li> <li> <p>progress_bar :  bool | None \u2014</p> <p>Whether progress bars should be shown. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>save_results :  bool | None \u2014</p> <p>Whether to save the benchmark results to 'scandeval_benchmark_results.jsonl'. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include, both for models and datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this to 'all' if all languages (also non-Scandinavian) should be considered. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>model_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for models. If specified then this overrides the <code>language</code> parameter for model languages. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>dataset_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for datasets. If specified then this overrides the <code>language</code> parameter for dataset languages. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>framework :  Framework | str | None \u2014</p> <p>The model framework to use. Only relevant if <code>model-id</code> refers to a local path. Otherwise, the framework will be set automatically. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>device :  Device | None \u2014</p> <p>The device to use for benchmarking. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>batch_size :  int | None \u2014</p> <p>The batch size to use. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>raise_errors :  bool | None \u2014</p> <p>Whether to raise errors instead of skipping the model evaluation.</p> </li> <li> <p>cache_dir :  str | None \u2014</p> <p>Directory to store cached models. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>api_key :  str | None \u2014</p> <p>The API key to use for a given inference server. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>force :  bool | None \u2014</p> <p>Whether to force evaluations of models, even if they have been benchmarked already. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>verbose :  bool | None \u2014</p> <p>Whether to output additional output. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>trust_remote_code :  bool | None \u2014</p> <p>Whether to trust remote code when loading models. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>load_in_4bit :  bool | None \u2014</p> <p>Whether to load models in 4-bit precision. If None then this will be done if CUDA is available and the model is a decoder model. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>use_flash_attention :  bool | None \u2014</p> <p>Whether to use Flash Attention. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>clear_model_cache :  bool | None \u2014</p> <p>Whether to clear the model cache after benchmarking each model. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>evaluate_test_split :  bool | None \u2014</p> <p>Whether to evaluate the test split of the datasets. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>few_shot :  bool | None \u2014</p> <p>Whether to only evaluate the model using few-shot evaluation. Only relevant if the model is generative. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>num_iterations :  int | None \u2014</p> <p>The number of times each model should be evaluated. This is only meant to be used for power users, and scores will not be allowed on the leaderboards if this is changed. Defaults to the value specified when initialising the benchmarker.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[BenchmarkResult] \u2014 A list of benchmark results.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014</p> <p>If both <code>task</code> and <code>dataset</code> are specified.</p> </li> <li> <p>benchmark_output_or_err</p> </li> </ul> <p> source block_terminal_output() </p> <p>Blocks libraries from writing output to the terminal.</p> <p>This filters warnings from some libraries, sets the logging level to ERROR for some libraries, disabled tokeniser progress bars when using Hugging Face tokenisers, and disables most of the logging from the <code>transformers</code> library.</p>"},{"location":"src/scandeval/","title":"scandeval","text":"scandeval<p> docs package scandeval </p> <pre><code>\"\"\"ScandEval - A benchmarking framework for language models.\"\"\"\n\nimport importlib.metadata\nimport logging\nimport os\nimport sys\n\nfrom dotenv import load_dotenv\nfrom termcolor import colored\n\nfrom .benchmarker import Benchmarker\nfrom .utils import block_terminal_output\n\n# Fetches the version of the package as defined in pyproject.toml\n__version__ = importlib.metadata.version(\"scandeval\")\n\n\n# Block unwanted terminal outputs\nblock_terminal_output()\n\n\n# Loads environment variables\nload_dotenv()\n\n\n# Set up logging\nfmt = colored(\"%(asctime)s\", \"light_blue\") + \" \u22c5 \" + colored(\"%(message)s\", \"green\")\nlogging.basicConfig(\n    level=logging.CRITICAL if hasattr(sys, \"_called_from_test\") else logging.INFO,\n    format=fmt,\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n)\n\n\n# Disable parallelisation when tokenizing, as that can lead to errors\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n\n# Enable MPS fallback\nos.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n\n\n# Set amount of threads per GPU - this is the default and is only set to prevent a\n# warning from showing\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\n</code></pre>"},{"location":"api/scandeval/benchmark_modules/","title":"scandeval.benchmark_modules","text":"scandeval.benchmark_modules<p> source package scandeval.benchmark_modules </p> <p>The different types of modules that can be benchmarked.</p> <p> Classes </p> <ul> <li> <p>BenchmarkModule \u2014 Abstract class for a benchmark module.</p> </li> <li> <p>FreshEncoderModel \u2014 A freshly initialised encoder model.</p> </li> <li> <p>HuggingFaceEncoderModel \u2014 An encoder model from the Hugging Face Hub.</p> </li> <li> <p>LiteLLMModel \u2014 A generative model from LiteLLM.</p> </li> <li> <p>VLLMModel \u2014 A generative model using the vLLM inference framework.</p> </li> </ul> <p> source class BenchmarkModule(dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) </p> <p><p>Bases : ABC</p></p> <p>Abstract class for a benchmark module.</p> <p>Initialise the benchmark module.</p> <p> Attributes </p> <ul> <li> <p>model_config \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config \u2014</p> <p>The benchmark configuration.</p> </li> <li> <p>is_generative \u2014</p> <p>Whether the model is generative.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>get_pytorch_module \u2014 Get the underlying PyTorch module.</p> </li> <li> <p>get_tokenizer \u2014 Get the underlying tokenizer.</p> </li> <li> <p>is_generative \u2014 Whether the model is generative.</p> </li> <li> <p>num_params \u2014 The number of parameters in the model.</p> </li> <li> <p>vocab_size \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length \u2014 The maximum length of the model.</p> </li> <li> <p>data_collator \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>compute_metrics \u2014 The function used to compute the metrics.</p> </li> <li> <p>extract_labels_from_generation \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class \u2014 The Trainer class to use for finetuning.</p> </li> <li> <p>prepare_datasets \u2014 Prepare the datasets for the model.</p> </li> <li> <p>prepare_dataset \u2014 Prepare the dataset for the model.</p> </li> <li> <p>generate \u2014 Generate outputs from the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> </ul> <p> source method BenchmarkModule.get_pytorch_module() \u2192 nn.Module </p> <p>Get the underlying PyTorch module.</p> <p> Returns </p> <ul> <li> <p>nn.Module \u2014 The PyTorch module.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method BenchmarkModule.get_tokenizer() \u2192 PreTrainedTokenizer </p> <p>Get the underlying tokenizer.</p> <p> Returns </p> <ul> <li> <p>PreTrainedTokenizer \u2014 The tokenizer.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method BenchmarkModule.is_generative() \u2192 bool </p> <p>Whether the model is generative.</p> <p> Returns </p> <ul> <li> <p>bool \u2014 Whether the model is generative.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method BenchmarkModule.num_params() \u2192 int </p> <p>The number of parameters in the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The number of parameters in the model.</p> </li> </ul> <p> source method BenchmarkModule.vocab_size() \u2192 int </p> <p>The vocabulary size of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The vocabulary size of the model.</p> </li> </ul> <p> source method BenchmarkModule.model_max_length() \u2192 int </p> <p>The maximum length of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum length of the model.</p> </li> </ul> <p> source method BenchmarkModule.data_collator() \u2192 c.Callable[[list[t.Any]], dict[str, t.Any]] </p> <p>The data collator used to prepare samples during finetuning.</p> <p> Returns </p> <ul> <li> <p>c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator.</p> </li> </ul> <p> source method BenchmarkModule.compute_metrics() \u2192 ComputeMetricsFunction </p> <p>The function used to compute the metrics.</p> <p> Returns </p> <ul> <li> <p>ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method BenchmarkModule.extract_labels_from_generation() \u2192 ExtractLabelsFunction </p> <p>The function used to extract the labels from the generated output.</p> <p> Returns </p> <ul> <li> <p>ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> </ul> <p> source method BenchmarkModule.trainer_class() \u2192 t.Type[Trainer] </p> <p>The Trainer class to use for finetuning.</p> <p> Returns </p> <ul> <li> <p>t.Type[Trainer] \u2014 The Trainer class.</p> </li> </ul> <p> source method BenchmarkModule.prepare_datasets(datasets: list[DatasetDict], task: Task) \u2192 list[DatasetDict] </p> <p>Prepare the datasets for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>datasets :  list[DatasetDict] \u2014</p> <p>The datasets to prepare.</p> </li> <li> <p>task :  Task \u2014</p> <p>The task to prepare the datasets for.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[DatasetDict] \u2014 The prepared datasets.</p> </li> </ul> <p> source method BenchmarkModule.prepare_dataset(dataset: DatasetDict, task: Task, itr_idx: int) \u2192 DatasetDict </p> <p>Prepare the dataset for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  DatasetDict \u2014</p> <p>The dataset to prepare.</p> </li> <li> <p>task :  Task \u2014</p> <p>The task to prepare the dataset for.</p> </li> <li> <p>itr_idx :  int \u2014</p> <p>The index of the dataset in the iterator.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetDict \u2014 The prepared dataset.</p> </li> </ul> <p> source method BenchmarkModule.generate(inputs: dict) \u2192 GenerativeModelOutput </p> <p>Generate outputs from the model.</p> <p> Parameters </p> <ul> <li> <p>inputs :  dict \u2014</p> <p>A batch of inputs to pass through the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>GenerativeModelOutput \u2014 The generated model outputs.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source classmethod BenchmarkModule.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> source classmethod BenchmarkModule.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> source class FreshEncoderModel() </p> <p><p>Bases : HuggingFaceEncoderModel</p></p> <p>A freshly initialised encoder model.</p> <p> Methods </p> <ul> <li> <p>num_params \u2014 The number of parameters in the model.</p> </li> <li> <p>vocab_size \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length \u2014 The maximum context length of the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> </ul> <p> source method FreshEncoderModel.num_params() \u2192 int </p> <p>The number of parameters in the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The number of parameters in the model.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method FreshEncoderModel.vocab_size() \u2192 int </p> <p>The vocabulary size of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The vocabulary size of the model.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method FreshEncoderModel.model_max_length() \u2192 int </p> <p>The maximum context length of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum context length of the model.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source classmethod FreshEncoderModel.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> source classmethod FreshEncoderModel.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> source class HuggingFaceEncoderModel(dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) </p> <p><p>Bases : BenchmarkModule</p></p> <p>An encoder model from the Hugging Face Hub.</p> <p>Initialise the model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>num_params \u2014 The number of parameters in the model.</p> </li> <li> <p>vocab_size \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length \u2014 The maximum context length of the model.</p> </li> <li> <p>data_collator \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>extract_labels_from_generation \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class \u2014 The Trainer class to use for finetuning.</p> </li> <li> <p>prepare_dataset \u2014 Prepare the dataset for the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> </ul> <p> source method HuggingFaceEncoderModel.num_params() \u2192 int </p> <p>The number of parameters in the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The number of parameters in the model.</p> </li> </ul> <p> source method HuggingFaceEncoderModel.vocab_size() \u2192 int </p> <p>The vocabulary size of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The vocabulary size of the model.</p> </li> </ul> <p> source method HuggingFaceEncoderModel.model_max_length() \u2192 int </p> <p>The maximum context length of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum context length of the model.</p> </li> </ul> <p> source method HuggingFaceEncoderModel.data_collator() \u2192 c.Callable[[list[t.Any]], dict[str, t.Any]] </p> <p>The data collator used to prepare samples during finetuning.</p> <p> Returns </p> <ul> <li> <p>c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method HuggingFaceEncoderModel.extract_labels_from_generation() \u2192 ExtractLabelsFunction </p> <p>The function used to extract the labels from the generated output.</p> <p> Returns </p> <ul> <li> <p>ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method HuggingFaceEncoderModel.trainer_class() \u2192 t.Type[Trainer] </p> <p>The Trainer class to use for finetuning.</p> <p> Returns </p> <ul> <li> <p>t.Type[Trainer] \u2014 The Trainer class.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method HuggingFaceEncoderModel.prepare_dataset(dataset: DatasetDict, task: Task, itr_idx: int) \u2192 DatasetDict </p> <p>Prepare the dataset for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  DatasetDict \u2014</p> <p>The dataset to prepare.</p> </li> <li> <p>task :  Task \u2014</p> <p>The task to prepare the dataset for.</p> </li> <li> <p>itr_idx :  int \u2014</p> <p>The index of the dataset in the iterator.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetDict \u2014 The prepared dataset.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source classmethod HuggingFaceEncoderModel.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> source classmethod HuggingFaceEncoderModel.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source class LiteLLMModel() </p> <p><p>Bases : BenchmarkModule</p></p> <p>A generative model from LiteLLM.</p> <p> Methods </p> <ul> <li> <p>generate \u2014 Generate outputs from the model.</p> </li> <li> <p>num_params \u2014 The number of parameters in the model.</p> </li> <li> <p>vocab_size \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length \u2014 The maximum length of the model.</p> </li> <li> <p>data_collator \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>extract_labels_from_generation \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class \u2014 The Trainer class to use for finetuning.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> <li> <p>prepare_dataset \u2014 Prepare the dataset for the model.</p> </li> </ul> <p> source method LiteLLMModel.generate(inputs: dict) \u2192 GenerativeModelOutput </p> <p>Generate outputs from the model.</p> <p> Parameters </p> <ul> <li> <p>inputs :  dict \u2014</p> <p>A batch of inputs to pass through the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>GenerativeModelOutput \u2014 The generated model outputs.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> <li> <p>NeedsAdditionalArgument</p> </li> </ul> <p> source method LiteLLMModel.num_params() \u2192 int </p> <p>The number of parameters in the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The number of parameters in the model.</p> </li> </ul> <p> source method LiteLLMModel.vocab_size() \u2192 int </p> <p>The vocabulary size of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The vocabulary size of the model.</p> </li> </ul> <p> source method LiteLLMModel.model_max_length() \u2192 int </p> <p>The maximum length of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum length of the model.</p> </li> </ul> <p> source method LiteLLMModel.data_collator() \u2192 c.Callable[[list[t.Any]], dict[str, t.Any]] </p> <p>The data collator used to prepare samples during finetuning.</p> <p> Returns </p> <ul> <li> <p>c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method LiteLLMModel.extract_labels_from_generation() \u2192 ExtractLabelsFunction </p> <p>The function used to extract the labels from the generated output.</p> <p> Returns </p> <ul> <li> <p>ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method LiteLLMModel.trainer_class() \u2192 t.Type[Trainer] </p> <p>The Trainer class to use for finetuning.</p> <p> Returns </p> <ul> <li> <p>t.Type[Trainer] \u2014 The Trainer class.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source classmethod LiteLLMModel.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>e</p> </li> </ul> <p> source classmethod LiteLLMModel.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> source method LiteLLMModel.prepare_dataset(dataset: DatasetDict, task: Task, itr_idx: int) \u2192 DatasetDict </p> <p>Prepare the dataset for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  DatasetDict \u2014</p> <p>The dataset to prepare.</p> </li> <li> <p>task :  Task \u2014</p> <p>The task to prepare the dataset for.</p> </li> <li> <p>itr_idx :  int \u2014</p> <p>The index of the dataset in the iterator.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetDict \u2014 The prepared dataset.</p> </li> </ul> <p> source class VLLMModel(dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) </p> <p><p>Bases : HuggingFaceEncoderModel</p></p> <p>A generative model using the vLLM inference framework.</p> <p>Initialise the vLLM model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>extract_labels_from_generation \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>prepare_dataset \u2014 Prepare the dataset for the model.</p> </li> <li> <p>generate \u2014 Generate outputs from the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> <li> <p>data_collator \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>trainer_class \u2014 The Trainer class to use for finetuning.</p> </li> </ul> <p> source method VLLMModel.extract_labels_from_generation() \u2192 ExtractLabelsFunction </p> <p>The function used to extract the labels from the generated output.</p> <p> Returns </p> <ul> <li> <p>ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method VLLMModel.prepare_dataset(dataset: DatasetDict, task: Task, itr_idx: int) \u2192 DatasetDict </p> <p>Prepare the dataset for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  DatasetDict \u2014</p> <p>The dataset to prepare.</p> </li> <li> <p>task :  Task \u2014</p> <p>The task to prepare the dataset for.</p> </li> <li> <p>itr_idx :  int \u2014</p> <p>The index of the dataset in the iterator.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetDict \u2014 The prepared dataset.</p> </li> </ul> <p> source method VLLMModel.generate(inputs: dict) \u2192 GenerativeModelOutput </p> <p>Generate outputs from the model.</p> <p> Parameters </p> <ul> <li> <p>inputs :  dict \u2014</p> <p>A batch of inputs to pass through the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>GenerativeModelOutput \u2014 The generated model outputs.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source classmethod VLLMModel.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> source classmethod VLLMModel.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source method VLLMModel.data_collator() \u2192 c.Callable[[list[t.Any]], dict[str, t.Any]] </p> <p>The data collator used to prepare samples during finetuning.</p> <p> Returns </p> <ul> <li> <p>c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method VLLMModel.trainer_class() \u2192 t.Type[Trainer] </p> <p>The Trainer class to use for finetuning.</p> <p> Returns </p> <ul> <li> <p>t.Type[Trainer] \u2014 The Trainer class.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul>"},{"location":"src/scandeval/benchmark_modules/","title":"scandeval.benchmark_modules","text":"scandeval.benchmark_modules<p> docs package scandeval.benchmark_modules </p> <pre><code>\"\"\"The different types of modules that can be benchmarked.\"\"\"\n\nfrom .base import BenchmarkModule\nfrom .fresh import FreshEncoderModel\nfrom .hf import HuggingFaceEncoderModel\nfrom .litellm import LiteLLMModel\nfrom .vllm import VLLMModel\n</code></pre>"},{"location":"api/scandeval/benchmark_modules/base/","title":"scandeval.benchmark_modules.base","text":"scandeval.benchmark_modules.base<p> source module scandeval.benchmark_modules.base </p> <p>Abstract benchmark module class that the model classes inherit from.</p> <p> Classes </p> <ul> <li> <p>BenchmarkModule \u2014 Abstract class for a benchmark module.</p> </li> </ul> <p> source class BenchmarkModule(dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) </p> <p><p>Bases : ABC</p></p> <p>Abstract class for a benchmark module.</p> <p>Initialise the benchmark module.</p> <p> Attributes </p> <ul> <li> <p>model_config \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config \u2014</p> <p>The benchmark configuration.</p> </li> <li> <p>is_generative \u2014</p> <p>Whether the model is generative.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>get_pytorch_module \u2014 Get the underlying PyTorch module.</p> </li> <li> <p>get_tokenizer \u2014 Get the underlying tokenizer.</p> </li> <li> <p>is_generative \u2014 Whether the model is generative.</p> </li> <li> <p>num_params \u2014 The number of parameters in the model.</p> </li> <li> <p>vocab_size \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length \u2014 The maximum length of the model.</p> </li> <li> <p>data_collator \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>compute_metrics \u2014 The function used to compute the metrics.</p> </li> <li> <p>extract_labels_from_generation \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class \u2014 The Trainer class to use for finetuning.</p> </li> <li> <p>prepare_datasets \u2014 Prepare the datasets for the model.</p> </li> <li> <p>prepare_dataset \u2014 Prepare the dataset for the model.</p> </li> <li> <p>generate \u2014 Generate outputs from the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> </ul> <p> source method BenchmarkModule.get_pytorch_module() \u2192 nn.Module </p> <p>Get the underlying PyTorch module.</p> <p> Returns </p> <ul> <li> <p>nn.Module \u2014 The PyTorch module.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method BenchmarkModule.get_tokenizer() \u2192 PreTrainedTokenizer </p> <p>Get the underlying tokenizer.</p> <p> Returns </p> <ul> <li> <p>PreTrainedTokenizer \u2014 The tokenizer.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method BenchmarkModule.is_generative() \u2192 bool </p> <p>Whether the model is generative.</p> <p> Returns </p> <ul> <li> <p>bool \u2014 Whether the model is generative.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method BenchmarkModule.num_params() \u2192 int </p> <p>The number of parameters in the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The number of parameters in the model.</p> </li> </ul> <p> source method BenchmarkModule.vocab_size() \u2192 int </p> <p>The vocabulary size of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The vocabulary size of the model.</p> </li> </ul> <p> source method BenchmarkModule.model_max_length() \u2192 int </p> <p>The maximum length of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum length of the model.</p> </li> </ul> <p> source method BenchmarkModule.data_collator() \u2192 c.Callable[[list[t.Any]], dict[str, t.Any]] </p> <p>The data collator used to prepare samples during finetuning.</p> <p> Returns </p> <ul> <li> <p>c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator.</p> </li> </ul> <p> source method BenchmarkModule.compute_metrics() \u2192 ComputeMetricsFunction </p> <p>The function used to compute the metrics.</p> <p> Returns </p> <ul> <li> <p>ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method BenchmarkModule.extract_labels_from_generation() \u2192 ExtractLabelsFunction </p> <p>The function used to extract the labels from the generated output.</p> <p> Returns </p> <ul> <li> <p>ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> </ul> <p> source method BenchmarkModule.trainer_class() \u2192 t.Type[Trainer] </p> <p>The Trainer class to use for finetuning.</p> <p> Returns </p> <ul> <li> <p>t.Type[Trainer] \u2014 The Trainer class.</p> </li> </ul> <p> source method BenchmarkModule.prepare_datasets(datasets: list[DatasetDict], task: Task) \u2192 list[DatasetDict] </p> <p>Prepare the datasets for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>datasets :  list[DatasetDict] \u2014</p> <p>The datasets to prepare.</p> </li> <li> <p>task :  Task \u2014</p> <p>The task to prepare the datasets for.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[DatasetDict] \u2014 The prepared datasets.</p> </li> </ul> <p> source method BenchmarkModule.prepare_dataset(dataset: DatasetDict, task: Task, itr_idx: int) \u2192 DatasetDict </p> <p>Prepare the dataset for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  DatasetDict \u2014</p> <p>The dataset to prepare.</p> </li> <li> <p>task :  Task \u2014</p> <p>The task to prepare the dataset for.</p> </li> <li> <p>itr_idx :  int \u2014</p> <p>The index of the dataset in the iterator.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetDict \u2014 The prepared dataset.</p> </li> </ul> <p> source method BenchmarkModule.generate(inputs: dict) \u2192 GenerativeModelOutput </p> <p>Generate outputs from the model.</p> <p> Parameters </p> <ul> <li> <p>inputs :  dict \u2014</p> <p>A batch of inputs to pass through the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>GenerativeModelOutput \u2014 The generated model outputs.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source classmethod BenchmarkModule.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> source classmethod BenchmarkModule.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul>"},{"location":"src/scandeval/benchmark_modules/base/","title":"scandeval.benchmark_modules.base","text":"scandeval.benchmark_modules.base<p> docs module scandeval.benchmark_modules.base </p> <pre><code>\"\"\"Abstract benchmark module class that the model classes inherit from.\"\"\"\n\nimport collections.abc as c\nimport logging\nimport sys\nimport typing as t\nfrom abc import ABC, abstractmethod\nfrom functools import cached_property, partial\n\nfrom datasets import DatasetDict\nfrom torch import nn\nfrom tqdm.auto import tqdm\nfrom transformers import PreTrainedTokenizer, Trainer\n\nfrom ..data_models import (\n    BenchmarkConfig,\n    DatasetConfig,\n    GenerativeModelOutput,\n    ModelConfig,\n    Task,\n)\nfrom ..enums import BatchingPreference\nfrom ..exceptions import NeedsEnvironmentVariable, NeedsExtraInstalled\nfrom ..task_utils import (\n    question_answering,\n    sequence_classification,\n    text_to_text,\n    token_classification,\n)\nfrom ..types import ComputeMetricsFunction, ExtractLabelsFunction\n\nlogger = logging.getLogger(\"scandeval\")\n\n\nclass BenchmarkModule(ABC):docs\n    \"\"\"Abstract class for a benchmark module.\n\n    Attributes:\n        model_config:\n            The model configuration.\n        dataset_config:\n            The dataset configuration.\n        benchmark_config:\n            The benchmark configuration.\n        is_generative:\n            Whether the model is generative.\n    \"\"\"\n\n    _is_generative: bool | None\n    batching_preference: BatchingPreference\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        dataset_config: DatasetConfig,\n        benchmark_config: BenchmarkConfig,\n    ) -&gt; None:\n        \"\"\"Initialise the benchmark module.\n\n        Args:\n            model_config:\n                The model configuration.\n            dataset_config:\n                The dataset configuration.\n            benchmark_config:\n                The benchmark configuration.\n        \"\"\"\n        self.model_config = model_config\n        self.dataset_config = dataset_config\n        self.benchmark_config = benchmark_config\n        self._log_metadata()\n\n    def _log_metadata(self) -&gt; None:\n        \"\"\"Log the metadata of the model.\"\"\"\n        # Set logging level based on verbosity\n        if hasattr(sys, \"_called_from_test\"):\n            logging_level = logging.CRITICAL\n        elif self.benchmark_config.verbose:\n            logging_level = logging.DEBUG\n        else:\n            logging_level = logging.INFO\n        logger.setLevel(logging_level)\n\n        logging_msg: str = \"\"\n        if self.num_params &lt; 0:\n            logging_msg += \"The model has an unknown number of parameters, \"\n        else:\n            logging_msg += f\"The model has {self.num_params:,} parameters, \"\n        if self.vocab_size &lt; 0:\n            logging_msg += \"an unknown vocabulary size, \"\n        else:\n            logging_msg += f\"a vocabulary size of {self.vocab_size:,}, \"\n        if self.model_max_length &lt; 0:\n            logging_msg += \"and an unknown maximum sequence length.\"\n        else:\n            logging_msg += f\"and a maximum context length of {self.model_max_length:,}.\"\n        logger.info(logging_msg)\n\n    def get_pytorch_module(self) -&gt; \"nn.Module\":docs\n        \"\"\"Get the underlying PyTorch module.\n\n        Returns:\n            The PyTorch module.\n        \"\"\"\n        if hasattr(self, \"_model\"):\n            return self._model\n        raise NotImplementedError(\n            \"The `get_pytorch_module` method has not been implemented for \"\n            f\"{self.__class__.__name__}.\"\n        )\n\n    def get_tokenizer(self) -&gt; \"PreTrainedTokenizer\":docs\n        \"\"\"Get the underlying tokenizer.\n\n        Returns:\n            The tokenizer.\n        \"\"\"\n        if hasattr(self, \"_tokenizer\"):\n            return self._tokenizer\n        raise NotImplementedError(\n            \"The `get_tokenizer` method has not been implemented for \"\n            f\"{self.__class__.__name__}.\"\n        )\n\n    @cached_property\n    def is_generative(self) -&gt; bool:docs\n        \"\"\"Whether the model is generative.\n\n        Returns:\n            Whether the model is generative.\n        \"\"\"\n        if self._is_generative is not None:\n            return self._is_generative\n        raise NotImplementedError(\n            \"The model type must define whether it is generative or not.\"\n        )\n\n    @cached_property\n    @abstractmethod\n    def num_params(self) -&gt; int:docs\n        \"\"\"The number of parameters in the model.\n\n        Returns:\n            The number of parameters in the model.\n        \"\"\"\n        ...\n\n    @cached_property\n    @abstractmethod\n    def vocab_size(self) -&gt; int:docs\n        \"\"\"The vocabulary size of the model.\n\n        Returns:\n            The vocabulary size of the model.\n        \"\"\"\n        ...\n\n    @cached_property\n    @abstractmethod\n    def model_max_length(self) -&gt; int:docs\n        \"\"\"The maximum length of the model.\n\n        Returns:\n            The maximum length of the model.\n        \"\"\"\n        ...\n\n    @cached_property\n    @abstractmethoddocs\n    def data_collator(self) -&gt; c.Callable[[list[t.Any]], dict[str, t.Any]]:\n        \"\"\"The data collator used to prepare samples during finetuning.\n\n        Returns:\n            The data collator.\n        \"\"\"\n        ...\n\n    @cached_property\n    def compute_metrics(self) -&gt; ComputeMetricsFunction:docs\n        \"\"\"The function used to compute the metrics.\n\n        Returns:\n            The function used to compute the metrics.\n        \"\"\"\n        match self.dataset_config.task.supertask:\n            case \"sequence-classification\":\n                return partial(\n                    sequence_classification.compute_metrics,\n                    id2label=self.dataset_config.id2label,\n                    dataset_config=self.dataset_config,\n                    benchmark_config=self.benchmark_config,\n                )\n            case \"text-to-text\":\n                return partial(\n                    text_to_text.compute_metrics,\n                    dataset_config=self.dataset_config,\n                    benchmark_config=self.benchmark_config,\n                )\n            case \"token-classification\":\n                return partial(\n                    token_classification.compute_metrics,\n                    has_misc_tags=self._has_misc_tags,\n                    dataset_config=self.dataset_config,\n                    benchmark_config=self.benchmark_config,\n                )\n            case \"question-answering\":\n                return partial(\n                    question_answering.compute_metrics,\n                    dataset_config=self.dataset_config,\n                    benchmark_config=self.benchmark_config,\n                )\n            case _:\n                raise NotImplementedError(\n                    f\"Unsupported task supertask: {self.dataset_config.task.supertask}.\"\n                )\n\n    @cached_property\n    @abstractmethod\n    def extract_labels_from_generation(self) -&gt; ExtractLabelsFunction:docs\n        \"\"\"The function used to extract the labels from the generated output.\n\n        Returns:\n            The function used to extract the labels from the generated output.\n        \"\"\"\n        ...\n\n    @cached_property\n    @abstractmethod\n    def trainer_class(self) -&gt; t.Type[\"Trainer\"]:docs\n        \"\"\"The Trainer class to use for finetuning.\n\n        Returns:\n            The Trainer class.\n        \"\"\"\n        ...\n\n    def prepare_datasets(docs\n        self, datasets: list[DatasetDict], task: Task\n    ) -&gt; list[DatasetDict]:\n        \"\"\"Prepare the datasets for the model.\n\n        This includes things like tokenisation.\n\n        Args:\n            datasets:\n                The datasets to prepare.\n            task:\n                The task to prepare the datasets for.\n\n        Returns:\n            The prepared datasets.\n        \"\"\"\n        for idx, dataset in enumerate(\n            tqdm(iterable=datasets, desc=\"Preparing datasets\")\n        ):\n            prepared_dataset = self.prepare_dataset(\n                dataset=dataset, task=task, itr_idx=idx\n            )\n            if self.dataset_config.task.supertask == \"token-classification\":\n                labels_in_train: set[str] = {\n                    tag for tag_list in dataset[\"train\"][\"labels\"] for tag in tag_list\n                }\n                self._has_misc_tags = (\n                    \"B-MISC\" in labels_in_train or \"I-MISC\" in labels_in_train\n                )\n            datasets[idx] = DatasetDict(\n                dict(\n                    train=prepared_dataset[\"train\"],\n                    val=prepared_dataset[\"val\"],\n                    test=prepared_dataset[\"test\"],\n                    original_train=dataset[\"train\"],\n                    original_val=dataset[\"val\"],\n                    original_test=dataset[\"test\"],\n                )\n            )\n        return datasets\n\n    @abstractmethod\n    def prepare_dataset(docs\n        self, dataset: DatasetDict, task: Task, itr_idx: int\n    ) -&gt; DatasetDict:\n        \"\"\"Prepare the dataset for the model.\n\n        This includes things like tokenisation.\n\n        Args:\n            dataset:\n                The dataset to prepare.\n            task:\n                The task to prepare the dataset for.\n            itr_idx:\n                The index of the dataset in the iterator.\n\n        Returns:\n            The prepared dataset.\n        \"\"\"\n        ...\n\n    def generate(self, inputs: dict) -&gt; GenerativeModelOutput:docs\n        \"\"\"Generate outputs from the model.\n\n        Args:\n            inputs:\n                A batch of inputs to pass through the model.\n\n        Returns:\n            The generated model outputs.\n        \"\"\"\n        raise NotImplementedError(\n            \"The `generate` method has not been implemented for \"\n            f\"{self.__class__.__name__}.\"\n        )\n\n    @classmethod\n    @abstractmethod\n    def model_exists(docs\n        cls, model_id: str, benchmark_config: BenchmarkConfig\n    ) -&gt; bool | NeedsExtraInstalled | NeedsEnvironmentVariable:\n        \"\"\"Check if a model exists.\n\n        Args:\n            model_id:\n                The model ID.\n            benchmark_config:\n                The benchmark configuration.\n\n        Returns:\n            Whether the model exists, or an error describing why we cannot check\n            whether the model exists.\n        \"\"\"\n        ...\n\n    @classmethod\n    @abstractmethod\n    def get_model_config(docs\n        cls, model_id: str, benchmark_config: BenchmarkConfig\n    ) -&gt; ModelConfig:\n        \"\"\"Fetch the model configuration.\n\n        Args:\n            model_id:\n                The model ID.\n            benchmark_config:\n                The benchmark configuration.\n\n        Returns:\n            The model configuration.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/scandeval/benchmark_modules/fresh/","title":"scandeval.benchmark_modules.fresh","text":"scandeval.benchmark_modules.fresh<p> source module scandeval.benchmark_modules.fresh </p> <p>Freshly initialised encoder models.</p> <p> Classes </p> <ul> <li> <p>FreshEncoderModel \u2014 A freshly initialised encoder model.</p> </li> </ul> <p> source class FreshEncoderModel() </p> <p><p>Bases : HuggingFaceEncoderModel</p></p> <p>A freshly initialised encoder model.</p> <p> Methods </p> <ul> <li> <p>num_params \u2014 The number of parameters in the model.</p> </li> <li> <p>vocab_size \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length \u2014 The maximum context length of the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> </ul> <p> source method FreshEncoderModel.num_params() \u2192 int </p> <p>The number of parameters in the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The number of parameters in the model.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method FreshEncoderModel.vocab_size() \u2192 int </p> <p>The vocabulary size of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The vocabulary size of the model.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method FreshEncoderModel.model_max_length() \u2192 int </p> <p>The maximum context length of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum context length of the model.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source classmethod FreshEncoderModel.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> source classmethod FreshEncoderModel.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul>"},{"location":"src/scandeval/benchmark_modules/fresh/","title":"scandeval.benchmark_modules.fresh","text":"scandeval.benchmark_modules.fresh<p> docs module scandeval.benchmark_modules.fresh </p> <pre><code>\"\"\"Freshly initialised encoder models.\"\"\"\n\nfrom functools import cached_property\nfrom json import JSONDecodeError\n\nfrom transformers import (\n    AutoConfig,\n    AutoTokenizer,\n    ElectraForQuestionAnswering,\n    ElectraForSequenceClassification,\n    ElectraForTokenClassification,\n    PretrainedConfig,\n    PreTrainedModel,\n    PreTrainedTokenizer,\n    XLMRobertaForQuestionAnswering,\n    XLMRobertaForSequenceClassification,\n    XLMRobertaForTokenClassification,\n)\n\nfrom ..data_models import BenchmarkConfig, ModelConfig\nfrom ..enums import Framework, ModelType\nfrom ..exceptions import (\n    InvalidBenchmark,\n    InvalidModel,\n    NeedsEnvironmentVariable,\n    NeedsExtraInstalled,\n)\nfrom ..utils import block_terminal_output, create_model_cache_dir\nfrom .hf import (\n    HuggingFaceEncoderModel,\n    align_model_and_tokenizer,\n    setup_model_for_question_answering,\n)\n\n\nclass FreshEncoderModel(HuggingFaceEncoderModel):docs\n    \"\"\"A freshly initialised encoder model.\"\"\"\n\n    @cached_property\n    def num_params(self) -&gt; int:docs\n        \"\"\"The number of parameters in the model.\n\n        Returns:\n            The number of parameters in the model.\n        \"\"\"\n        match self.model_config.model_id:\n            case \"fresh-xlm-roberta-base\":\n                return 278_885_778\n            case \"fresh-electra-small\":\n                return 13_738_755\n            case _:\n                raise NotImplementedError(\n                    f\"Number of parameters for model {self.model_config.model_id} is \"\n                    \"not implemented.\"\n                )\n\n    @cached_property\n    def vocab_size(self) -&gt; int:docs\n        \"\"\"The vocabulary size of the model.\n\n        Returns:\n            The vocabulary size of the model.\n        \"\"\"\n        match self.model_config.model_id:\n            case \"fresh-xlm-roberta-base\":\n                return 250_002\n            case \"fresh-electra-small\":\n                return 32_000\n            case _:\n                raise NotImplementedError(\n                    f\"Vocabulary size for model {self.model_config.model_id} is not \"\n                    \"implemented.\"\n                )\n\n    @cached_property\n    def model_max_length(self) -&gt; int:docs\n        \"\"\"The maximum context length of the model.\n\n        Returns:\n            The maximum context length of the model.\n        \"\"\"\n        match self.model_config.model_id:\n            case \"fresh-xlm-roberta-base\":\n                return 512\n            case \"fresh-electra-small\":\n                return 128\n            case _:\n                raise NotImplementedError(\n                    f\"Maximum context length for model {self.model_config.model_id} is \"\n                    \"not implemented.\"\n                )\n\n    @classmethod\n    def model_exists(docs\n        cls, model_id: str, benchmark_config: BenchmarkConfig\n    ) -&gt; bool | NeedsExtraInstalled | NeedsEnvironmentVariable:\n        \"\"\"Check if a model exists.\n\n        Args:\n            model_id:\n                The model ID.\n            benchmark_config:\n                The benchmark configuration.\n\n        Returns:\n            Whether the model exists, or an error describing why we cannot check\n            whether the model exists.\n        \"\"\"\n        valid_models = [\"fresh-electra-small\", \"fresh-xlm-roberta-base\"]\n        return model_id in valid_models\n\n    @classmethod\n    def get_model_config(docs\n        cls, model_id: str, benchmark_config: BenchmarkConfig\n    ) -&gt; ModelConfig:\n        \"\"\"Fetch the model configuration.\n\n        Args:\n            model_id:\n                The model ID.\n            benchmark_config:\n                The benchmark configuration.\n\n        Returns:\n            The model configuration.\n        \"\"\"\n        return ModelConfig(\n            model_id=model_id,\n            framework=Framework.PYTORCH,\n            task=\"fill-mask\",\n            languages=list(),\n            revision=\"main\",\n            model_type=ModelType.FRESH,\n            model_cache_dir=create_model_cache_dir(\n                cache_dir=benchmark_config.cache_dir, model_id=model_id\n            ),\n            adapter_base_model_id=None,\n        )\n\n    def _load_model_and_tokenizer(self) -&gt; tuple[PreTrainedModel, PreTrainedTokenizer]:\n        \"\"\"Load the model and tokenizer.\n\n        Returns:\n            The loaded model and tokenizer.\n        \"\"\"\n        config: \"PretrainedConfig\"\n        block_terminal_output()\n\n        # Get the fresh model ID and the corresponding real model ID\n        model_id = self.model_config.model_id.replace(\"-\", \"_\")\n        fresh_to_real_model_id_mapping = dict(\n            fresh_xlm_roberta_base=\"FacebookAI/xlm-roberta-base\",\n            fresh_electra_small=\"google/electra-small-discriminator\",\n        )\n        real_model_id = fresh_to_real_model_id_mapping[model_id]\n\n        match self.dataset_config.task.supertask:\n            case \"sequence-classification\":\n                model_cls_mapping = dict(\n                    fresh_xlm_roberta_base=XLMRobertaForSequenceClassification,\n                    fresh_electra_small=ElectraForSequenceClassification,\n                )\n            case \"token-classification\":\n                model_cls_mapping = dict(\n                    fresh_xlm_roberta_base=XLMRobertaForTokenClassification,\n                    fresh_electra_small=ElectraForTokenClassification,\n                )\n            case \"question-answering\":\n                model_cls_mapping = dict(\n                    fresh_xlm_roberta_base=XLMRobertaForQuestionAnswering,\n                    fresh_electra_small=ElectraForQuestionAnswering,\n                )\n            case _:\n                raise InvalidBenchmark(\n                    f\"Supertask {self.dataset_config.task.supertask} is not supported \"\n                    f\"for model {self.model_config.model_id}\"\n                )\n        model_cls = model_cls_mapping[model_id]\n\n        config = AutoConfig.from_pretrained(\n            real_model_id,\n            token=self.benchmark_config.api_key or True,\n            num_labels=self.dataset_config.num_labels,\n            id2label=self.dataset_config.id2label,\n            label2id=self.dataset_config.label2id,\n            cache_dir=self.model_config.model_cache_dir,\n        )\n        model = model_cls(config)\n\n        if self.dataset_config.task.supertask == \"question-answering\":\n            model = setup_model_for_question_answering(model=model)\n\n        # Load the tokenizer. If the model is a subclass of a RoBERTa model then we\n        # have to add a prefix space to the tokens, by the way the model is constructed\n        prefix_models = [\"Roberta\", \"GPT\", \"Deberta\"]\n        prefix = any(model_type in type(model).__name__ for model_type in prefix_models)\n        try:\n            tokenizer: \"PreTrainedTokenizer\" = AutoTokenizer.from_pretrained(\n                real_model_id,\n                revision=self.model_config.revision,\n                token=self.benchmark_config.api_key or True,\n                add_prefix_space=prefix,\n                cache_dir=self.model_config.model_cache_dir,\n                use_fast=True,\n                verbose=False,\n            )\n        except (JSONDecodeError, OSError):\n            raise InvalidModel(f\"Could not load tokenizer for model {real_model_id!r}.\")\n\n        model, tokenizer = align_model_and_tokenizer(\n            model=model,\n            tokenizer=tokenizer,\n            raise_errors=self.benchmark_config.raise_errors,\n        )\n\n        return model, tokenizer\n</code></pre>"},{"location":"api/scandeval/benchmark_modules/hf/","title":"scandeval.benchmark_modules.hf","text":"scandeval.benchmark_modules.hf<p> source module scandeval.benchmark_modules.hf </p> <p>Encoder models from the Hugging Face Hub.</p> <p> Classes </p> <ul> <li> <p>HuggingFaceEncoderModel \u2014 An encoder model from the Hugging Face Hub.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>get_model_repo_info \u2014 Get the information about the model from the Hugging Face Hub.</p> </li> <li> <p>load_tokenizer \u2014 Load the tokenizer.</p> </li> <li> <p>get_torch_dtype \u2014 Get the torch dtype, used for loading the model.</p> </li> <li> <p>load_hf_model_config \u2014 Load the Hugging Face model configuration.</p> </li> <li> <p>setup_model_for_question_answering \u2014 Setup a model for question answering.</p> </li> <li> <p>get_children_of_module \u2014 Get the children of a module.</p> </li> <li> <p>align_model_and_tokenizer \u2014 Aligns the model and the tokenizer.</p> </li> <li> <p>get_model_max_length \u2014 Get the maximum context length of a model.</p> </li> </ul> <p> source class HuggingFaceEncoderModel(dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) </p> <p><p>Bases : BenchmarkModule</p></p> <p>An encoder model from the Hugging Face Hub.</p> <p>Initialise the model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>num_params \u2014 The number of parameters in the model.</p> </li> <li> <p>vocab_size \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length \u2014 The maximum context length of the model.</p> </li> <li> <p>data_collator \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>extract_labels_from_generation \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class \u2014 The Trainer class to use for finetuning.</p> </li> <li> <p>prepare_dataset \u2014 Prepare the dataset for the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> </ul> <p> source method HuggingFaceEncoderModel.num_params() \u2192 int </p> <p>The number of parameters in the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The number of parameters in the model.</p> </li> </ul> <p> source method HuggingFaceEncoderModel.vocab_size() \u2192 int </p> <p>The vocabulary size of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The vocabulary size of the model.</p> </li> </ul> <p> source method HuggingFaceEncoderModel.model_max_length() \u2192 int </p> <p>The maximum context length of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum context length of the model.</p> </li> </ul> <p> source method HuggingFaceEncoderModel.data_collator() \u2192 c.Callable[[list[t.Any]], dict[str, t.Any]] </p> <p>The data collator used to prepare samples during finetuning.</p> <p> Returns </p> <ul> <li> <p>c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method HuggingFaceEncoderModel.extract_labels_from_generation() \u2192 ExtractLabelsFunction </p> <p>The function used to extract the labels from the generated output.</p> <p> Returns </p> <ul> <li> <p>ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method HuggingFaceEncoderModel.trainer_class() \u2192 t.Type[Trainer] </p> <p>The Trainer class to use for finetuning.</p> <p> Returns </p> <ul> <li> <p>t.Type[Trainer] \u2014 The Trainer class.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method HuggingFaceEncoderModel.prepare_dataset(dataset: DatasetDict, task: Task, itr_idx: int) \u2192 DatasetDict </p> <p>Prepare the dataset for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  DatasetDict \u2014</p> <p>The dataset to prepare.</p> </li> <li> <p>task :  Task \u2014</p> <p>The task to prepare the dataset for.</p> </li> <li> <p>itr_idx :  int \u2014</p> <p>The index of the dataset in the iterator.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetDict \u2014 The prepared dataset.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source classmethod HuggingFaceEncoderModel.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> source classmethod HuggingFaceEncoderModel.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source get_model_repo_info(model_id: str, revision: str, benchmark_config: BenchmarkConfig) \u2192 HFModelInfo | None </p> <p>Get the information about the model from the Hugging Face Hub.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>revision :  str \u2014</p> <p>The revision of the model.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>HFModelInfo | None \u2014 The information about the model, or None if the model could not be found.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>HuggingFaceHubDown</p> </li> <li> <p>NoInternetConnection</p> </li> <li> <p>NeedsAdditionalArgument</p> </li> </ul> <p> source load_tokenizer(model: PreTrainedModel | None, model_id: str, trust_remote_code: bool) \u2192 PreTrainedTokenizer </p> <p>Load the tokenizer.</p> <p> Parameters </p> <ul> <li> <p>model :  PreTrainedModel | None \u2014</p> <p>The model, which is used to determine whether to add a prefix space to the tokens. Can be None.</p> </li> <li> <p>model_id :  str \u2014</p> <p>The model identifier. Used for logging.</p> </li> <li> <p>trust_remote_code :  bool \u2014</p> <p>Whether to trust remote code.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>PreTrainedTokenizer \u2014 The loaded tokenizer.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source get_torch_dtype(device: torch.device, torch_dtype_is_set: bool, bf16_available: bool) \u2192 str | torch.dtype </p> <p>Get the torch dtype, used for loading the model.</p> <p> Parameters </p> <ul> <li> <p>device :  torch.device \u2014</p> <p>The device to use.</p> </li> <li> <p>torch_dtype_is_set :  bool \u2014</p> <p>Whether the torch data type is set in the model configuration.</p> </li> <li> <p>bf16_available :  bool \u2014</p> <p>Whether bfloat16 is available.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str | torch.dtype \u2014 The torch dtype.</p> </li> </ul> <p> source load_hf_model_config(model_id: str, num_labels: int, id2label: dict[int, str], label2id: dict[str, int], revision: str, model_cache_dir: str | None, api_key: str | None, trust_remote_code: bool, run_with_cli: bool) \u2192 PretrainedConfig </p> <p>Load the Hugging Face model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The Hugging Face model ID.</p> </li> <li> <p>num_labels :  int \u2014</p> <p>The number of labels in the dataset.</p> </li> <li> <p>id2label :  dict[int, str] \u2014</p> <p>The mapping from label IDs to labels.</p> </li> <li> <p>label2id :  dict[str, int] \u2014</p> <p>The mapping from labels to label IDs.</p> </li> <li> <p>revision :  str \u2014</p> <p>The revision of the model.</p> </li> <li> <p>model_cache_dir :  str | None \u2014</p> <p>The directory to cache the model in.</p> </li> <li> <p>api_key :  str | None \u2014</p> <p>The Hugging Face API key.</p> </li> <li> <p>trust_remote_code :  bool \u2014</p> <p>Whether to trust remote code.</p> </li> <li> <p>run_with_cli :  bool \u2014</p> <p>Whether the script is being run with the CLI.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>PretrainedConfig \u2014 The Hugging Face model configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> <li> <p>e</p> </li> <li> <p>NeedsAdditionalArgument</p> </li> </ul> <p> source setup_model_for_question_answering(model: PreTrainedModel) \u2192 PreTrainedModel </p> <p>Setup a model for question answering.</p> <p> Parameters </p> <ul> <li> <p>model :  PreTrainedModel \u2014</p> <p>The model to setup.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>PreTrainedModel \u2014 The setup model.</p> </li> </ul> <p> source get_children_of_module(name: str, module: nn.Module) \u2192 nn.Module | dict[str, t.Any] | None </p> <p>Get the children of a module.</p> <p> Parameters </p> <ul> <li> <p>name :  str \u2014</p> <p>The name of the module.</p> </li> <li> <p>module :  nn.Module \u2014</p> <p>The module to get the children of.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>nn.Module | dict[str, t.Any] | None \u2014 The children of the module, or None if the module has no children.</p> </li> </ul> <p> source align_model_and_tokenizer(model: PreTrainedModel, tokenizer: PreTrainedTokenizer, raise_errors: bool = False) \u2192 tuple[PreTrainedModel, PreTrainedTokenizer] </p> <p>Aligns the model and the tokenizer.</p> <p> Parameters </p> <ul> <li> <p>model :  PreTrainedModel \u2014</p> <p>The model to fix.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer \u2014</p> <p>The tokenizer to fix.</p> </li> <li> <p>raise_errors :  bool \u2014</p> <p>Whether to raise errors instead of trying to fix them silently.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[PreTrainedModel, PreTrainedTokenizer] \u2014 The fixed model and tokenizer.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source get_model_max_length(model: PreTrainedModel, tokenizer: PreTrainedTokenizer) \u2192 int </p> <p>Get the maximum context length of a model.</p> <p> Parameters </p> <ul> <li> <p>model :  PreTrainedModel \u2014</p> <p>The model.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer \u2014</p> <p>The tokenizer.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum context length.</p> </li> </ul>"},{"location":"src/scandeval/benchmark_modules/hf/","title":"scandeval.benchmark_modules.hf","text":"scandeval.benchmark_modules.hf<p> docs module scandeval.benchmark_modules.hf </p> <pre><code>\"\"\"Encoder models from the Hugging Face Hub.\"\"\"\n\nimport collections.abc as c\nimport logging\nimport typing as t\nfrom functools import cached_property, partial\nfrom json import JSONDecodeError\nfrom time import sleep\n\nimport torch\nfrom datasets import DatasetDict\nfrom huggingface_hub import HfApi\nfrom huggingface_hub import whoami as hf_whoami\nfrom huggingface_hub.hf_api import RepositoryNotFoundError, RevisionNotFoundError\nfrom huggingface_hub.utils import (\n    GatedRepoError,\n    HFValidationError,\n    LocalTokenNotFoundError,\n)\nfrom requests.exceptions import RequestException\nfrom torch import nn\nfrom transformers import (\n    AutoConfig,\n    AutoTokenizer,\n    DataCollatorForTokenClassification,\n    DataCollatorWithPadding,\n    PretrainedConfig,\n    PreTrainedModel,\n    PreTrainedTokenizer,\n    Trainer,\n)\nfrom urllib3.exceptions import RequestError\n\nfrom ..constants import DUMMY_FILL_VALUE, GENERATIVE_MODEL_TASKS, GENERATIVE_TAGS\nfrom ..data_models import BenchmarkConfig, DatasetConfig, HFModelInfo, ModelConfig, Task\nfrom ..enums import BatchingPreference, Framework, ModelType\nfrom ..exceptions import (\n    HuggingFaceHubDown,\n    InvalidBenchmark,\n    InvalidModel,\n    NeedsAdditionalArgument,\n    NeedsEnvironmentVariable,\n    NeedsExtraInstalled,\n    NoInternetConnection,\n)\nfrom ..languages import get_all_languages\nfrom ..task_utils import question_answering, token_classification\nfrom ..types import ExtractLabelsFunction\nfrom ..utils import (\n    block_terminal_output,\n    create_model_cache_dir,\n    get_class_by_name,\n    internet_connection_available,\n)\nfrom .base import BenchmarkModule\n\nlogger = logging.getLogger(\"scandeval\")\n\n\nclass HuggingFaceEncoderModel(BenchmarkModule):\n    \"\"\"An encoder model from the Hugging Face Hub.\"\"\"\n\n    _is_generative = False\n    batching_preference = BatchingPreference.NO_PREFERENCE\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        dataset_config: DatasetConfig,\n        benchmark_config: BenchmarkConfig,\n    ) -&gt; None:\n        \"\"\"Initialise the model.\n\n        Args:\n            model_config:\n                The model configuration.\n            dataset_config:\n                The dataset configuration.\n            benchmark_config:\n                The benchmark configuration.\n        \"\"\"\n        self.model_config = model_config\n        self.dataset_config = dataset_config\n        self.benchmark_config = benchmark_config\n        model, tokenizer = self._load_model_and_tokenizer()\n        self._model: PreTrainedModel = model\n        self._tokenizer: PreTrainedTokenizer = tokenizer\n        self._log_metadata()\n\n    @cached_property\n    def num_params(self) -&gt; int:\n        \"\"\"The number of parameters in the model.\n\n        Returns:\n            The number of parameters in the model.\n        \"\"\"\n        hf_api = HfApi()\n        try:\n            repo_info = hf_api.model_info(\n                repo_id=self.model_config.adapter_base_model_id\n                or self.model_config.model_id,\n                revision=self.model_config.revision,\n                token=self.benchmark_config.api_key or True,\n            )\n        except (\n            RepositoryNotFoundError,\n            RevisionNotFoundError,\n            RequestException,\n            HFValidationError,\n        ):\n            repo_info = None\n\n        if (\n            repo_info is not None\n            and hasattr(repo_info, \"safetensors\")\n            and repo_info.safetensors is not None\n            and \"total\" in repo_info.safetensors\n        ):\n            num_params = repo_info.safetensors[\"total\"]\n        elif (\n            hasattr(self._model.config, \"num_params\")\n            and self._model.config.num_params is not None\n        ):\n            num_params = self._model.config.num_params\n        else:\n            num_params = sum(p.numel() for p in self._model.parameters())\n        return num_params\n\n    @cached_property\n    def vocab_size(self) -&gt; int:\n        \"\"\"The vocabulary size of the model.\n\n        Returns:\n            The vocabulary size of the model.\n        \"\"\"\n        if (\n            hasattr(self._model.config, \"vocab_size\")\n            and self._model.config.vocab_size is not None\n        ):\n            vocab_size = self._model.config.vocab_size\n        elif (\n            hasattr(self._tokenizer, \"vocab_size\")\n            and self._tokenizer.vocab_size is not None\n        ):\n            vocab_size = self._tokenizer.vocab_size\n        else:\n            vocab_size = -1\n        return vocab_size\n\n    @cached_property\n    def model_max_length(self) -&gt; int:\n        \"\"\"The maximum context length of the model.\n\n        Returns:\n            The maximum context length of the model.\n        \"\"\"\n        all_max_lengths: list[int] = list()\n\n        # Add the registered max length of the tokenizer\n        if hasattr(\n            self._tokenizer, \"model_max_length\"\n        ) and self._tokenizer.model_max_length &lt; int(1e30):\n            all_max_lengths.append(self._tokenizer.model_max_length)\n\n        # Add the max length derived from the model's input sizes\n        if hasattr(self._tokenizer, \"max_model_input_sizes\"):\n            all_max_lengths.extend(\n                [\n                    size\n                    for size in self._tokenizer.max_model_input_sizes.values()\n                    if size is not None\n                ]\n            )\n\n        # Add max length candidates from the model's configuration\n        candidate_config_max_lengths = [\n            \"max_position_embeddings\",\n            \"max_sequence_length\",\n            \"model_max_length\",\n            \"sliding_window\",\n            \"sliding_window_size\",\n            \"n_positions\",\n        ]\n        for candidate_config_max_length in candidate_config_max_lengths:\n            if (\n                hasattr(self._model.config, candidate_config_max_length)\n                and (value := getattr(self._model.config, candidate_config_max_length))\n                is not None\n            ):\n                all_max_lengths.append(value)\n\n        # To avoid models having artificially low max lengths, we remove any max lengths\n        # that are less than 128\n        all_max_lengths = [\n            max_length for max_length in all_max_lengths if max_length &gt;= 128\n        ]\n\n        if len(list(all_max_lengths)) &gt; 0:\n            model_max_length = min(list(all_max_lengths))\n        else:\n            model_max_length = -1\n\n        return model_max_length\n\n    @cached_property\n    def data_collator(self) -&gt; c.Callable[[list[t.Any]], dict[str, t.Any]]:\n        \"\"\"The data collator used to prepare samples during finetuning.\n\n        Returns:\n            The data collator.\n        \"\"\"\n        match self.dataset_config.task.supertask:\n            case \"sequence-classification\" | \"text-to-text\" | \"question-answering\":\n                return DataCollatorWithPadding(self._tokenizer, padding=\"longest\")\n            case \"token-classification\":\n                return DataCollatorForTokenClassification(\n                    tokenizer=self._tokenizer, label_pad_token_id=-100\n                )\n            case _:\n                raise NotImplementedError(\n                    f\"Unsupported task supertask: {self.dataset_config.task.supertask}.\"\n                )\n\n    @cached_property\n    def extract_labels_from_generation(self) -&gt; ExtractLabelsFunction:\n        \"\"\"The function used to extract the labels from the generated output.\n\n        Returns:\n            The function used to extract the labels from the generated output.\n        \"\"\"\n        raise NotImplementedError(\n            \"The `extract_labels_from_generation` property has not been implemented \"\n            \"for Hugging Face Encoder models.\"\n        )\n\n    @cached_property\n    def trainer_class(self) -&gt; t.Type[\"Trainer\"]:\n        \"\"\"The Trainer class to use for finetuning.\n\n        Returns:\n            The Trainer class.\n        \"\"\"\n        match self.dataset_config.task.supertask:\n            case \"sequence-classification\" | \"text-to-text\" | \"token-classification\":\n                return Trainer\n            case \"question-answering\":\n                return question_answering.QuestionAnsweringTrainer\n            case _:\n                raise NotImplementedError(\n                    f\"Unsupported task supertask: {self.dataset_config.task.supertask}.\"\n                )\n\n    def prepare_dataset(\n        self, dataset: DatasetDict, task: Task, itr_idx: int\n    ) -&gt; DatasetDict:\n        \"\"\"Prepare the dataset for the model.\n\n        This includes things like tokenisation.\n\n        Args:\n            dataset:\n                The dataset to prepare.\n            task:\n                The task to prepare the dataset for.\n            itr_idx:\n                The index of the dataset in the iterator.\n\n        Returns:\n            The prepared dataset.\n        \"\"\"\n\n        def numericalise_labels(examples: dict):\n            if \"label\" in examples:\n                try:\n                    examples[\"label\"] = [\n                        self._model.config.label2id[lbl.lower()]\n                        for lbl in examples[\"label\"]\n                    ]\n                except KeyError:\n                    raise InvalidBenchmark(\n                        f\"One of the labels in the dataset, \"\n                        f\"{examples['label'].lower()}, does not occur in the \"\n                        f\"label2id dictionary {self._model.config.label2id}.\"\n                    )\n            return examples\n\n        def tokenise(examples: dict):\n            return self._tokenizer(text=examples[\"text\"], truncation=True, padding=True)\n\n        match task.supertask:\n            case \"sequence-classification\":\n                dataset = dataset.map(\n                    numericalise_labels, batched=True, load_from_cache_file=False\n                ).map(tokenise, batched=True, load_from_cache_file=False)\n\n            case \"text-to-text\":\n                dataset = dataset.map(\n                    tokenise,\n                    batched=True,\n                    load_from_cache_file=False,\n                    keep_in_memory=True,\n                )\n\n            case \"token-classification\":\n                dataset = dataset.map(\n                    partial(\n                        token_classification.tokenize_and_align_labels,\n                        tokenizer=self._tokenizer,\n                        label2id=self._model.config.label2id,\n                    ),\n                    batched=True,\n                    load_from_cache_file=False,\n                    keep_in_memory=True,\n                )\n\n            case \"question-answering\":\n                dataset = DatasetDict(\n                    dict(\n                        train=dataset[\"train\"].map(\n                            partial(\n                                question_answering.prepare_train_examples,\n                                tokenizer=self._tokenizer,\n                            ),\n                            batched=True,\n                            batch_size=10,\n                            remove_columns=dataset[\"test\"].column_names,\n                            load_from_cache_file=False,\n                            keep_in_memory=True,\n                        ),\n                        val=dataset[\"val\"].map(\n                            partial(\n                                question_answering.prepare_train_examples,\n                                tokenizer=self._tokenizer,\n                            ),\n                            batched=True,\n                            batch_size=10,\n                            remove_columns=dataset[\"test\"].column_names,\n                            load_from_cache_file=False,\n                            keep_in_memory=True,\n                        ),\n                        test=dataset[\"test\"].map(\n                            partial(\n                                question_answering.prepare_test_examples,\n                                tokenizer=self._tokenizer,\n                            ),\n                            batched=True,\n                            batch_size=10,\n                            remove_columns=dataset[\"test\"].column_names,\n                            load_from_cache_file=False,\n                            keep_in_memory=True,\n                        ),\n                    )\n                )\n\n                # The Trainer hides the columns that are not used by the model (here\n                # `id` and `offset_mapping` which we will need for our post-processing),\n                # so we put them back\n                for split_name, split in dataset.items():\n                    dataset[split_name].set_format(\n                        type=split.format[\"type\"], columns=list(split.features.keys())\n                    )\n\n            case _:\n                raise NotImplementedError(\n                    f\"Unsupported task supertask: {task.supertask}.\"\n                )\n\n        return dataset\n\n    @classmethod\n    def model_exists(\n        cls, model_id: str, benchmark_config: BenchmarkConfig\n    ) -&gt; bool | NeedsExtraInstalled | NeedsEnvironmentVariable:\n        \"\"\"Check if a model exists.\n\n        Args:\n            model_id:\n                The model ID.\n            benchmark_config:\n                The benchmark configuration.\n\n        Returns:\n            Whether the model exists, or an error describing why we cannot check\n            whether the model exists.\n        \"\"\"\n        model_id, revision = (\n            model_id.split(\"@\") if \"@\" in model_id else (model_id, \"main\")\n        )\n        model_info = get_model_repo_info(\n            model_id=model_id, revision=revision, benchmark_config=benchmark_config\n        )\n        return (\n            model_info is not None\n            and model_info.pipeline_tag not in GENERATIVE_MODEL_TASKS\n        )\n\n    @classmethod\n    def get_model_config(\n        cls, model_id: str, benchmark_config: BenchmarkConfig\n    ) -&gt; ModelConfig:\n        \"\"\"Fetch the model configuration.\n\n        Args:\n            model_id:\n                The model ID.\n            benchmark_config:\n                The benchmark configuration.\n\n        Returns:\n            The model configuration.\n        \"\"\"\n        model_id, revision = (\n            model_id.split(\"@\") if \"@\" in model_id else (model_id, \"main\")\n        )\n        model_info = get_model_repo_info(\n            model_id=model_id, revision=revision, benchmark_config=benchmark_config\n        )\n        if model_info is None:\n            raise InvalidModel(f\"The model {model_id!r} could not be found.\")\n\n        framework = Framework.PYTORCH\n        if \"pytorch\" in model_info.tags:\n            pass\n        elif \"jax\" in model_info.tags:\n            framework = Framework.JAX\n        elif \"spacy\" in model_info.tags:\n            raise InvalidModel(\"SpaCy models are not supported.\")\n        elif any(tag in model_info.tags for tag in {\"tf\", \"tensorflow\", \"keras\"}):\n            raise InvalidModel(\"TensorFlow/Keras models are not supported.\")\n\n        language_mapping = get_all_languages()\n        language_codes = list(language_mapping.keys())\n\n        model_config = ModelConfig(\n            model_id=model_id,\n            revision=revision,\n            framework=framework,\n            task=model_info.pipeline_tag,\n            languages=[\n                language_mapping[tag]\n                for tag in model_info.tags\n                if tag in language_codes\n            ],\n            model_type=ModelType.HF_HUB_ENCODER,\n            model_cache_dir=create_model_cache_dir(\n                cache_dir=benchmark_config.cache_dir, model_id=model_id\n            ),\n            adapter_base_model_id=None,\n        )\n\n        return model_config\n\n    def _load_model_and_tokenizer(self) -&gt; tuple[PreTrainedModel, PreTrainedTokenizer]:\n        \"\"\"Load the model and tokenizer.\n\n        Returns:\n            The loaded model and tokenizer.\n        \"\"\"\n        config: \"PretrainedConfig\"\n        block_terminal_output()\n\n        model_id = self.model_config.model_id\n        supertask = self.dataset_config.task.supertask\n        from_flax = self.model_config.framework == Framework.JAX\n        ignore_mismatched_sizes = False\n\n        config = load_hf_model_config(\n            model_id=model_id,\n            num_labels=self.dataset_config.num_labels,\n            id2label=self.dataset_config.id2label,\n            label2id=self.dataset_config.label2id,\n            revision=self.model_config.revision,\n            model_cache_dir=self.model_config.model_cache_dir,\n            api_key=self.benchmark_config.api_key,\n            trust_remote_code=self.benchmark_config.trust_remote_code,\n            run_with_cli=self.benchmark_config.run_with_cli,\n        )\n\n        model_kwargs = dict(\n            config=config,\n            from_flax=from_flax,\n            ignore_mismatched_sizes=ignore_mismatched_sizes,\n            revision=self.model_config.revision,\n            token=self.benchmark_config.api_key or True,\n            cache_dir=self.model_config.model_cache_dir,\n            trust_remote_code=self.benchmark_config.trust_remote_code,\n            torch_dtype=get_torch_dtype(\n                device=self.benchmark_config.device,\n                torch_dtype_is_set=config.to_dict().get(\"torch_dtype\") is not None,\n                bf16_available=(\n                    torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n                ),\n            ),\n        )\n\n        # These are used when a timeout occurs\n        attempts_left = 5\n\n        model: PreTrainedModel | None = None\n        while True:\n            try:\n                # Get the model class associated with the supertask\n                model_cls_or_none: t.Type[\"PreTrainedModel\"] | None = get_class_by_name(\n                    class_name=f\"auto-model-for-{supertask}\", module_name=\"transformers\"\n                )\n\n                # If the model class could not be found then raise an error\n                if not model_cls_or_none:\n                    raise InvalidBenchmark(\n                        f\"The supertask {supertask!r} does not correspond to a \"\n                        \"Hugging Face AutoModel type (such as \"\n                        \"`AutoModelForSequenceClassification`).\"\n                    )\n\n                # If the model is a DeBERTaV2 model then we ensure that\n                # `pooler_hidden_size` is the same size as `hidden_size`\n                if config.model_type == \"deberta-v2\":\n                    config.pooler_hidden_size = config.hidden_size\n\n                try:\n                    model_or_tuple = model_cls_or_none.from_pretrained(\n                        self.model_config.model_id, **model_kwargs\n                    )\n                except (KeyError, RuntimeError) as e:\n                    if not model_kwargs[\"ignore_mismatched_sizes\"]:\n                        logger.debug(\n                            f\"{type(e).__name__} occurred during the loading \"\n                            f\"of the {model_id!r} model. Retrying with \"\n                            \"`ignore_mismatched_sizes` set to True.\"\n                        )\n                        model_kwargs[\"ignore_mismatched_sizes\"] = True\n                        continue\n                    else:\n                        raise InvalidModel(str(e))\n                except (TimeoutError, RequestError):\n                    attempts_left -= 1\n                    if attempts_left == 0:\n                        raise InvalidModel(\n                            \"The model could not be loaded after 5 attempts.\"\n                        )\n                    logger.info(f\"Couldn't load the model {model_id!r}. Retrying.\")\n                    sleep(5)\n                    continue\n\n                if isinstance(model_or_tuple, tuple):\n                    model = model_or_tuple[0]\n                else:\n                    model = model_or_tuple\n                break\n\n            except (OSError, ValueError) as e:\n                # If `from_flax` is False but only Flax models are available then\n                # try again with `from_flax` set to True\n                if not from_flax and \"Use `from_flax=True` to load this model\" in str(\n                    e\n                ):\n                    from_flax = True\n                    continue\n\n                if \"checkpoint seems to be incorrect\" in str(e):\n                    raise InvalidModel(\n                        f\"The model {model_id!r} has an incorrect checkpoint.\"\n                    )\n                if \"trust_remote_code\" in str(e):\n                    raise InvalidModel(\n                        f\"Loading the model {model_id!r} needs to trust remote code. \"\n                        \"If you trust the suppliers of this model, then you can enable \"\n                        \"this by setting the `--trust-remote-code` flag.\"\n                    )\n                raise InvalidModel(\n                    f\"The model {model_id!r} could not be loaded. The error was {e!r}.\"\n                )\n\n        assert model is not None\n\n        model.eval()\n        model.to(self.benchmark_config.device)\n\n        if isinstance(model, PreTrainedModel) and supertask == \"question-answering\":\n            model = setup_model_for_question_answering(model=model)\n\n        tokenizer = load_tokenizer(\n            model=model,\n            model_id=model_id,\n            trust_remote_code=self.benchmark_config.trust_remote_code,\n        )\n\n        model, tokenizer = align_model_and_tokenizer(\n            model=model,\n            tokenizer=tokenizer,\n            raise_errors=self.benchmark_config.raise_errors,\n        )\n\n        return model, tokenizer\n\n\ndef get_model_repo_info(docs\n    model_id: str, revision: str, benchmark_config: BenchmarkConfig\n) -&gt; HFModelInfo | None:\n    \"\"\"Get the information about the model from the Hugging Face Hub.\n\n    Args:\n        model_id:\n            The model ID.\n        revision:\n            The revision of the model.\n        benchmark_config:\n            The benchmark configuration.\n\n    Returns:\n        The information about the model, or None if the model could not be found.\n    \"\"\"\n    hf_api = HfApi(token=benchmark_config.api_key or True)\n    model_id, revision = model_id.split(\"@\") if \"@\" in model_id else (model_id, \"main\")\n\n    try:\n        model_info = hf_api.model_info(repo_id=model_id, revision=revision)\n\n    # Case where the model is gated; note this to the user\n    except (GatedRepoError, LocalTokenNotFoundError) as e:\n        try:\n            hf_whoami()\n            logger.warning(\n                f\"Could not access the model {model_id} with the revision \"\n                f\"{revision}. The error was {str(e)!r}.\"\n            )\n            return None\n        except LocalTokenNotFoundError:\n            raise NeedsAdditionalArgument(\n                cli_argument=\"--api-key\",\n                script_argument=\"api_key=&lt;your-api-key&gt;\",\n                run_with_cli=benchmark_config.run_with_cli,\n            )\n\n    # Case where the model could not be found\n    except (RepositoryNotFoundError, HFValidationError):\n        return None\n\n    # Other internet-related errors\n    except (OSError, RequestException):\n        if internet_connection_available():\n            raise HuggingFaceHubDown()\n        else:\n            raise NoInternetConnection()\n\n    tags = model_info.tags or list()\n\n    has_base_model_tag = any(\n        tag.startswith(\"base_model:\") and tag.count(\":\") == 1 for tag in tags\n    )\n    base_model_id: str | None = None\n    if has_base_model_tag:\n        has_adapter_config = model_info.siblings is not None and any(\n            sibling.rfilename == \"adapter_config.json\"\n            for sibling in model_info.siblings\n        )\n        if has_adapter_config:\n            base_model_id = [\n                tag.split(\":\")[1]\n                for tag in tags\n                if tag.startswith(\"base_model:\") and tag.count(\":\") == 1\n            ][0]\n            base_model_info = hf_api.model_info(\n                repo_id=base_model_id,\n                revision=revision,\n                token=benchmark_config.api_key or True,\n            )\n            tags += base_model_info.tags or list()\n            tags = list(set(tags))\n\n    pipeline_tag = model_info.pipeline_tag\n    if pipeline_tag is None:\n        if any(tag in GENERATIVE_TAGS for tag in tags):\n            pipeline_tag = \"text-generation\"\n        else:\n            pipeline_tag = \"fill-mask\"\n\n    return HFModelInfo(\n        pipeline_tag=pipeline_tag, tags=tags, adapter_base_model_id=base_model_id\n    )\n\n\ndef load_tokenizer(docs\n    model: \"PreTrainedModel | None\", model_id: str, trust_remote_code: bool\n) -&gt; \"PreTrainedTokenizer\":\n    \"\"\"Load the tokenizer.\n\n    Args:\n        model:\n            The model, which is used to determine whether to add a prefix space to\n            the tokens. Can be None.\n        model_id:\n            The model identifier. Used for logging.\n        trust_remote_code:\n            Whether to trust remote code.\n\n    Returns:\n        The loaded tokenizer.\n    \"\"\"\n    loading_kwargs: dict[str, bool | str] = dict(\n        use_fast=True,\n        verbose=False,\n        trust_remote_code=trust_remote_code,\n        padding_side=\"right\",\n        truncation_side=\"right\",\n    )\n\n    # If the model is a subclass of a certain model types then we have to add a prefix\n    # space to the tokens, by the way the model is constructed.\n    if model is not None:\n        prefix_models = [\"Roberta\", \"GPT\", \"Deberta\"]\n        add_prefix = any(\n            model_type in type(model).__name__ for model_type in prefix_models\n        )\n        if add_prefix:\n            loading_kwargs[\"add_prefix_space\"] = True\n\n    while True:\n        try:\n            return AutoTokenizer.from_pretrained(model_id, **loading_kwargs)\n        except (JSONDecodeError, OSError, TypeError):\n            raise InvalidModel(f\"Could not load tokenizer for model {model_id!r}.\")\n        except (TimeoutError, RequestError):\n            logger.info(f\"Couldn't load tokenizer for {model_id!r}. Retrying.\")\n            sleep(5)\n            continue\n\n\ndef get_torch_dtype(docs\n    device: torch.device, torch_dtype_is_set: bool, bf16_available: bool\n) -&gt; str | torch.dtype:\n    \"\"\"Get the torch dtype, used for loading the model.\n\n    Args:\n        device:\n            The device to use.\n        torch_dtype_is_set:\n            Whether the torch data type is set in the model configuration.\n        bf16_available:\n            Whether bfloat16 is available.\n\n    Returns:\n        The torch dtype.\n    \"\"\"\n    using_cuda = device == torch.device(\"cuda\")\n    if using_cuda and torch_dtype_is_set:\n        return \"auto\"\n    elif using_cuda and bf16_available:\n        return torch.bfloat16\n    elif using_cuda:\n        return torch.float16\n    return torch.float32\n\n\ndef load_hf_model_config(docs\n    model_id: str,\n    num_labels: int,\n    id2label: dict[int, str],\n    label2id: dict[str, int],\n    revision: str,\n    model_cache_dir: str | None,\n    api_key: str | None,\n    trust_remote_code: bool,\n    run_with_cli: bool,\n) -&gt; \"PretrainedConfig\":\n    \"\"\"Load the Hugging Face model configuration.\n\n    Args:\n        model_id:\n            The Hugging Face model ID.\n        num_labels:\n            The number of labels in the dataset.\n        id2label:\n            The mapping from label IDs to labels.\n        label2id:\n            The mapping from labels to label IDs.\n        revision:\n            The revision of the model.\n        model_cache_dir:\n            The directory to cache the model in.\n        api_key:\n            The Hugging Face API key.\n        trust_remote_code:\n            Whether to trust remote code.\n        run_with_cli:\n            Whether the script is being run with the CLI.\n\n    Returns:\n        The Hugging Face model configuration.\n    \"\"\"\n    while True:\n        try:\n            config = AutoConfig.from_pretrained(\n                model_id,\n                num_labels=num_labels,\n                id2label=id2label,\n                label2id=label2id,\n                revision=revision,\n                token=api_key or True,\n                trust_remote_code=trust_remote_code,\n                cache_dir=model_cache_dir,\n            )\n            if config.eos_token_id is not None and config.pad_token_id is None:\n                if isinstance(config.eos_token_id, list):\n                    config.pad_token_id = config.eos_token_id[0]\n                else:\n                    config.pad_token_id = config.eos_token_id\n            return config\n        except KeyError as e:\n            key = e.args[0]\n            raise InvalidModel(\n                f\"The model config for the model {model_id!r} could not be \"\n                f\"loaded, as the key {key!r} was not found in the config.\"\n            )\n        except OSError as e:\n            # TEMP: When the model is gated then we cannot set cache dir, for some\n            # reason (transformers==4.38.2). This should be included back in when\n            # this is fixed.\n            if \"gated repo\" in str(e):\n                model_cache_dir = None\n                continue\n            raise InvalidModel(\n                f\"Couldn't load model config for {model_id!r}. The error was \"\n                f\"{e!r}. Skipping\"\n            )\n        except (TimeoutError, RequestError):\n            logger.info(f\"Couldn't load model config for {model_id!r}. Retrying.\")\n            sleep(5)\n            continue\n        except ValueError as e:\n            requires_trust_remote_code = \"trust_remote_code\" in str(e)\n            if requires_trust_remote_code:\n                raise NeedsAdditionalArgument(\n                    cli_argument=\"--trust-remote-code\",\n                    script_argument=\"trust_remote_code=True\",\n                    run_with_cli=run_with_cli,\n                )\n            raise e\n\ndocs\ndef setup_model_for_question_answering(model: \"PreTrainedModel\") -&gt; \"PreTrainedModel\":\n    \"\"\"Setup a model for question answering.\n\n    Args:\n        model:\n            The model to setup.\n\n    Returns:\n        The setup model.\n    \"\"\"\n    # Get the models' token type embedding children, if they exist\n    children = get_children_of_module(name=\"model\", module=model)\n\n    # If the model has token type embeddings then get them\n    if children:\n        # Get the list of attributes that are token type embeddings\n        attribute_list = list()\n        done = False\n        while not done:\n            for key, value in children.items():\n                attribute_list.append(key)\n                if isinstance(value, dict):\n                    children = value\n                else:\n                    done = True\n                break\n\n        # Get the token type embeddings\n        token_type_embeddings = model\n        for attribute in attribute_list:\n            token_type_embeddings = getattr(token_type_embeddings, attribute)\n\n        # If the token type embeddings has shape (1, ...) then set the shape to\n        # (2, ...) by randomly initializing the second token type embedding\n        if token_type_embeddings.weight.data.shape[0] == 1:\n            token_type_embeddings.weight.data = torch.cat(\n                (\n                    token_type_embeddings.weight.data,\n                    torch.rand_like(token_type_embeddings.weight.data),\n                ),\n                dim=0,\n            )\n            token_type_embeddings.num_embeddings = 2\n\n        # Set the model config to use the new type vocab size\n        model.config.type_vocab_size = 2\n\n    return model\n\n\ndef get_children_of_module(docs\n    name: str, module: nn.Module\n) -&gt; nn.Module | dict[str, t.Any] | None:\n    \"\"\"Get the children of a module.\n\n    Args:\n        name:\n            The name of the module.\n        module:\n            The module to get the children of.\n\n    Returns:\n        The children of the module, or None if the module has no children.\n    \"\"\"\n    if len(list(module.children())) == 0:\n        if name == \"token_type_embeddings\":\n            return module\n        else:\n            return None\n    else:\n        submodules = dict()\n        for subname, submodule in module.named_children():\n            children = get_children_of_module(name=subname, module=submodule)\n            if children:\n                submodules[subname] = children\n        return submodules\n\n\ndef align_model_and_tokenizer(docs\n    model: \"PreTrainedModel\",\n    tokenizer: \"PreTrainedTokenizer\",\n    raise_errors: bool = False,\n) -&gt; tuple[\"PreTrainedModel\", \"PreTrainedTokenizer\"]:\n    \"\"\"Aligns the model and the tokenizer.\n\n    Args:\n        model:\n            The model to fix.\n        tokenizer:\n            The tokenizer to fix.\n        raise_errors:\n            Whether to raise errors instead of trying to fix them silently.\n\n    Returns:\n        The fixed model and tokenizer.\n    \"\"\"\n    model_max_length = get_model_max_length(model=model, tokenizer=tokenizer)\n\n    # Ensure that the model max length is at least 5,000, to avoid OOM errors\n    model_max_length = min(model_max_length, 5_000)\n\n    if model_max_length &gt; 0:\n        tokenizer.model_max_length = model_max_length\n    else:\n        tokenizer.model_max_length = 512\n\n    # If we're not dealing with a generative model then we move it to CPU to avoid OOM\n    # errors\n    device: torch.device = torch.device(\"cpu\")\n    model_device = model.device\n    model.to(device)\n\n    # Manually check that this model max length is valid for the model, and adjust\n    # otherwise\n    initial_max_length = tokenizer.model_max_length\n    for max_length in range(initial_max_length, 0, -1):\n        tokenizer.model_max_length = max_length\n        dummy_inputs = torch.full(\n            size=(1, max_length),\n            fill_value=DUMMY_FILL_VALUE,\n            dtype=torch.long,\n            device=device,\n        )\n\n        with torch.inference_mode():\n            try:\n                model(dummy_inputs)\n                break\n\n            # This happens if `max_length` is too large\n            except IndexError:\n                continue\n\n    # If there is a mismatch between the vocab size according to the tokenizer and\n    # the vocab size according to the model, we raise an error\n    if hasattr(model.config, \"vocab_size\") and hasattr(tokenizer, \"vocab_size\"):\n        if model.config.vocab_size &lt; tokenizer.vocab_size:\n            if raise_errors:\n                raise InvalidModel(\n                    \"The vocab size of the tokenizer is larger than the vocab size of \"\n                    \"the model. As the --raise-errors option was specified, the \"\n                    \"embeddings of the model will not be automatically adjusted.\"\n                )\n            if hasattr(model, \"resize_token_embeddings\"):\n                model.resize_token_embeddings(new_num_tokens=tokenizer.vocab_size + 1)\n\n    if tokenizer.bos_token is None and tokenizer.eos_token is not None:\n        tokenizer.bos_token = tokenizer.eos_token\n        tokenizer.bos_token_id = tokenizer.eos_token_id\n\n    model.to(model_device)\n\n    return model, tokenizer\n\n\ndef get_model_max_length(docs\n    model: \"PreTrainedModel\", tokenizer: \"PreTrainedTokenizer\"\n) -&gt; int:\n    \"\"\"Get the maximum context length of a model.\n\n    Args:\n        model:\n            The model.\n        tokenizer:\n            The tokenizer.\n\n    Returns:\n        The maximum context length.\n    \"\"\"\n    all_max_lengths: list[int] = list()\n\n    if tokenizer is not None:\n        # Add the registered max length of the tokenizer\n        if hasattr(tokenizer, \"model_max_length\") and tokenizer.model_max_length &lt; int(\n            1e30\n        ):\n            all_max_lengths.append(tokenizer.model_max_length)\n\n        # Add the max length derived from the model's input sizes\n        if hasattr(tokenizer, \"max_model_input_sizes\"):\n            all_max_lengths.extend(\n                [\n                    size\n                    for size in tokenizer.max_model_input_sizes.values()\n                    if size is not None\n                ]\n            )\n\n    # Add max length candidates from the model's configuration\n    candidate_config_max_lengths = [\n        \"max_position_embeddings\",\n        \"model_max_length\",\n        \"max_sequence_length\",\n        \"sliding_window\",\n        \"sliding_window_size\",\n    ]\n    for candidate_config_max_length in candidate_config_max_lengths:\n        if (\n            hasattr(model.config, candidate_config_max_length)\n            and (value := getattr(model.config, candidate_config_max_length))\n            is not None\n        ):\n            all_max_lengths.append(value)\n\n    # To avoid models having artificially low max lengths, we remove any max lengths\n    # that are less than 128\n    all_max_lengths = [\n        max_length for max_length in all_max_lengths if max_length &gt;= 128\n    ]\n\n    if len(list(all_max_lengths)) &gt; 0:\n        model_max_length = min(list(all_max_lengths))\n    else:\n        model_max_length = -1\n\n    return model_max_length\n</code></pre>"},{"location":"api/scandeval/benchmark_modules/litellm/","title":"scandeval.benchmark_modules.litellm","text":"scandeval.benchmark_modules.litellm<p> source module scandeval.benchmark_modules.litellm </p> <p>Generative models from an inference API, using the LiteLLM framework.</p> <p> Classes </p> <ul> <li> <p>LiteLLMModel \u2014 A generative model from LiteLLM.</p> </li> </ul> <p> source class LiteLLMModel() </p> <p><p>Bases : BenchmarkModule</p></p> <p>A generative model from LiteLLM.</p> <p> Methods </p> <ul> <li> <p>generate \u2014 Generate outputs from the model.</p> </li> <li> <p>num_params \u2014 The number of parameters in the model.</p> </li> <li> <p>vocab_size \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length \u2014 The maximum length of the model.</p> </li> <li> <p>data_collator \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>extract_labels_from_generation \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class \u2014 The Trainer class to use for finetuning.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> <li> <p>prepare_dataset \u2014 Prepare the dataset for the model.</p> </li> </ul> <p> source method LiteLLMModel.generate(inputs: dict) \u2192 GenerativeModelOutput </p> <p>Generate outputs from the model.</p> <p> Parameters </p> <ul> <li> <p>inputs :  dict \u2014</p> <p>A batch of inputs to pass through the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>GenerativeModelOutput \u2014 The generated model outputs.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> <li> <p>NeedsAdditionalArgument</p> </li> </ul> <p> source method LiteLLMModel.num_params() \u2192 int </p> <p>The number of parameters in the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The number of parameters in the model.</p> </li> </ul> <p> source method LiteLLMModel.vocab_size() \u2192 int </p> <p>The vocabulary size of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The vocabulary size of the model.</p> </li> </ul> <p> source method LiteLLMModel.model_max_length() \u2192 int </p> <p>The maximum length of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum length of the model.</p> </li> </ul> <p> source method LiteLLMModel.data_collator() \u2192 c.Callable[[list[t.Any]], dict[str, t.Any]] </p> <p>The data collator used to prepare samples during finetuning.</p> <p> Returns </p> <ul> <li> <p>c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method LiteLLMModel.extract_labels_from_generation() \u2192 ExtractLabelsFunction </p> <p>The function used to extract the labels from the generated output.</p> <p> Returns </p> <ul> <li> <p>ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method LiteLLMModel.trainer_class() \u2192 t.Type[Trainer] </p> <p>The Trainer class to use for finetuning.</p> <p> Returns </p> <ul> <li> <p>t.Type[Trainer] \u2014 The Trainer class.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source classmethod LiteLLMModel.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>e</p> </li> </ul> <p> source classmethod LiteLLMModel.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> source method LiteLLMModel.prepare_dataset(dataset: DatasetDict, task: Task, itr_idx: int) \u2192 DatasetDict </p> <p>Prepare the dataset for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  DatasetDict \u2014</p> <p>The dataset to prepare.</p> </li> <li> <p>task :  Task \u2014</p> <p>The task to prepare the dataset for.</p> </li> <li> <p>itr_idx :  int \u2014</p> <p>The index of the dataset in the iterator.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetDict \u2014 The prepared dataset.</p> </li> </ul>"},{"location":"src/scandeval/benchmark_modules/litellm/","title":"scandeval.benchmark_modules.litellm","text":"scandeval.benchmark_modules.litellm<p> docs module scandeval.benchmark_modules.litellm </p> <pre><code>\"\"\"Generative models from an inference API, using the LiteLLM framework.\"\"\"\n\nimport collections.abc as c\nimport itertools as it\nimport json\nimport logging\nimport random\nimport re\nimport typing as t\nfrom functools import cached_property, partial\nfrom time import sleep\n\nimport litellm\nfrom datasets import DatasetDict\nfrom huggingface_hub import HfApi\nfrom huggingface_hub.hf_api import RepositoryNotFoundError, RevisionNotFoundError\nfrom huggingface_hub.utils import HFValidationError\nfrom litellm.exceptions import (\n    APIError,\n    AuthenticationError,\n    BadRequestError,\n    InternalServerError,\n    NotFoundError,\n)\nfrom litellm.types.utils import ModelResponse\nfrom requests.exceptions import RequestException\nfrom transformers import Trainer\n\nfrom scandeval.benchmark_modules.hf import (\n    HuggingFaceEncoderModel,\n    load_hf_model_config,\n    load_tokenizer,\n)\n\nfrom ..constants import MAX_LOGPROBS, SUPERTASKS_USING_LOGPROBS, TASKS_USING_JSON\nfrom ..data_models import BenchmarkConfig, GenerativeModelOutput, ModelConfig, Task\nfrom ..enums import BatchingPreference, Framework, ModelType\nfrom ..exceptions import (\n    InvalidBenchmark,\n    NeedsAdditionalArgument,\n    NeedsEnvironmentVariable,\n    NeedsExtraInstalled,\n)\nfrom ..task_utils import (\n    question_answering,\n    sequence_classification,\n    text_to_text,\n    token_classification,\n)\nfrom ..types import ExtractLabelsFunction\nfrom ..utils import create_model_cache_dir\nfrom .base import BenchmarkModule\n\nlogger = logging.getLogger(\"scandeval\")\n\n\nVOCAB_SIZE_MAPPING = {\n    # OpenAI models\n    \"(text-)?(ada|babbage|curie|davinci)(-001)?\": 50_257,\n    \"(code|text)-davinci-00[2-9]\": 50_281,\n    \"gpt-3.5-turbo(-16k)?(-[0-9]{4})?\": 100_256,\n    \"gpt-4-(32k)?(-[0-9]{4})?\": 100_256,\n    \"gpt-4-[0-9]{4}-preview\": 100_256,\n    \"gpt-4-turbo(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": 100_256,\n    \"gpt-4-(vision|turbo)(-preview)?\": 100_256,\n    \"gpt-3.5-turbo-instruct(-[0-9]{4})?\": 100_256,\n    \"gpt-4o(-mini)?(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": 200_019,\n    # Anthropic models\n    \"claude-[1-9](-[1-9])?-(opus|sonnet|haiku)-[0-9]{8}\": -1,\n}\n\n\nMODEL_MAX_LENGTH_MAPPING = {\n    # OpenAI models\n    \"(text-)?(ada|babbage|curie|davinci)(-001)?\": 2_050,\n    \"text-davinci-00[2-9]\": 4_098,\n    \"code-davinci-00[1-9]\": 8_002,\n    \"gpt-3.5-turbo-0613\": 4_096,\n    \"gpt-3.5-turbo(-[0-9]{4})?\": 16_385,\n    \"gpt-3.5-turbo-16k(-[0-9]{4})?\": 16_384,\n    \"gpt-4(-[0-9]{4})?\": 8_191,\n    \"gpt-4-32k(-[0-9]{4})?\": 32_767,\n    \"gpt-4-[0-9]{4}-preview\": 128_000,\n    \"gpt-4-turbo(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": 128_000,\n    \"gpt-4-(vision|turbo)(-preview)?\": 128_000,\n    \"gpt-3.5-turbo-instruct(-[0-9]{4})?\": 4_095,\n    \"gpt-4o(-mini)?(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": 128_000,\n    # Anthropic models\n    \"claude-[1-9](-[1-9])?-(opus|sonnet|haiku)-[0-9]{8}\": 200_000,\n}\n\n\nNUM_PARAMS_MAPPING = {\n    \"(text-)?ada(-001)?\": 350_000_000,\n    \"(text-)?babbage(-001)?\": 3_000_000_000,\n    \"(text-)?curie(-001)?\": 13_000_000_000,\n    \"((text|code)-)?davinci(-00[1-9])?\": 175_000_000_000,\n    \"gpt-(3.5|4)-turbo-((16|32)k)?(-[0-9]{4})?\": -1,\n    \"gpt-4-[0-9]{4}-preview\": -1,\n    \"gpt-4-turbo(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": -1,\n    \"gpt-4-(vision|turbo)(-preview)?\": -1,\n    \"gpt-3.5-turbo-instruct(-[0-9]{4})?\": -1,\n    \"gpt-4o(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": -1,\n    \"gpt-4o-mini(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": -1,\n    # Anthropic models\n    \"claude-[1-9](-[1-9])?-(opus|sonnet|haiku)-[0-9]{8}\": -1,\n}\n\n\nclass LiteLLMModel(BenchmarkModule):docs\n    \"\"\"A generative model from LiteLLM.\"\"\"\n\n    _is_generative = True\n    batching_preference = BatchingPreference.SINGLE_SAMPLE\n\n    def generate(self, inputs: dict) -&gt; GenerativeModelOutput:docs\n        \"\"\"Generate outputs from the model.\n\n        Args:\n            inputs:\n                A batch of inputs to pass through the model.\n\n        Returns:\n            The generated model outputs.\n        \"\"\"\n        assert \"messages\" in inputs, \"The input must contain a 'messages' key.\"\n        assert (\n            len(inputs[\"messages\"]) == 1\n        ), \"API models only support single-sample batching.\"\n        messages = inputs[\"messages\"][0]\n\n        generation_kwargs: dict[str, t.Any] = dict(\n            model=self.model_config.model_id,\n            max_tokens=self.dataset_config.max_generated_tokens,\n            stop=[\"\\n\\n\"],\n            temperature=0.0,\n            seed=4242,\n            api_key=self.benchmark_config.api_key,\n            api_base=self.benchmark_config.api_base,\n            api_version=self.benchmark_config.api_version,\n        )\n\n        if self.dataset_config.task.supertask in SUPERTASKS_USING_LOGPROBS:\n            generation_kwargs[\"logprobs\"] = True\n            generation_kwargs[\"top_logprobs\"] = MAX_LOGPROBS\n\n        if self.dataset_config.task.name in TASKS_USING_JSON:\n            assert (\n                \"json\" in messages[0][\"content\"]\n            ), \"Prompt must contain 'json' for JSON tasks.\"\n            generation_kwargs[\"response_format\"] = dict(type=\"json_object\")\n\n        # This drops generation kwargs that are not supported by the model\n        litellm.drop_params = True\n\n        # Extract the generated sequences from the model response. Some APIs cannot\n        # handle using newlines as stop sequences, so we try both.\n        num_attempts = 10\n        for _ in range(num_attempts):\n            try:\n                model_response = litellm.completion(\n                    messages=messages, max_retries=3, **generation_kwargs\n                )\n                break\n            except BadRequestError as e:\n                if \"stop_sequences\" not in str(e).lower():\n                    raise InvalidBenchmark(\n                        f\"Failed to generate text. The error message was: {e}\"\n                    )\n                generation_kwargs[\"stop\"] = None\n            except APIError as e:\n                raise InvalidBenchmark(\n                    f\"Failed to generate text. The error message was: {e}\"\n                )\n            except InternalServerError as e:\n                if \"overloaded\" not in str(e).lower():\n                    raise InvalidBenchmark(\n                        f\"Failed to generate text. The error message was: {e}\"\n                    )\n                sleep(1)\n                continue\n            except AuthenticationError:\n                raise NeedsAdditionalArgument(\n                    cli_argument=\"--api-key\",\n                    script_argument=\"api_key=&lt;your-api-key&gt;\",\n                    run_with_cli=self.benchmark_config.run_with_cli,\n                )\n        else:\n            raise InvalidBenchmark(\n                message=f\"Failed to generate text, after {num_attempts} attempts.\"\n            )\n\n        assert isinstance(model_response, ModelResponse)\n        model_response_choices = model_response.choices[0]\n        assert isinstance(model_response_choices, litellm.Choices)\n        generation_output = model_response_choices.message[\"content\"] or \"\"\n        generation_output = generation_output.strip()\n\n        # Structure the model output as a GenerativeModelOutput object\n        model_output = GenerativeModelOutput(sequences=[generation_output])\n        if hasattr(model_response_choices, \"logprobs\"):\n            logprobs_list: list[list[tuple[str, float]]] = [\n                [(dct[\"token\"], dct[\"logprob\"]) for dct in content[\"top_logprobs\"]]\n                for content in model_response_choices.logprobs[\"content\"]\n            ]\n            model_output.scores = [logprobs_list]\n\n        return model_output\n\n    @cached_property\n    def num_params(self) -&gt; int:docs\n        \"\"\"The number of parameters in the model.\n\n        Returns:\n            The number of parameters in the model.\n        \"\"\"\n        for key, value in NUM_PARAMS_MAPPING.items():\n            if re.match(pattern=key, string=self.model_config.model_id) is not None:\n                return value\n\n        if self.model_config.model_id.startswith(\"huggingface/\"):\n            model_id = self.model_config.model_id.split(sep=\"/\", maxsplit=1)[-1]\n            if HuggingFaceEncoderModel.model_exists(\n                model_id=model_id, benchmark_config=self.benchmark_config\n            ):\n                hf_config = load_hf_model_config(\n                    model_id=model_id,\n                    num_labels=self.dataset_config.num_labels,\n                    id2label=self.dataset_config.id2label,\n                    label2id=self.dataset_config.label2id,\n                    revision=self.model_config.revision,\n                    model_cache_dir=self.model_config.model_cache_dir,\n                    api_key=self.benchmark_config.api_key,\n                    trust_remote_code=self.benchmark_config.trust_remote_code,\n                    run_with_cli=self.benchmark_config.run_with_cli,\n                )\n\n                hf_api = HfApi()\n                try:\n                    repo_info = hf_api.model_info(\n                        repo_id=model_id,\n                        revision=self.model_config.revision,\n                        token=self.benchmark_config.api_key or True,\n                    )\n                except (\n                    RepositoryNotFoundError,\n                    RevisionNotFoundError,\n                    RequestException,\n                    HFValidationError,\n                ):\n                    repo_info = None\n\n                if (\n                    repo_info is not None\n                    and hasattr(repo_info, \"safetensors\")\n                    and repo_info.safetensors is not None\n                    and \"total\" in repo_info.safetensors\n                ):\n                    return repo_info.safetensors[\"total\"]\n                elif (\n                    hasattr(hf_config, \"num_params\")\n                    and hf_config.num_params is not None\n                ):\n                    return hf_config.num_params\n\n        return -1\n\n    @cached_property\n    def vocab_size(self) -&gt; int:docs\n        \"\"\"The vocabulary size of the model.\n\n        Returns:\n            The vocabulary size of the model.\n        \"\"\"\n        for key, value in VOCAB_SIZE_MAPPING.items():\n            if re.match(pattern=key, string=self.model_config.model_id) is not None:\n                return value\n\n        if self.model_config.model_id.startswith(\"huggingface/\"):\n            model_id = self.model_config.model_id.split(sep=\"/\", maxsplit=1)[-1]\n            if HuggingFaceEncoderModel.model_exists(\n                model_id=model_id, benchmark_config=self.benchmark_config\n            ):\n                hf_config = load_hf_model_config(\n                    model_id=model_id,\n                    num_labels=self.dataset_config.num_labels,\n                    id2label=self.dataset_config.id2label,\n                    label2id=self.dataset_config.label2id,\n                    revision=self.model_config.revision,\n                    model_cache_dir=self.model_config.model_cache_dir,\n                    api_key=self.benchmark_config.api_key,\n                    trust_remote_code=self.benchmark_config.trust_remote_code,\n                    run_with_cli=self.benchmark_config.run_with_cli,\n                )\n\n                tokenizer = load_tokenizer(\n                    model=None,\n                    model_id=model_id,\n                    trust_remote_code=self.benchmark_config.trust_remote_code,\n                )\n\n                if (\n                    hasattr(hf_config, \"vocab_size\")\n                    and hf_config.vocab_size is not None\n                ):\n                    vocab_size = hf_config.vocab_size\n                elif (\n                    hasattr(tokenizer, \"vocab_size\")\n                    and tokenizer.vocab_size is not None\n                ):\n                    vocab_size = tokenizer.vocab_size\n                else:\n                    vocab_size = -1\n                return vocab_size\n\n        return -1\n\n    @cached_property\n    def model_max_length(self) -&gt; int:docs\n        \"\"\"The maximum length of the model.\n\n        Returns:\n            The maximum length of the model.\n        \"\"\"\n        for key, value in MODEL_MAX_LENGTH_MAPPING.items():\n            if re.match(pattern=key, string=self.model_config.model_id) is not None:\n                return value\n\n        if self.model_config.model_id.startswith(\"huggingface/\"):\n            model_id = self.model_config.model_id.split(sep=\"/\", maxsplit=1)[-1]\n            if HuggingFaceEncoderModel.model_exists(\n                model_id=model_id, benchmark_config=self.benchmark_config\n            ):\n                hf_config = load_hf_model_config(\n                    model_id=model_id,\n                    num_labels=self.dataset_config.num_labels,\n                    id2label=self.dataset_config.id2label,\n                    label2id=self.dataset_config.label2id,\n                    revision=self.model_config.revision,\n                    model_cache_dir=self.model_config.model_cache_dir,\n                    api_key=self.benchmark_config.api_key,\n                    trust_remote_code=self.benchmark_config.trust_remote_code,\n                    run_with_cli=self.benchmark_config.run_with_cli,\n                )\n\n                tokenizer = load_tokenizer(\n                    model=None,\n                    model_id=model_id,\n                    trust_remote_code=self.benchmark_config.trust_remote_code,\n                )\n\n                all_max_lengths: list[int] = list()\n\n                # Add the registered max length of the tokenizer\n                if hasattr(\n                    tokenizer, \"model_max_length\"\n                ) and tokenizer.model_max_length &lt; int(1e30):\n                    all_max_lengths.append(tokenizer.model_max_length)\n\n                # Add the max length derived from the model's input sizes\n                if hasattr(tokenizer, \"max_model_input_sizes\"):\n                    all_max_lengths.extend(\n                        [\n                            size\n                            for size in tokenizer.max_model_input_sizes.values()\n                            if size is not None\n                        ]\n                    )\n\n                # Add max length candidates from the model's configuration\n                candidate_config_max_lengths = [\n                    \"max_position_embeddings\",\n                    \"max_sequence_length\",\n                    \"model_max_length\",\n                    \"sliding_window\",\n                    \"sliding_window_size\",\n                    \"n_positions\",\n                ]\n                for candidate_config_max_length in candidate_config_max_lengths:\n                    if (\n                        hasattr(hf_config, candidate_config_max_length)\n                        and (value := getattr(hf_config, candidate_config_max_length))\n                        is not None\n                    ):\n                        all_max_lengths.append(value)\n\n                # To avoid models having artificially low max lengths, we remove any max\n                # lengths that are less than 128\n                all_max_lengths = [\n                    max_length for max_length in all_max_lengths if max_length &gt;= 128\n                ]\n\n                if len(list(all_max_lengths)) &gt; 0:\n                    return min(list(all_max_lengths))\n\n        return -1\n\n    @cached_propertydocs\n    def data_collator(self) -&gt; c.Callable[[list[t.Any]], dict[str, t.Any]]:\n        \"\"\"The data collator used to prepare samples during finetuning.\n\n        Returns:\n            The data collator.\n        \"\"\"\n        raise NotImplementedError(\n            \"The `data_collator` property has not been implemented for LiteLLM models.\"\n        )\n\n    @cached_property\n    def extract_labels_from_generation(self) -&gt; ExtractLabelsFunction:docs\n        \"\"\"The function used to extract the labels from the generated output.\n\n        Returns:\n            The function used to extract the labels from the generated output.\n        \"\"\"\n        match self.dataset_config.task.supertask:\n            case \"sequence-classification\":\n                return partial(\n                    sequence_classification.extract_labels_from_generation,\n                    dataset_config=self.dataset_config,\n                )\n            case \"text-to-text\":\n                return text_to_text.extract_labels_from_generation\n            case \"token-classification\":\n                return partial(\n                    token_classification.extract_labels_from_generation,\n                    dataset_config=self.dataset_config,\n                )\n            case \"question-answering\":\n                return question_answering.extract_labels_from_generation\n            case _:\n                raise NotImplementedError(\n                    f\"Unsupported task supertask: {self.dataset_config.task.supertask}.\"\n                )\n\n    @cached_property\n    def trainer_class(self) -&gt; t.Type[\"Trainer\"]:docs\n        \"\"\"The Trainer class to use for finetuning.\n\n        Returns:\n            The Trainer class.\n        \"\"\"\n        raise NotImplementedError(\n            \"The `trainer_class` property has not been implemented for LiteLLM models.\"\n        )\n\n    @classmethod\n    def model_exists(docs\n        cls, model_id: str, benchmark_config: BenchmarkConfig\n    ) -&gt; bool | NeedsExtraInstalled | NeedsEnvironmentVariable:\n        \"\"\"Check if a model exists.\n\n        Args:\n            model_id:\n                The model ID.\n            benchmark_config:\n                The benchmark configuration.\n\n        Returns:\n            Whether the model exists, or an error describing why we cannot check\n            whether the model exists.\n        \"\"\"\n        if model_id in litellm.model_list:\n            return True\n\n        num_attempts = 10\n        for _ in range(num_attempts):\n            try:\n                litellm.completion(\n                    messages=[dict(role=\"user\", content=\"X\")],\n                    model=model_id,\n                    max_tokens=1,\n                    api_key=benchmark_config.api_key,\n                    api_base=benchmark_config.api_base,\n                    api_version=benchmark_config.api_version,\n                )\n                return True\n            except APIError as e:\n                if \"'503 Service Unavailable\" not in str(e):\n                    raise e\n                logger.warning(\n                    f\"Failed to check if model {model_id!r} exists. Retrying in \"\n                    f\"{num_attempts} seconds...\"\n                )\n                sleep(10)\n            except (BadRequestError, NotFoundError):\n                candidate_models = [\n                    candidate_model_id\n                    for candidate_model_id in litellm.model_list\n                    if candidate_model_id.startswith(model_id)\n                ]\n                match len(candidate_models):\n                    case 0:\n                        pass\n                    case 1:\n                        logger.warning(\n                            f\"Could not find the model ID {model_id!r}. Did you mean \"\n                            f\"{candidate_models[0]!r}?\"\n                        )\n                    case _:\n                        candidate_models_str = \"', '\".join(candidate_models)\n                        logger.warning(\n                            f\"Could not find the model ID {model_id!r}. Did you mean \"\n                            f\"any of the following model IDs: '{candidate_models_str}'?\"\n                        )\n                return False\n        else:\n            logger.error(\n                f\"Failed to check if model {model_id!r} exists after {num_attempts} \"\n                \"attempts. Assuming it does not exist.\"\n            )\n            return False\n\n    @classmethod\n    def get_model_config(docs\n        cls, model_id: str, benchmark_config: BenchmarkConfig\n    ) -&gt; ModelConfig:\n        \"\"\"Fetch the model configuration.\n\n        Args:\n            model_id:\n                The model ID.\n            benchmark_config:\n                The benchmark configuration.\n\n        Returns:\n            The model configuration.\n        \"\"\"\n        return ModelConfig(\n            model_id=model_id,\n            revision=\"main\",\n            framework=Framework.API,\n            task=\"text-generation\",\n            languages=list(),\n            model_cache_dir=create_model_cache_dir(\n                cache_dir=benchmark_config.cache_dir, model_id=model_id\n            ),\n            model_type=ModelType.API,\n            adapter_base_model_id=None,\n        )\n\n    def prepare_dataset(docs\n        self, dataset: DatasetDict, task: Task, itr_idx: int\n    ) -&gt; DatasetDict:\n        \"\"\"Prepare the dataset for the model.\n\n        This includes things like tokenisation.\n\n        Args:\n            dataset:\n                The dataset to prepare.\n            task:\n                The task to prepare the dataset for.\n            itr_idx:\n                The index of the dataset in the iterator.\n\n        Returns:\n            The prepared dataset.\n        \"\"\"\n        if task.supertask == \"question-answering\":\n            dataset = dataset.map(\n                lambda examples: dict(\n                    label=[\n                        dict(\n                            id=id,\n                            answers=dict(\n                                answer_start=answer_dct[\"answer_start\"],\n                                text=[\n                                    answer_text.lower()\n                                    for answer_text in answer_dct[\"text\"]\n                                ],\n                            ),\n                        )\n                        for id, answer_dct in zip(examples[\"id\"], examples[\"answers\"])\n                    ]\n                ),\n                batched=True,\n                load_from_cache_file=False,\n                keep_in_memory=True,\n            )\n\n        if self.benchmark_config.few_shot:\n            few_shot_examples = self._extract_few_shot_examples(\n                dataset=dataset, task=task, itr_idx=itr_idx\n            )\n        else:\n            few_shot_examples = list()\n\n        dataset[\"test\"] = dataset[\"test\"].map(\n            partial(self._apply_prompt, few_shot_examples=few_shot_examples, task=task),\n            batched=True,\n            load_from_cache_file=False,\n            keep_in_memory=True,\n        )\n\n        return dataset\n\n    def _extract_few_shot_examples(\n        self, dataset: DatasetDict, task: Task, itr_idx: int\n    ) -&gt; list[dict[str, t.Any]]:\n        \"\"\"Extract few-shot examples from a dataset.\n\n        This will always extract the examples from the training split.\n\n        We ensure that the few-shot examples are unique by picking them one at a time.\n\n        Args:\n            dataset:\n                The dataset to extract the few-shot examples from.\n            task:\n                The task that is being benchmarked.\n            itr_idx:\n                The index of the dataset in the iterator.\n\n        Returns:\n            The few-shot examples.\n        \"\"\"\n        random_seed = 4242 + itr_idx\n        num_few_shots = self.dataset_config.num_few_shot_examples\n        few_shot_examples: list[dict[str, t.Any]] = list()\n        shuffled_train = dataset[\"train\"].shuffle(seed=random_seed)\n\n        match task.supertask:\n            case \"sequence-classification\":\n                labels = it.cycle(self.dataset_config.task.labels)\n                while (\n                    len(few_shot_examples) &lt; num_few_shots and len(shuffled_train) &gt; 0\n                ):\n                    label = next(labels)\n                    possible_examples = shuffled_train.filter(\n                        lambda x: x[\"label\"].lower() == label.lower()\n                    )\n                    if len(possible_examples) == 0:\n                        continue\n                    example = possible_examples.select(range(1))[0]\n                    few_shot_examples.append(example)\n                    shuffled_train = shuffled_train.filter(\n                        lambda x: x[\"text\"] != example[\"text\"]\n                    )\n\n            case \"text-to-text\":\n                while (\n                    len(few_shot_examples) &lt; num_few_shots and len(shuffled_train) &gt; 0\n                ):\n                    example = shuffled_train.select(range(1))[0]\n                    few_shot_examples.append(example)\n                    shuffled_train = shuffled_train.filter(\n                        lambda x: x[\"text\"] != example[\"text\"]\n                    )\n\n            case \"token-classification\":\n                labels = it.cycle(\n                    [\n                        label.lower()\n                        for label in self.dataset_config.task.labels\n                        if label.lower().startswith(\"b-\")\n                    ]\n                )\n                while (\n                    len(few_shot_examples) &lt; num_few_shots and len(shuffled_train) &gt; 0\n                ):\n                    label = next(labels)\n                    possible_examples = shuffled_train.filter(\n                        lambda x: label in [tag.lower() for tag in x[\"labels\"]]\n                    )\n                    if len(possible_examples) == 0:\n                        continue\n                    example = possible_examples.select(range(1))[0]\n                    few_shot_examples.append(example)\n                    shuffled_train = shuffled_train.filter(\n                        lambda x: x[\"text\"] != example[\"text\"]\n                    )\n\n            case \"question-answering\":\n                # Locate the maximum number of tokens that constitutes a short example\n                for max_num_tokens in [512, 1024, 2048, 4096, 8192]:\n                    train_with_short_examples = dataset[\"train\"].filter(\n                        lambda example: len(example[\"context\"]) &lt; max_num_tokens\n                    )\n                    num_short_examples = len(train_with_short_examples)\n                    if num_short_examples &gt;= self.dataset_config.num_few_shot_examples:\n                        break\n                else:\n                    raise InvalidBenchmark(\n                        \"Could not find enough short examples for few-shot learning.\"\n                    )\n\n                shuffled_train = train_with_short_examples.shuffle(seed=random_seed)\n                while (\n                    len(few_shot_examples) &lt; num_few_shots and len(shuffled_train) &gt; 0\n                ):\n                    example = shuffled_train.select(range(1))[0]\n                    few_shot_examples.append(example)\n                    shuffled_train = shuffled_train.filter(\n                        lambda x: x[\"context\"] != example[\"context\"]\n                    )\n\n            case _:\n                raise NotImplementedError(\n                    f\"Unsupported task supertask: {task.supertask}.\"\n                )\n\n        random.seed(random_seed)\n        random.shuffle(few_shot_examples)\n        return few_shot_examples\n\n    def _apply_prompt(\n        self,\n        examples: dict[str, t.Any],\n        few_shot_examples: list[dict[str, t.Any]],\n        task: Task,\n    ) -&gt; dict[str, t.Any]:\n        \"\"\"Apply prompt template to an example, potentially with few-shot examples.\n\n        Args:\n            examples:\n                The examples to apply the few-shot examples to.\n            few_shot_examples:\n                The few-shot examples to apply.\n            task:\n                The task that is being benchmarked.\n\n        Returns:\n            The example with the few-shot examples applied.\n        \"\"\"\n\n        def split_section(section: str) -&gt; tuple[str, str]:\n            \"\"\"Split a section of the prompt to user and assistant messages.\"\"\"\n            user_part = \"\\n\".join(section.split(\"\\n\")[:-1])\n            assistant_part = section.split(\"\\n\")[-1]\n            return user_part, assistant_part\n\n        match task.supertask:\n            case \"sequence-classification\":\n                few_shot_sections = [\n                    self.dataset_config.prompt_template.format(\n                        text=example[\"text\"].replace(\"\\n\", \" \").strip(),\n                        label=example[\"label\"].replace(\"\\n\", \" \").strip(),\n                    )\n                    for example in few_shot_examples\n                ]\n                new_sections = [\n                    self.dataset_config.prompt_template.format(\n                        text=text.replace(\"\\n\", \" \").strip(), label=\"\"\n                    )\n                    for text in examples[\"text\"]\n                ]\n\n            case \"text-to-text\":\n                few_shot_sections = [\n                    self.dataset_config.prompt_template.format(\n                        text=example[\"text\"].replace(\"\\n\", \" \").strip(),\n                        target_text=example[\"target_text\"].replace(\"\\n\", \" \").strip(),\n                    )\n                    for example in few_shot_examples\n                ]\n                new_sections = [\n                    self.dataset_config.prompt_template.format(\n                        text=text.replace(\"\\n\", \" \").strip(), target_text=\"\"\n                    )\n                    for text in examples[\"text\"]\n                ]\n\n            case \"token-classification\":\n\n                def create_label(example: dict) -&gt; str:\n                    prompt_labels = self.dataset_config.prompt_label_mapping.values()\n                    labels: dict[str, list[str]] = {\n                        prompt_label: list() for prompt_label in prompt_labels\n                    }\n                    for token, label in zip(example[\"tokens\"], example[\"labels\"]):\n                        label = label.lower()\n                        if label == \"o\":\n                            continue\n                        prompt_label = self.dataset_config.prompt_label_mapping[label]\n                        if label.startswith(\"b-\"):\n                            labels[prompt_label].append(token)\n                        elif label.startswith(\"i-\"):\n                            labels[prompt_label][-1] += \" \" + token\n                    return json.dumps(labels, ensure_ascii=False)\n\n                few_shot_sections = [\n                    self.dataset_config.prompt_template.format(\n                        text=\" \".join(example[\"tokens\"]).replace(\"\\n\", \" \").strip(),\n                        label=create_label(example=example),\n                    )\n                    for example in few_shot_examples\n                ]\n                new_sections = [\n                    self.dataset_config.prompt_template.format(\n                        text=\" \".join(tokens).replace(\"\\n\", \" \").strip(), label=\"\"\n                    )\n                    for tokens in examples[\"tokens\"]\n                ]\n\n            case \"question-answering\":\n                few_shot_sections = [\n                    self.dataset_config.prompt_template.format(\n                        text=example[\"context\"].replace(\"\\n\", \" \").strip(),\n                        question=example[\"question\"].replace(\"\\n\", \" \").strip(),\n                        label=example[\"answers\"][\"text\"][0].replace(\"\\n\", \" \"),\n                    )\n                    for example in few_shot_examples\n                ]\n                new_sections = [\n                    self.dataset_config.prompt_template.format(\n                        text=context.replace(\"\\n\", \" \").strip(),\n                        question=question.replace(\"\\n\", \" \").strip(),\n                        label=\"\",\n                    )\n                    for context, question in zip(\n                        examples[\"context\"], examples[\"question\"]\n                    )\n                ]\n\n            case _:\n                raise NotImplementedError(\n                    f\"Unsupported task supertask: {task.supertask}.\"\n                )\n\n        few_shot_messages = [\n            dict(role=role, content=content.split(\":\", 1)[1].strip())\n            for section in few_shot_sections\n            for role, content in zip(\n                it.cycle([\"user\", \"assistant\"]), split_section(section=section)\n            )\n            if content.split(\":\", 1)[1].strip() != \"\"\n        ]\n\n        if self.dataset_config.prompt_prefix:\n            few_shot_messages[0][\"content\"] = (\n                self.dataset_config.prompt_prefix\n                + \"\\n\\n\"\n                + few_shot_messages[0][\"content\"]\n            )\n\n        examples[\"messages\"] = [\n            few_shot_messages\n            + [\n                dict(\n                    role=\"user\",\n                    content=split_section(section=new_section)[0]\n                    .split(\":\", 1)[1]\n                    .strip(),\n                )\n            ]\n            for new_section in new_sections\n        ]\n\n        return examples\n</code></pre>"},{"location":"api/scandeval/benchmark_modules/vllm/","title":"scandeval.benchmark_modules.vllm","text":"scandeval.benchmark_modules.vllm<p> source module scandeval.benchmark_modules.vllm </p> <p>Generative models using the vLLM inference framework.</p> <p> Classes </p> <ul> <li> <p>VLLMModel \u2014 A generative model using the vLLM inference framework.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>load_tokenizer \u2014 Load the tokenizer.</p> </li> <li> <p>clear_vllm \u2014 Clear the GPU memory used by the vLLM model, enabling re-initialisation.</p> </li> </ul> <p> source class VLLMModel(dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) </p> <p><p>Bases : HuggingFaceEncoderModel</p></p> <p>A generative model using the vLLM inference framework.</p> <p>Initialise the vLLM model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>extract_labels_from_generation \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>prepare_dataset \u2014 Prepare the dataset for the model.</p> </li> <li> <p>generate \u2014 Generate outputs from the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> <li> <p>data_collator \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>trainer_class \u2014 The Trainer class to use for finetuning.</p> </li> </ul> <p> source method VLLMModel.extract_labels_from_generation() \u2192 ExtractLabelsFunction </p> <p>The function used to extract the labels from the generated output.</p> <p> Returns </p> <ul> <li> <p>ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method VLLMModel.prepare_dataset(dataset: DatasetDict, task: Task, itr_idx: int) \u2192 DatasetDict </p> <p>Prepare the dataset for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  DatasetDict \u2014</p> <p>The dataset to prepare.</p> </li> <li> <p>task :  Task \u2014</p> <p>The task to prepare the dataset for.</p> </li> <li> <p>itr_idx :  int \u2014</p> <p>The index of the dataset in the iterator.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetDict \u2014 The prepared dataset.</p> </li> </ul> <p> source method VLLMModel.generate(inputs: dict) \u2192 GenerativeModelOutput </p> <p>Generate outputs from the model.</p> <p> Parameters </p> <ul> <li> <p>inputs :  dict \u2014</p> <p>A batch of inputs to pass through the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>GenerativeModelOutput \u2014 The generated model outputs.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source classmethod VLLMModel.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> source classmethod VLLMModel.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source method VLLMModel.data_collator() \u2192 c.Callable[[list[t.Any]], dict[str, t.Any]] </p> <p>The data collator used to prepare samples during finetuning.</p> <p> Returns </p> <ul> <li> <p>c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method VLLMModel.trainer_class() \u2192 t.Type[Trainer] </p> <p>The Trainer class to use for finetuning.</p> <p> Returns </p> <ul> <li> <p>t.Type[Trainer] \u2014 The Trainer class.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source load_tokenizer(model_id: str, revision: str, adapter_base_model_id: str | None, trust_remote_code: bool, model_max_length: int, model_cache_dir: str) \u2192 PreTrainedTokenizer </p> <p>Load the tokenizer.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model identifier.</p> </li> <li> <p>revision :  str \u2014</p> <p>The revision of the model.</p> </li> <li> <p>adapter_base_model_id :  str | None \u2014</p> <p>The base model ID for the adapter model. Can be None if the model is not an adapter model.</p> </li> <li> <p>trust_remote_code :  bool \u2014</p> <p>Whether to trust remote code.</p> </li> <li> <p>model_max_length :  int \u2014</p> <p>The maximum length of the model.</p> </li> <li> <p>model_cache_dir :  str \u2014</p> <p>The cache directory for the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>PreTrainedTokenizer \u2014 The loaded tokenizer.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source clear_vllm() \u2192 None </p> <p>Clear the GPU memory used by the vLLM model, enabling re-initialisation.</p>"},{"location":"src/scandeval/benchmark_modules/vllm/","title":"scandeval.benchmark_modules.vllm","text":"scandeval.benchmark_modules.vllm<p> docs module scandeval.benchmark_modules.vllm </p> <pre><code>\"\"\"Generative models using the vLLM inference framework.\"\"\"\n\nimport collections.abc as c\nimport importlib.util\nimport itertools as it\nimport json\nimport logging\nimport random\nimport sys\nimport typing as t\nfrom functools import cached_property, partial\nfrom pathlib import Path\nfrom time import sleep\nfrom types import MethodType\n\nimport torch\nfrom datasets import DatasetDict\nfrom huggingface_hub import snapshot_download\nfrom tqdm.auto import tqdm\nfrom transformers import AutoConfig, AutoTokenizer, PreTrainedTokenizer, Trainer\nfrom urllib3.exceptions import RequestError\n\nfrom ..constants import (\n    GENERATIVE_MODEL_TASKS,\n    MAX_LOGPROBS,\n    SUPERTASKS_USING_LOGPROBS,\n    TASKS_USING_JSON,\n)\nfrom ..data_models import (\n    BenchmarkConfig,\n    DatasetConfig,\n    GenerativeModelOutput,\n    ModelConfig,\n    Task,\n)\nfrom ..enums import BatchingPreference, Framework, ModelType\nfrom ..exceptions import (\n    InvalidBenchmark,\n    InvalidModel,\n    NeedsEnvironmentVariable,\n    NeedsExtraInstalled,\n)\nfrom ..languages import get_all_languages\nfrom ..structured_generation_utils import get_ner_logits_processors\nfrom ..task_utils import (\n    question_answering,\n    sequence_classification,\n    text_to_text,\n    token_classification,\n)\nfrom ..types import ExtractLabelsFunction\nfrom ..utils import (\n    clear_memory,\n    create_model_cache_dir,\n    get_end_of_chat_token_ids,\n    log_once,\n    should_prompts_be_stripped,\n)\nfrom .hf import HuggingFaceEncoderModel, get_model_repo_info\n\nif t.TYPE_CHECKING or importlib.util.find_spec(\"vllm\") is not None:\n    from vllm import LLM, RequestOutput, SamplingParams\n    from vllm.lora.request import LoRARequest\n\n    try:\n        from vllm.model_executor.parallel_utils.parallel_state import (\n            destroy_model_parallel,\n        )\n    except ImportError:\n        from vllm.distributed.parallel_state import destroy_model_parallel\n\nif t.TYPE_CHECKING or importlib.util.find_spec(\"ray\") is not None:\n    import ray\n\nlogger = logging.getLogger(\"scandeval\")\n\n\nclass VLLMModel(HuggingFaceEncoderModel):docs\n    \"\"\"A generative model using the vLLM inference framework.\"\"\"\n\n    _is_generative = True\n    batching_preference = BatchingPreference.ALL_AT_ONCE\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        dataset_config: DatasetConfig,\n        benchmark_config: BenchmarkConfig,\n    ) -&gt; None:\n        \"\"\"Initialise the vLLM model.\n\n        Args:\n            model_config:\n                The model configuration.\n            dataset_config:\n                The dataset configuration.\n            benchmark_config:\n                The benchmark configuration.\n        \"\"\"\n        if (\n            importlib.util.find_spec(\"vllm\") is None\n            or importlib.util.find_spec(\"ray\") is None\n        ):\n            raise NeedsExtraInstalled(extra=\"generative\")\n\n        self.model_config = model_config\n        self.dataset_config = dataset_config\n        self.benchmark_config = benchmark_config\n        self.output_scores = (\n            self.dataset_config.task.supertask in SUPERTASKS_USING_LOGPROBS\n        )\n\n        model, tokenizer = self._load_model_and_tokenizer()\n        self._model: LLM = model\n        self._tokenizer: PreTrainedTokenizer = tokenizer\n\n        self.instruction_model = self._tokenizer.chat_template is not None\n\n        self.lora_request: LoRARequest | None = None\n        if self.model_config.adapter_base_model_id is not None:\n            adapter_path = snapshot_download(\n                repo_id=self.model_config.model_id,\n                cache_dir=Path(self.model_config.model_cache_dir),\n            )\n            self.lora_request = LoRARequest(\n                lora_name=\"adapter\", lora_int_id=1, lora_path=adapter_path\n            )\n\n        self._log_metadata()\n\n    @cached_property\n    def extract_labels_from_generation(self) -&gt; ExtractLabelsFunction:docs\n        \"\"\"The function used to extract the labels from the generated output.\n\n        Returns:\n            The function used to extract the labels from the generated output.\n        \"\"\"\n        match self.dataset_config.task.supertask:\n            case \"sequence-classification\":\n                return partial(\n                    sequence_classification.extract_labels_from_generation,\n                    dataset_config=self.dataset_config,\n                )\n            case \"text-to-text\":\n                return text_to_text.extract_labels_from_generation\n            case \"token-classification\":\n                return partial(\n                    token_classification.extract_labels_from_generation,\n                    dataset_config=self.dataset_config,\n                )\n            case \"question-answering\":\n                return question_answering.extract_labels_from_generation\n            case _:\n                raise NotImplementedError(\n                    f\"Unsupported task supertask: {self.dataset_config.task.supertask}.\"\n                )\n\n    def prepare_dataset(docs\n        self, dataset: DatasetDict, task: Task, itr_idx: int\n    ) -&gt; DatasetDict:\n        \"\"\"Prepare the dataset for the model.\n\n        This includes things like tokenisation.\n\n        Args:\n            dataset:\n                The dataset to prepare.\n            task:\n                The task to prepare the dataset for.\n            itr_idx:\n                The index of the dataset in the iterator.\n\n        Returns:\n            The prepared dataset.\n        \"\"\"\n        if task.supertask == \"question-answering\":\n            dataset = dataset.map(\n                lambda examples: dict(\n                    label=[\n                        dict(\n                            id=id,\n                            answers=dict(\n                                answer_start=answer_dct[\"answer_start\"],\n                                text=[\n                                    answer_text.lower()\n                                    for answer_text in answer_dct[\"text\"]\n                                ],\n                            ),\n                        )\n                        for id, answer_dct in zip(examples[\"id\"], examples[\"answers\"])\n                    ]\n                ),\n                batched=True,\n                load_from_cache_file=False,\n                keep_in_memory=True,\n            )\n\n        if self.benchmark_config.few_shot:\n            few_shot_examples = self._extract_few_shot_examples(\n                dataset=dataset, task=task, itr_idx=itr_idx\n            )\n        else:\n            few_shot_examples = list()\n\n        dataset[\"test\"] = dataset[\"test\"].map(\n            partial(self._apply_prompt, few_shot_examples=few_shot_examples, task=task),\n            batched=True,\n            load_from_cache_file=False,\n            keep_in_memory=True,\n        )\n\n        return dataset\n\n    def generate(self, inputs: dict) -&gt; GenerativeModelOutput:docs\n        \"\"\"Generate outputs from the model.\n\n        Args:\n            inputs:\n                A batch of inputs to pass through the model.\n\n        Returns:\n            The generated model outputs.\n        \"\"\"\n        # Define which tokens to use as stopping criteria. We want to use the padding\n        # token, end-of-sentence token, and a double newline (since these separate the\n        # few-shot examples in the input)\n        stop_tokens: list[str] = [\"\\n\\n\"]\n        if self._tokenizer.pad_token_id is not None:\n            stop_tokens.append(self._tokenizer.pad_token)\n        if self._tokenizer.eos_token_id is not None:\n            stop_tokens.append(self._tokenizer.eos_token)\n            if self._tokenizer.pad_token_id is None:\n                self._tokenizer.pad_token_id = self._tokenizer.eos_token_id\n                self._tokenizer.pad_token = self._tokenizer.eos_token\n        if (\n            self._tokenizer.bos_token_id is not None\n            and self._tokenizer.pad_token_id is None\n        ):\n            self._tokenizer.pad_token_id = self._tokenizer.bos_token_id\n            self._tokenizer.pad_token = self._tokenizer.bos_token\n        elif (\n            self._tokenizer.eos_token_id is not None\n            and self._tokenizer.pad_token_id is None\n        ):\n            self._tokenizer.pad_token_id = self._tokenizer.eos_token_id\n            self._tokenizer.pad_token = self._tokenizer.eos_token\n        elif self._tokenizer.pad_token_id is None:\n            pad_token_candidates = [\"&lt;pad&gt;\", \"[pad]\", \"&lt;|endoftext|&gt;\", \"&lt;|im_end|&gt;\"]\n            pad_token_candidates.extend([c.upper() for c in pad_token_candidates])\n            for candidate in pad_token_candidates:\n                if candidate in self._tokenizer.get_vocab():\n                    pad_token_id = self._tokenizer.get_vocab()[candidate]\n                    self._tokenizer.pad_token = candidate\n                    self._tokenizer.pad_token_id = pad_token_id\n                    break\n            else:\n                raise InvalidModel(\n                    \"Could not find a suitable token to use as a padding token, since \"\n                    \"the model does not have a BOS, EOS, or padding token, and does \"\n                    f\"not have any of the following tokens in its vocabulary: \"\n                    f\"{pad_token_candidates}.\"\n                )\n\n        assert self._tokenizer.pad_token_id is not None\n\n        # Add end of chat token as a stopping token, if it exists\n        end_of_chat_token_ids = get_end_of_chat_token_ids(tokenizer=self._tokenizer)\n        if end_of_chat_token_ids is not None:\n            end_of_chat_token = self._tokenizer.decode(end_of_chat_token_ids).strip()\n            if end_of_chat_token:\n                stop_tokens.append(end_of_chat_token)\n\n        if self.dataset_config.task.name in TASKS_USING_JSON:\n            ner_tag_names = list(self.dataset_config.prompt_label_mapping.values())\n            logits_processors = get_ner_logits_processors(\n                ner_tag_names=ner_tag_names, llm=self._model\n            )\n        else:\n            logits_processors = None\n\n        # Define the parameters used for vLLM generation\n        max_tokens: int = self.dataset_config.max_generated_tokens\n        sampling_params = SamplingParams(\n            max_tokens=max_tokens,\n            logprobs=MAX_LOGPROBS if self.output_scores else None,\n            temperature=0.0,\n            stop=[stop_token for stop_token in stop_tokens if stop_token],\n            logits_processors=logits_processors,\n        )\n\n        # If any of the prompts are empty then we need to replace them with a BOS token\n        # so that the vLLM model can generate from them\n        prompts = inputs[\"text\"]\n        if any(len(prompt) == 0 for prompt in prompts):\n            logger.debug(\"Found empty prompts, replacing with BOS token.\")\n            prompts = [\n                prompt if len(prompt) &gt; 0 else self._tokenizer.bos_token\n                for prompt in prompts\n            ]\n\n        # Strip the prompts if the model's tokeniser requires it\n        labels_to_be_generated = list(self.dataset_config.prompt_label_mapping.values())\n        if (\n            not self.instruction_model\n            and len(labels_to_be_generated) &gt; 0\n            and should_prompts_be_stripped(\n                labels_to_be_generated=labels_to_be_generated, tokenizer=self._tokenizer\n            )\n        ):\n            prompts = [prompt.strip() for prompt in prompts]\n\n        # Generate sequences using vLLM\n        input_is_a_test = len(prompts) == 1 and len(set(prompts[0])) == 1\n        raw_outputs = self._model.generate(\n            prompts=prompts,\n            sampling_params=sampling_params,\n            use_tqdm=(not input_is_a_test),\n            lora_request=self.lora_request,\n        )\n        completions = self._tokenizer.batch_decode(\n            sequences=[\n                torch.LongTensor(output.outputs[0].token_ids) for output in raw_outputs\n            ],\n            skip_special_tokens=True,\n        )\n        completions = [completion.strip() for completion in completions]\n\n        # Add logprobs scores to the output\n        if self.output_scores:\n            scores: list[list[list[tuple[str, float]]]] = [\n                [\n                    [\n                        (obj.decoded_token, obj.logprob)\n                        for obj in token_logprobs_dict.values()\n                    ]\n                    for token_logprobs_dict in raw_output.outputs[0].logprobs\n                ]\n                for raw_output in raw_outputs\n            ]\n            output = GenerativeModelOutput(sequences=completions, scores=scores)\n        else:\n            output = GenerativeModelOutput(sequences=completions)\n\n        return output\n\n    @classmethod\n    def model_exists(docs\n        cls, model_id: str, benchmark_config: BenchmarkConfig\n    ) -&gt; bool | NeedsExtraInstalled | NeedsEnvironmentVariable:\n        \"\"\"Check if a model exists.\n\n        Args:\n            model_id:\n                The model ID.\n            benchmark_config:\n                The benchmark configuration.\n\n        Returns:\n            Whether the model exists, or an error describing why we cannot check\n            whether the model exists.\n        \"\"\"\n        using_api = (\n            benchmark_config.api_base is not None\n            or benchmark_config.api_version is not None\n        )\n        if using_api:\n            return False\n\n        model_id, revision = (\n            model_id.split(\"@\") if \"@\" in model_id else (model_id, \"main\")\n        )\n        model_info = get_model_repo_info(\n            model_id=model_id, revision=revision, benchmark_config=benchmark_config\n        )\n        return (\n            model_info is not None and model_info.pipeline_tag in GENERATIVE_MODEL_TASKS\n        )\n\n    @classmethod\n    def get_model_config(docs\n        cls, model_id: str, benchmark_config: BenchmarkConfig\n    ) -&gt; ModelConfig:\n        \"\"\"Fetch the model configuration.\n\n        Args:\n            model_id:\n                The model ID.\n            benchmark_config:\n                The benchmark configuration.\n\n        Returns:\n            The model configuration.\n        \"\"\"\n        model_id, revision = (\n            model_id.split(\"@\") if \"@\" in model_id else (model_id, \"main\")\n        )\n        model_info = get_model_repo_info(\n            model_id=model_id, revision=revision, benchmark_config=benchmark_config\n        )\n        if model_info is None:\n            raise InvalidModel(f\"The model {model_id!r} could not be found.\")\n\n        framework = Framework.PYTORCH\n        if \"pytorch\" in model_info.tags:\n            pass\n        elif \"jax\" in model_info.tags:\n            framework = Framework.JAX\n        elif \"spacy\" in model_info.tags:\n            raise InvalidModel(\"SpaCy models are not supported.\")\n        elif any(tag in model_info.tags for tag in {\"tf\", \"tensorflow\", \"keras\"}):\n            raise InvalidModel(\"TensorFlow/Keras models are not supported.\")\n\n        language_mapping = get_all_languages()\n        language_codes = list(language_mapping.keys())\n\n        model_config = ModelConfig(\n            model_id=model_id,\n            revision=revision,\n            framework=framework,\n            task=model_info.pipeline_tag,\n            languages=[\n                language_mapping[tag]\n                for tag in model_info.tags\n                if tag in language_codes\n            ],\n            model_type=ModelType.HF_HUB_GENERATIVE,\n            model_cache_dir=create_model_cache_dir(\n                cache_dir=benchmark_config.cache_dir, model_id=model_id\n            ),\n            adapter_base_model_id=model_info.adapter_base_model_id,\n        )\n\n        return model_config\n\n    def _load_model_and_tokenizer(self) -&gt; \"tuple[LLM, PreTrainedTokenizer]\":\n        \"\"\"Load the model and tokenizer.\n\n        Returns:\n            The loaded model and tokenizer.\n        \"\"\"\n        try:\n            hf_model_config = AutoConfig.from_pretrained(\n                self.model_config.adapter_base_model_id or self.model_config.model_id,\n                revision=self.model_config.revision,\n                cache_dir=self.model_config.model_cache_dir,\n            )\n        except ValueError as e:\n            raise InvalidModel(\n                \"Could not load model configuration for \"\n                f\"{self.model_config.model_id!r}. The error was: {str(e)}\"\n            )\n\n        quantization = None\n        if hasattr(hf_model_config, \"quantization_config\"):\n            quantization = hf_model_config.quantization_config.get(\"quant_method\")\n\n        # The quantised models require extra dependencies\n        if quantization == \"gptq\" and (\n            importlib.util.find_spec(\"auto_gptq\") is None\n            or importlib.util.find_spec(\"optimum\") is None\n        ):\n            raise NeedsExtraInstalled(extra=\"quantization\")\n        if quantization == \"awq\" and importlib.util.find_spec(\"awq\") is None:\n            raise NeedsExtraInstalled(extra=\"quantization\")\n\n        dtype: str | torch.dtype = \"auto\"\n        if quantization is not None and hf_model_config.torch_dtype != torch.float16:\n            logger.info(\n                \"You are loading a quantized model with dtype \"\n                f\"{hf_model_config.torch_dtype}, which vLLM does not support. Setting \"\n                \"dtype to float16 instead.\"\n            )\n            dtype = torch.float16\n\n        if self.model_config.adapter_base_model_id is not None:\n            download_dir = str(Path(self.model_config.model_cache_dir) / \"base_model\")\n        else:\n            download_dir = str(self.model_config.model_cache_dir)\n\n        potential_max_model_length_config_names = [\n            \"max_position_embeddings\",\n            \"max_sequence_length\",\n            \"model_max_length\",\n            \"sliding_window\",\n            \"sliding_window_size\",\n            \"n_positions\",\n        ]\n        true_max_model_len_candidates: list[int] = list()\n        for config_name in potential_max_model_length_config_names:\n            if hasattr(hf_model_config, config_name):\n                model_len = getattr(hf_model_config, config_name)\n                if model_len is not None:\n                    true_max_model_len_candidates.append(model_len)\n\n        if len(true_max_model_len_candidates) &gt; 0:\n            true_max_model_len = min(true_max_model_len_candidates)\n        else:\n            true_max_model_len = 5_000\n\n        clear_vllm()\n\n        # Prefer base model ID if the model is an adapter - the adapter will be added on\n        # during inference in this case\n        model_id = self.model_config.adapter_base_model_id or self.model_config.model_id\n\n        try:\n            model = LLM(\n                model=model_id,\n                tokenizer=model_id,\n                gpu_memory_utilization=0.95,\n                max_model_len=min(true_max_model_len, 5_000),\n                download_dir=download_dir,\n                trust_remote_code=self.benchmark_config.trust_remote_code,\n                revision=self.model_config.revision,\n                seed=4242,\n                distributed_executor_backend=\"mp\",  # \"ray\",  #\u00a0TEMP?\n                tensor_parallel_size=torch.cuda.device_count(),\n                disable_custom_all_reduce=True,\n                quantization=quantization,\n                dtype=dtype,\n                enforce_eager=True,\n                max_logprobs=MAX_LOGPROBS if self.output_scores else None,\n                # TEMP: Prefix caching isn't supported with sliding window in vLLM yet,\n                # so we disable it for now\n                enable_prefix_caching=False,\n                enable_lora=self.model_config.adapter_base_model_id is not None,\n                max_lora_rank=256,\n                guided_decoding_backend=\"outlines\",\n            )\n        except ValueError as e:\n            if \"trust_remote_code\" in str(e):\n                raise InvalidModel(\n                    f\"Loading the model {model_id!r} needs to trust remote code. \"\n                    \"If you trust the suppliers of this model, then you can enable \"\n                    \"this by setting the `--trust-remote-code` flag.\"\n                )\n            raise InvalidModel(\n                f\"The model {model_id!r} could not be loaded. The error was {e!r}.\"\n            )\n\n        model._run_engine = MethodType(_run_engine_with_fixed_progress_bars, model)\n        model.config = hf_model_config\n\n        tokenizer = load_tokenizer(\n            model_id=self.model_config.model_id,\n            revision=self.model_config.revision,\n            adapter_base_model_id=self.model_config.adapter_base_model_id,\n            trust_remote_code=self.benchmark_config.trust_remote_code,\n            model_max_length=true_max_model_len,\n            model_cache_dir=self.model_config.model_cache_dir,\n        )\n\n        return model, tokenizer\n\n    def _extract_few_shot_examples(\n        self, dataset: DatasetDict, task: Task, itr_idx: int\n    ) -&gt; list[dict[str, t.Any]]:\n        \"\"\"Extract few-shot examples from a dataset.\n\n        This will always extract the examples from the training split.\n\n        We ensure that the few-shot examples are unique by picking them one at a time.\n\n        Args:\n            dataset:\n                The dataset to extract the few-shot examples from.\n            task:\n                The task that is being benchmarked.\n            itr_idx:\n                The index of the dataset in the iterator.\n\n        Returns:\n            The few-shot examples.\n        \"\"\"\n        random_seed = 4242 + itr_idx\n        num_few_shots = self.dataset_config.num_few_shot_examples\n        few_shot_examples: list[dict[str, t.Any]] = list()\n        shuffled_train = dataset[\"train\"].shuffle(seed=random_seed)\n\n        match task.supertask:\n            case \"sequence-classification\":\n                labels = it.cycle(self.dataset_config.task.labels)\n                while (\n                    len(few_shot_examples) &lt; num_few_shots and len(shuffled_train) &gt; 0\n                ):\n                    label = next(labels)\n                    possible_examples = shuffled_train.filter(\n                        lambda x: x[\"label\"].lower() == label.lower()\n                    )\n                    if len(possible_examples) == 0:\n                        continue\n                    example = possible_examples.select(range(1))[0]\n                    few_shot_examples.append(example)\n                    shuffled_train = shuffled_train.filter(\n                        lambda x: x[\"text\"] != example[\"text\"]\n                    )\n\n            case \"text-to-text\":\n                while (\n                    len(few_shot_examples) &lt; num_few_shots and len(shuffled_train) &gt; 0\n                ):\n                    example = shuffled_train.select(range(1))[0]\n                    few_shot_examples.append(example)\n                    shuffled_train = shuffled_train.filter(\n                        lambda x: x[\"text\"] != example[\"text\"]\n                    )\n\n            case \"token-classification\":\n                labels = it.cycle(\n                    [\n                        label.lower()\n                        for label in self.dataset_config.task.labels\n                        if label.lower().startswith(\"b-\")\n                    ]\n                )\n                while (\n                    len(few_shot_examples) &lt; num_few_shots and len(shuffled_train) &gt; 0\n                ):\n                    label = next(labels)\n                    possible_examples = shuffled_train.filter(\n                        lambda x: label in [tag.lower() for tag in x[\"labels\"]]\n                    )\n                    if len(possible_examples) == 0:\n                        continue\n                    example = possible_examples.select(range(1))[0]\n                    few_shot_examples.append(example)\n                    shuffled_train = shuffled_train.filter(\n                        lambda x: x[\"text\"] != example[\"text\"]\n                    )\n\n            case \"question-answering\":\n                # Locate the maximum number of tokens that constitutes a short example\n                for max_num_tokens in [512, 1024, 2048, 4096, 8192]:\n                    train_with_short_examples = dataset[\"train\"].filter(\n                        lambda example: len(example[\"context\"]) &lt; max_num_tokens\n                    )\n                    num_short_examples = len(train_with_short_examples)\n                    if num_short_examples &gt;= self.dataset_config.num_few_shot_examples:\n                        break\n                else:\n                    raise InvalidBenchmark(\n                        \"Could not find enough short examples for few-shot learning.\"\n                    )\n\n                shuffled_train = train_with_short_examples.shuffle(seed=random_seed)\n                while (\n                    len(few_shot_examples) &lt; num_few_shots and len(shuffled_train) &gt; 0\n                ):\n                    example = shuffled_train.select(range(1))[0]\n                    few_shot_examples.append(example)\n                    shuffled_train = shuffled_train.filter(\n                        lambda x: x[\"context\"] != example[\"context\"]\n                    )\n\n            case _:\n                raise NotImplementedError(\n                    f\"Unsupported task supertask: {task.supertask}.\"\n                )\n\n        random.seed(random_seed)\n        random.shuffle(few_shot_examples)\n        return few_shot_examples\n\n    def _apply_prompt(\n        self,\n        examples: dict[str, t.Any],\n        few_shot_examples: list[dict[str, t.Any]],\n        task: Task,\n    ) -&gt; dict[str, t.Any]:\n        \"\"\"Apply prompt template to an example, potentially with few-shot examples.\n\n        Args:\n            examples:\n                The examples to apply the few-shot examples to.\n            few_shot_examples:\n                The few-shot examples to apply.\n            task:\n                The task that is being benchmarked.\n\n        Returns:\n            The example with the few-shot examples applied.\n        \"\"\"\n\n        def create_prompt(**kwargs) -&gt; tuple[str, str]:\n            \"\"\"Create a prompt from the given keyword arguments.\n\n            Args:\n                kwargs:\n                    The keyword arguments to use in the prompt.\n\n            Returns:\n                A pair (prompt, label), where \"label\" is an empty string if the model is\n                not instruction tuned (as in this case it is included in the prompt).\n            \"\"\"\n            if self.instruction_model:\n                label_key = \"label\" if \"label\" in kwargs else \"target_text\"\n                label = kwargs.pop(label_key)\n                label_mapping = self.dataset_config.prompt_label_mapping\n                label = label_mapping.get(label, label)\n                prompt = self.dataset_config.instruction_prompt.format(**kwargs)\n                return prompt, label\n            else:\n                return self.dataset_config.prompt_template.format(**kwargs), \"\"\n\n        match task.supertask:\n            case \"sequence-classification\":\n                few_shot_sections = [\n                    create_prompt(\n                        text=example[\"text\"].replace(\"\\n\", \" \").strip(),\n                        label=example[\"label\"].replace(\"\\n\", \" \").strip(),\n                    )\n                    for example in few_shot_examples\n                ]\n                new_sections = [\n                    create_prompt(text=text.replace(\"\\n\", \" \").strip(), label=\"\")\n                    for text in examples[\"text\"]\n                ]\n\n            case \"text-to-text\":\n                few_shot_sections = [\n                    create_prompt(\n                        text=example[\"text\"].replace(\"\\n\", \" \").strip(),\n                        target_text=example[\"target_text\"].replace(\"\\n\", \" \").strip(),\n                    )\n                    for example in few_shot_examples\n                ]\n                new_sections = [\n                    create_prompt(text=text.replace(\"\\n\", \" \").strip(), target_text=\"\")\n                    for text in examples[\"text\"]\n                ]\n\n            case \"token-classification\":\n\n                def create_label(example: dict) -&gt; str:\n                    prompt_labels = self.dataset_config.prompt_label_mapping.values()\n                    labels: dict[str, list[str]] = {\n                        prompt_label: list() for prompt_label in prompt_labels\n                    }\n                    for token, label in zip(example[\"tokens\"], example[\"labels\"]):\n                        label = label.lower()\n                        if label == \"o\":\n                            continue\n                        prompt_label = self.dataset_config.prompt_label_mapping[label]\n                        if label.startswith(\"b-\"):\n                            labels[prompt_label].append(token)\n                        elif label.startswith(\"i-\"):\n                            labels[prompt_label][-1] += \" \" + token\n                    return json.dumps(labels, ensure_ascii=False)\n\n                few_shot_sections = [\n                    create_prompt(\n                        text=\" \".join(example[\"tokens\"]).replace(\"\\n\", \" \").strip(),\n                        label=create_label(example=example),\n                    )\n                    for example in few_shot_examples\n                ]\n                new_sections = [\n                    create_prompt(\n                        text=\" \".join(tokens).replace(\"\\n\", \" \").strip(), label=\"\"\n                    )\n                    for tokens in examples[\"tokens\"]\n                ]\n\n            case \"question-answering\":\n                few_shot_sections = [\n                    create_prompt(\n                        text=example[\"context\"].replace(\"\\n\", \" \").strip(),\n                        question=example[\"question\"].replace(\"\\n\", \" \").strip(),\n                        label=example[\"answers\"][\"text\"][0].replace(\"\\n\", \" \"),\n                    )\n                    for example in few_shot_examples\n                ]\n                new_sections = [\n                    create_prompt(\n                        text=context.replace(\"\\n\", \" \").strip(),\n                        question=question.replace(\"\\n\", \" \").strip(),\n                        label=\"\",\n                    )\n                    for context, question in zip(\n                        examples[\"context\"], examples[\"question\"]\n                    )\n                ]\n\n            case _:\n                raise NotImplementedError(\n                    f\"Unsupported task supertask: {task.supertask}.\"\n                )\n\n        if self.instruction_model:\n            few_shot_messages = [\n                dict(role=role, content=content)\n                for prompt, label in few_shot_sections\n                for role, content in [(\"user\", prompt), (\"assistant\", label)]\n            ]\n\n            messages_list = [\n                few_shot_messages + [dict(role=\"user\", content=prompt)]\n                for prompt, _ in new_sections\n            ]\n\n            # Pick the chat template that matches the language of the dataset, if such a\n            # template exists\n            chat_template: str | None = None\n            if isinstance(self._tokenizer.chat_template, dict):\n                language_codes = [\n                    language.code for language in self.dataset_config.languages\n                ]\n                for name, candidate_template in self._tokenizer.chat_template.items():\n                    if name.lower() in language_codes:\n                        chat_template = candidate_template\n                        log_once(\n                            f\"Using the {name!r} chat template for the tokenizer.\",\n                            level=logging.DEBUG,\n                        )\n                        break\n\n            texts = [\n                self._tokenizer.apply_chat_template(\n                    conversation=messages,\n                    tokenize=False,\n                    add_generation_prompt=True,\n                    chat_template=chat_template,\n                )\n                for messages in messages_list\n            ]\n\n            examples[\"text\"] = texts\n\n        else:\n            prompt_prefix = \"\"\n            if self.dataset_config.prompt_prefix:\n                prompt_prefix = self.dataset_config.prompt_prefix + \"\\n\\n\"\n\n            few_shot_prompt = \"\\n\\n\".join([prompt for prompt, _ in few_shot_sections])\n            if few_shot_prompt:\n                few_shot_prompt += \"\\n\\n\"\n\n            examples[\"text\"] = [\n                prompt_prefix + few_shot_prompt + new_prompt\n                for new_prompt, _ in new_sections\n            ]\n\n        return examples\n\n    @cached_propertydocs\n    def data_collator(self) -&gt; c.Callable[[list[t.Any]], dict[str, t.Any]]:\n        \"\"\"The data collator used to prepare samples during finetuning.\n\n        Returns:\n            The data collator.\n        \"\"\"\n        raise NotImplementedError(\n            \"The `data_collator` property has not been implemented for vLLM models.\"\n        )\n\n    @cached_property\n    def trainer_class(self) -&gt; t.Type[\"Trainer\"]:docs\n        \"\"\"The Trainer class to use for finetuning.\n\n        Returns:\n            The Trainer class.\n        \"\"\"\n        raise NotImplementedError(\n            \"The `trainer_class` property has not been implemented for vLLM models.\"\n        )\n\n\ndef load_tokenizer(docs\n    model_id: str,\n    revision: str,\n    adapter_base_model_id: str | None,\n    trust_remote_code: bool,\n    model_max_length: int,\n    model_cache_dir: str,\n) -&gt; \"PreTrainedTokenizer\":\n    \"\"\"Load the tokenizer.\n\n    Args:\n        model_id:\n            The model identifier.\n        revision:\n            The revision of the model.\n        adapter_base_model_id:\n            The base model ID for the adapter model. Can be None if the model is not an\n            adapter model.\n        trust_remote_code:\n            Whether to trust remote code.\n        model_max_length:\n            The maximum length of the model.\n        model_cache_dir:\n            The cache directory for the model.\n\n    Returns:\n        The loaded tokenizer.\n    \"\"\"\n    config = AutoConfig.from_pretrained(\n        adapter_base_model_id or model_id, revision=revision, cache_dir=model_cache_dir\n    )\n    num_retries = 5\n    for _ in range(num_retries):\n        try:\n            tokenizer = AutoTokenizer.from_pretrained(\n                model_id,\n                use_fast=True,\n                verbose=False,\n                trust_remote_code=trust_remote_code,\n                padding_side=\"left\",\n                truncation_side=\"left\",\n                model_max_length=model_max_length,\n                config=config,\n            )\n            break\n        except (json.JSONDecodeError, OSError, TypeError) as e:\n            if adapter_base_model_id is None or model_id == adapter_base_model_id:\n                raise InvalidModel(\n                    f\"Could not load tokenizer for model {model_id!r}. The error was \"\n                    f\"{str(e)}.\"\n                )\n            logger.debug(\n                f\"Could not load tokenizer for {model_id!r}. Falling back to \"\n                f\"{adapter_base_model_id!r}.\"\n            )\n            model_id = adapter_base_model_id\n        except (TimeoutError, RequestError):\n            logger.info(f\"Couldn't load tokenizer for {model_id!r}. Retrying.\")\n            sleep(5)\n            continue\n    else:\n        raise InvalidModel(\n            f\"Could not load tokenizer for model {model_id!r} after {num_retries} \"\n            \"attempts.\"\n        )\n\n    if tokenizer.pad_token_id is None:\n        tokenizer.pad_token = tokenizer.eos_token\n\n    return tokenizer\n\n\ndef _run_engine_with_fixed_progress_bars(\n    self: \"LLM\", use_tqdm: bool\n) -&gt; list[\"RequestOutput\"]:\n    if use_tqdm:\n        num_requests = self.llm_engine.get_num_unfinished_requests()\n        pbar = tqdm(\n            total=num_requests, leave=False, disable=hasattr(sys, \"_called_from_test\")\n        )\n    else:\n        pbar = None\n\n    # Run the engine.\n    outputs: list[\"RequestOutput\"] = list()\n    while self.llm_engine.has_unfinished_requests():\n        step_outputs = self.llm_engine.step()\n        for output in step_outputs:\n            if output.finished:\n                outputs.append(output)\n                if pbar is not None:\n                    pbar.update(1)\n\n    if pbar is not None:\n        pbar.close()\n\n    # Sort the outputs by request ID. This is necessary because some requests may be\n    # finished earlier than its previous requests.\n    outputs = sorted(outputs, key=lambda x: int(x.request_id))\n\n    return outputs\n\n\ndef clear_vllm() -&gt; None:docs\n    \"\"\"Clear the GPU memory used by the vLLM model, enabling re-initialisation.\"\"\"\n    try:\n        destroy_model_parallel()\n    except ImportError:\n        pass\n    clear_memory()\n    if ray.is_initialized():\n        ray.shutdown()\n</code></pre>"},{"location":"api/scandeval/task_utils/","title":"scandeval.task_utils","text":"scandeval.task_utils<p> source package scandeval.task_utils </p> <p>Utility functions related to the different tasks and supertasks.</p> <p> Modules </p> <ul> <li> <p>scandeval.task_utils.question_answering \u2014 Utility functions related to the question-answering supertask.</p> </li> <li> <p>scandeval.task_utils.sequence_classification \u2014 Utility functions related to the sequence-classification supertask.</p> </li> <li> <p>scandeval.task_utils.text_to_text \u2014 Utility functions related to the text-to-text supertask.</p> </li> <li> <p>scandeval.task_utils.token_classification \u2014 Utility functions related to the token-classification supertask.</p> </li> </ul>"},{"location":"src/scandeval/task_utils/","title":"scandeval.task_utils","text":"scandeval.task_utils<p> docs package scandeval.task_utils </p> <pre><code>\"\"\"Utility functions related to the different tasks and supertasks.\"\"\"\n</code></pre>"},{"location":"api/scandeval/task_utils/question_answering/","title":"scandeval.task_utils.question_answering","text":"scandeval.task_utils.question_answering<p> source module scandeval.task_utils.question_answering </p> <p>Utility functions related to the question-answering supertask.</p> <p> Classes </p> <ul> <li> <p>QuestionAnsweringTrainer \u2014 Trainer subclass for question answering tasks.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>compute_metrics \u2014 Compute the metrics needed for evaluation.</p> </li> <li> <p>extract_labels_from_generation \u2014 Extract the predicted labels from the generated output.</p> </li> <li> <p>prepare_train_examples \u2014 Prepare the features for training.</p> </li> <li> <p>prepare_test_examples \u2014 Prepare test examples.</p> </li> <li> <p>postprocess_predictions_and_labels \u2014 Postprocess the predictions and labels, to allow easier metric computation.</p> </li> <li> <p>find_best_answer \u2014 Find the best answer for a given example.</p> </li> <li> <p>find_valid_answers \u2014 Find the valid answers from the start and end indexes.</p> </li> </ul> <p> source class QuestionAnsweringTrainer(**kwargs) </p> <p><p>Bases : Trainer</p></p> <p>Trainer subclass for question answering tasks.</p> <p>Initialize the trainer.</p> <p> Methods </p> <ul> <li> <p>evaluate \u2014 Evaluate the model on the given dataset.</p> </li> </ul> <p> source method QuestionAnsweringTrainer.evaluate(eval_dataset: Dataset | None = None, orig_eval_dataset: Dataset | None = None, ignore_keys: list[str] | None = None, metric_key_prefix: str = 'eval') \u2192 dict[str, float] | None </p> <p>Evaluate the model on the given dataset.</p> <p> Parameters </p> <ul> <li> <p>eval_dataset :  Dataset | None \u2014</p> <p>The dataset to evaluate on. If None, then use the stored evaluation dataset.</p> </li> <li> <p>orig_eval_dataset :  Dataset | None \u2014</p> <p>The original evaluation dataset, before any postprocessing. If None, then use the stored original evaluation dataset.</p> </li> <li> <p>ignore_keys :  list[str] | None \u2014</p> <p>The keys to ignore when computing the metrics.</p> </li> <li> <p>metric_key_prefix :  str \u2014</p> <p>The prefix to use for the metric keys.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] | None \u2014 The metrics computed on the evaluation dataset.</p> </li> </ul> <p> source compute_metrics(model_outputs_and_labels: tuple[Predictions, Labels], id2label: dict[int, str], dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 dict[str, float] </p> <p>Compute the metrics needed for evaluation.</p> <p> Parameters </p> <ul> <li> <p>model_outputs_and_labels :  tuple[Predictions, Labels] \u2014</p> <p>The first sequence contains the model outputs and the second sequence contains the true labels.</p> </li> <li> <p>id2label :  dict[int, str] \u2014</p> <p>Conversion of indices to labels.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The configuration of the benchmark.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] \u2014 A dictionary with the names of the metrics as keys and the metric values as values.</p> </li> </ul> <p> source extract_labels_from_generation(input_batch: dict[str, list], model_output: GenerativeModelOutput) \u2192 list[t.Any] </p> <p>Extract the predicted labels from the generated output.</p> <p> Parameters </p> <ul> <li> <p>input_batch :  dict[str, list] \u2014</p> <p>The input batch, where the keys are the feature names and the values are lists with the feature values.</p> </li> <li> <p>model_output :  GenerativeModelOutput \u2014</p> <p>The raw generated output of the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[t.Any] \u2014 The predicted labels.</p> </li> </ul> <p> source prepare_train_examples(examples: BatchEncoding, tokenizer: PreTrainedTokenizer) \u2192 BatchEncoding </p> <p>Prepare the features for training.</p> <p> Parameters </p> <ul> <li> <p>examples :  BatchEncoding \u2014</p> <p>The examples to prepare.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer \u2014</p> <p>The tokenizer to use to prepare the examples.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BatchEncoding \u2014 The prepared examples.</p> </li> </ul> <p> source prepare_test_examples(examples: BatchEncoding, tokenizer: PreTrainedTokenizer) \u2192 BatchEncoding </p> <p>Prepare test examples.</p> <p> Parameters </p> <ul> <li> <p>examples :  BatchEncoding \u2014</p> <p>Dictionary of test examples.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer \u2014</p> <p>The tokenizer used to preprocess the examples.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BatchEncoding \u2014 The prepared test examples.</p> </li> </ul> <p> source postprocess_predictions_and_labels(predictions: list, dataset: Dataset, prepared_dataset: Dataset, cls_token_index: int) \u2192 tuple[list[dict], list[dict]] </p> <p>Postprocess the predictions and labels, to allow easier metric computation.</p> <p> Parameters </p> <ul> <li> <p>predictions :  list \u2014</p> <p>A pair of (start_logits, end_logits) predictions.</p> </li> <li> <p>dataset :  Dataset \u2014</p> <p>The dataset containing the examples.</p> </li> <li> <p>prepared_dataset :  Dataset \u2014</p> <p>The dataset containing the prepared examples.</p> </li> <li> <p>cls_token_index :  int \u2014</p> <p>The index of the CLS token.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[list[dict], list[dict]] \u2014 The postprocessed predictions and labels.</p> </li> </ul> <p> source find_best_answer(all_start_logits: np.ndarray, all_end_logits: np.ndarray, prepared_dataset: Dataset, feature_indices: list[int], context: str, max_answer_length: int, num_best_logits: int, min_null_score: float, cls_token_index: int) \u2192 str </p> <p>Find the best answer for a given example.</p> <p> Parameters </p> <ul> <li> <p>all_start_logits :  np.ndarray \u2014</p> <p>The start logits for all the features.</p> </li> <li> <p>all_end_logits :  np.ndarray \u2014</p> <p>The end logits for all the features.</p> </li> <li> <p>prepared_dataset :  Dataset \u2014</p> <p>The dataset containing the prepared examples.</p> </li> <li> <p>feature_indices :  list[int] \u2014</p> <p>The indices of the features associated with the current example.</p> </li> <li> <p>context :  str \u2014</p> <p>The context of the example.</p> </li> <li> <p>max_answer_length :  int \u2014</p> <p>The maximum length of the answer.</p> </li> <li> <p>num_best_logits :  int \u2014</p> <p>The number of best logits to consider.</p> </li> <li> <p>min_null_score :  float \u2014</p> <p>The minimum score an answer can have.</p> </li> <li> <p>cls_token_index :  int \u2014</p> <p>The index of the CLS token.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The best answer for the example.</p> </li> </ul> <p> source find_valid_answers(start_logits: np.ndarray, end_logits: np.ndarray, offset_mapping: list[tuple[int, int]], context: str, max_answer_length: int, num_best_logits: int, min_null_score: float) \u2192 list[dict] </p> <p>Find the valid answers from the start and end indexes.</p> <p> Parameters </p> <ul> <li> <p>start_logits :  np.ndarray \u2014</p> <p>The logits for the start of the answer.</p> </li> <li> <p>end_logits :  np.ndarray \u2014</p> <p>The logits for the end of the answer.</p> </li> <li> <p>offset_mapping :  list[tuple[int, int]] \u2014</p> <p>The offset mapping, being a list of pairs of integers for each token index, containing the start and end character index in the original context.</p> </li> <li> <p>context :  str \u2014</p> <p>The context of the example.</p> </li> <li> <p>max_answer_length :  int \u2014</p> <p>The maximum length of the answer.</p> </li> <li> <p>num_best_logits :  int \u2014</p> <p>The number of best logits to consider. Note that this function will run in O(<code>num_best_logits</code> ^ 2) time.</p> </li> <li> <p>min_null_score :  float \u2014</p> <p>The minimum score an answer can have.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[dict] \u2014 A list of the valid answers, each being a dictionary with keys \"text\" and \"score\", the score being the sum of the start and end logits.</p> </li> </ul>"},{"location":"src/scandeval/task_utils/question_answering/","title":"scandeval.task_utils.question_answering","text":"scandeval.task_utils.question_answering<p> docs module scandeval.task_utils.question_answering </p> <pre><code>\"\"\"Utility functions related to the question-answering supertask.\"\"\"\n\nimport logging\nimport typing as t\nfrom collections import defaultdict\n\nimport evaluate\nimport numpy as np\nfrom evaluate import EvaluationModule\nfrom transformers import PreTrainedTokenizer\nfrom transformers.trainer import Trainer\n\nfrom ..data_models import BenchmarkConfig, DatasetConfig, GenerativeModelOutput\nfrom ..utils import (\n    get_special_token_metadata,\n    raise_if_model_output_contains_nan_values,\n)\n\nif t.TYPE_CHECKING:\n    from datasets.arrow_dataset import Dataset\n    from transformers.tokenization_utils_base import BatchEncoding\n\n    from ..types import Labels, Predictions\n\nlogger = logging.getLogger(\"scandeval\")\n\n\nclass QuestionAnsweringTrainer(Trainer):docs\n    \"\"\"Trainer subclass for question answering tasks.\"\"\"\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        \"\"\"Initialize the trainer.\"\"\"\n        super().__init__(*args, **kwargs)\n\n        # Get the CLS token id for the tokenizer\n        special_token_metadata = get_special_token_metadata(self.tokenizer)\n        self.cls_token_id = special_token_metadata[\"cls_token_id\"]\n\n        # Set the label names\n        self.label_names = [\"start_positions\", \"end_positions\"]\n\n    def evaluate(docs\n        self,\n        eval_dataset: \"Dataset | None\" = None,\n        orig_eval_dataset: \"Dataset | None\" = None,\n        ignore_keys: list[str] | None = None,\n        metric_key_prefix: str = \"eval\",\n    ) -&gt; dict[str, float] | None:\n        \"\"\"Evaluate the model on the given dataset.\n\n        Args:\n            eval_dataset:\n                The dataset to evaluate on. If None, then use the stored evaluation\n                dataset.\n            orig_eval_dataset:\n                The original evaluation dataset, before any postprocessing. If None,\n                then use the stored original evaluation dataset.\n            ignore_keys:\n                The keys to ignore when computing the metrics.\n            metric_key_prefix:\n                The prefix to use for the metric keys.\n\n        Returns:\n            The metrics computed on the evaluation dataset.\n        \"\"\"\n        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n\n        # Temporarily disable metric computation, we will do it in the loop here.\n        compute_metrics = self.compute_metrics  # type: ignore[has-type]\n        self.compute_metrics = None\n        eval_loop = (\n            self.prediction_loop\n            if self.args.use_legacy_prediction_loop\n            else self.evaluation_loop\n        )\n        try:\n            output = eval_loop(\n                eval_dataloader,\n                description=\"Evaluation\",\n                prediction_loss_only=True if compute_metrics is None else None,\n                ignore_keys=ignore_keys,\n                metric_key_prefix=metric_key_prefix,\n            )\n        finally:\n            self.compute_metrics = compute_metrics\n\n        if orig_eval_dataset is not None:\n            preds_and_labels = postprocess_predictions_and_labels(\n                predictions=output.predictions,\n                dataset=orig_eval_dataset,\n                prepared_dataset=eval_dataset,\n                cls_token_index=self.cls_token_id,\n            )\n            output.metrics.update(self.compute_metrics(preds_and_labels))\n\n            # Prefix all keys with metric_key_prefix + '_'\n            for key in list(output.metrics.keys()):\n                if not key.startswith(f\"{metric_key_prefix}_\"):\n                    output.metrics[f\"{metric_key_prefix}_{key}\"] = output.metrics.pop(\n                        key\n                    )\n\n        # Only the main node log the results by default\n        if self.args.should_log:\n            self.log(output.metrics)\n\n        self.control = self.callback_handler.on_evaluate(\n            self.args,\n            self.state,\n            self.control,  # type: ignore[has-type]\n            output.metrics,\n        )\n        return output.metrics\n\n\ndef compute_metrics(docs\n    model_outputs_and_labels: tuple[\"Predictions\", \"Labels\"],\n    id2label: dict[int, str],\n    dataset_config: \"DatasetConfig\",\n    benchmark_config: \"BenchmarkConfig\",\n) -&gt; dict[str, float]:\n    \"\"\"Compute the metrics needed for evaluation.\n\n    Args:\n        model_outputs_and_labels:\n            The first sequence contains the model outputs and the second sequence\n            contains the true labels.\n        id2label:\n            Conversion of indices to labels.\n        dataset_config:\n            The configuration of the dataset.\n        benchmark_config:\n            The configuration of the benchmark.\n\n    Returns:\n        A dictionary with the names of the metrics as keys and the metric values as\n        values.\n    \"\"\"\n    model_outputs, labels = model_outputs_and_labels\n    raise_if_model_output_contains_nan_values(model_output=model_outputs)\n\n    metrics = {\n        metric_cfg.name: (\n            evaluate.load(\n                path=metric_cfg.huggingface_id, cache_dir=benchmark_config.cache_dir\n            )\n            if metric_cfg.huggingface_id != \"\"\n            else None\n        )\n        for metric_cfg in dataset_config.task.metrics\n    }\n\n    model_output_dtype = np.asarray(model_outputs).dtype\n    if model_output_dtype in [np.float16, np.float32, np.float64]:\n        predictions = np.asarray(model_outputs).argmax(axis=-1)\n    else:\n        predictions = model_outputs\n\n    results: dict[str, float] = dict()\n    for cfg in dataset_config.task.metrics:\n        metric = metrics[cfg.name]\n        assert isinstance(metric, EvaluationModule)\n        score_dict: dict[str, float] | None = metric.compute(\n            predictions=predictions, references=labels, **cfg.compute_kwargs\n        )\n\n        # The metric returns None if we are running on multi-GPU and the current\n        # process is not the main process\n        if score_dict is not None:\n            scores = score_dict[cfg.results_key]\n            if isinstance(scores, list):\n                scores = sum(scores) / len(scores)\n            results[cfg.name] = scores\n\n    return results\n\n\ndef extract_labels_from_generation(docs\n    input_batch: dict[str, list], model_output: \"GenerativeModelOutput\"\n) -&gt; list[t.Any]:\n    \"\"\"Extract the predicted labels from the generated output.\n\n    Args:\n        input_batch:\n            The input batch, where the keys are the feature names and the values\n            are lists with the feature values.\n        model_output:\n            The raw generated output of the model.\n\n    Returns:\n        The predicted labels.\n    \"\"\"\n    raw_predictions = model_output.sequences\n    predictions = [\n        dict(id=id, prediction_text=predicted_answer.lower(), no_answer_probability=0.0)\n        for id, predicted_answer in zip(input_batch[\"id\"], raw_predictions)\n    ]\n    return predictions\n\n\ndef prepare_train_examples(docs\n    examples: \"BatchEncoding\", tokenizer: \"PreTrainedTokenizer\"\n) -&gt; \"BatchEncoding\":\n    \"\"\"Prepare the features for training.\n\n    Args:\n        examples:\n            The examples to prepare.\n        tokenizer:\n            The tokenizer to use to prepare the examples.\n\n    Returns:\n        The prepared examples.\n    \"\"\"\n    # Some of the questions have lots of whitespace on the left, which is not useful\n    # and will make the truncation of the context fail (the tokenized question will\n    # take a lots of space). So we remove that left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Extract special token metadata from the tokenizer\n    special_token_metadata = get_special_token_metadata(tokenizer=tokenizer)\n    has_cls_token = special_token_metadata[\"has_cls_token\"]\n    has_sep_token = special_token_metadata[\"has_sep_token\"]\n    cls_token_id = special_token_metadata[\"cls_token_id\"]\n    cls_token = special_token_metadata[\"cls_token\"]\n    sep_token = special_token_metadata[\"sep_token\"]\n\n    # If the tokenizer is not adding special tokens, then we add them manually\n    if not has_cls_token and not has_sep_token:\n        examples[\"question\"] = [\n            f\"{cls_token}{q}{sep_token}\" for q in examples[\"question\"]\n        ]\n        examples[\"context\"] = [f\"{c}{sep_token}\" for c in examples[\"context\"]]\n\n    # Set the stride used during tokenization, when the context is long enough to be\n    # split into several features. Since we are always keeping the question tokens, we\n    # need to make sure that the stride does not exceed the resulting maximum context\n    # length.\n    max_question_tokens = max(len(tokenizer(q).input_ids) for q in examples[\"question\"])\n    num_special_tokens = int(has_cls_token) + int(has_sep_token)\n    stride = tokenizer.model_max_length // 4\n    max_length = tokenizer.model_max_length - stride\n    stride = min(stride, max_length - max_question_tokens - num_special_tokens)\n    max_length = tokenizer.model_max_length - stride\n\n    # Tokenize our examples with truncation and padding, but keep the overflows using a\n    # stride. This results in one example possible giving several features when a\n    # context is long, each of those features having a context that overlaps a bit the\n    # context of the previous feature.\n    tokenized_examples = tokenizer(\n        text=examples[\"question\"],\n        text_pair=examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we\n    # need a map from a feature to its corresponding example. This key gives us just\n    # that\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n    # The offset mappings will give us a map from token to character position in the\n    # original context. This will help us compute the start_positions and\n    # end_positions.\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n\n    # Initialise the start- and end positions of the answers\n    tokenized_examples[\"start_positions\"] = list()\n    tokenized_examples[\"end_positions\"] = list()\n\n    for i, offsets in enumerate(offset_mapping):\n        # Get the input IDs for the current example\n        input_ids = tokenized_examples.input_ids[i]\n\n        # We will label impossible answers with the index of the CLS token\n        cls_index = input_ids.index(cls_token_id)\n\n        # Grab the sequence corresponding to that example (to know what is the context\n        # and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n\n        # Manually ensure that the special tokens are set to None in `sequence_ids`\n        for special_token in tokenizer.special_tokens_map.keys():\n            if hasattr(tokenizer, f\"{special_token}_id\"):\n                special_token_id = getattr(tokenizer, f\"{special_token}_id\")\n                if special_token_id is not None:\n                    sequence_ids = [\n                        None if token_id == special_token_id else seq_id\n                        for token_id, seq_id in zip(input_ids, sequence_ids)\n                    ]\n\n        # One example can give several spans, this is the index of the example\n        # containing this span of text.\n        sample_index = sample_mapping[i]\n        answers = examples[\"answers\"][sample_index]\n\n        # If no answers are given, set the cls_index as answer.\n        if len(answers[\"answer_start\"]) == 0:\n            tokenized_examples.start_positions.append(cls_index)\n            tokenized_examples.end_positions.append(cls_index)\n\n        else:\n            # Start/end character index of the answer in the text.\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            # Start token index of the current span in the text.\n            token_start_index = 0\n            while sequence_ids[token_start_index] != 1:\n                token_start_index += 1\n\n            # End token index of the current span in the text.\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != 1:\n                token_end_index -= 1\n\n            # Detect if the answer is out of the span (in which case this feature is\n            # labeled with the CLS index).\n            if not (\n                offsets[token_start_index][0] &lt;= start_char\n                and offsets[token_end_index][1] &gt;= end_char\n            ):\n                tokenized_examples.start_positions.append(cls_index)\n                tokenized_examples.end_positions.append(cls_index)\n\n            # Otherwise move the token_start_index and token_end_index to the two ends\n            # of the answer. Note: we could go after the last offset if the answer is\n            # the last word (edge case).\n            else:\n                while (\n                    token_start_index &lt;= token_end_index\n                    and offsets[token_start_index][0] &lt;= start_char\n                ):\n                    token_start_index += 1\n                token_start_index -= 1\n                tokenized_examples.start_positions.append(token_start_index)\n                while (\n                    token_start_index &lt;= token_end_index\n                    and offsets[token_end_index][1] &gt;= end_char\n                ):\n                    token_end_index -= 1\n                token_end_index += 1\n                tokenized_examples.end_positions.append(token_end_index)\n                assert token_end_index &gt;= token_start_index\n\n    return tokenized_examples\n\n\ndef prepare_test_examples(docs\n    examples: \"BatchEncoding\", tokenizer: \"PreTrainedTokenizer\"\n) -&gt; \"BatchEncoding\":\n    \"\"\"Prepare test examples.\n\n    Args:\n        examples:\n            Dictionary of test examples.\n        tokenizer:\n            The tokenizer used to preprocess the examples.\n\n    Returns:\n        The prepared test examples.\n    \"\"\"\n    # Some of the questions have lots of whitespace on the left, which is not useful\n    # and will make the truncation of the context fail (the tokenized question will\n    # take a lots of space). So we remove that left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Extract special token metadata from the tokenizer\n    special_token_metadata = get_special_token_metadata(tokenizer=tokenizer)\n    has_cls_token = special_token_metadata[\"has_cls_token\"]\n    has_sep_token = special_token_metadata[\"has_sep_token\"]\n    cls_token = special_token_metadata[\"cls_token\"]\n    sep_token = special_token_metadata[\"sep_token\"]\n\n    # If the tokenizer is not adding special tokens, then we add them manually\n    if not has_cls_token and not has_sep_token:\n        examples[\"question\"] = [\n            f\"{cls_token}{q}{sep_token}\" for q in examples[\"question\"]\n        ]\n        examples[\"context\"] = [f\"{c}{sep_token}\" for c in examples[\"context\"]]\n\n    # Set the stride used during tokenization, when the context is long enough to be\n    # split into several features. Since we are always keeping the question tokens, we\n    # need to make sure that the stride does not exceed the resulting maximum context\n    # length.\n    max_question_tokens = max(len(tokenizer(q).input_ids) for q in examples[\"question\"])\n    num_special_tokens = int(has_cls_token) + int(has_sep_token)\n    stride = tokenizer.model_max_length // 4\n    max_length = tokenizer.model_max_length - stride\n    stride = min(stride, max_length - max_question_tokens - num_special_tokens)\n    max_length = tokenizer.model_max_length - stride\n\n    # Tokenize our examples with truncation and maybe padding, but keep the overflows\n    # using a stride. This results in one example possible giving several features when\n    # a context is long, each of those features having a context that overlaps a bit\n    # the context of the previous feature.\n    tokenized_examples = tokenizer(\n        text=examples[\"question\"],\n        text_pair=examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we\n    # need a map from a feature to its corresponding example. This key gives us just\n    # that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n    # We keep the id that gave us this feature and we will store the offset mappings.\n    tokenized_examples[\"id\"] = list()\n\n    for i in range(len(tokenized_examples.input_ids)):\n        # Grab the sequence corresponding to that example (to know what is the context\n        # and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1\n\n        # One example can give several spans, this is the index of the example\n        # containing this span of text.\n        sample_index = sample_mapping[i]\n        tokenized_examples.id.append(examples[\"id\"][sample_index])\n\n        # Set to (-1, -1) the offset_mapping that are not part of the context so it's\n        # easy to determine if a token position is part of the context or not.\n        tokenized_examples.offset_mapping[i] = [\n            (o if sequence_ids[k] == context_index else (-1, -1))\n            for k, o in enumerate(tokenized_examples.offset_mapping[i])\n        ]\n\n    return tokenized_examples\n\n\ndef postprocess_predictions_and_labels(docs\n    predictions: list,\n    dataset: \"Dataset\",\n    prepared_dataset: \"Dataset\",\n    cls_token_index: int,\n) -&gt; tuple[list[dict], list[dict]]:\n    \"\"\"Postprocess the predictions and labels, to allow easier metric computation.\n\n    Args:\n        predictions:\n            A pair of (start_logits, end_logits) predictions.\n        dataset:\n            The dataset containing the examples.\n        prepared_dataset:\n            The dataset containing the prepared examples.\n        cls_token_index:\n            The index of the CLS token.\n\n    Returns:\n        The postprocessed predictions and labels.\n    \"\"\"\n    # Extract the logits from the predictions\n    all_start_logits = predictions[0]\n    all_end_logits = predictions[1]\n\n    # Build a map from an example to its corresponding features, being the blocks of\n    # text from the context that we're feeding into the model. An example can have\n    # multiple features/blocks if it has a long context.\n    id_to_index = {k: i for i, k in enumerate(dataset[\"id\"])}\n    features_per_example = defaultdict(list)\n    for i, feature in enumerate(prepared_dataset):\n        id = feature[\"id\"]\n        example_index = id_to_index[id]\n        features_per_example[example_index].append(i)\n\n    # Loop over all the examples\n    predictions = list()\n    labels = list()\n    for example_index, example in enumerate(dataset):\n        # Extract the best valid answer associated with the current example\n        best_answer = find_best_answer(\n            all_start_logits=all_start_logits,\n            all_end_logits=all_end_logits,\n            prepared_dataset=prepared_dataset,\n            feature_indices=features_per_example[example_index],\n            context=example[\"context\"],\n            max_answer_length=30,\n            num_best_logits=20,\n            min_null_score=0.0,\n            cls_token_index=cls_token_index,\n        )\n\n        # Create the final prediction dictionary, to be added to the list of\n        # predictions\n        prediction = dict(\n            id=example[\"id\"], prediction_text=best_answer, no_answer_probability=0.0\n        )\n\n        # Add the answer to the list of predictions\n        predictions.append(prediction)\n\n        # Create the associated reference dictionary, to be added to the list of\n        # references\n        label = dict(\n            id=example[\"id\"],\n            answers=dict(\n                text=example[\"answers\"][\"text\"],\n                answer_start=example[\"answers\"][\"answer_start\"],\n            ),\n        )\n\n        # Add the answer and label to the list of predictions and labels, respectively\n        labels.append(label)\n\n    return predictions, labels\n\n\ndef find_best_answer(docs\n    all_start_logits: np.ndarray,\n    all_end_logits: np.ndarray,\n    prepared_dataset: \"Dataset\",\n    feature_indices: list[int],\n    context: str,\n    max_answer_length: int,\n    num_best_logits: int,\n    min_null_score: float,\n    cls_token_index: int,\n) -&gt; str:\n    \"\"\"Find the best answer for a given example.\n\n    Args:\n        all_start_logits:\n            The start logits for all the features.\n        all_end_logits:\n            The end logits for all the features.\n        prepared_dataset:\n            The dataset containing the prepared examples.\n        feature_indices:\n            The indices of the features associated with the current example.\n        context:\n            The context of the example.\n        max_answer_length:\n            The maximum length of the answer.\n        num_best_logits:\n            The number of best logits to consider.\n        min_null_score:\n            The minimum score an answer can have.\n        cls_token_index:\n            The index of the CLS token.\n\n    Returns:\n        The best answer for the example.\n    \"\"\"\n    # Loop through all the features associated to the current example\n    valid_answers = list()\n    for feature_index in feature_indices:\n        # Get the features associated with the current example\n        features = prepared_dataset[feature_index]\n\n        # Get the predictions of the model for this feature\n        start_logits = all_start_logits[feature_index]\n        end_logits = all_end_logits[feature_index]\n\n        # Update minimum null prediction\n        cls_index = features[\"input_ids\"].index(cls_token_index)\n        feature_null_score = (start_logits[cls_index] + end_logits[cls_index]).item()\n        if min_null_score &lt; feature_null_score:\n            min_null_score = feature_null_score\n\n        # Find the valid answers for the feature\n        valid_answers_for_feature = find_valid_answers(\n            start_logits=start_logits,\n            end_logits=end_logits,\n            offset_mapping=features[\"offset_mapping\"],\n            context=context,\n            max_answer_length=max_answer_length,\n            num_best_logits=num_best_logits,\n            min_null_score=min_null_score,\n        )\n        valid_answers.extend(valid_answers_for_feature)\n\n    # In the very rare edge case we have not a single non-null prediction, we create a\n    # fake prediction to avoid failure\n    if not valid_answers:\n        return \"\"\n\n    # Otherwise, we select the answer with the largest score as the best answer, and\n    # return it\n    best_answer_dict = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n    return best_answer_dict[\"text\"]\n\n\ndef find_valid_answers(docs\n    start_logits: np.ndarray,\n    end_logits: np.ndarray,\n    offset_mapping: list[tuple[int, int]],\n    context: str,\n    max_answer_length: int,\n    num_best_logits: int,\n    min_null_score: float,\n) -&gt; list[dict]:\n    \"\"\"Find the valid answers from the start and end indexes.\n\n    Args:\n        start_logits:\n            The logits for the start of the answer.\n        end_logits:\n            The logits for the end of the answer.\n        offset_mapping:\n            The offset mapping, being a list of pairs of integers for each token index,\n            containing the start and end character index in the original context.\n        context:\n            The context of the example.\n        max_answer_length:\n            The maximum length of the answer.\n        num_best_logits:\n            The number of best logits to consider. Note that this function will run in\n            O(`num_best_logits` ^ 2) time.\n        min_null_score:\n            The minimum score an answer can have.\n\n    Returns:\n        A list of the valid answers, each being a dictionary with keys \"text\" and\n        \"score\", the score being the sum of the start and end logits.\n    \"\"\"\n    # Fetch the top-k predictions for the start- and end token indices\n    start_indexes = np.argsort(start_logits)[-1 : -num_best_logits - 1 : -1].tolist()\n    end_indexes = np.argsort(end_logits)[-1 : -num_best_logits - 1 : -1].tolist()\n\n    # We loop over all combinations of starting and ending indexes for valid answers\n    valid_answers = list()\n    for start_index in start_indexes:\n        for end_index in end_indexes:\n            # If the starting or ending index is out-of-scope, meaning that they are\n            # either out of bounds or correspond to part of the input_ids that are not\n            # in the context, then we skip this index\n            if (\n                start_index &gt;= len(offset_mapping)\n                or end_index &gt;= len(offset_mapping)\n                or tuple(offset_mapping[start_index]) == (-1, -1)\n                or tuple(offset_mapping[end_index]) == (-1, -1)\n            ):\n                continue\n\n            # Do not consider answers with a length that is either negative or greater\n            # than the context length\n            max_val = max_answer_length + start_index - 1\n            if end_index &lt; start_index or end_index &gt; max_val:\n                continue\n\n            # If we got to this point then the answer is valid, so we store the\n            # corresponding start- and end character indices in the original context,\n            # and from these extract the answer\n            start_char = offset_mapping[start_index][0]\n            end_char = offset_mapping[end_index][1]\n            text = context[start_char:end_char]\n\n            # Compute the score of the answer, being the sum of the start and end\n            # logits. Intuitively, this indicates how likely the answer is to be\n            # correct, and allows us to pick the best valid answer.\n            score = start_logits[start_index] + end_logits[end_index]\n\n            # Add the answer to the list of valid answers, if the score is greater\n            # than the minimum null score\n            if score &gt; min_null_score:\n                valid_answers.append(dict(score=score, text=text))\n\n    return valid_answers\n</code></pre>"},{"location":"api/scandeval/task_utils/sequence_classification/","title":"scandeval.task_utils.sequence_classification","text":"scandeval.task_utils.sequence_classification<p> source module scandeval.task_utils.sequence_classification </p> <p>Utility functions related to the sequence-classification supertask.</p> <p> Functions </p> <ul> <li> <p>compute_metrics \u2014 Compute the metrics needed for evaluation.</p> </li> <li> <p>extract_labels_from_generation \u2014 Extract the predicted labels from the generated output.</p> </li> <li> <p>get_closest_logprobs_labels \u2014 Get the labels with the highest predicted logprob value.</p> </li> <li> <p>get_closest_word_edit_labels \u2014 Get the labels with the smallest edit distance to the predicted labels.</p> </li> </ul> <p> source compute_metrics(model_outputs_and_labels: tuple[Predictions, Labels], id2label: dict[int, str], dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 dict[str, float] </p> <p>Compute the metrics needed for evaluation.</p> <p> Parameters </p> <ul> <li> <p>model_outputs_and_labels :  tuple[Predictions, Labels] \u2014</p> <p>The first sequence contains the model outputs and the second sequence contains the true labels.</p> </li> <li> <p>id2label :  dict[int, str] \u2014</p> <p>Conversion of indices to labels.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The configuration of the benchmark.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] \u2014 A dictionary with the names of the metrics as keys and the metric values as values.</p> </li> </ul> <p> source extract_labels_from_generation(input_batch: dict[str, list], model_output: GenerativeModelOutput, dataset_config: DatasetConfig) \u2192 list[str] </p> <p>Extract the predicted labels from the generated output.</p> <p> Parameters </p> <ul> <li> <p>input_batch :  dict[str, list] \u2014</p> <p>The input batch, where the keys are the feature names and the values are lists with the feature values.</p> </li> <li> <p>model_output :  GenerativeModelOutput \u2014</p> <p>The raw generated output of the model.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 The predicted labels.</p> </li> </ul> <p> source get_closest_logprobs_labels(generation_logprobs: list[list[list[tuple[str, float]]]], dataset_config: DatasetConfig) \u2192 list[str] </p> <p>Get the labels with the highest predicted logprob value.</p> <p>In case a candidate label is split into multiple tokens, we only use the first token to compute the logprob value. E.g., if the candidate label \"positive\" is tokenised as [\"pos\", \"itive\"], we only use the logprob value of \"pos\" to represent the logprob value of the entire label.</p> <p> Parameters </p> <ul> <li> <p>generation_logprobs :  list[list[list[tuple[str, float]]]] \u2014</p> <p>The logprobs of the generated tokens, for all samples in the batch. Of shape (batch_size, num_tokens, num_logprobs).</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 The predicted labels.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark \u2014</p> <p>If no candidate label can be found for any of the generated labels.</p> </li> </ul> <p> source get_closest_word_edit_labels(generated_sequences: list[str], dataset_config: DatasetConfig) \u2192 list[str] </p> <p>Get the labels with the smallest edit distance to the predicted labels.</p> <p> Parameters </p> <ul> <li> <p>generated_sequences :  list[str] \u2014</p> <p>The generated sequences from the model.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 The candidate labels with the smallest edit distance to the predicted labels.</p> </li> </ul>"},{"location":"src/scandeval/task_utils/sequence_classification/","title":"scandeval.task_utils.sequence_classification","text":"scandeval.task_utils.sequence_classification<p> docs module scandeval.task_utils.sequence_classification </p> <pre><code>\"\"\"Utility functions related to the sequence-classification supertask.\"\"\"\n\nimport logging\nimport re\nimport typing as t\n\nimport evaluate\nimport Levenshtein\nimport numpy as np\nfrom evaluate import EvaluationModule\n\nfrom ..data_models import BenchmarkConfig, GenerativeModelOutput\nfrom ..utils import log_once, raise_if_model_output_contains_nan_values\n\nif t.TYPE_CHECKING:\n    from ..data_models import DatasetConfig\n    from ..types import Labels, Predictions\n\n\nlogger = logging.getLogger(\"scandeval\")\n\n\ndef compute_metrics(docs\n    model_outputs_and_labels: tuple[\"Predictions\", \"Labels\"],\n    id2label: dict[int, str],\n    dataset_config: \"DatasetConfig\",\n    benchmark_config: \"BenchmarkConfig\",\n) -&gt; dict[str, float]:\n    \"\"\"Compute the metrics needed for evaluation.\n\n    Args:\n        model_outputs_and_labels:\n            The first sequence contains the model outputs and the second sequence\n            contains the true labels.\n        id2label:\n            Conversion of indices to labels.\n        dataset_config:\n            The configuration of the dataset.\n        benchmark_config:\n            The configuration of the benchmark.\n\n    Returns:\n        A dictionary with the names of the metrics as keys and the metric values as\n        values.\n    \"\"\"\n    model_outputs, labels = model_outputs_and_labels\n    label2id = {label: idx for idx, label in id2label.items()}\n    raise_if_model_output_contains_nan_values(model_output=model_outputs)\n\n    metrics = {\n        metric_cfg.name: (\n            evaluate.load(\n                path=metric_cfg.huggingface_id, cache_dir=benchmark_config.cache_dir\n            )\n            if metric_cfg.huggingface_id != \"\"\n            else None\n        )\n        for metric_cfg in dataset_config.task.metrics\n    }\n\n    model_output_dtype = np.asarray(model_outputs).dtype\n    if model_output_dtype in [np.float16, np.float32, np.float64]:\n        predictions = np.asarray(model_outputs).argmax(axis=-1)\n    else:\n        predictions = model_outputs\n\n    prompt_label_to_label_mapping = {\n        prompt_label: label\n        for label, prompt_label in dataset_config.prompt_label_mapping.items()\n    }\n    predictions = [\n        (\n            label2id[prompt_label_to_label_mapping[pred.lower()]]\n            if isinstance(pred, str)\n            else pred\n        )\n        for pred in predictions\n    ]\n\n    label_ids = [\n        label2id[label.lower()] if isinstance(label, str) else label for label in labels\n    ]\n\n    results: dict[str, float] = dict()\n    for cfg in dataset_config.task.metrics:\n        metric = metrics[cfg.name]\n        assert isinstance(metric, EvaluationModule)\n        score_dict: dict[str, float] | None = metric.compute(\n            predictions=predictions, references=label_ids, **cfg.compute_kwargs\n        )\n\n        # The metric returns None if we are running on multi-GPU and the current\n        # process is not the main process\n        if score_dict is not None:\n            scores = score_dict[cfg.results_key]\n            if isinstance(scores, list):\n                scores = sum(scores) / len(scores)\n            results[cfg.name] = scores\n\n    return results\n\n\ndef extract_labels_from_generation(docs\n    input_batch: dict[str, list],\n    model_output: GenerativeModelOutput,\n    dataset_config: \"DatasetConfig\",\n) -&gt; list[str]:\n    \"\"\"Extract the predicted labels from the generated output.\n\n    Args:\n        input_batch:\n            The input batch, where the keys are the feature names and the values\n            are lists with the feature values.\n        model_output:\n            The raw generated output of the model.\n        dataset_config:\n            The configuration of the dataset.\n\n    Returns:\n        The predicted labels.\n    \"\"\"\n    if model_output.scores is not None:\n        return get_closest_logprobs_labels(\n            generation_logprobs=model_output.scores, dataset_config=dataset_config\n        )\n    else:\n        return get_closest_word_edit_labels(\n            generated_sequences=model_output.sequences, dataset_config=dataset_config\n        )\n\n\ndef get_closest_logprobs_labels(docs\n    generation_logprobs: list[list[list[tuple[str, float]]]],\n    dataset_config: \"DatasetConfig\",\n) -&gt; list[str]:\n    \"\"\"Get the labels with the highest predicted logprob value.\n\n    In case a candidate label is split into multiple tokens, we only use the first\n    token to compute the logprob value. E.g., if the candidate label \"positive\" is\n    tokenised as [\"pos\", \"itive\"], we only use the logprob value of \"pos\" to\n    represent the logprob value of the entire label.\n\n    Args:\n        generation_logprobs:\n            The logprobs of the generated tokens, for all samples in the batch. Of shape\n            (batch_size, num_tokens, num_logprobs).\n        dataset_config:\n            The configuration of the dataset.\n\n    Returns:\n        The predicted labels.\n\n    Raises:\n        InvalidBenchmark:\n            If no candidate label can be found for any of the generated labels.\n    \"\"\"\n    english_labels = list(dataset_config.id2label.values())\n    english2local = dataset_config.prompt_label_mapping\n    candidate_labels = [\n        english2local[lbl].lower() for lbl in english_labels\n    ] + english_labels\n\n    output_labels: list[str] = list()\n    for sample in generation_logprobs:\n        for logprob_list in sample:\n            generated_labels = [\n                re.sub(\n                    pattern=r\"^[^a-z\u00e6\u00f8\u00e5\u00fc\u00f6\u00e4]+|[^a-z\u00e6\u00f8\u00e5\u00fc\u00f6\u00e4]+$\",\n                    repl=\"\",\n                    string=label.lower(),\n                )\n                for label, _ in logprob_list\n            ]\n            generated_labels = [label for label in generated_labels if label != \"\"]\n\n            # We want to use the first generated label which starts with a candidate\n            # label, as the output label\n            output_label: str | None = None\n            for generated_label in generated_labels:\n                candidate_output_labels = [\n                    candidate_label\n                    for candidate_label in candidate_labels\n                    if candidate_label.startswith(generated_label)\n                ]\n                if candidate_output_labels:\n                    output_label = candidate_output_labels[0]\n                    break\n\n            if output_label is not None:\n                output_label = english2local.get(output_label, output_label)\n                output_labels.append(output_label)\n                break\n        else:\n            if len(sample) == 0:\n                log_once(\n                    \"The model outputted an empty string, so no candidate labels could \"\n                    f\"be determined. Using {candidate_labels[0]!r} as the output \"\n                    \"label.\",\n                    level=logging.DEBUG,\n                )\n            else:\n                log_once(\n                    \"Could not find a candidate label for any of the generated \"\n                    f\"labels in the sample {sample}. Using {candidate_labels[0]!r} \"\n                    \"as the output label.\",\n                    level=logging.DEBUG,\n                )\n            output_labels.append(candidate_labels[0])\n\n    assert len(output_labels) == len(generation_logprobs)\n    return output_labels\n\n\ndef get_closest_word_edit_labels(docs\n    generated_sequences: list[str], dataset_config: \"DatasetConfig\"\n) -&gt; list[str]:\n    \"\"\"Get the labels with the smallest edit distance to the predicted labels.\n\n    Args:\n        generated_sequences:\n            The generated sequences from the model.\n        dataset_config:\n            The configuration of the dataset.\n\n    Returns:\n        The candidate labels with the smallest edit distance to the predicted labels.\n    \"\"\"\n    candidate_labels = [\n        dataset_config.prompt_label_mapping[lbl]\n        for lbl in dataset_config.id2label.values()\n    ]\n    new_predicted_labels: list[str] = list()\n    for predicted_label in generated_sequences:\n        edit_distances = [\n            Levenshtein.distance(s1=predicted_label.lower(), s2=candidate_label.lower())\n            for candidate_label in candidate_labels\n        ]\n        closest_label = candidate_labels[np.argmin(edit_distances).item()]\n        new_predicted_labels.append(closest_label)\n    return new_predicted_labels\n</code></pre>"},{"location":"api/scandeval/task_utils/text_to_text/","title":"scandeval.task_utils.text_to_text","text":"scandeval.task_utils.text_to_text<p> source module scandeval.task_utils.text_to_text </p> <p>Utility functions related to the text-to-text supertask.</p> <p> Functions </p> <ul> <li> <p>compute_metrics \u2014 Compute the metrics needed for evaluation.</p> </li> <li> <p>extract_labels_from_generation \u2014 Extract the predicted labels from the generated output.</p> </li> </ul> <p> source compute_metrics(model_outputs_and_labels: tuple[Predictions, Labels], id2label: dict[int, str], dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 dict[str, float] </p> <p>Compute the metrics needed for evaluation.</p> <p> Parameters </p> <ul> <li> <p>model_outputs_and_labels :  tuple[Predictions, Labels] \u2014</p> <p>The first sequence contains the model outputs and the second sequence contains the true labels.</p> </li> <li> <p>id2label :  dict[int, str] \u2014</p> <p>Conversion of indices to labels.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The configuration of the benchmark.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] \u2014 A dictionary with the names of the metrics as keys and the metric values as values.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source extract_labels_from_generation(input_batch: dict[str, list], model_output: GenerativeModelOutput) \u2192 list[t.Any] </p> <p>Extract the predicted labels from the generated output.</p> <p> Parameters </p> <ul> <li> <p>input_batch :  dict[str, list] \u2014</p> <p>The input batch, where the keys are the feature names and the values are lists with the feature values.</p> </li> <li> <p>model_output :  GenerativeModelOutput \u2014</p> <p>The raw generated output of the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[t.Any] \u2014 The predicted labels.</p> </li> </ul>"},{"location":"src/scandeval/task_utils/text_to_text/","title":"scandeval.task_utils.text_to_text","text":"scandeval.task_utils.text_to_text<p> docs module scandeval.task_utils.text_to_text </p> <pre><code>\"\"\"Utility functions related to the text-to-text supertask.\"\"\"\n\nimport logging\nimport typing as t\n\nimport evaluate\nimport numpy as np\nfrom evaluate import EvaluationModule\n\nfrom ..constants import METRIC_ATTRIBUTES_TAKING_UP_MEMORY\nfrom ..data_models import BenchmarkConfig, DatasetConfig, GenerativeModelOutput\nfrom ..exceptions import InvalidBenchmark\nfrom ..utils import (\n    HiddenPrints,\n    clear_memory,\n    raise_if_model_output_contains_nan_values,\n)\n\nif t.TYPE_CHECKING:\n    from ..types import Labels, Predictions\n\n\nlogger = logging.getLogger(\"scandeval\")\n\n\ndef compute_metrics(docs\n    model_outputs_and_labels: tuple[\"Predictions\", \"Labels\"],\n    id2label: dict[int, str],\n    dataset_config: \"DatasetConfig\",\n    benchmark_config: \"BenchmarkConfig\",\n) -&gt; dict[str, float]:\n    \"\"\"Compute the metrics needed for evaluation.\n\n    Args:\n        model_outputs_and_labels:\n            The first sequence contains the model outputs and the second sequence\n            contains the true labels.\n        id2label:\n            Conversion of indices to labels.\n        dataset_config:\n            The configuration of the dataset.\n        benchmark_config:\n            The configuration of the benchmark.\n\n    Returns:\n        A dictionary with the names of the metrics as keys and the metric values as\n        values.\n    \"\"\"\n    model_outputs, labels = model_outputs_and_labels\n    raise_if_model_output_contains_nan_values(model_output=model_outputs)\n\n    metrics = {\n        metric_cfg.name: (\n            evaluate.load(\n                path=metric_cfg.huggingface_id, cache_dir=benchmark_config.cache_dir\n            )\n            if metric_cfg.huggingface_id != \"\"\n            else None\n        )\n        for metric_cfg in dataset_config.task.metrics\n    }\n\n    model_output_dtype = np.asarray(model_outputs).dtype\n    output_is_prob = model_output_dtype in [np.float16, np.float32, np.float64]\n    if output_is_prob:\n        predictions = np.asarray(model_outputs).argmax(axis=-1)\n    else:\n        predictions = model_outputs\n\n    results: dict[str, float] = dict()\n    for cfg in dataset_config.task.metrics:\n        metric = metrics[cfg.name]\n        assert isinstance(metric, EvaluationModule)\n\n        # Some metrics can be computed on hardware accelerators. In this case we\n        # start by setting the device to the same device as the model\n        if cfg.compute_kwargs.get(\"device\", None) == \"auto\":\n            cfg.compute_kwargs[\"device\"] = benchmark_config.device.type\n\n        while True:\n            try:\n                with HiddenPrints():\n                    score_dict: dict[str, float] | None = metric.compute(\n                        predictions=predictions, references=labels, **cfg.compute_kwargs\n                    )\n\n                # Clear the cache of the BERTScorer to avoid memory leaks\n                for attribute in METRIC_ATTRIBUTES_TAKING_UP_MEMORY:\n                    if hasattr(metric, attribute):\n                        delattr(metric, attribute)\n\n                clear_memory()\n                break\n            except Exception as e:\n                # Clear the cache of the BERTScorer to avoid memory leaks\n                if hasattr(metric, \"cached_bertscorer\"):\n                    del metric.cached_bertscorer\n                    clear_memory()\n\n                oom_error = [\n                    \"CUDA out of memory\",\n                    \"CUDA error\",\n                    \"MPS backend out of memory\",\n                ]\n                if not any(error in str(e) for error in oom_error):\n                    raise InvalidBenchmark(str(e))\n\n                if cfg.compute_kwargs.get(\"batch_size\", 1) &gt; 1:\n                    batch_size = cfg.compute_kwargs[\"batch_size\"]\n                    cfg.compute_kwargs[\"batch_size\"] = batch_size // 2\n                    logger.debug(\n                        \"Out of memory error occurred during the computation of \"\n                        f\"the metric {cfg.pretty_name}. Reducing the batch size to \"\n                        f\"{cfg.compute_kwargs['batch_size']}.\"\n                    )\n                elif cfg.compute_kwargs.get(\"device\", \"cpu\") != \"cpu\":\n                    cfg.compute_kwargs[\"batch_size\"] = 32\n                    cfg.compute_kwargs[\"device\"] = \"cpu\"\n                    logger.debug(\n                        \"Out of memory error occurred during the computation of \"\n                        f\"the metric {cfg.pretty_name}. Moving the computation to \"\n                        \"the CPU.\"\n                    )\n                else:\n                    raise InvalidBenchmark(str(e))\n\n        # The metric returns None if we are running on multi-GPU and the current\n        # process is not the main process\n        if score_dict is not None:\n            scores = score_dict[cfg.results_key]\n            if isinstance(scores, list):\n                scores = sum(scores) / len(scores)\n            results[cfg.name] = scores\n\n    return results\n\n\ndef extract_labels_from_generation(docs\n    input_batch: dict[str, list], model_output: \"GenerativeModelOutput\"\n) -&gt; list[t.Any]:\n    \"\"\"Extract the predicted labels from the generated output.\n\n    Args:\n        input_batch:\n            The input batch, where the keys are the feature names and the values\n            are lists with the feature values.\n        model_output:\n            The raw generated output of the model.\n\n    Returns:\n        The predicted labels.\n    \"\"\"\n    return model_output.sequences\n</code></pre>"},{"location":"api/scandeval/task_utils/token_classification/","title":"scandeval.task_utils.token_classification","text":"scandeval.task_utils.token_classification<p> source module scandeval.task_utils.token_classification </p> <p>Utility functions related to the token-classification supertask.</p> <p> Functions </p> <ul> <li> <p>compute_metrics \u2014 Compute the metrics needed for evaluation.</p> </li> <li> <p>extract_labels_from_generation \u2014 Extract the predicted labels from the generated output.</p> </li> <li> <p>tokenize_and_align_labels \u2014 Tokenise all texts and align the labels with them.</p> </li> <li> <p>handle_unk_tokens \u2014 Replace unknown tokens in the tokens with the corresponding word.</p> </li> </ul> <p> source compute_metrics(model_outputs_and_labels: tuple[Predictions, Labels], id2label: dict[int, str], has_misc_tags: bool, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 dict[str, float] </p> <p>Compute the metrics needed for evaluation.</p> <p> Parameters </p> <ul> <li> <p>model_outputs_and_labels :  tuple[Predictions, Labels] \u2014</p> <p>The first array contains the probability predictions and the second array contains the true labels.</p> </li> <li> <p>id2label :  dict[int, str] \u2014</p> <p>Conversion of indices to labels.</p> </li> <li> <p>has_misc_tags :  bool \u2014</p> <p>Whether the dataset has MISC tags.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The configuration of the benchmark.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] \u2014 A dictionary with the names of the metrics as keys and the metric values as values.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source extract_labels_from_generation(input_batch: dict[str, list], model_output: GenerativeModelOutput, dataset_config: DatasetConfig) \u2192 list[t.Any] </p> <p>Extract the predicted labels from the generated output.</p> <p> Parameters </p> <ul> <li> <p>input_batch :  dict[str, list] \u2014</p> <p>The input batch, where the keys are the feature names and the values are lists with the feature values.</p> </li> <li> <p>model_output :  GenerativeModelOutput \u2014</p> <p>The raw generated output of the model.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[t.Any] \u2014 The predicted labels.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NeedsExtraInstalled</p> </li> </ul> <p> source tokenize_and_align_labels(examples: dict, tokenizer: PreTrainedTokenizer, label2id: dict[str, int]) \u2192 BatchEncoding </p> <p>Tokenise all texts and align the labels with them.</p> <p> Parameters </p> <ul> <li> <p>examples :  dict \u2014</p> <p>The examples to be tokenised.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer \u2014</p> <p>A pretrained tokenizer.</p> </li> <li> <p>label2id :  dict[str, int] \u2014</p> <p>A dictionary that converts NER tags to IDs.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BatchEncoding \u2014 A dictionary containing the tokenized data as well as labels.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source handle_unk_tokens(tokenizer: PreTrainedTokenizer, tokens: list[str], words: list[str]) \u2192 list[str] </p> <p>Replace unknown tokens in the tokens with the corresponding word.</p> <p> Parameters </p> <ul> <li> <p>tokenizer :  PreTrainedTokenizer \u2014</p> <p>The tokenizer used to tokenize the words.</p> </li> <li> <p>tokens :  list[str] \u2014</p> <p>The list of tokens.</p> </li> <li> <p>words :  list[str] \u2014</p> <p>The list of words.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 The list of tokens with unknown tokens replaced by the corresponding word.</p> </li> </ul>"},{"location":"src/scandeval/task_utils/token_classification/","title":"scandeval.task_utils.token_classification","text":"scandeval.task_utils.token_classification<p> docs module scandeval.task_utils.token_classification </p> <pre><code>\"\"\"Utility functions related to the token-classification supertask.\"\"\"\n\nimport importlib.util\nimport logging\nimport re\nimport typing as t\nfrom copy import deepcopy\n\nimport evaluate\nimport numpy as np\nfrom evaluate import EvaluationModule\nfrom transformers import PreTrainedTokenizer\n\nfrom ..data_models import BenchmarkConfig, DatasetConfig, GenerativeModelOutput\nfrom ..exceptions import InvalidBenchmark, NeedsExtraInstalled\nfrom ..utils import raise_if_model_output_contains_nan_values\n\nif t.TYPE_CHECKING:\n    from transformers import BatchEncoding\n\n    from ..types import Labels, Predictions\n\nif importlib.util.find_spec(\"demjson3\") is not None:\n    import demjson3\n\n\nlogger = logging.getLogger(\"scandeval\")\n\n\ndef compute_metrics(docs\n    model_outputs_and_labels: tuple[\"Predictions\", \"Labels\"],\n    id2label: dict[int, str],\n    has_misc_tags: bool,\n    dataset_config: \"DatasetConfig\",\n    benchmark_config: \"BenchmarkConfig\",\n) -&gt; dict[str, float]:\n    \"\"\"Compute the metrics needed for evaluation.\n\n    Args:\n        model_outputs_and_labels:\n            The first array contains the probability predictions and the second\n            array contains the true labels.\n        id2label:\n            Conversion of indices to labels.\n        has_misc_tags:\n            Whether the dataset has MISC tags.\n        dataset_config:\n            The configuration of the dataset.\n        benchmark_config:\n            The configuration of the benchmark.\n\n    Returns:\n        A dictionary with the names of the metrics as keys and the metric values as\n        values.\n    \"\"\"\n    model_outputs, labels = model_outputs_and_labels\n    raise_if_model_output_contains_nan_values(model_output=model_outputs)\n\n    metrics = {\n        metric_cfg.name: (\n            evaluate.load(\n                path=metric_cfg.huggingface_id, cache_dir=benchmark_config.cache_dir\n            )\n            if metric_cfg.huggingface_id != \"\"\n            else None\n        )\n        for metric_cfg in dataset_config.task.metrics\n    }\n\n    predictions: list[list[str]]\n    if not isinstance(model_outputs[0][0], str):\n        raw_predictions: list[list[int]] = np.argmax(model_outputs, axis=-1).tolist()\n\n        # Remove ignored index (special tokens)\n        predictions = [\n            [\n                id2label[pred_id]\n                for pred_id, lbl_id in zip(pred, label)\n                if lbl_id != -100\n            ]\n            for pred, label in zip(raw_predictions, labels)\n        ]\n        labels = [\n            [\n                (\n                    id2label[int(lbl_id)]\n                    if isinstance(lbl_id, int) or isinstance(lbl_id, np.int_)\n                    else lbl_id\n                )\n                for lbl_id in label\n                if lbl_id != -100\n            ]\n            for label in labels\n        ]\n\n    else:\n        predictions = model_outputs  # type: ignore[assignment]\n\n    # Replace predicted tag with either MISC or O tags if they are not part of the\n    # dataset\n    labels_without_misc = {\n        label\n        for label in dataset_config.id2label.values()\n        if label not in {\"b-misc\", \"i-misc\"}\n    }\n    ner_tag: str\n    for i, prediction_list in enumerate(predictions):\n        for j, ner_tag in enumerate(prediction_list):\n            if ner_tag not in labels_without_misc:\n                if has_misc_tags and ner_tag[:2] == \"b-\":\n                    predictions[i][j] = \"b-misc\"\n                elif has_misc_tags and ner_tag[:2] == \"i-\":\n                    predictions[i][j] = \"i-misc\"\n                else:\n                    predictions[i][j] = \"o\"\n\n    # Remove MISC labels from predictions\n    predictions_no_misc = deepcopy(predictions)\n    for i, prediction_list in enumerate(predictions_no_misc):\n        for j, ner_tag in enumerate(prediction_list):\n            if ner_tag[-4:] == \"misc\":\n                predictions_no_misc[i][j] = \"o\"\n\n    # Remove MISC labels from labels\n    labels_no_misc: list[list[str]] = deepcopy(labels)  # type: ignore[arg-type]\n    for i, label_list in enumerate(labels_no_misc):\n        for j, ner_tag in enumerate(label_list):\n            if (\n                isinstance(ner_tag, str)\n                and len(ner_tag) &gt;= 4\n                and ner_tag[-4:] == \"misc\"\n            ):\n                labels_no_misc[i][j] = \"o\"\n\n    # Compute the metrics\n    # We manually set the F1 metric to be 100% if both the labels and the models\n    # have no NER tags in them, since this causes an error with the `compute`\n    # method otherwise\n    predictions_all_zero = all(\n        all(ner_tag == \"o\" for ner_tag in prediction_list)\n        for prediction_list in predictions\n    )\n    labels_all_zero = all(\n        all(ner_tag == \"o\" for ner_tag in label_list) for label_list in labels\n    )\n    if predictions_all_zero and labels_all_zero:\n        results = dict(overall_f1=1.0)\n    else:\n        metric = metrics[\"micro_f1\"]\n        assert isinstance(metric, EvaluationModule)\n        results = metric.compute(predictions=predictions, references=labels)\n\n    # Compute the metrics without MISC tags\n    # We manually set the F1 metric to be 100% if both the labels and the models\n    # have no NER tags in them, since this causes an error with the `compute`\n    # method otherwise\n    predictions_no_misc_all_zero = all(\n        all(ner_tag == \"o\" for ner_tag in prediction_list)\n        for prediction_list in predictions_no_misc\n    )\n    labels_no_misc_all_zero = all(\n        all(ner_tag == \"o\" for ner_tag in label_list) for label_list in labels_no_misc\n    )\n    if predictions_no_misc_all_zero and labels_no_misc_all_zero:\n        results_no_misc = dict(overall_f1=1.0)\n    else:\n        metric = metrics[\"micro_f1_no_misc\"]\n        assert isinstance(metric, EvaluationModule)\n        results_no_misc = metric.compute(\n            predictions=predictions_no_misc, references=labels_no_misc\n        )\n\n    # Raise error if the metrics are invalid\n    if results is None or results_no_misc is None:\n        raise InvalidBenchmark(\"The predictions and labels are not of the same length.\")\n\n    return dict(\n        micro_f1_no_misc=results_no_misc[\"overall_f1\"], micro_f1=results[\"overall_f1\"]\n    )\n\n\ndef extract_labels_from_generation(docs\n    input_batch: dict[str, list],\n    model_output: \"GenerativeModelOutput\",\n    dataset_config: \"DatasetConfig\",\n) -&gt; list[t.Any]:\n    \"\"\"Extract the predicted labels from the generated output.\n\n    Args:\n        input_batch:\n            The input batch, where the keys are the feature names and the values\n            are lists with the feature values.\n        model_output:\n            The raw generated output of the model.\n        dataset_config:\n            The configuration of the dataset.\n\n    Returns:\n        The predicted labels.\n    \"\"\"\n    if importlib.util.find_spec(\"demjson3\") is None:\n        raise NeedsExtraInstalled(extra=\"generative\")\n\n    raw_predictions = model_output.sequences\n\n    # Attempt to extract the JSON dictionary from the predictions\n    json_regex = r\"\\{.+?\\}\"\n    json_matches = [\n        re.search(pattern=json_regex, string=raw_prediction, flags=re.DOTALL)\n        or raw_prediction\n        for raw_prediction in raw_predictions\n    ]\n    raw_predictions = [\n        json_match.group() if isinstance(json_match, re.Match) else json_match\n        for json_match in json_matches\n    ]\n\n    tokens = input_batch[\"tokens\"]\n    predicted_labels: list[list[str]] = [[\"o\"] * len(token_ids) for token_ids in tokens]\n    for idx, raw_prediction in enumerate(raw_predictions):\n        try:\n            json_output = demjson3.decode(txt=raw_prediction)\n            if not isinstance(json_output, dict):\n                logger.debug(\n                    \"The model output is not a JSON dictionary, so cannot parse \"\n                    f\"it. Skipping. Here is the output: {raw_prediction}\"\n                )\n                continue\n            elif not all(isinstance(key, str) for key in json_output.keys()):\n                logger.debug(\n                    \"The model output is not a JSON dictionary with string keys, \"\n                    \"so cannot parse it. Skipping. Here is the output: \"\n                    f\"{raw_prediction}\"\n                )\n                continue\n            elif not all(isinstance(value, list) for value in json_output.values()):\n                logger.debug(\n                    \"The model output is not a JSON dictionary with list values, \"\n                    \"so cannot parse it. Skipping. Here is the output: \"\n                    f\"{raw_prediction}\"\n                )\n                continue\n            prediction_dict: dict[str, list[str]] = json_output\n        except demjson3.JSONDecodeError:\n            logger.debug(\n                \"The model output is not valid JSON, so cannot parse it. Skipping. \"\n                f\"Here is the output: {raw_prediction!r}\"\n            )\n            continue\n\n        prompt_label_mapping = dataset_config.prompt_label_mapping\n        for prompt_tag_name, named_entities in prediction_dict.items():\n            try:\n                tag_name = [\n                    tag[2:]\n                    for tag, prompt_tag in prompt_label_mapping.items()\n                    if prompt_tag == prompt_tag_name\n                ][0]\n            except IndexError:\n                logger.debug(\n                    \"The model produced an invalid prompt tag name, \"\n                    f\"{prompt_tag_name}. Skipping.\"\n                )\n                continue\n\n            named_entities = [str(named_entity) for named_entity in named_entities]\n            for named_entity in named_entities:\n                for ne_idx, named_entity_word in enumerate(named_entity.split()):\n                    for token_idx, token in enumerate(tokens[idx]):\n                        if named_entity_word in token:\n                            if ne_idx == 0:\n                                predicted_labels[idx][token_idx] = f\"b-{tag_name}\"\n                            elif (\n                                predicted_labels[idx][token_idx] == \"o\"\n                                and predicted_labels[idx][token_idx - 1][2:] == tag_name\n                            ):\n                                predicted_labels[idx][token_idx] = f\"i-{tag_name}\"\n    return predicted_labels\n\n\ndef tokenize_and_align_labels(docs\n    examples: dict, tokenizer: \"PreTrainedTokenizer\", label2id: dict[str, int]\n) -&gt; \"BatchEncoding\":\n    \"\"\"Tokenise all texts and align the labels with them.\n\n    Args:\n        examples:\n            The examples to be tokenised.\n        tokenizer:\n            A pretrained tokenizer.\n        label2id:\n            A dictionary that converts NER tags to IDs.\n\n    Returns:\n        A dictionary containing the tokenized data as well as labels.\n    \"\"\"\n    # Tokenize the texts. We use the `is_split_into_words` argument here because\n    # the texts in our dataset are lists of words (with a label for each word)\n    tokenized_inputs = tokenizer(\n        examples[\"tokens\"], is_split_into_words=True, truncation=True, padding=True\n    )\n\n    # Extract a mapping between all the tokens and their corresponding word. If the\n    # tokenizer is of a \"fast\" variant then this can be accessed through the\n    # `word_ids` method. Otherwise, we have to extract it manually.\n    all_labels: list[list[int]] = list()\n    labels: list[str]\n    word_ids: list[int | None]\n    for i, labels in enumerate(examples[\"labels\"]):\n        # Try to get the word IDs from the tokenizer\n        try:\n            word_ids = tokenized_inputs.word_ids(batch_index=i)\n\n        # If the tokenizer is not of a \"fast\" variant, we have to extract the word\n        # IDs manually\n        except ValueError:\n            # Get the list of words in the document\n            words: list[str] = examples[\"tokens\"][i]\n\n            # Get the list of token IDs in the document\n            tok_ids: list[int] = tokenized_inputs.input_ids[i]\n\n            # Decode the token IDs\n            tokens = tokenizer.convert_ids_to_tokens(tok_ids)\n            assert isinstance(tokens, list)\n\n            # Remove prefixes from the tokens\n            prefixes_to_remove = [\"\u2581\", \"##\"]\n            for tok_idx, tok in enumerate(tokens):\n                if tok:\n                    for prefix in prefixes_to_remove:\n                        if tok.startswith(prefix):\n                            tokens[tok_idx] = tok[len(prefix) :]\n\n            # Replace UNK tokens with the correct word\n            tokens = handle_unk_tokens(tokenizer=tokenizer, tokens=tokens, words=words)\n\n            # Get list of special tokens. Some tokenizers do not record these\n            # properly, which is why we convert the values to their indices and\n            # then back to strings\n            sp_toks = [\n                tokenizer.convert_ids_to_tokens(tokenizer.convert_tokens_to_ids(sp_tok))\n                for sp_tok in tokenizer.special_tokens_map.values()\n            ]\n\n            # Replace special tokens with `None`\n            tokens_with_none = [None if tok in sp_toks else tok for tok in tokens]\n\n            # Get the alignment between the words and the tokens, on a character\n            # level\n            word_idxs = [\n                word_idx for word_idx, word in enumerate(words) for _ in str(word)\n            ]\n            token_idxs = [\n                tok_idx\n                for tok_idx, tok_or_none in enumerate(tokens_with_none)\n                for _ in str(tok_or_none)\n                if tok_or_none is not None\n            ]\n            alignment = list(zip(word_idxs, token_idxs))\n\n            # Raise error if there are not as many characters in the words as in\n            # the tokens. This can be due to the use of a different prefix.\n            if len(word_idxs) != len(token_idxs):\n                raise InvalidBenchmark(\n                    \"The tokens could not be aligned with the words during manual \"\n                    \"word-token alignment. It seems that the tokenizer is neither \"\n                    \"of the fast variant nor of a SentencePiece/WordPiece variant.\"\n                )\n\n            # Get the aligned word IDs\n            word_ids = list()\n            for tok_idx, tok_or_none in enumerate(tokens_with_none):\n                if tok_or_none is None or tok_or_none == \"\":\n                    word_ids.append(None)\n                else:\n                    word_idx = [\n                        word_idx\n                        for word_idx, token_idx in alignment\n                        if token_idx == tok_idx\n                    ][0]\n                    word_ids.append(word_idx)\n\n        previous_word_idx: int | None = None\n        label_ids: list[int] = list()\n        for word_id in word_ids:\n            # Special tokens have a word id that is None. We set the label to -100\n            # so they are automatically ignored in the loss function\n            if word_id is None:\n                label_ids.append(-100)\n\n            # We set the label for the first token of each word\n            elif word_id != previous_word_idx:\n                label = labels[word_id]\n                try:\n                    label_id = label2id[label.lower()]\n                except KeyError:\n                    msg = f\"The label {label} was not found in the model's config.\"\n                    raise InvalidBenchmark(msg)\n                label_ids.append(label_id)\n\n            # For the other tokens in a word, we set the label to -100\n            else:\n                label_ids.append(-100)\n\n            previous_word_idx = word_id\n\n        all_labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = all_labels\n    return tokenized_inputs\n\n\ndef handle_unk_tokens(docs\n    tokenizer: \"PreTrainedTokenizer\", tokens: list[str], words: list[str]\n) -&gt; list[str]:\n    \"\"\"Replace unknown tokens in the tokens with the corresponding word.\n\n    Args:\n        tokenizer:\n            The tokenizer used to tokenize the words.\n        tokens:\n            The list of tokens.\n        words:\n            The list of words.\n\n    Returns:\n        The list of tokens with unknown tokens replaced by the corresponding word.\n    \"\"\"\n    # Locate the token indices of the unknown tokens\n    token_unk_idxs = [i for i, tok in enumerate(tokens) if tok == tokenizer.unk_token]\n\n    # Locate the word indices of the words which contain an unknown token\n    word_unk_idxs = [\n        i\n        for i, word in enumerate(words)\n        if tokenizer.unk_token\n        in tokenizer.convert_ids_to_tokens(\n            tokenizer.encode(word, add_special_tokens=False)\n        )\n    ]\n\n    # Iterate over the token index and word index pairs\n    for tok_idx, word_idx in zip(token_unk_idxs, word_unk_idxs):\n        # Fetch the word\n        word = words[word_idx]\n\n        # Tokenize the word, which is now a list containing at least one UNK token\n        tokens_with_unk = tokenizer.convert_ids_to_tokens(\n            tokenizer.encode(word, add_special_tokens=False)\n        )\n\n        # Iterate over the tokens in the word\n        for possible_unk_token in tokens_with_unk:\n            # If the token is not an UNK token then we remove the first occurence\n            # of the content of this token from the word. The result of the `word`\n            # variable will be the content of the UNK token.\n            # NOTE: This is a bit hacky and not bulletproof. For instance, if the\n            # word is \"1925-1950\" and the tokenizer splits it into [\"[UNK]\", \"-\",\n            # \"19\", \"50\"], then the result will be 2519 instead of 1925. This\n            # happens almost never, however, so we can live with it.\n            if possible_unk_token != tokenizer.unk_token:\n                word = word.replace(possible_unk_token, \"\", 1)\n\n        # Replace the token with the word\n        tokens[tok_idx] = word\n\n    return tokens\n</code></pre>"},{"location":"api/scandeval/benchmarker/","title":"scandeval.benchmarker","text":"scandeval.benchmarker<p> source module scandeval.benchmarker </p> <p>Class that benchmarks Scandinavian language models.</p> <p> Classes </p> <ul> <li> <p>Benchmarker \u2014 Benchmarking all the Scandinavian language models.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>model_has_been_benchmarked \u2014 Checks whether a model has already been benchmarked on a dataset.</p> </li> <li> <p>adjust_logging_level \u2014 Adjust the logging level based on verbosity.</p> </li> <li> <p>clear_model_cache_fn \u2014 Clear the model cache.</p> </li> <li> <p>prepare_dataset_configs \u2014 Prepare the dataset configuration(s) to be benchmarked.</p> </li> </ul> <p> source class Benchmarker(save_results: bool = True, task: str | list[str] | None = None, dataset: list[str] | str | None = None, language: str | list[str] = 'all', model_language: str | list[str] | None = None, dataset_language: str | list[str] | None = None, framework: Framework | str | None = None, device: Device | None = None, batch_size: int = 32, raise_errors: bool = False, cache_dir: str = '.scandeval_cache', api_key: str | None = None, force: bool = False, verbose: bool = False, trust_remote_code: bool = False, load_in_4bit: bool | None = None, use_flash_attention: bool | None = None, clear_model_cache: bool = False, evaluate_test_split: bool = False, few_shot: bool = True, num_iterations: int = 10, api_base: str | None = None, api_version: str | None = None, debug: bool = False, run_with_cli: bool = False) </p> <p>Benchmarking all the Scandinavian language models.</p> <p>Initialise the benchmarker.</p> <p> Attributes </p> <ul> <li> <p>benchmark_config_default_params \u2014</p> <p>The default parameters for the benchmark configuration.</p> </li> <li> <p>benchmark_config \u2014</p> <p>The benchmark configuration.</p> </li> <li> <p>force \u2014</p> <p>Whether to force evaluations of models, even if they have been benchmarked already.</p> </li> <li> <p>results_path \u2014</p> <p>The path to the results file.</p> </li> <li> <p>benchmark_results :  list[BenchmarkResult] \u2014</p> <p>The benchmark results.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>progress_bar :  bool \u2014</p> <p>Whether progress bars should be shown. Defaults to True.</p> </li> <li> <p>save_results :  bool \u2014</p> <p>Whether to save the benchmark results to 'scandeval_benchmark_results.jsonl'. Defaults to True.</p> </li> <li> <p>task :  str | list[str] | None \u2014</p> <p>The tasks benchmark the model(s) on. Mutually exclusive with <code>dataset</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked.</p> </li> <li> <p>dataset :  list[str] | str | None \u2014</p> <p>The datasets to benchmark on. Mutually exclusive with <code>task</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked.</p> </li> <li> <p>language :  str | list[str] \u2014</p> <p>The language codes of the languages to include, both for models and datasets. Set this to 'all' if all languages should be considered. Defaults to \"all\".</p> </li> <li> <p>model_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for models. If specified then this overrides the <code>language</code> parameter for model languages. Defaults to None.</p> </li> <li> <p>dataset_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for datasets. If specified then this overrides the <code>language</code> parameter for dataset languages. Defaults to None.</p> </li> <li> <p>framework :  Framework | str | None \u2014</p> <p>The model framework to use. Only relevant if <code>model-id</code> refers to a local path. Otherwise, the framework will be set automatically. Defaults to None.</p> </li> <li> <p>device :  Device | None \u2014</p> <p>The device to use for benchmarking. Defaults to None.</p> </li> <li> <p>batch_size :  int \u2014</p> <p>The batch size to use. Defaults to 32.</p> </li> <li> <p>raise_errors :  bool \u2014</p> <p>Whether to raise errors instead of skipping the model evaluation. Defaults to False.</p> </li> <li> <p>cache_dir :  str \u2014</p> <p>Directory to store cached models. Defaults to '.scandeval_cache'.</p> </li> <li> <p>api_key :  str | None \u2014</p> <p>The API key to use for a given inference API.</p> </li> <li> <p>force :  bool \u2014</p> <p>Whether to force evaluations of models, even if they have been benchmarked already. Defaults to False.</p> </li> <li> <p>verbose :  bool \u2014</p> <p>Whether to output additional output. This is automatically set if <code>debug</code> is True. Defaults to False.</p> </li> <li> <p>trust_remote_code :  bool \u2014</p> <p>Whether to trust remote code when loading models. Defaults to False.</p> </li> <li> <p>load_in_4bit :  bool | None \u2014</p> <p>Whether to load models in 4-bit precision. If None then this will be done if CUDA is available and the model is a decoder model. Defaults to None.</p> </li> <li> <p>use_flash_attention :  bool | None \u2014</p> <p>Whether to use Flash Attention. If None then it will be used if it is installed and the model is a decoder model. Defaults to None.</p> </li> <li> <p>clear_model_cache :  bool \u2014</p> <p>Whether to clear the model cache after benchmarking each model. Defaults to False.</p> </li> <li> <p>evaluate_test_split :  bool \u2014</p> <p>Whether to evaluate the test split of the datasets. Defaults to False.</p> </li> <li> <p>few_shot :  bool \u2014</p> <p>Whether to only evaluate the model using few-shot evaluation. Only relevant if the model is generative. Defaults to True.</p> </li> <li> <p>num_iterations :  int \u2014</p> <p>The number of times each model should be evaluated. This is only meant to be used for power users, and scores will not be allowed on the leaderboards if this is changed. Defaults to 10.</p> </li> <li> <p>api_base :  str | None \u2014</p> <p>The base URL for a given inference API. Only relevant if <code>model</code> refers to a model on an inference API. Defaults to None.</p> </li> <li> <p>api_version :  str | None \u2014</p> <p>The version of the API to use. Defaults to None.</p> </li> <li> <p>debug :  bool \u2014</p> <p>Whether to output debug information. Defaults to False.</p> </li> <li> <p>run_with_cli :  bool \u2014</p> <p>Whether the benchmarker is being run from the command-line interface. Defaults to False.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014</p> <p>If both <code>task</code> and <code>dataset</code> are specified.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>benchmark \u2014 Benchmarks models on datasets.</p> </li> </ul> <p> source property Benchmarker.benchmark_results: list[BenchmarkResult] </p> <p>The benchmark results.</p> <p> source method Benchmarker.benchmark(model: list[str] | str, task: str | list[str] | None = None, dataset: list[str] | str | None = None, progress_bar: bool | None = None, save_results: bool | None = None, language: str | list[str] | None = None, model_language: str | list[str] | None = None, dataset_language: str | list[str] | None = None, framework: Framework | str | None = None, device: Device | None = None, batch_size: int | None = None, raise_errors: bool | None = None, cache_dir: str | None = None, api_key: str | None = None, force: bool | None = None, verbose: bool | None = None, trust_remote_code: bool | None = None, load_in_4bit: bool | None = None, use_flash_attention: bool | None = None, clear_model_cache: bool | None = None, evaluate_test_split: bool | None = None, few_shot: bool | None = None, num_iterations: int | None = None) \u2192 list[BenchmarkResult] </p> <p>Benchmarks models on datasets.</p> <p> Parameters </p> <ul> <li> <p>model :  list[str] | str \u2014</p> <p>The full Hugging Face Hub path(s) to the pretrained transformer model. The specific model version to use can be added after the suffix '@': \"model@v1.0.0\". It can be a branch name, a tag name, or a commit id, and defaults to the latest version if not specified.</p> </li> <li> <p>task :  str | list[str] | None \u2014</p> <p>The tasks benchmark the model(s) on. Mutually exclusive with <code>dataset</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked. Defaults to None.</p> </li> <li> <p>dataset :  list[str] | str | None \u2014</p> <p>The datasets to benchmark on. Mutually exclusive with <code>task</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked. Defaults to None.</p> </li> <li> <p>progress_bar :  bool | None \u2014</p> <p>Whether progress bars should be shown. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>save_results :  bool | None \u2014</p> <p>Whether to save the benchmark results to 'scandeval_benchmark_results.jsonl'. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include, both for models and datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this to 'all' if all languages (also non-Scandinavian) should be considered. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>model_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for models. If specified then this overrides the <code>language</code> parameter for model languages. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>dataset_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for datasets. If specified then this overrides the <code>language</code> parameter for dataset languages. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>framework :  Framework | str | None \u2014</p> <p>The model framework to use. Only relevant if <code>model-id</code> refers to a local path. Otherwise, the framework will be set automatically. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>device :  Device | None \u2014</p> <p>The device to use for benchmarking. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>batch_size :  int | None \u2014</p> <p>The batch size to use. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>raise_errors :  bool | None \u2014</p> <p>Whether to raise errors instead of skipping the model evaluation.</p> </li> <li> <p>cache_dir :  str | None \u2014</p> <p>Directory to store cached models. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>api_key :  str | None \u2014</p> <p>The API key to use for a given inference server. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>force :  bool | None \u2014</p> <p>Whether to force evaluations of models, even if they have been benchmarked already. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>verbose :  bool | None \u2014</p> <p>Whether to output additional output. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>trust_remote_code :  bool | None \u2014</p> <p>Whether to trust remote code when loading models. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>load_in_4bit :  bool | None \u2014</p> <p>Whether to load models in 4-bit precision. If None then this will be done if CUDA is available and the model is a decoder model. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>use_flash_attention :  bool | None \u2014</p> <p>Whether to use Flash Attention. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>clear_model_cache :  bool | None \u2014</p> <p>Whether to clear the model cache after benchmarking each model. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>evaluate_test_split :  bool | None \u2014</p> <p>Whether to evaluate the test split of the datasets. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>few_shot :  bool | None \u2014</p> <p>Whether to only evaluate the model using few-shot evaluation. Only relevant if the model is generative. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>num_iterations :  int | None \u2014</p> <p>The number of times each model should be evaluated. This is only meant to be used for power users, and scores will not be allowed on the leaderboards if this is changed. Defaults to the value specified when initialising the benchmarker.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[BenchmarkResult] \u2014 A list of benchmark results.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014</p> <p>If both <code>task</code> and <code>dataset</code> are specified.</p> </li> <li> <p>benchmark_output_or_err</p> </li> </ul> <p> source model_has_been_benchmarked(model_id: str, dataset: str, few_shot: bool, validation_split: bool, benchmark_results: list[BenchmarkResult]) \u2192 bool </p> <p>Checks whether a model has already been benchmarked on a dataset.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>dataset :  str \u2014</p> <p>The dataset.</p> </li> <li> <p>few_shot :  bool \u2014</p> <p>Whether the model was evaluated using few-shot evaluation.</p> </li> <li> <p>validation_split :  bool \u2014</p> <p>Whether the model was evaluated on the validation split.</p> </li> <li> <p>benchmark_results :  list[BenchmarkResult] \u2014</p> <p>The benchmark results.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool \u2014 Whether the model has already been evaluated on the dataset.</p> </li> </ul> <p> source adjust_logging_level(verbose: bool, ignore_testing: bool = False) \u2192 int </p> <p>Adjust the logging level based on verbosity.</p> <p> Parameters </p> <ul> <li> <p>verbose :  bool \u2014</p> <p>Whether to output additional output.</p> </li> <li> <p>ignore_testing :  bool \u2014</p> <p>Whether to ignore the testing flag.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>int \u2014 The logging level that was set.</p> </li> </ul> <p> source clear_model_cache_fn(cache_dir: str) \u2192 None </p> <p>Clear the model cache.</p> <p>Note that this will not remove the stored completions.</p> <p> Parameters </p> <ul> <li> <p>cache_dir :  str \u2014</p> <p>The path to the cache directory.</p> </li> </ul> <p> source prepare_dataset_configs(dataset_names: list[str]) \u2192 list[DatasetConfig] </p> <p>Prepare the dataset configuration(s) to be benchmarked.</p> <p> Parameters </p> <ul> <li> <p>dataset_names :  list[str] \u2014</p> <p>The dataset names to benchmark.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[DatasetConfig] \u2014 The prepared list of model IDs.</p> </li> </ul>"},{"location":"src/scandeval/benchmarker/","title":"scandeval.benchmarker","text":"scandeval.benchmarker<p> docs module scandeval.benchmarker </p> <pre><code>\"\"\"Class that benchmarks Scandinavian language models.\"\"\"\n\nimport json\nimport logging\nimport re\nimport sys\nimport typing as t\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom shutil import rmtree\nfrom time import sleep\n\nfrom torch.distributed import destroy_process_group\n\nfrom .benchmark_config_factory import build_benchmark_config\nfrom .data_loading import load_data\nfrom .data_models import BenchmarkConfigParams, BenchmarkResult\nfrom .dataset_configs import get_all_dataset_configs\nfrom .enums import Device, Framework\nfrom .exceptions import InvalidBenchmark, InvalidModel\nfrom .finetuning import finetune\nfrom .generation import generate\nfrom .model_config import get_model_config\nfrom .model_loading import load_model\nfrom .scores import log_scores\nfrom .speed_benchmark import benchmark_speed\nfrom .tasks import SPEED\nfrom .utils import enforce_reproducibility\n\nif t.TYPE_CHECKING:\n    from .benchmark_modules import BenchmarkModule\n    from .data_models import BenchmarkConfig, DatasetConfig, ModelConfig\n\n\nlogger = logging.getLogger(\"scandeval\")\n\n\nclass Benchmarker:docs\n    \"\"\"Benchmarking all the Scandinavian language models.\n\n    Attributes:\n        benchmark_config_default_params:\n            The default parameters for the benchmark configuration.\n        benchmark_config:\n            The benchmark configuration.\n        force:\n            Whether to force evaluations of models, even if they have been benchmarked\n            already.\n        results_path:\n            The path to the results file.\n        benchmark_results:\n            The benchmark results.\n    \"\"\"\n\n    def __init__(\n        self,\n        progress_bar: bool = True,\n        save_results: bool = True,\n        task: str | list[str] | None = None,\n        dataset: list[str] | str | None = None,\n        language: str | list[str] = \"all\",\n        model_language: str | list[str] | None = None,\n        dataset_language: str | list[str] | None = None,\n        framework: Framework | str | None = None,\n        device: Device | None = None,\n        batch_size: int = 32,\n        raise_errors: bool = False,\n        cache_dir: str = \".scandeval_cache\",\n        api_key: str | None = None,\n        force: bool = False,\n        verbose: bool = False,\n        trust_remote_code: bool = False,\n        load_in_4bit: bool | None = None,\n        use_flash_attention: bool | None = None,\n        clear_model_cache: bool = False,\n        evaluate_test_split: bool = False,\n        few_shot: bool = True,\n        num_iterations: int = 10,\n        api_base: str | None = None,\n        api_version: str | None = None,\n        debug: bool = False,\n        run_with_cli: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialise the benchmarker.\n\n        Args:\n            progress_bar:\n                Whether progress bars should be shown. Defaults to True.\n            save_results:\n                Whether to save the benchmark results to\n                'scandeval_benchmark_results.jsonl'. Defaults to True.\n            task:\n                The tasks benchmark the model(s) on. Mutually exclusive with `dataset`.\n                If both `task` and `dataset` are None then all datasets will be\n                benchmarked.\n            dataset:\n                The datasets to benchmark on. Mutually exclusive with `task`. If both\n                `task` and `dataset` are None then all datasets will be benchmarked.\n            language:\n                The language codes of the languages to include, both for models and\n                datasets. Set this to 'all' if all languages should be considered.\n                Defaults to \"all\".\n            model_language:\n                The language codes of the languages to include for models. If specified\n                then this overrides the `language` parameter for model languages.\n                Defaults to None.\n            dataset_language:\n                The language codes of the languages to include for datasets. If\n                specified then this overrides the `language` parameter for dataset\n                languages. Defaults to None.\n            framework:\n                The model framework to use. Only relevant if `model-id` refers to a\n                local path. Otherwise, the framework will be set automatically.\n                Defaults to None.\n            device:\n                The device to use for benchmarking. Defaults to None.\n            batch_size:\n                The batch size to use. Defaults to 32.\n            raise_errors:\n                Whether to raise errors instead of skipping the model evaluation.\n                Defaults to False.\n            cache_dir:\n                Directory to store cached models. Defaults to '.scandeval_cache'.\n            api_key:\n                The API key to use for a given inference API.\n            force:\n                Whether to force evaluations of models, even if they have been\n                benchmarked already. Defaults to False.\n            verbose:\n                Whether to output additional output. This is automatically set if\n                `debug` is True. Defaults to False.\n            trust_remote_code:\n                Whether to trust remote code when loading models. Defaults to False.\n            load_in_4bit:\n                Whether to load models in 4-bit precision. If None then this will be\n                done if CUDA is available and the model is a decoder model. Defaults to\n                None.\n            use_flash_attention:\n                Whether to use Flash Attention. If None then it will be used if it is\n                installed and the model is a decoder model. Defaults to None.\n            clear_model_cache:\n                Whether to clear the model cache after benchmarking each model.\n                Defaults to False.\n            evaluate_test_split:\n                Whether to evaluate the test split of the datasets. Defaults to False.\n            few_shot:\n                Whether to only evaluate the model using few-shot evaluation. Only\n                relevant if the model is generative. Defaults to True.\n            num_iterations:\n                The number of times each model should be evaluated. This is only meant\n                to be used for power users, and scores will not be allowed on the\n                leaderboards if this is changed. Defaults to 10.\n            api_base:\n                The base URL for a given inference API. Only relevant if `model` refers\n                to a model on an inference API. Defaults to None.\n            api_version:\n                The version of the API to use. Defaults to None.\n            debug:\n                Whether to output debug information. Defaults to False.\n            run_with_cli:\n                Whether the benchmarker is being run from the command-line interface.\n                Defaults to False.\n\n        Raises:\n            ValueError:\n                If both `task` and `dataset` are specified.\n        \"\"\"\n        if task is not None and dataset is not None:\n            raise ValueError(\"Only one of `task` and `dataset` can be specified.\")\n\n        self.benchmark_config_default_params = BenchmarkConfigParams(\n            progress_bar=progress_bar,\n            save_results=save_results,\n            task=task,\n            dataset=dataset,\n            language=language,\n            model_language=model_language,\n            dataset_language=dataset_language,\n            framework=framework,\n            device=device,\n            batch_size=batch_size,\n            raise_errors=raise_errors,\n            cache_dir=cache_dir,\n            api_key=api_key,\n            force=force,\n            verbose=verbose,\n            trust_remote_code=trust_remote_code,\n            load_in_4bit=load_in_4bit,\n            use_flash_attention=use_flash_attention,\n            clear_model_cache=clear_model_cache,\n            evaluate_test_split=evaluate_test_split,\n            few_shot=few_shot,\n            num_iterations=num_iterations,\n            api_base=api_base,\n            api_version=api_version,\n            debug=debug,\n            run_with_cli=run_with_cli,\n        )\n\n        self.benchmark_config = build_benchmark_config(\n            first_time=True, **self.benchmark_config_default_params.model_dump()\n        )\n\n        # Initialise variable storing model lists, so we only have to fetch it once\n        self._model_lists: dict[str, list[str]] | None = None\n\n        self.results_path = Path.cwd() / \"scandeval_benchmark_results.jsonl\"\n        adjust_logging_level(verbose=self.benchmark_config.verbose)\n\n    @property\n    def benchmark_results(self) -&gt; list[BenchmarkResult]:docs\n        \"\"\"The benchmark results.\"\"\"\n        if self.results_path.exists():\n            with self.results_path.open() as f:\n                return [\n                    BenchmarkResult.from_dict(json.loads(line))\n                    for line in f\n                    if line.strip()\n                ]\n        else:\n            return list()\n\n    def benchmark(docs\n        self,\n        model: list[str] | str,\n        task: str | list[str] | None = None,\n        dataset: list[str] | str | None = None,\n        progress_bar: bool | None = None,\n        save_results: bool | None = None,\n        language: str | list[str] | None = None,\n        model_language: str | list[str] | None = None,\n        dataset_language: str | list[str] | None = None,\n        framework: Framework | str | None = None,\n        device: Device | None = None,\n        batch_size: int | None = None,\n        raise_errors: bool | None = None,\n        cache_dir: str | None = None,\n        api_key: str | None = None,\n        force: bool | None = None,\n        verbose: bool | None = None,\n        trust_remote_code: bool | None = None,\n        load_in_4bit: bool | None = None,\n        use_flash_attention: bool | None = None,\n        clear_model_cache: bool | None = None,\n        evaluate_test_split: bool | None = None,\n        few_shot: bool | None = None,\n        num_iterations: int | None = None,\n    ) -&gt; list[BenchmarkResult]:\n        \"\"\"Benchmarks models on datasets.\n\n        Args:\n            model:\n                The full Hugging Face Hub path(s) to the pretrained transformer model.\n                The specific model version to use can be added after the suffix '@':\n                \"model@v1.0.0\". It can be a branch name, a tag name, or a commit id,\n                and defaults to the latest version if not specified.\n            task:\n                The tasks benchmark the model(s) on. Mutually exclusive with `dataset`.\n                If both `task` and `dataset` are None then all datasets will be\n                benchmarked. Defaults to None.\n            dataset:\n                The datasets to benchmark on. Mutually exclusive with `task`. If both\n                `task` and `dataset` are None then all datasets will be benchmarked.\n                Defaults to None.\n            progress_bar:\n                Whether progress bars should be shown. Defaults to the value specified\n                when initialising the benchmarker.\n            save_results:\n                Whether to save the benchmark results to\n                'scandeval_benchmark_results.jsonl'. Defaults to the value specified\n                when initialising the benchmarker.\n            language:\n                The language codes of the languages to include, both for models and\n                datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this\n                to 'all' if all languages (also non-Scandinavian) should be considered.\n                Defaults to the value specified when initialising the benchmarker.\n            model_language:\n                The language codes of the languages to include for models. If specified\n                then this overrides the `language` parameter for model languages.\n                Defaults to the value specified when initialising the benchmarker.\n            dataset_language:\n                The language codes of the languages to include for datasets. If\n                specified then this overrides the `language` parameter for dataset\n                languages. Defaults to the value specified when initialising the\n                benchmarker.\n            framework:\n                The model framework to use. Only relevant if `model-id` refers to a\n                local path. Otherwise, the framework will be set automatically.\n                Defaults to the value specified when initialising the benchmarker.\n            device:\n                The device to use for benchmarking. Defaults to the value specified when\n                initialising the benchmarker.\n            batch_size:\n                The batch size to use. Defaults to the value specified when initialising\n                the benchmarker.\n            raise_errors:\n                Whether to raise errors instead of skipping the model evaluation.\n            cache_dir:\n                Directory to store cached models. Defaults to the value specified when\n                initialising the benchmarker.\n            api_key:\n                The API key to use for a given inference server. Defaults to the value\n                specified when initialising the benchmarker.\n            force:\n                Whether to force evaluations of models, even if they have been\n                benchmarked already. Defaults to the value specified when initialising\n                the benchmarker.\n            verbose:\n                Whether to output additional output. Defaults to the value specified when\n                initialising the benchmarker.\n            trust_remote_code:\n                Whether to trust remote code when loading models. Defaults to the value\n                specified when initialising the benchmarker.\n            load_in_4bit:\n                Whether to load models in 4-bit precision. If None then this will be done\n                if CUDA is available and the model is a decoder model. Defaults to the\n                value specified when initialising the benchmarker.\n            use_flash_attention:\n                Whether to use Flash Attention. Defaults to the value specified when\n                initialising the benchmarker.\n            clear_model_cache:\n                Whether to clear the model cache after benchmarking each model. Defaults\n                to the value specified when initialising the benchmarker.\n            evaluate_test_split:\n                Whether to evaluate the test split of the datasets. Defaults to the\n                value specified when initialising the benchmarker.\n            few_shot:\n                Whether to only evaluate the model using few-shot evaluation. Only\n                relevant if the model is generative. Defaults to the value specified\n                when initialising the benchmarker.\n            num_iterations:\n                The number of times each model should be evaluated. This is only meant\n                to be used for power users, and scores will not be allowed on the\n                leaderboards if this is changed. Defaults to the value specified when\n                initialising the benchmarker.\n\n        Returns:\n            A list of benchmark results.\n\n        Raises:\n            ValueError:\n                If both `task` and `dataset` are specified.\n        \"\"\"\n        if task is not None and dataset is not None:\n            raise ValueError(\"Only one of `task` and `dataset` can be specified.\")\n\n        benchmark_config = self._get_updated_benchmark_config(\n            task=task,\n            dataset=dataset,\n            progress_bar=progress_bar,\n            save_results=save_results,\n            language=language,\n            model_language=model_language,\n            dataset_language=dataset_language,\n            framework=framework,\n            device=device,\n            batch_size=batch_size,\n            raise_errors=raise_errors,\n            cache_dir=cache_dir,\n            api_key=api_key,\n            force=force,\n            verbose=verbose,\n            trust_remote_code=trust_remote_code,\n            load_in_4bit=load_in_4bit,\n            use_flash_attention=use_flash_attention,\n            clear_model_cache=clear_model_cache,\n            evaluate_test_split=evaluate_test_split,\n            few_shot=few_shot,\n            num_iterations=num_iterations,\n        )\n\n        adjust_logging_level(verbose=benchmark_config.verbose)\n\n        if benchmark_config.clear_model_cache:\n            clear_model_cache_fn(cache_dir=benchmark_config.cache_dir)\n\n        model_ids = self._prepare_model_ids(model_id=model)\n        dataset_configs = prepare_dataset_configs(\n            dataset_names=benchmark_config.datasets\n        )\n\n        current_benchmark_results: list[BenchmarkResult] = list()\n        for m_id in model_ids:\n            try:\n                model_config = get_model_config(\n                    model_id=m_id, benchmark_config=benchmark_config\n                )\n            except InvalidModel as e:\n                logger.info(e.message)\n                continue\n\n            for dataset_config in dataset_configs:\n                # Skip if we have already benchmarked this model on this dataset and\n                # we are not forcing the benchmark\n                if not benchmark_config.force and model_has_been_benchmarked(\n                    model_id=m_id,\n                    dataset=dataset_config.name,\n                    few_shot=benchmark_config.few_shot,\n                    validation_split=not benchmark_config.evaluate_test_split,\n                    benchmark_results=self.benchmark_results,\n                ):\n                    logger.debug(\n                        f\"Skipping benchmarking {m_id} on {dataset_config.pretty_name},\"\n                        \" as it has already been benchmarked.\"\n                    )\n                    continue\n\n                # Benchmark a single model on a single dataset\n                benchmark_output_or_err = self._benchmark_single(\n                    model_config=model_config,\n                    dataset_config=dataset_config,\n                    benchmark_config=benchmark_config,\n                )\n\n                if (\n                    isinstance(benchmark_output_or_err, Exception)\n                    and benchmark_config.raise_errors\n                ):\n                    raise benchmark_output_or_err\n\n                elif isinstance(benchmark_output_or_err, InvalidBenchmark):\n                    logger.info(\n                        f\"{m_id} could not be benchmarked on \"\n                        f\"{dataset_config.pretty_name}. Skipping. The error message \"\n                        f\"raised was {benchmark_output_or_err.message!r}.\"\n                    )\n                    continue\n\n                elif isinstance(benchmark_output_or_err, InvalidModel):\n                    logger.info(benchmark_output_or_err.message)\n                    break\n\n                else:\n                    record = benchmark_output_or_err\n                    current_benchmark_results.append(record)\n                    if benchmark_config.save_results:\n                        record.append_to_results(results_path=self.results_path)\n\n            if benchmark_config.clear_model_cache:\n                clear_model_cache_fn(cache_dir=benchmark_config.cache_dir)\n\n        # This avoids the following warning at the end of the benchmarking:\n        #   Warning: WARNING: process group has NOT been destroyed before we destruct\n        #   ProcessGroupNCCL. On normal program exit, the application should call\n        #   destroy_process_group to ensure that any pending NCCL operations have\n        #   finished in this process. In rare cases this process can exit before this\n        #   point and block the progress of another member of the process group. This\n        #   constraint has always been present,  but this warning has only been added\n        #   since PyTorch 2.4 (function operator())\n        try:\n            destroy_process_group()\n        except AssertionError:\n            pass\n\n        return current_benchmark_results\n\n    def _get_updated_benchmark_config(self, **kwargs) -&gt; \"BenchmarkConfig\":\n        \"\"\"Get an updated benchmark configuration.\n\n        Args:\n            **kwargs:\n                The new parameters for the benchmark configuration.\n\n        Returns:\n            The updated benchmark configuration.\n        \"\"\"\n        benchmark_config_params = deepcopy(self.benchmark_config_default_params)\n        for key, value in kwargs.items():\n            if value is not None and hasattr(benchmark_config_params, key):\n                setattr(benchmark_config_params, key, value)\n                if key == \"task\":\n                    benchmark_config_params.dataset = None\n                elif key == \"dataset\":\n                    benchmark_config_params.task = None\n        return build_benchmark_config(**benchmark_config_params.model_dump())\n\n    def _prepare_model_ids(self, model_id: list[str] | str) -&gt; list[str]:\n        \"\"\"Prepare the model ID(s) to be benchmarked.\n\n        Args:\n            model_id:\n                The model ID(s) of the models to benchmark.\n\n        Returns:\n            The prepared list of model IDs.\n        \"\"\"\n        model_ids = [model_id] if isinstance(model_id, str) else model_id\n\n        # Reorder the `model_ids` list to include the ones present in the benchmark\n        # results first\n        benchmarked_model_ids = [\n            re.sub(r\"\\(.+\\)\", \"\", record.model).strip()\n            for record in self.benchmark_results\n        ]\n        model_ids_sorted = [m_id for m_id in model_ids if m_id in benchmarked_model_ids]\n        model_ids_sorted += [\n            m_id for m_id in model_ids if m_id not in benchmarked_model_ids\n        ]\n\n        return [m_id.rstrip(\" /\") for m_id in model_ids_sorted]\n\n    def _benchmark_single(\n        self,\n        model_config: \"ModelConfig\",\n        dataset_config: \"DatasetConfig\",\n        benchmark_config: \"BenchmarkConfig\",\n    ) -&gt; BenchmarkResult | InvalidBenchmark | InvalidModel:\n        \"\"\"Benchmark a single model on a single dataset.\n\n        Args:\n            model_config:\n                The configuration of the model we are evaluating.\n            dataset_config:\n                The configuration of the dataset we are evaluating on.\n            benchmark_config:\n                The general benchmark configuration.\n\n        Returns:\n            The benchmark result, or an error if the benchmark was unsuccessful.\n        \"\"\"\n        # Initial logging\n        logger.info(\n            f\"Benchmarking {model_config.model_id} on {dataset_config.pretty_name}\"\n        )\n        if dataset_config.unofficial:\n            logger.info(\n                f\"Note that the {dataset_config.name!r} dataset is unofficial, \"\n                \"meaning that the resulting evaluation will not be included in the \"\n                \"official leaderboard.\"\n            )\n        if benchmark_config.debug:\n            logger.info(\n                \"Running in debug mode. This will output additional information, as \"\n                \"well as store the model outputs in the current directory after each \"\n                \"batch. For this reason, evaluation will be slower.\"\n            )\n\n        model: BenchmarkModule | None = None\n        while True:\n            try:\n                # Set random seeds to enforce reproducibility of the randomly\n                # initialised weights\n                rng = enforce_reproducibility(framework=model_config.framework)\n\n                if model is None or not model.is_generative:\n                    logger.info(\"Loading model...\")\n                    model = load_model(\n                        model_config=model_config,\n                        dataset_config=dataset_config,\n                        benchmark_config=benchmark_config,\n                    )\n                assert model is not None\n\n                if dataset_config.task == SPEED:\n                    scores = benchmark_speed(\n                        model=model, benchmark_config=self.benchmark_config\n                    )\n\n                else:\n                    bootstrapped_datasets = load_data(\n                        rng=rng,\n                        dataset_config=dataset_config,\n                        benchmark_config=benchmark_config,\n                    )\n                    prepared_datasets = model.prepare_datasets(\n                        datasets=bootstrapped_datasets, task=dataset_config.task\n                    )\n                    if model.is_generative:\n                        scores = generate(\n                            model=model,\n                            datasets=prepared_datasets,\n                            model_config=model_config,\n                            dataset_config=dataset_config,\n                            benchmark_config=self.benchmark_config,\n                        )\n                    else:\n                        scores = finetune(\n                            model=model,\n                            datasets=prepared_datasets,\n                            model_config=model_config,\n                            dataset_config=dataset_config,\n                            benchmark_config=benchmark_config,\n                        )\n\n                results = log_scores(\n                    dataset_name=dataset_config.pretty_name,\n                    metric_configs=dataset_config.task.metrics,\n                    scores=scores,\n                    model_id=model_config.model_id,\n                )\n\n                record = BenchmarkResult(\n                    dataset=dataset_config.name,\n                    task=dataset_config.task.name,\n                    dataset_languages=[\n                        language.code for language in dataset_config.languages\n                    ],\n                    model=model_config.model_id,\n                    results=results,\n                    num_model_parameters=model.num_params,\n                    max_sequence_length=model.model_max_length,\n                    vocabulary_size=model.vocab_size,\n                    generative=model.is_generative,\n                    few_shot=benchmark_config.few_shot,\n                    validation_split=not benchmark_config.evaluate_test_split,\n                )\n                logger.debug(f\"Results:\\n{results}\")\n                return record\n\n            except (InvalidBenchmark, InvalidModel) as e:\n                # If the model ID is not valid then raise an error\n                model_err_msg = \"does not exist on the Hugging Face Hub\"\n                if benchmark_config.raise_errors and model_err_msg in str(e):\n                    raise e\n\n                # Otherwise, if the error is due to Hugging Face Hub being down, then\n                # wait a bit and try again\n                elif \"The Hugging Face Hub seems to be down.\" in str(e):\n                    wait_time = 30\n                    logger.debug(\n                        \"The Hugging Face Hub seems to be down. Retrying in \"\n                        f\"{wait_time} seconds.\"\n                    )\n                    sleep(wait_time)\n                    continue\n\n                # Otherwise, if the error is due to the MPS fallback not being enabled,\n                # then raise an error asking the user to enable it\n                elif \"PYTORCH_ENABLE_MPS_FALLBACK\" in str(e):\n                    raise RuntimeError(\n                        \"The benchmark failed because the environment variable \"\n                        \"`PYTORCH_ENABLE_MPS_FALLBACK` is not set. Please set this \"\n                        \"environment variable to `1` and try again.\"\n                    )\n\n                elif benchmark_config.raise_errors:\n                    raise e\n                return e\n\n    def __call__(self, *args, **kwargs) -&gt; list[BenchmarkResult]:\n        \"\"\"Call the benchmarker. See `Benchmarker.benchmark`.\"\"\"\n        return self.benchmark(*args, **kwargs)\n\n\ndef model_has_been_benchmarked(docs\n    model_id: str,\n    dataset: str,\n    few_shot: bool,\n    validation_split: bool,\n    benchmark_results: list[BenchmarkResult],\n) -&gt; bool:\n    \"\"\"Checks whether a model has already been benchmarked on a dataset.\n\n    Args:\n        model_id:\n            The model ID.\n        dataset:\n            The dataset.\n        few_shot:\n            Whether the model was evaluated using few-shot evaluation.\n        validation_split:\n            Whether the model was evaluated on the validation split.\n        benchmark_results:\n            The benchmark results.\n\n    Returns:\n        Whether the model has already been evaluated on the dataset.\n    \"\"\"\n    for record in benchmark_results:\n        same_evaluation = record.model == model_id and record.dataset == dataset\n        same_validation_split_setting = record.validation_split == validation_split\n        same_few_shot_setting = record.few_shot == few_shot or not record.generative\n        if same_evaluation and same_validation_split_setting and same_few_shot_setting:\n            return True\n    return False\n\ndocs\ndef adjust_logging_level(verbose: bool, ignore_testing: bool = False) -&gt; int:\n    \"\"\"Adjust the logging level based on verbosity.\n\n    Args:\n        verbose:\n            Whether to output additional output.\n        ignore_testing:\n            Whether to ignore the testing flag.\n\n    Returns:\n        The logging level that was set.\n    \"\"\"\n    if hasattr(sys, \"_called_from_test\") and not ignore_testing:\n        logging_level = logging.CRITICAL\n    elif verbose:\n        logging_level = logging.DEBUG\n    else:\n        logging_level = logging.INFO\n    logger.setLevel(logging_level)\n    return logging_level\n\n\ndef clear_model_cache_fn(cache_dir: str) -&gt; None:docs\n    \"\"\"Clear the model cache.\n\n    Note that this will not remove the stored completions.\n\n    Args:\n        cache_dir:\n            The path to the cache directory.\n    \"\"\"\n    model_cache_path = Path(cache_dir) / \"model_cache\"\n    model_cache_path.mkdir(parents=True, exist_ok=True)\n    for model_dir in model_cache_path.iterdir():\n        if model_dir.is_dir():\n            for sub_model_dir in model_dir.iterdir():\n                if sub_model_dir.is_dir():\n                    rmtree(sub_model_dir)\n\ndocs\ndef prepare_dataset_configs(dataset_names: list[str]) -&gt; list[\"DatasetConfig\"]:\n    \"\"\"Prepare the dataset configuration(s) to be benchmarked.\n\n    Args:\n        dataset_names:\n            The dataset names to benchmark.\n\n    Returns:\n        The prepared list of model IDs.\n    \"\"\"\n    return [\n        cfg for cfg in get_all_dataset_configs().values() if cfg.name in dataset_names\n    ]\n</code></pre>"},{"location":"api/scandeval/benchmark_config_factory/","title":"scandeval.benchmark_config_factory","text":"scandeval.benchmark_config_factory<p> source module scandeval.benchmark_config_factory </p> <p>Factory class for creating dataset configurations.</p> <p> Functions </p> <ul> <li> <p>build_benchmark_config \u2014 Create a benchmark configuration.</p> </li> <li> <p>get_correct_language_codes \u2014 Get correct language code(s).</p> </li> <li> <p>prepare_languages \u2014 Prepare language(s) for benchmarking.</p> </li> <li> <p>prepare_tasks_and_datasets \u2014 Prepare task(s) and dataset(s) for benchmarking.</p> </li> <li> <p>prepare_device \u2014 Prepare device for benchmarking.</p> </li> </ul> <p> source build_benchmark_config(progress_bar: bool, save_results: bool, task: str | list[str] | None, dataset: str | list[str] | None, language: str | list[str], model_language: str | list[str] | None, dataset_language: str | list[str] | None, framework: Framework | str | None, device: Device | None, batch_size: int, raise_errors: bool, cache_dir: str, api_key: str | None, force: bool, verbose: bool, trust_remote_code: bool, load_in_4bit: bool | None, use_flash_attention: bool | None, clear_model_cache: bool, evaluate_test_split: bool, few_shot: bool, num_iterations: int, api_base: str | None, api_version: str | None, debug: bool, run_with_cli: bool, first_time: bool = False) \u2192 BenchmarkConfig </p> <p>Create a benchmark configuration.</p> <p> Parameters </p> <ul> <li> <p>progress_bar :  bool \u2014</p> <p>Whether to show a progress bar when running the benchmark.</p> </li> <li> <p>save_results :  bool \u2014</p> <p>Whether to save the benchmark results to a file.</p> </li> <li> <p>task :  str | list[str] | None \u2014</p> <p>The tasks to include for dataset. If None then datasets will not be filtered based on their task.</p> </li> <li> <p>dataset :  str | list[str] | None \u2014</p> <p>The datasets to include for task. If None then all datasets will be included, limited by the <code>task</code> parameter.</p> </li> <li> <p>language :  str | list[str] \u2014</p> <p>The language codes of the languages to include, both for models and datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this to 'all' if all languages (also non-Scandinavian) should be considered.</p> </li> <li> <p>model_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for models. If None then the <code>language</code> parameter will be used.</p> </li> <li> <p>dataset_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for datasets. If None then the <code>language</code> parameter will be used.</p> </li> <li> <p>framework :  Framework | str | None \u2014</p> <p>The framework to use for running the models. If None then the framework will be set automatically.</p> </li> <li> <p>device :  Device | None \u2014</p> <p>The device to use for running the models. If None then the device will be set automatically.</p> </li> <li> <p>batch_size :  int \u2014</p> <p>The batch size to use for running the models.</p> </li> <li> <p>raise_errors :  bool \u2014</p> <p>Whether to raise errors when running the benchmark.</p> </li> <li> <p>cache_dir :  str \u2014</p> <p>The directory to use for caching the models.</p> </li> <li> <p>api_key :  str | None \u2014</p> <p>The API key to use for a given inference server.</p> </li> <li> <p>force :  bool \u2014</p> <p>Whether to force the benchmark to run even if the results are already cached.</p> </li> <li> <p>verbose :  bool \u2014</p> <p>Whether to print verbose output when running the benchmark. This is automatically set if <code>debug</code> is True.</p> </li> <li> <p>trust_remote_code :  bool \u2014</p> <p>Whether to trust remote code when running the benchmark.</p> </li> <li> <p>load_in_4bit :  bool | None \u2014</p> <p>Whether to load the models in 4-bit precision.</p> </li> <li> <p>use_flash_attention :  bool | None \u2014</p> <p>Whether to use Flash Attention for the models. If None then it will be used if it is available.</p> </li> <li> <p>clear_model_cache :  bool \u2014</p> <p>Whether to clear the model cache before running the benchmark.</p> </li> <li> <p>evaluate_test_split :  bool \u2014</p> <p>Whether to use the test split for the datasets.</p> </li> <li> <p>few_shot :  bool \u2014</p> <p>Whether to use few-shot learning for the models.</p> </li> <li> <p>num_iterations :  int \u2014</p> <p>The number of iterations each model should be evaluated for.</p> </li> <li> <p>api_base :  str | None \u2014</p> <p>The base URL for a given inference API. Only relevant if <code>model</code> refers to a model on an inference API.</p> </li> <li> <p>api_version :  str | None \u2014</p> <p>The version of the API to use for a given inference API.</p> </li> <li> <p>debug :  bool \u2014</p> <p>Whether to run the benchmark in debug mode.</p> </li> <li> <p>run_with_cli :  bool \u2014</p> <p>Whether the benchmark is being run with the CLI.</p> </li> <li> <p>first_time :  bool \u2014</p> <p>Whether this is the first time the benchmark configuration is being created. Defaults to False.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> source get_correct_language_codes(language_codes: str | list[str]) \u2192 list[str] </p> <p>Get correct language code(s).</p> <p> Parameters </p> <ul> <li> <p>language_codes :  str | list[str] \u2014</p> <p>The language codes of the languages to include, both for models and datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this to 'all' if all languages (also non-Scandinavian) should be considered.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 The correct language codes.</p> </li> </ul> <p> source prepare_languages(language_codes: str | list[str] | None, default_language_codes: list[str]) \u2192 list[Language] </p> <p>Prepare language(s) for benchmarking.</p> <p> Parameters </p> <ul> <li> <p>language_codes :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for models or datasets. If specified then this overrides the <code>language</code> parameter for model or dataset languages.</p> </li> <li> <p>default_language_codes :  list[str] \u2014</p> <p>The default language codes of the languages to include.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[Language] \u2014 The prepared model or dataset languages.</p> </li> </ul> <p> source prepare_tasks_and_datasets(task: str | list[str] | None, dataset_languages: list[Language], dataset: str | list[str] | None) \u2192 tuple[list[Task], list[str]] </p> <p>Prepare task(s) and dataset(s) for benchmarking.</p> <p> Parameters </p> <ul> <li> <p>task :  str | list[str] | None \u2014</p> <p>The tasks to include for dataset. If None then datasets will not be filtered based on their task.</p> </li> <li> <p>dataset_languages :  list[Language] \u2014</p> <p>The languages of the datasets in the benchmark.</p> </li> <li> <p>dataset :  str | list[str] | None \u2014</p> <p>The datasets to include for task. If None then all datasets will be included, limited by the <code>task</code> and <code>dataset_languages</code> parameters.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[list[Task], list[str]] \u2014 The prepared tasks and datasets.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark \u2014</p> <p>If the task or dataset is not found in the benchmark tasks or datasets.</p> </li> </ul> <p> source prepare_device(device: Device | None) \u2192 torch.device </p> <p>Prepare device for benchmarking.</p> <p> Parameters </p> <ul> <li> <p>device :  Device | None \u2014</p> <p>The device to use for running the models. If None then the device will be set automatically.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>torch.device \u2014 The prepared device.</p> </li> </ul>"},{"location":"src/scandeval/benchmark_config_factory/","title":"scandeval.benchmark_config_factory","text":"scandeval.benchmark_config_factory<p> docs module scandeval.benchmark_config_factory </p> <pre><code>\"\"\"Factory class for creating dataset configurations.\"\"\"\n\nimport importlib.util\nimport logging\nimport sys\nimport typing as t\n\nimport torch\n\nfrom .data_models import BenchmarkConfig\nfrom .dataset_configs import get_all_dataset_configs\nfrom .enums import Device, Framework\nfrom .exceptions import InvalidBenchmark\nfrom .languages import get_all_languages\nfrom .tasks import get_all_tasks\n\nif t.TYPE_CHECKING:\n    from .data_models import Language, Task\n\n\nlogger = logging.getLogger(\"scandeval\")\n\n\ndef build_benchmark_config(docs\n    progress_bar: bool,\n    save_results: bool,\n    task: str | list[str] | None,\n    dataset: str | list[str] | None,\n    language: str | list[str],\n    model_language: str | list[str] | None,\n    dataset_language: str | list[str] | None,\n    framework: Framework | str | None,\n    device: Device | None,\n    batch_size: int,\n    raise_errors: bool,\n    cache_dir: str,\n    api_key: str | None,\n    force: bool,\n    verbose: bool,\n    trust_remote_code: bool,\n    load_in_4bit: bool | None,\n    use_flash_attention: bool | None,\n    clear_model_cache: bool,\n    evaluate_test_split: bool,\n    few_shot: bool,\n    num_iterations: int,\n    api_base: str | None,\n    api_version: str | None,\n    debug: bool,\n    run_with_cli: bool,\n    first_time: bool = False,\n) -&gt; BenchmarkConfig:\n    \"\"\"Create a benchmark configuration.\n\n    Args:\n        progress_bar:\n            Whether to show a progress bar when running the benchmark.\n        save_results:\n            Whether to save the benchmark results to a file.\n        task:\n            The tasks to include for dataset. If None then datasets will not be\n            filtered based on their task.\n        dataset:\n            The datasets to include for task. If None then all datasets will be\n            included, limited by the `task` parameter.\n        language:\n            The language codes of the languages to include, both for models and\n            datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this\n            to 'all' if all languages (also non-Scandinavian) should be considered.\n        model_language:\n            The language codes of the languages to include for models. If None then\n            the `language` parameter will be used.\n        dataset_language:\n            The language codes of the languages to include for datasets. If None then\n            the `language` parameter will be used.\n        framework:\n            The framework to use for running the models. If None then the framework\n            will be set automatically.\n        device:\n            The device to use for running the models. If None then the device will be\n            set automatically.\n        batch_size:\n            The batch size to use for running the models.\n        raise_errors:\n            Whether to raise errors when running the benchmark.\n        cache_dir:\n            The directory to use for caching the models.\n        api_key:\n            The API key to use for a given inference server.\n        force:\n            Whether to force the benchmark to run even if the results are already\n            cached.\n        verbose:\n            Whether to print verbose output when running the benchmark. This is\n            automatically set if `debug` is True.\n        trust_remote_code:\n            Whether to trust remote code when running the benchmark.\n        load_in_4bit:\n            Whether to load the models in 4-bit precision.\n        use_flash_attention:\n            Whether to use Flash Attention for the models. If None then it will be used\n            if it is available.\n        clear_model_cache:\n            Whether to clear the model cache before running the benchmark.\n        evaluate_test_split:\n            Whether to use the test split for the datasets.\n        few_shot:\n            Whether to use few-shot learning for the models.\n        num_iterations:\n            The number of iterations each model should be evaluated for.\n        api_base:\n            The base URL for a given inference API. Only relevant if `model` refers to a\n            model on an inference API.\n        api_version:\n            The version of the API to use for a given inference API.\n        debug:\n            Whether to run the benchmark in debug mode.\n        run_with_cli:\n            Whether the benchmark is being run with the CLI.\n        first_time:\n            Whether this is the first time the benchmark configuration is being created.\n            Defaults to False.\n\n    Returns:\n        The benchmark configuration.\n    \"\"\"\n    language_codes = get_correct_language_codes(language_codes=language)\n    model_languages = prepare_languages(\n        language_codes=model_language, default_language_codes=language_codes\n    )\n    dataset_languages = prepare_languages(\n        language_codes=dataset_language, default_language_codes=language_codes\n    )\n\n    tasks, datasets = prepare_tasks_and_datasets(\n        task=task, dataset=dataset, dataset_languages=dataset_languages\n    )\n\n    torch_device = prepare_device(device=device)\n\n    framework_obj = Framework(framework) if framework is not None else None\n\n    if use_flash_attention is None:\n        if torch_device.type != \"cuda\":\n            use_flash_attention = False\n        elif (\n            importlib.util.find_spec(\"flash_attn\") is None\n            and importlib.util.find_spec(\"vllm_flash_attn\") is None\n        ):\n            use_flash_attention = False\n            if first_time and torch_device.type == \"cuda\":\n                message = (\n                    \"Flash attention has not been installed, so this will not be used. \"\n                    \"To install it, run `pip install -U wheel &amp;&amp; \"\n                    \"FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE pip install flash-attn \"\n                    \"--no-build-isolation`. Alternatively, you can disable this \"\n                    \"message by setting \"\n                )\n                if run_with_cli:\n                    message += \"the flag `--no-use-flash-attention`.\"\n                else:\n                    message += (\n                        \"the argument `use_flash_attention=False` in the `Benchmarker`.\"\n                    )\n                logger.info(message)\n\n    # Set variable with number of iterations\n    if hasattr(sys, \"_called_from_test\"):\n        num_iterations = 1\n\n    return BenchmarkConfig(\n        model_languages=model_languages,\n        dataset_languages=dataset_languages,\n        tasks=tasks,\n        datasets=datasets,\n        batch_size=batch_size,\n        raise_errors=raise_errors,\n        cache_dir=cache_dir,\n        api_key=api_key,\n        force=force,\n        progress_bar=progress_bar,\n        save_results=save_results,\n        verbose=verbose or debug,\n        framework=framework_obj,\n        device=torch_device,\n        trust_remote_code=trust_remote_code,\n        load_in_4bit=load_in_4bit,\n        use_flash_attention=use_flash_attention,\n        clear_model_cache=clear_model_cache,\n        evaluate_test_split=evaluate_test_split,\n        few_shot=few_shot,\n        num_iterations=num_iterations,\n        api_base=api_base,\n        api_version=api_version,\n        debug=debug,\n        run_with_cli=run_with_cli,\n    )\n\ndocs\ndef get_correct_language_codes(language_codes: str | list[str]) -&gt; list[str]:\n    \"\"\"Get correct language code(s).\n\n    Args:\n        language_codes:\n            The language codes of the languages to include, both for models and\n            datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this\n            to 'all' if all languages (also non-Scandinavian) should be considered.\n\n    Returns:\n        The correct language codes.\n    \"\"\"\n    # Create a dictionary that maps languages to their associated language objects\n    language_mapping = get_all_languages()\n\n    # Create the list `languages`\n    if \"all\" in language_codes:\n        languages = list(language_mapping.keys())\n    elif isinstance(language_codes, str):\n        languages = [language_codes]\n    else:\n        languages = language_codes\n\n    # If `languages` contains 'no' then also include 'nb' and 'nn'. Conversely, if\n    # either 'nb' or 'nn' are specified then also include 'no'.\n    if \"no\" in languages:\n        languages = list(set(languages) | {\"nb\", \"nn\"})\n    elif \"nb\" in languages or \"nn\" in languages:\n        languages = list(set(languages) | {\"no\"})\n\n    return languages\n\n\ndef prepare_languages(docs\n    language_codes: str | list[str] | None, default_language_codes: list[str]\n) -&gt; list[\"Language\"]:\n    \"\"\"Prepare language(s) for benchmarking.\n\n    Args:\n        language_codes:\n            The language codes of the languages to include for models or datasets.\n            If specified then this overrides the `language` parameter for model or\n            dataset languages.\n        default_language_codes:\n            The default language codes of the languages to include.\n\n    Returns:\n        The prepared model or dataset languages.\n    \"\"\"\n    # Create a dictionary that maps languages to their associated language objects\n    language_mapping = get_all_languages()\n\n    # Create the list `languages_str` of language codes to use for models or datasets\n    languages_str: list[str]\n    if language_codes is None:\n        languages_str = default_language_codes\n    elif isinstance(language_codes, str):\n        languages_str = [language_codes]\n    else:\n        languages_str = language_codes\n\n    # Convert the model languages to language objects\n    if \"all\" in languages_str:\n        prepared_languages = list(language_mapping.values())\n    else:\n        prepared_languages = [language_mapping[language] for language in languages_str]\n\n    return prepared_languages\n\n\ndef prepare_tasks_and_datasets(docs\n    task: str | list[str] | None,\n    dataset_languages: list[\"Language\"],\n    dataset: str | list[str] | None,\n) -&gt; tuple[list[\"Task\"], list[str]]:\n    \"\"\"Prepare task(s) and dataset(s) for benchmarking.\n\n    Args:\n        task:\n            The tasks to include for dataset. If None then datasets will not be\n            filtered based on their task.\n        dataset_languages:\n            The languages of the datasets in the benchmark.\n        dataset:\n            The datasets to include for task. If None then all datasets will be\n            included, limited by the `task` and `dataset_languages` parameters.\n\n    Returns:\n        The prepared tasks and datasets.\n\n    Raises:\n        InvalidBenchmark:\n            If the task or dataset is not found in the benchmark tasks or datasets.\n    \"\"\"\n    # Create a dictionary that maps benchmark tasks to their associated benchmark\n    # task objects, and a dictionary that maps dataset names to their associated\n    # dataset configuration objects\n    task_mapping = get_all_tasks()\n    all_dataset_configs = get_all_dataset_configs()\n\n    # Create the list of dataset tasks\n    try:\n        if task is None:\n            tasks = list(task_mapping.values())\n        elif isinstance(task, str):\n            tasks = [task_mapping[task]]\n        else:\n            tasks = [task_mapping[t] for t in task]\n    except KeyError as e:\n        raise InvalidBenchmark(f\"Task {e} not found in the benchmark tasks.\") from e\n\n    all_official_datasets = [\n        dataset_name\n        for dataset_name, dataset_config in all_dataset_configs.items()\n        if not dataset_config.unofficial\n    ]\n    if dataset is None:\n        dataset = all_official_datasets\n    elif isinstance(dataset, str):\n        dataset = [dataset]\n\n    all_datasets = list(all_dataset_configs.keys())\n    invalid_datasets = set(dataset) - set(all_datasets)\n    if invalid_datasets:\n        raise InvalidBenchmark(\n            f\"Dataset(s) {', '.join(invalid_datasets)} not found in the benchmark \"\n            \"datasets.\"\n        )\n\n    datasets = [\n        dataset_name\n        for dataset_name, dataset_config in all_dataset_configs.items()\n        if dataset_name in dataset\n        and dataset_config.task in tasks\n        and set(dataset_config.languages).intersection(dataset_languages)\n    ]\n\n    return tasks, datasets\n\n\ndef prepare_device(device: Device | None) -&gt; torch.device:docs\n    \"\"\"Prepare device for benchmarking.\n\n    Args:\n        device:\n            The device to use for running the models. If None then the device will be\n            set automatically.\n\n    Returns:\n        The prepared device.\n    \"\"\"\n    device_mapping = {\n        Device.CPU: torch.device(\"cpu\"),\n        Device.CUDA: torch.device(\"cuda\"),\n        Device.MPS: torch.device(\"mps\"),\n    }\n    if isinstance(device, Device):\n        return device_mapping[device]\n\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    elif torch.backends.mps.is_available():\n        return torch.device(\"mps\")\n    else:\n        return torch.device(\"cpu\")\n</code></pre>"},{"location":"api/scandeval/callbacks/","title":"scandeval.callbacks","text":"scandeval.callbacks<p> source module scandeval.callbacks </p> <p>Callbacks for the Hugging Face Trainer.</p> <p> Classes </p> <ul> <li> <p>NeverLeaveProgressCallback \u2014 Progress callback which never leaves the progress bar.</p> </li> </ul> <p> source class NeverLeaveProgressCallback(**kwargs) </p> <p><p>Bases : ProgressCallback</p></p> <p>Progress callback which never leaves the progress bar.</p> <p>Initialise the callback.</p> <p> Methods </p> <ul> <li> <p>on_train_begin \u2014 Callback actions when training begins.</p> </li> <li> <p>on_step_end \u2014 Callback actions when a training step ends.</p> </li> <li> <p>on_prediction_step \u2014 Callback actions when a prediction step ends.</p> </li> </ul> <p> source method NeverLeaveProgressCallback.on_train_begin(args, state, control, **kwargs) </p> <p>Callback actions when training begins.</p> <p> source method NeverLeaveProgressCallback.on_step_end(args, state, control, **kwargs) </p> <p>Callback actions when a training step ends.</p> <p> source method NeverLeaveProgressCallback.on_prediction_step(args, state, control, eval_dataloader=None, **kwargs) </p> <p>Callback actions when a prediction step ends.</p>"},{"location":"src/scandeval/callbacks/","title":"scandeval.callbacks","text":"scandeval.callbacks<p> docs module scandeval.callbacks </p> <pre><code>\"\"\"Callbacks for the Hugging Face Trainer.\"\"\"\n\nimport sys\nfrom collections.abc import Sized\n\nfrom tqdm.auto import tqdm\nfrom transformers.trainer_callback import ProgressCallback\n\n\nclass NeverLeaveProgressCallback(ProgressCallback):docs\n    \"\"\"Progress callback which never leaves the progress bar.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialise the callback.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.training_bar: tqdm\n        self.prediction_bar: tqdm | None\n\n    def on_train_begin(self, args, state, control, **kwargs):docs\n        \"\"\"Callback actions when training begins.\"\"\"\n        if state.is_local_process_zero:\n            desc = \"Finetuning model\"\n            self.training_bar = tqdm(\n                total=None,\n                leave=False,\n                desc=desc,\n                disable=hasattr(sys, \"_called_from_test\"),\n            )\n        self.current_step = 0\n\n    def on_step_end(self, args, state, control, **kwargs):docs\n        \"\"\"Callback actions when a training step ends.\"\"\"\n        if state.is_local_process_zero:\n            self.training_bar.update(state.global_step - self.current_step)\n            self.current_step = state.global_step\ndocs\n    def on_prediction_step(self, args, state, control, eval_dataloader=None, **kwargs):\n        \"\"\"Callback actions when a prediction step ends.\"\"\"\n        if eval_dataloader is None:\n            return\n        correct_dtype = isinstance(eval_dataloader.dataset, Sized)\n        if state.is_local_process_zero and correct_dtype:\n            if self.prediction_bar is None:\n                desc = \"Evaluating model\"\n                self.prediction_bar = tqdm(\n                    total=len(eval_dataloader),\n                    leave=False,\n                    desc=desc,\n                    disable=hasattr(sys, \"_called_from_test\"),\n                )\n            self.prediction_bar.update(1)\n</code></pre>"},{"location":"api/scandeval/cli/","title":"scandeval.cli","text":"scandeval.cli<p> source module scandeval.cli </p> <p>Command-line interface for benchmarking.</p> <p> Functions </p> <ul> <li> <p>benchmark \u2014 Benchmark pretrained language models on language tasks.</p> </li> </ul> <p> source benchmark(model: tuple[str], dataset: tuple[str], language: tuple[str], model_language: tuple[str], dataset_language: tuple[str], raise_errors: bool, task: tuple[str], batch_size: str, progress_bar: bool, save_results: bool, cache_dir: str, api_key: str | None, force: bool, verbose: bool, framework: str | None, device: str | None, trust_remote_code: bool, load_in_4bit: bool | None, use_flash_attention: bool | None, clear_model_cache: bool, evaluate_test_split: bool, few_shot: bool, num_iterations: int, api_base: str | None, api_version: str | None, debug: bool) \u2192 None </p> <p>Benchmark pretrained language models on language tasks.</p>"},{"location":"src/scandeval/cli/","title":"scandeval.cli","text":"scandeval.cli<p> docs module scandeval.cli </p> <pre><code>\"\"\"Command-line interface for benchmarking.\"\"\"\n\nimport click\n\nfrom .benchmarker import Benchmarker\nfrom .dataset_configs import get_all_dataset_configs\nfrom .enums import Device, Framework\nfrom .languages import get_all_languages\nfrom .tasks import get_all_tasks\n\n\n@click.command()\n@click.option(\n    \"--model\",\n    \"-m\",\n    required=True,\n    multiple=True,\n    help=\"The ID of the model to benchmark.\",\n)\n@click.option(\n    \"--task\",\n    \"-t\",\n    default=None,\n    show_default=True,\n    multiple=True,\n    type=click.Choice(list(get_all_tasks().keys())),\n    help=\"The dataset tasks to benchmark the model(s) on.\",\n)\n@click.option(\n    \"--language\",\n    \"-l\",\n    default=[\"all\"],\n    show_default=True,\n    multiple=True,\n    metavar=\"ISO 639-1 LANGUAGE CODE\",\n    type=click.Choice([\"all\"] + list(get_all_languages().keys())),\n    help=\"\"\"The languages to benchmark, both for models and datasets. If \"all\" then all\n    models will be benchmarked on all datasets.\"\"\",\n)\n@click.option(\n    \"--model-language\",\n    \"-ml\",\n    default=None,\n    show_default=True,\n    multiple=True,\n    metavar=\"ISO 639-1 LANGUAGE CODE\",\n    type=click.Choice([\"all\"] + list(get_all_languages().keys())),\n    help=\"\"\"The model languages to benchmark. If not specified then this will use the\n    `language` value.\"\"\",\n)\n@click.option(\n    \"--dataset-language\",\n    \"-dl\",\n    default=None,\n    show_default=True,\n    multiple=True,\n    metavar=\"ISO 639-1 LANGUAGE CODE\",\n    type=click.Choice([\"all\"] + list(get_all_languages().keys())),\n    help=\"\"\"The dataset languages to benchmark. If \"all\" then the models will be\n    benchmarked on all datasets. If not specified then this will use the `language`\n    value.\"\"\",\n)\n@click.option(\n    \"--dataset\",\n    default=None,\n    show_default=True,\n    multiple=True,\n    type=click.Choice(list(get_all_dataset_configs().keys())),\n    help=\"\"\"The name of the benchmark dataset. We recommend to use the `task` and\n    `language` options instead of this option.\"\"\",\n)\n@click.option(\n    \"--batch-size\",\n    default=\"32\",\n    type=click.Choice([\"1\", \"2\", \"4\", \"8\", \"16\", \"32\"]),\n    help=\"The batch size to use.\",\n)\n@click.option(\n    \"--progress-bar/--no-progress-bar\",\n    default=True,\n    show_default=True,\n    help=\"Whether to show a progress bar.\",\n)\n@click.option(\n    \"--raise-errors/--no-raise-errors\",\n    default=False,\n    show_default=True,\n    help=\"Whether to raise errors instead of skipping the evaluation.\",\n)\n@click.option(\n    \"--verbose/--no-verbose\",\n    \"-v/-nv\",\n    default=False,\n    show_default=True,\n    help=\"Whether extra input should be outputted during benchmarking\",\n)\n@click.option(\n    \"--save-results/--no-save-results\",\n    \"-s/-ns\",\n    default=True,\n    show_default=True,\n    help=\"Whether results should not be stored to disk.\",\n)\n@click.option(\n    \"--cache-dir\",\n    \"-c\",\n    default=\".scandeval_cache\",\n    show_default=True,\n    help=\"The directory where models are datasets are cached.\",\n)\n@click.option(\n    \"--api-key\",\n    type=str,\n    default=None,\n    show_default=True,\n    help=\"\"\"The API key to use for a given inference API. If you are benchmarking an \"\n    \"OpenAI model then this would be the OpenAI API key, if you are benchmarking a \"\n    \"model on the Hugging Face inference API then this would be the Hugging Face API \"\n    \"key, and so on.\"\"\",\n)\n@click.option(\n    \"--force/--no-force\",\n    \"-f\",\n    default=False,\n    show_default=True,\n    help=\"\"\"Whether to force evaluation of models which have already been evaluated,\n    with scores lying in the 'scandeval_benchmark_results.jsonl' file.\"\"\",\n)\n@click.option(\n    \"--framework\",\n    default=None,\n    show_default=True,\n    type=click.Choice([framework.lower() for framework in Framework.__members__]),\n    help=\"\"\"The model framework to use. Only relevant if `model` refers to a local\n    path. Otherwise, the framework will be set automatically.\"\"\",\n)\n@click.option(\n    \"--device\",\n    default=None,\n    show_default=True,\n    type=click.Choice([device.lower() for device in Device.__members__]),\n    help=\"\"\"The device to use for evaluation. If not specified then the device will be\n    set automatically.\"\"\",\n)\n@click.option(\n    \"--trust-remote-code/--no-trust-remote-code\",\n    default=False,\n    show_default=True,\n    help=\"\"\"Whether to trust remote code. Only set this flag if you trust the supplier\n    of the model.\"\"\",\n)\n@click.option(\n    \"--load-in-4bit/--no-load-in-4bit\",\n    default=None,\n    show_default=True,\n    help=\"\"\"Whether to load the model in 4-bit precision. If not specified then the\n    model will be loaded in 4-bit precision if possible.\"\"\",\n)\n@click.option(\n    \"--use-flash-attention/--no-use-flash-attention\",\n    default=None,\n    show_default=True,\n    help=\"\"\"Whether to use Flash Attention. If not specified then the model will use\n    Flash Attention for generative models if a CUDA GPU is available and `flash-attn`\n    or `vllm-flash-attn` are installed.\"\"\",\n)\n@click.option(\n    \"--clear-model-cache/--no-clear-model-cache\",\n    default=False,\n    show_default=True,\n    help=\"\"\"Whether to clear the model cache after benchmarking each model. Note that\n    this will only remove the model files, and not the cached model outputs (which\n    don't take up a lot of disk space). This is useful when benchmarking many models,\n    to avoid running out of disk space.\"\"\",\n)\n@click.option(\n    \"--evaluate-test-split/--evaluate-val-split\",\n    default=False,\n    show_default=True,\n    help=\"\"\"Whether to only evaluate on the test split. Only use this for your final\n    evaluation, as the test split should not be used for development.\"\"\",\n)\n@click.option(\n    \"--few-shot/--zero-shot\",\n    default=True,\n    show_default=True,\n    help=\"Whether to only evaluate the model using few-shot evaluation. Only relevant \"\n    \"if the model is generative.\",\n)\n@click.option(\n    \"--num-iterations\",\n    default=10,\n    show_default=True,\n    help=\"\"\"The number of times each model should be evaluated. This is only meant to\n    be used for power users, and scores will not be allowed on the leaderboards if this\n    is changed.\"\"\",\n)\n@click.option(\n    \"--api-base\",\n    default=None,\n    show_default=True,\n    help=\"The base URL for a given inference API. Only relevant if `model` refers to a \"\n    \"model on an inference API.\",\n)\n@click.option(\n    \"--api-version\",\n    default=None,\n    show_default=True,\n    help=\"The version of the API to use. Only relevant if `model` refers to a model on \"\n    \"an inference API.\",\n)\n@click.option(\n    \"--debug/--no-debug\",\n    default=False,\n    show_default=True,\n    help=\"Whether to run the benchmark in debug mode. This prints out extra information \"\n    \"and stores all outputs to the current working directory. Only relevant if the \"\n    \"model is generative.\",\n)\ndef benchmark(docs\n    model: tuple[str],\n    dataset: tuple[str],\n    language: tuple[str],\n    model_language: tuple[str],\n    dataset_language: tuple[str],\n    raise_errors: bool,\n    task: tuple[str],\n    batch_size: str,\n    progress_bar: bool,\n    save_results: bool,\n    cache_dir: str,\n    api_key: str | None,\n    force: bool,\n    verbose: bool,\n    framework: str | None,\n    device: str | None,\n    trust_remote_code: bool,\n    load_in_4bit: bool | None,\n    use_flash_attention: bool | None,\n    clear_model_cache: bool,\n    evaluate_test_split: bool,\n    few_shot: bool,\n    num_iterations: int,\n    api_base: str | None,\n    api_version: str | None,\n    debug: bool,\n) -&gt; None:\n    \"\"\"Benchmark pretrained language models on language tasks.\"\"\"\n    models = list(model)\n    datasets = None if len(dataset) == 0 else list(dataset)\n    languages: list[str] = list(language)\n    model_languages = None if len(model_language) == 0 else list(model_language)\n    dataset_languages = None if len(dataset_language) == 0 else list(dataset_language)\n    tasks = None if len(task) == 0 else list(task)\n    batch_size_int = int(batch_size)\n    device = Device[device.upper()] if device is not None else None\n\n    benchmarker = Benchmarker(\n        language=languages,\n        model_language=model_languages,\n        dataset_language=dataset_languages,\n        task=tasks,\n        dataset=datasets,\n        batch_size=batch_size_int,\n        progress_bar=progress_bar,\n        save_results=save_results,\n        raise_errors=raise_errors,\n        verbose=verbose,\n        api_key=api_key,\n        force=force,\n        cache_dir=cache_dir,\n        framework=framework,\n        device=device,\n        trust_remote_code=trust_remote_code,\n        load_in_4bit=load_in_4bit,\n        use_flash_attention=use_flash_attention,\n        clear_model_cache=clear_model_cache,\n        evaluate_test_split=evaluate_test_split,\n        few_shot=few_shot,\n        num_iterations=num_iterations,\n        api_base=api_base,\n        api_version=api_version,\n        debug=debug,\n        run_with_cli=True,\n    )\n\n    # Perform the benchmark evaluation\n    benchmarker(model=models)\n\n\nif __name__ == \"__main__\":\n    benchmark()\n</code></pre>"},{"location":"api/scandeval/constants/","title":"scandeval.constants","text":"scandeval.constants<p> source module scandeval.constants </p> <p>Constants used throughout the project.</p>"},{"location":"src/scandeval/constants/","title":"scandeval.constants","text":"scandeval.constants<p> docs module scandeval.constants </p> <pre><code>\"\"\"Constants used throughout the project.\"\"\"\n\n# This is used as input to generative models; it cannot be a special token\nDUMMY_FILL_VALUE = 100\n\n\nGENERATIVE_MODEL_TASKS = [\"text-generation\", \"conversational\", \"text2text-generation\"]\n\n\nGENERATIVE_DATASET_TASKS = [\n    \"knowledge\",\n    \"common-sense-reasoning\",\n    \"multiple-choice-reading-comprehension\",\n]\n\n\nGENERATIVE_DATASET_SUPERTASKS = [\"text-to-text\", \"text-modelling\"]\n\n\nTASKS_USING_JSON = [\"ner\"]\n\n\nSUPERTASKS_USING_LOGPROBS = [\"sequence-classification\"]\n\n\nMETRIC_ATTRIBUTES_TAKING_UP_MEMORY = [\"cached_bertscorer\"]\n\n\nGENERATIVE_TAGS = [\n    \"trl\",\n    \"mistral\",\n    \"text-generation-inference\",\n    \"unsloth\",\n    \"text-generation\",\n    \"llama\",\n]\n\n\nMAX_LOGPROBS = 10\n</code></pre>"},{"location":"api/scandeval/dataset_configs/","title":"scandeval.dataset_configs","text":"scandeval.dataset_configs<p> source module scandeval.dataset_configs </p> <p>All dataset configurations used in ScandEval.</p> <p> Functions </p> <ul> <li> <p>get_all_dataset_configs \u2014 Get a mapping of all the dataset configurations.</p> </li> <li> <p>get_dataset_config \u2014 Get the dataset configuration for a dataset.</p> </li> </ul> <p> source get_all_dataset_configs() \u2192 dict[str, DatasetConfig] </p> <p>Get a mapping of all the dataset configurations.</p> <p> Returns </p> <ul> <li> <p>dict[str, DatasetConfig] \u2014 A mapping between names of datasets and their configurations.</p> </li> </ul> <p> source get_dataset_config(dataset_name: str) \u2192 DatasetConfig </p> <p>Get the dataset configuration for a dataset.</p> <p> Parameters </p> <ul> <li> <p>dataset_name :  str \u2014</p> <p>The name of the dataset.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetConfig \u2014 The dataset configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014</p> <p>If the dataset is not found.</p> </li> </ul>"},{"location":"src/scandeval/dataset_configs/","title":"scandeval.dataset_configs","text":"scandeval.dataset_configs<p> docs module scandeval.dataset_configs </p> <pre><code>\"\"\"All dataset configurations used in ScandEval.\"\"\"\n\nfrom .data_models import DatasetConfig\nfrom .languages import DA, DE, EN, FO, IS, NB, NL, NN, NO, SV, get_all_languages\nfrom .tasks import COMMON_SENSE, KNOW, LA, MCRC, NER, RC, SENT, SPEED, SUMM\n\n\ndef get_all_dataset_configs() -&gt; dict[str, DatasetConfig]:docs\n    \"\"\"Get a mapping of all the dataset configurations.\n\n    Returns:\n        A mapping between names of datasets and their configurations.\n    \"\"\"\n    dataset_configs = [\n        cfg for cfg in globals().values() if isinstance(cfg, DatasetConfig)\n    ]\n    assert len(dataset_configs) == len({cfg.name for cfg in dataset_configs}), (\n        \"There are duplicate dataset configurations. Please ensure that each dataset \"\n        \"has a unique name.\"\n    )\n    return {cfg.name: cfg for cfg in dataset_configs}\n\n\ndef get_dataset_config(dataset_name: str) -&gt; DatasetConfig:docs\n    \"\"\"Get the dataset configuration for a dataset.\n\n    Args:\n        dataset_name:\n            The name of the dataset.\n\n    Returns:\n        The dataset configuration.\n\n    Raises:\n        ValueError:\n            If the dataset is not found.\n    \"\"\"\n    # Get mapping of all dataset configs\n    dataset_configs = get_all_dataset_configs()\n\n    # If there are no matches for the dataset name, raise an error\n    if dataset_name not in dataset_configs:\n        raise ValueError(f\"No dataset config found for dataset {dataset_name}.\")\n\n    # Otherwise, return the dataset configuration\n    return dataset_configs[dataset_name]\n\n\n### SENTIMENT DATASETS ###\n\nSWEREC_CONFIG = DatasetConfig(\n    name=\"swerec\",\n    pretty_name=\"the truncated version of the Swedish sentiment classification \"\n    \"dataset SweReC\",\n    huggingface_id=\"ScandEval/swerec-mini\",\n    task=SENT,\n    languages=[SV],\n    prompt_prefix=\"F\u00f6ljande \u00e4r recensioner och deras sentiment, som kan vara \"\n    \"'positiv', 'neutral' eller 'negativ'.\",\n    prompt_template=\"Recension: {text}\\nSentiment: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positiv\", neutral=\"neutral\", negative=\"negativ\"\n    ),\n    instruction_prompt=\"Recension: {text}\\n\\nKlassificera sentimentet i recensionen. \"\n    \"Svara med 'positiv', 'neutral' eller 'negativ'.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nANGRY_TWEETS_CONFIG = DatasetConfig(\n    name=\"angry-tweets\",\n    pretty_name=\"the truncated version of the Danish sentiment classification \"\n    \"dataset AngryTweets\",\n    huggingface_id=\"ScandEval/angry-tweets-mini\",\n    task=SENT,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er tweets og deres sentiment, som kan v\u00e6re 'positiv', \"\n    \"'neutral' eller 'negativ'.\",\n    prompt_template=\"Tweet: {text}\\nSentiment: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positiv\", neutral=\"neutral\", negative=\"negativ\"\n    ),\n    instruction_prompt=\"Tweet: {text}\\n\\nKlassificer sentimentet i tweetet. Svar kun \"\n    \"med 'positiv', 'neutral' eller 'negativ'.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nNOREC_CONFIG = DatasetConfig(\n    name=\"norec\",\n    pretty_name=\"the truncated version of the Norwegian sentiment classification \"\n    \"dataset NoReC\",\n    huggingface_id=\"ScandEval/norec-mini\",\n    task=SENT,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"F\u00f8lgende er anmeldelser og deres sentiment, som kan v\u00e6re 'positiv', \"\n    \"'n\u00f8ytral' eller 'negativ'.\",\n    prompt_template=\"Anmeldelse: {text}\\nSentiment: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positiv\", neutral=\"n\u00f8ytral\", negative=\"negativ\"\n    ),\n    instruction_prompt=\"Anmeldelse: {text}\\n\\nKlassifiser sentimentet i anmeldelsen. \"\n    \"Svar med 'positiv', 'n\u00f8ytral' eller 'negativ'.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nHOTTER_AND_COLDER_SENTIMENT_CONFIG = DatasetConfig(\n    name=\"hotter-and-colder-sentiment\",\n    pretty_name=\"the sentiment classification part of the Icelandic dataset Hotter \"\n    \"and Colder\",\n    huggingface_id=\"ScandEval/hotter-and-colder-sentiment\",\n    task=SENT,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru yfirfer\u00f0ir \u00e1samt lyndisgildi \u00feeirra, sem getur \"\n    \"veri\u00f0 'j\u00e1kv\u00e6tt', 'hlutlaust' e\u00f0a 'neikv\u00e6tt'.\",\n    prompt_template=\"Yfirfer\u00f0: {text}\\nLyndi: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"j\u00e1kv\u00e6tt\", neutral=\"hlutlaust\", negative=\"neikv\u00e6tt\"\n    ),\n    instruction_prompt=\"Texti: {text}\\n\\nFlokka\u00f0u tilfinninguna \u00ed textanum. \"\n    \"Svara\u00f0u me\u00f0 'j\u00e1kv\u00e6tt', 'hlutlaust' e\u00f0a 'neikv\u00e6tt'.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nSB10K_CONFIG = DatasetConfig(\n    name=\"sb10k\",\n    pretty_name=\"the truncated version of the German sentiment classification \"\n    \"dataset SB10k\",\n    huggingface_id=\"ScandEval/sb10k-mini\",\n    task=SENT,\n    languages=[DE],\n    prompt_prefix=\"Im Folgenden sind Tweets und ihre Stimmung aufgef\u00fchrt, die \"\n    \"'positiv', 'neutral' oder 'negativ' sein kann.\",\n    prompt_template=\"Tweet: {text}\\nStimmungslage: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positiv\", neutral=\"neutral\", negative=\"negativ\"\n    ),\n    instruction_prompt=\"Tweet: {text}\\n\\nKlassifizieren Sie die Stimmung im Tweet. \"\n    \"Antworten Sie mit 'positiv', 'neutral' oder 'negativ'.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nDUTCH_SOCIAL_CONFIG = DatasetConfig(\n    name=\"dutch-social\",\n    pretty_name=\"the truncated version of the Dutch sentiment classification \"\n    \"dataset Dutch Social\",\n    huggingface_id=\"ScandEval/dutch-social-mini\",\n    task=SENT,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan tweets en hun sentiment, dat 'positief', \"\n    \"'neutraal' of 'negatief' kan zijn.\",\n    prompt_template=\"Tweet: {text}\\nSentiment: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positief\", neutral=\"neutraal\", negative=\"negatief\"\n    ),\n    instruction_prompt=\"Tweet: {text}\\n\\nClassificeer het sentiment in de tweet. \"\n    \"Antwoord met 'positief', 'neutraal' of 'negatief'.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nSST5_CONFIG = DatasetConfig(\n    name=\"sst5\",\n    pretty_name=\"the truncated version of the English sentiment classification \"\n    \"dataset SST5\",\n    huggingface_id=\"ScandEval/sst5-mini\",\n    task=SENT,\n    languages=[EN],\n    prompt_prefix=\"The following are texts and their sentiment, which can be \"\n    \"'positive', 'neutral' or 'negative'.\",\n    prompt_template=\"Text: {text}\\nSentiment: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positive\", neutral=\"neutral\", negative=\"negative\"\n    ),\n    instruction_prompt=\"Text: {text}\\n\\nClassify the sentiment in the text. Answer \"\n    \"with 'positive', 'neutral' or 'negative'.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nFOSENT_CONFIG = DatasetConfig(\n    name=\"fosent\",\n    pretty_name=\"the Faroese sentiment classification dataset FoSent\",\n    huggingface_id=\"ScandEval/fosent\",\n    task=SENT,\n    languages=[FO],\n    prompt_prefix=\"Her eru nakrir tekstir flokka\u00f0ir eftir lyndi, sum kann vera \"\n    \"'positivt', 'neutralt' ella 'negativt'.\",\n    prompt_template=\"Text: {text}\\nLyndi: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positivt\", neutral=\"neutralt\", negative=\"negativt\"\n    ),\n    instruction_prompt=\"Tekstur: {text}\\n\\nFlokka lyndi\u00f0 \u00ed tekstinum. Svara vi\u00f0 \"\n    \"'positivt', 'neutralt' ella 'negativt'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\n\n### NAMED ENTITY RECOGNITION DATASETS ###\n\nSUC3_CONFIG = DatasetConfig(\n    name=\"suc3\",\n    pretty_name=\"the truncated version of the Swedish named entity recognition \"\n    \"dataset SUC 3.0\",\n    huggingface_id=\"ScandEval/suc3-mini\",\n    task=NER,\n    languages=[SV],\n    prompt_prefix=\"F\u00f6ljande \u00e4r meningar och JSON-ordb\u00f6cker med de namngivna enheter \"\n    \"som f\u00f6rekommer i den givna meningen.\",\n    prompt_template=\"Mening: {text}\\nNamngivna entiteter: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"plats\",\n        \"i-loc\": \"plats\",\n        \"b-org\": \"organisation\",\n        \"i-org\": \"organisation\",\n        \"b-misc\": \"diverse\",\n        \"i-misc\": \"diverse\",\n    },\n    instruction_prompt=\"Mening: {text}\\n\\nIdentifiera de namngivna enheterna i \"\n    \"meningen. Du ska outputta detta som en JSON-ordbok med nycklarna 'person', \"\n    \"'plats', 'organisation' och 'diverse'. V\u00e4rdena ska vara listor \u00f6ver de namngivna \"\n    \"enheter av den typen, precis som de f\u00f6rekommer i meningen.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nDANSK_CONFIG = DatasetConfig(\n    name=\"dansk\",\n    pretty_name=\"the truncated version of the Danish named entity recognition \"\n    \"dataset DANSK\",\n    huggingface_id=\"ScandEval/dansk-mini\",\n    task=NER,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er s\u00e6tninger og JSON-ordb\u00f8ger med de navngivne enheder, \"\n    \"som forekommer i den givne s\u00e6tning.\",\n    prompt_template=\"S\u00e6tning: {text}\\nNavngivne enheder: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"sted\",\n        \"i-loc\": \"sted\",\n        \"b-org\": \"organisation\",\n        \"i-org\": \"organisation\",\n        \"b-misc\": \"diverse\",\n        \"i-misc\": \"diverse\",\n    },\n    instruction_prompt=\"S\u00e6tning: {text}\\n\\nIdentific\u00e9r de navngivne enheder i \"\n    \"s\u00e6tningen. Du skal outputte dette som en JSON-ordbog med n\u00f8glerne 'person', \"\n    \"'sted', 'organisation' og 'diverse'. V\u00e6rdierne skal v\u00e6re lister over de navngivne \"\n    \"enheder af den type, pr\u00e6cis som de forekommer i s\u00e6tningen.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nNORNE_NB_CONFIG = DatasetConfig(\n    name=\"norne-nb\",\n    pretty_name=\"the truncated version of the Bokm\u00e5l part of the Norwegian named \"\n    \"entity recognition dataset NorNE\",\n    huggingface_id=\"ScandEval/norne-nb-mini\",\n    task=NER,\n    languages=[NB, NO],\n    prompt_prefix=\"F\u00f8lgende er fraser og JSON-ordb\u00f8ker med de navngitte enhetene \"\n    \"som forekommer i den gitte frasen.\",\n    prompt_template=\"Frase: {text}\\nNavngitte enheter: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"sted\",\n        \"i-loc\": \"sted\",\n        \"b-org\": \"organisasjon\",\n        \"i-org\": \"organisasjon\",\n        \"b-misc\": \"diverse\",\n        \"i-misc\": \"diverse\",\n    },\n    instruction_prompt=\"Frase: {text}\\n\\nIdentifiser de navngitte enhetene i frasen. \"\n    \"Du b\u00f8r outputte dette som en JSON-ordbok med n\u00f8klene 'person', 'sted', \"\n    \"'organisasjon' og 'diverse'. Verdiene skal v\u00e6re lister over de navngitte enhetene \"\n    \"av den typen, akkurat som de vises i frasen.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nNORNE_NN_CONFIG = DatasetConfig(\n    name=\"norne-nn\",\n    pretty_name=\"the truncated version of the Nynorsk part of the Norwegian named \"\n    \"entity recognition dataset NorNE\",\n    huggingface_id=\"ScandEval/norne-nn-mini\",\n    task=NER,\n    languages=[NN],\n    prompt_prefix=\"F\u00f8lgende er fraser og JSON-ordb\u00f8ker med de navngitte enhetene \"\n    \"som forekommer i den gitte frasen.\",\n    prompt_template=\"Frase: {text}\\nNavngitte enheter: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"sted\",\n        \"i-loc\": \"sted\",\n        \"b-org\": \"organisasjon\",\n        \"i-org\": \"organisasjon\",\n        \"b-misc\": \"diverse\",\n        \"i-misc\": \"diverse\",\n    },\n    instruction_prompt=\"Frase: {text}\\n\\nIdentifiser de navngitte enhetene i frasen. \"\n    \"Du b\u00f8r outputte dette som en JSON-ordbok med n\u00f8klene 'person', 'sted', \"\n    \"'organisasjon' og 'diverse'. Verdiene skal v\u00e6re lister over de navngitte enhetene \"\n    \"av den typen, akkurat som de vises i frasen.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nMIM_GOLD_NER_CONFIG = DatasetConfig(\n    name=\"mim-gold-ner\",\n    pretty_name=\"the truncated version of the Icelandic named entity recognition \"\n    \"dataset MIM-GOLD-NER\",\n    huggingface_id=\"ScandEval/mim-gold-ner-mini\",\n    task=NER,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru setningar \u00e1samt JSON lyklum me\u00f0 nefndum einingum \"\n    \"sem koma fyrir \u00ed setningunum.\",\n    prompt_template=\"Setning: {text}\\nNefndar einingar: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"einstaklingur\",\n        \"i-per\": \"einstaklingur\",\n        \"b-loc\": \"sta\u00f0setning\",\n        \"i-loc\": \"sta\u00f0setning\",\n        \"b-org\": \"stofnun\",\n        \"i-org\": \"stofnun\",\n        \"b-misc\": \"\u00fdmislegt\",\n        \"i-misc\": \"\u00fdmislegt\",\n    },\n    instruction_prompt=\"Setning: {text}\\n\\nGreini\u00f0 nefndu einingarnar \u00ed setningunni. \"\n    \"\u00de\u00fa \u00e6ttir a\u00f0 skila \u00feessu sem JSON or\u00f0ab\u00f3k me\u00f0 lyklunum 'einstaklingur', \"\n    \"'sta\u00f0setning', 'stofnun' og '\u00fdmislegt'. Gildin \u00e6ttu a\u00f0 vera listi yfir nefndu \"\n    \"einingarnar af \u00feeirri ger\u00f0, n\u00e1kv\u00e6mlega eins og \u00fe\u00e6r koma fram \u00ed setningunni.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nFONE_CONFIG = DatasetConfig(\n    name=\"fone\",\n    pretty_name=\"the truncated version of the Faroese named entity recognition \"\n    \"dataset FoNE\",\n    huggingface_id=\"ScandEval/fone-mini\",\n    task=NER,\n    languages=[FO],\n    prompt_prefix=\"Her eru nakrir setningar og nakrar JSON or\u00f0ab\u00f8kur vi\u00f0 nevndar \"\n    \"eindir, sum eru \u00ed setningunum.\",\n    prompt_template=\"Setningur: {text}\\nNevndar eindir: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"pers\u00f3nur\",\n        \"i-per\": \"pers\u00f3nur\",\n        \"b-loc\": \"sta\u00f0ur\",\n        \"i-loc\": \"sta\u00f0ur\",\n        \"b-org\": \"felagsskapur\",\n        \"i-org\": \"felagsskapur\",\n        \"b-misc\": \"ymiskt\",\n        \"i-misc\": \"ymiskt\",\n    },\n    instruction_prompt=\"Setningur: {text}\\n\\nGreini\u00f0 nevndu einingarnar \u00ed setningunni. \"\n    \"\u00de\u00fa \u00e6ttir a\u00f0 skila \u00feessu sem JSON or\u00f0ab\u00f3k me\u00f0 lyklunum 'pers\u00f3nur', 'sta\u00f0ur', \"\n    \"'felagsskapur' og 'ymiskt'. Gildin \u00e6ttu a\u00f0 vera listi yfir nevndu einingarnar af \"\n    \"\u00feeirri ger\u00f0, n\u00e1kv\u00e6mlega eins og \u00fe\u00e6r koma fram \u00ed setningunni.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nGERMEVAL_CONFIG = DatasetConfig(\n    name=\"germeval\",\n    pretty_name=\"the truncated version of the German named entity recognition \"\n    \"dataset GermEval\",\n    huggingface_id=\"ScandEval/germeval-mini\",\n    task=NER,\n    languages=[DE],\n    prompt_prefix=\"Es folgen S\u00e4tze und JSON-W\u00f6rterb\u00fccher mit den benannten \"\n    \"Entit\u00e4ten, die in der angegebenen Phrase vorkommen.\",\n    prompt_template=\"Satz: {text}\\nBenannte Entit\u00e4ten: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"ort\",\n        \"i-loc\": \"ort\",\n        \"b-org\": \"organisation\",\n        \"i-org\": \"organisation\",\n        \"b-misc\": \"verschiedenes\",\n        \"i-misc\": \"verschiedenes\",\n    },\n    instruction_prompt=\"Satz: {text}\\n\\nIdentifizieren Sie die benannten Entit\u00e4ten im \"\n    \"Satz. Sie sollten dies als JSON-W\u00f6rterbuch mit den Schl\u00fcsseln 'person', 'ort', \"\n    \"'organisation' und 'verschiedenes' ausgeben. Die Werte sollten Listen der \"\n    \"benannten Entit\u00e4ten dieses Typs sein, genau wie sie im Satz erscheinen.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nCONLL_NL_CONFIG = DatasetConfig(\n    name=\"conll-nl\",\n    pretty_name=\"the Dutch part of the truncated version of the named entity \"\n    \"recognition dataset CoNLL 2002\",\n    huggingface_id=\"ScandEval/conll-nl-mini\",\n    task=NER,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan zinnen en JSON woordenboeken met de genoemde \"\n    \"entiteiten die voorkomen in de gegeven zin.\",\n    prompt_template=\"Zin: {text}\\nGenoemde entiteiten: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"persoon\",\n        \"i-per\": \"persoon\",\n        \"b-loc\": \"locatie\",\n        \"i-loc\": \"locatie\",\n        \"b-org\": \"organisatie\",\n        \"i-org\": \"organisatie\",\n        \"b-misc\": \"diversen\",\n        \"i-misc\": \"diversen\",\n    },\n    instruction_prompt=\"Zin: {text}\\n\\nIdentificeer de genoemde entiteiten in de zin. \"\n    \"Je moet dit uitvoeren als een JSON-woordenboek met de sleutels 'persoon', \"\n    \"'locatie', 'organisatie' en 'diversen'. De waarden moeten lijsten zijn van de \"\n    \"genoemde entiteiten van dat type, precies zoals ze voorkomen in de zin.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nCONLL_EN_CONFIG = DatasetConfig(\n    name=\"conll-en\",\n    pretty_name=\"the truncated version of the English named entity recognition \"\n    \"dataset CoNLL 2003\",\n    huggingface_id=\"ScandEval/conll-en-mini\",\n    task=NER,\n    languages=[EN],\n    prompt_prefix=\"Below are sentences and JSON dictionaries with the named \"\n    \"entities that occur in the given sentence.\",\n    prompt_template=\"Sentence: {text}\\nNamed entities: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"location\",\n        \"i-loc\": \"location\",\n        \"b-org\": \"organization\",\n        \"i-org\": \"organization\",\n        \"b-misc\": \"miscellaneous\",\n        \"i-misc\": \"miscellaneous\",\n    },\n    instruction_prompt=\"Sentence: {text}\\n\\nIdentify the named entities in the \"\n    \"sentence. You should output this as a JSON dictionary with the keys being \"\n    \"'person', 'location', 'organization' and 'miscellaneous'. The values should be \"\n    \"lists of the named entities of that type, exactly as they appear in the sentence.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nDANE_CONFIG = DatasetConfig(\n    name=\"dane\",\n    pretty_name=\"the truncated version of the Danish named entity recognition \"\n    \"dataset DaNE\",\n    huggingface_id=\"ScandEval/dane-mini\",\n    task=NER,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er s\u00e6tninger og JSON-ordb\u00f8ger med de navngivne enheder, \"\n    \"som forekommer i den givne s\u00e6tning.\",\n    prompt_template=\"S\u00e6tning: {text}\\nNavngivne enheder: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"sted\",\n        \"i-loc\": \"sted\",\n        \"b-org\": \"organisation\",\n        \"i-org\": \"organisation\",\n        \"b-misc\": \"diverse\",\n        \"i-misc\": \"diverse\",\n    },\n    instruction_prompt=\"S\u00e6tning: {text}\\n\\nIdentific\u00e9r de navngivne enheder i \"\n    \"s\u00e6tningen. Du skal outputte dette som en JSON-ordbog med n\u00f8glerne 'person', \"\n    \"'sted', 'organisation' og 'diverse'. V\u00e6rdierne skal v\u00e6re lister over de navngivne \"\n    \"enheder af den type, pr\u00e6cis som de forekommer i s\u00e6tningen.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n    unofficial=True,\n)\n\nWIKIANN_FO_CONFIG = DatasetConfig(\n    name=\"wikiann-fo\",\n    pretty_name=\"the truncated version of the Faroese part of the named entity \"\n    \"recognition dataset WikiANN\",\n    huggingface_id=\"ScandEval/wikiann-fo-mini\",\n    task=NER,\n    languages=[FO],\n    prompt_prefix=\"Her eru nakrir setningar og nakrar JSON or\u00f0ab\u00f8kur vi\u00f0 nevndar \"\n    \"eindir, sum eru \u00ed setningunum.\",\n    prompt_template=\"Setningur: {text}\\nNevndar eindir: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"pers\u00f3nur\",\n        \"i-per\": \"pers\u00f3nur\",\n        \"b-loc\": \"sta\u00f0ur\",\n        \"i-loc\": \"sta\u00f0ur\",\n        \"b-org\": \"felagsskapur\",\n        \"i-org\": \"felagsskapur\",\n        \"b-misc\": \"ymiskt\",\n        \"i-misc\": \"ymiskt\",\n    },\n    instruction_prompt=\"Setningur: {text}\\n\\nGreini\u00f0 nevndu einingarnar \u00ed setningunni. \"\n    \"\u00de\u00fa \u00e6ttir a\u00f0 skila \u00feessu sem JSON or\u00f0ab\u00f3k me\u00f0 lyklunum 'pers\u00f3nur', 'sta\u00f0ur', \"\n    \"'felagsskapur' og 'ymiskt'. Gildin \u00e6ttu a\u00f0 vera listi yfir nevndu einingarnar af \"\n    \"\u00feeirri ger\u00f0, n\u00e1kv\u00e6mlega eins og \u00fe\u00e6r koma fram \u00ed setningunni.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n    unofficial=True,\n)\n\n\n### LINGUISTIC ACCEPTABILITY DATASETS ###\n\nSCALA_SV_CONFIG = DatasetConfig(\n    name=\"scala-sv\",\n    pretty_name=\"The Swedish part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-sv\",\n    task=LA,\n    languages=[SV],\n    prompt_prefix=\"F\u00f6ljande \u00e4r meningar och huruvida de \u00e4r grammatiskt korrekta.\",\n    prompt_template=\"Mening: {text}\\nGrammatisk korrekt: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nej\"),\n    instruction_prompt=\"Mening: {text}\\n\\nBest\u00e4m om meningen \u00e4r grammatiskt korrekt \"\n    \"eller inte. Svara med 'ja' om meningen \u00e4r korrekt och 'nej' om den inte \u00e4r.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nSCALA_DA_CONFIG = DatasetConfig(\n    name=\"scala-da\",\n    pretty_name=\"the Danish part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-da\",\n    task=LA,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er s\u00e6tninger og om de er grammatisk korrekte.\",\n    prompt_template=\"S\u00e6tning: {text}\\nGrammatisk korrekt: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nej\"),\n    instruction_prompt=\"S\u00e6tning: {text}\\n\\nBestem om s\u00e6tningen er grammatisk korrekt \"\n    \"eller ej. Svar med 'ja', hvis s\u00e6tningen er korrekt, og 'nej', hvis den ikke er.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nSCALA_NB_CONFIG = DatasetConfig(\n    name=\"scala-nb\",\n    pretty_name=\"the Bokm\u00e5l part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-nb\",\n    task=LA,\n    languages=[NB, NO],\n    prompt_prefix=\"F\u00f8lgende er setninger og hvorvidt de er grammatisk korrekte.\",\n    prompt_template=\"Setning: {text}\\nGrammatisk korrekt: {label}\",\n    instruction_prompt=\"Setning: {text}\\n\\nBestem om setningen er grammatisk korrekt \"\n    \"eller ikke. Svar med 'ja' hvis setningen er korrekt og 'nei' hvis den ikke er.\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nei\"),\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nSCALA_NN_CONFIG = DatasetConfig(\n    name=\"scala-nn\",\n    pretty_name=\"the Nynorsk part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-nn\",\n    task=LA,\n    languages=[NN],\n    prompt_prefix=\"F\u00f8lgende er setninger og hvorvidt de er grammatisk korrekte.\",\n    prompt_template=\"Setning: {text}\\nGrammatisk korrekt: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nei\"),\n    instruction_prompt=\"Setning: {text}\\n\\nBestem om setningen er grammatisk korrekt \"\n    \"eller ikke. Svar med 'ja' hvis setningen er korrekt og 'nei' hvis den ikke er.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nSCALA_IS_CONFIG = DatasetConfig(\n    name=\"scala-is\",\n    pretty_name=\"the Icelandic part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-is\",\n    task=LA,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru setningar og hvort \u00fe\u00e6r eru m\u00e1lfr\u00e6\u00f0ilega r\u00e9ttar.\",\n    prompt_template=\"Setning: {text}\\nM\u00e1lfr\u00e6\u00f0ilega r\u00e9tt: {label}\",\n    prompt_label_mapping=dict(correct=\"j\u00e1\", incorrect=\"nei\"),\n    instruction_prompt=\"Setning: {text}\\n\\nGreini\u00f0 hvort setningin er m\u00e1lfr\u00e6\u00f0ilega \"\n    \"r\u00e9tt e\u00f0a ekki. Svari\u00f0 skal vera 'j\u00e1' ef setningin er r\u00e9tt og 'nei' ef h\u00fan er \"\n    \"ekki.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nSCALA_FO_CONFIG = DatasetConfig(\n    name=\"scala-fo\",\n    pretty_name=\"the Faroese part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-fo\",\n    task=LA,\n    languages=[FO],\n    prompt_prefix=\"Hetta eru nakrir setningar og um teir eru m\u00e1ll\u00e6ruliga r\u00e6ttir.\",\n    prompt_template=\"Setningur: {text}\\nM\u00e1ll\u00e6ruliga r\u00e6ttur: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nei\"),\n    instruction_prompt=\"Setningur: {text}\\n\\nGreini\u00f0 hvort setningurin er m\u00e1ll\u00e6ruliga \"\n    \"r\u00e6ttur ella ikki. Svari\u00f0 skal vera 'ja' um setningurin er r\u00e6ttur og 'nei' um \"\n    \"hann ikki er.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nSCALA_DE_CONFIG = DatasetConfig(\n    name=\"scala-de\",\n    pretty_name=\"the German part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-de\",\n    task=LA,\n    languages=[DE],\n    prompt_prefix=\"Die folgenden S\u00e4tze und ob sie grammatikalisch korrekt sind.\",\n    prompt_template=\"Satz: {text}\\nGrammatikalisch richtig: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nein\"),\n    instruction_prompt=\"Satz: {text}\\n\\nBestimmen Sie, ob der Satz grammatikalisch \"\n    \"korrekt ist oder nicht. Antworten Sie mit 'ja', wenn der Satz korrekt ist und \"\n    \"'nein', wenn er es nicht ist.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nSCALA_NL_CONFIG = DatasetConfig(\n    name=\"scala-nl\",\n    pretty_name=\"the Dutch part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-nl\",\n    task=LA,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan zinnen en of ze grammaticaal correct zijn.\",\n    prompt_template=\"Zin: {text}\\nGrammaticaal correct: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nee\"),\n    instruction_prompt=\"Zin: {text}\\n\\nBepaal of de zin grammaticaal correct is of \"\n    \"niet. Antwoord met 'ja' als de zin correct is en 'nee' als dat niet het geval is.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nSCALA_EN_CONFIG = DatasetConfig(\n    name=\"scala-en\",\n    pretty_name=\"the English part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-en\",\n    task=LA,\n    languages=[EN],\n    prompt_prefix=\"The following are sentences and whether they are grammatically \"\n    \"correct.\",\n    prompt_template=\"Sentence: {text}\\nGrammatically correct: {label}\",\n    prompt_label_mapping=dict(correct=\"yes\", incorrect=\"no\"),\n    instruction_prompt=\"Sentence: {text}\\n\\nDetermine whether the sentence is \"\n    \"grammatically correct or not. Reply with 'yes' if the sentence is correct and \"\n    \"'no' if it is not.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nDUTCH_COLA_CONFIG = DatasetConfig(\n    name=\"dutch-cola\",\n    pretty_name=\"the truncated version of the Dutch linguistic acceptability dataset \"\n    \"Dutch CoLA\",\n    huggingface_id=\"ScandEval/dutch-cola\",\n    task=LA,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan zinnen en of ze grammaticaal correct ('ja') of \"\n    \"incorrect ('nee') zijn.\",\n    prompt_template=\"Zin: {text}\\nGrammaticaal correct: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nee\"),\n    instruction_prompt=\"Zin: {text}\\n\\nBepaal of de zin grammaticaal correct is of \"\n    \"niet. Antwoord met 'ja' als de zin correct is en 'nee' als dat niet het geval is.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=3,\n    unofficial=True,\n)\n\nDUTCH_COLA_FULL_CONFIG = DatasetConfig(\n    name=\"dutch-cola-full\",\n    pretty_name=\"the Dutch linguistic acceptability dataset Dutch CoLA\",\n    huggingface_id=\"ScandEval/dutch-cola-full\",\n    task=LA,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan zinnen en of ze grammaticaal correct ('ja') of \"\n    \"incorrect ('nee') zijn.\",\n    prompt_template=\"Zin: {text}\\nGrammaticaal correct: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nee\"),\n    instruction_prompt=\"Zin: {text}\\n\\nBepaal of de zin grammaticaal correct is of \"\n    \"niet. Antwoord met 'ja' als de zin correct is en 'nee' als dat niet het geval is.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=3,\n    unofficial=True,\n)\n\nICE_EC_CONFIG = DatasetConfig(\n    name=\"ice-ec\",\n    pretty_name=\"the truncated version of the Icelandic Error Corpus\",\n    huggingface_id=\"ScandEval/ice-ec\",\n    task=LA,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru setningar og hvort \u00fe\u00e6r eru m\u00e1lfr\u00e6\u00f0ilega r\u00e9ttar.\",\n    prompt_template=\"Setning: {text}\\nM\u00e1lfr\u00e6\u00f0ilega r\u00e9tt: {label}\",\n    prompt_label_mapping=dict(correct=\"j\u00e1\", incorrect=\"nei\"),\n    instruction_prompt=\"Setning: {text}\\n\\nGreini\u00f0 hvort setningin er m\u00e1lfr\u00e6\u00f0ilega \"\n    \"r\u00e9tt e\u00f0a ekki. Svari\u00f0 skal vera 'j\u00e1' ef setningin er r\u00e9tt og 'nei' ef h\u00fan er \"\n    \"ekki.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nICE_EC_FULL_CONFIG = DatasetConfig(\n    name=\"ice-ec-full\",\n    pretty_name=\"the Icelandic Error Corpus\",\n    huggingface_id=\"ScandEval/ice-ec-full\",\n    task=LA,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru setningar og hvort \u00fe\u00e6r eru m\u00e1lfr\u00e6\u00f0ilega r\u00e9ttar.\",\n    prompt_template=\"Setning: {text}\\nM\u00e1lfr\u00e6\u00f0ilega r\u00e9tt: {label}\",\n    prompt_label_mapping=dict(correct=\"j\u00e1\", incorrect=\"nei\"),\n    instruction_prompt=\"Setning: {text}\\n\\nGreini\u00f0 hvort setningin er m\u00e1lfr\u00e6\u00f0ilega \"\n    \"r\u00e9tt e\u00f0a ekki. Svari\u00f0 skal vera 'j\u00e1' ef setningin er r\u00e9tt og 'nei' ef h\u00fan er ekki.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nICE_LINGUISTIC_CONFIG = DatasetConfig(\n    name=\"ice-linguistic\",\n    pretty_name=\"the Icelandic linguistic acceptability dataset IceLinguistic\",\n    huggingface_id=\"ScandEval/ice-linguistic\",\n    task=LA,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru setningar og hvort \u00fe\u00e6r eru m\u00e1lfr\u00e6\u00f0ilega r\u00e9ttar.\",\n    prompt_template=\"Setning: {text}\\nM\u00e1lfr\u00e6\u00f0ilega r\u00e9tt: {label}\",\n    prompt_label_mapping=dict(correct=\"j\u00e1\", incorrect=\"nei\"),\n    instruction_prompt=\"Setning: {text}\\n\\nGreini\u00f0 hvort setningin er m\u00e1lfr\u00e6\u00f0ilega \"\n    \"r\u00e9tt e\u00f0a ekki. Svari\u00f0 skal vera 'j\u00e1' ef setningin er r\u00e9tt og 'nei' ef h\u00fan er ekki.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\n\n### READING COMPREHENSION DATASETS ###\n\nSCANDIQA_DA_CONFIG = DatasetConfig(\n    name=\"scandiqa-da\",\n    pretty_name=\"the Danish part of the truncated version of the question answering \"\n    \"dataset ScandiQA\",\n    huggingface_id=\"ScandEval/scandiqa-da-mini\",\n    task=RC,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er tekster med tilh\u00f8rende sp\u00f8rgsm\u00e5l og svar.\",\n    prompt_template=\"Tekst: {text}\\nSp\u00f8rgsm\u00e5l: {question}\\nSvar med maks. 3 ord: \"\n    \"{label}\",\n    instruction_prompt=\"Tekst: {text}\\n\\nBesvar f\u00f8lgende sp\u00f8rgsm\u00e5l om teksten ovenfor \"\n    \"med maks. 3 ord.\\n\\nSp\u00f8rgsm\u00e5l: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nNORQUAD_CONFIG = DatasetConfig(\n    name=\"norquad\",\n    pretty_name=\"the truncated version of the Norwegian question answering \"\n    \"dataset NorQuAD\",\n    huggingface_id=\"ScandEval/norquad-mini\",\n    task=RC,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"Her f\u00f8lger tekster med tilh\u00f8rende sp\u00f8rsm\u00e5l og svar.\",\n    prompt_template=\"Tekst: {text}\\nSp\u00f8rsm\u00e5l: {question}\\nSvar p\u00e5 maks 3 ord: {label}\",\n    instruction_prompt=\"Tekst: {text}\\n\\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l om teksten ovenfor \"\n    \"med maks 3 ord.\\n\\nSp\u00f8rsm\u00e5l: {question}\",\n    num_few_shot_examples=2,\n    max_generated_tokens=32,\n)\n\nNORGLM_MULTI_QA = DatasetConfig(\n    name=\"norglm-multi-qa\",\n    pretty_name=\"the question answering part of the Norwegian NorGLM multi-task human annotated dataset \"\n    \"NO-Multi-QA-Sum\",\n    huggingface_id=\"ScandEval/norglm-multi-qa\",\n    task=RC,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"Her f\u00f8lger tekster med tilh\u00f8rende sp\u00f8rsm\u00e5l og svar.\",\n    prompt_template=\"Tekst: {text}\\nSp\u00f8rsm\u00e5l: {question}\\nSvar p\u00e5 maks 3 ord: {label}\",\n    instruction_prompt=\"Tekst: {text}\\n\\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l om teksten ovenfor \"\n    \"med maks 3 ord.\\n\\nSp\u00f8rsm\u00e5l: {question}\",\n    num_few_shot_examples=2,\n    max_generated_tokens=32,\n    unofficial=True,\n)\n\nSCANDIQA_SV_CONFIG = DatasetConfig(\n    name=\"scandiqa-sv\",\n    pretty_name=\"the Swedish part of the truncated version of the question answering \"\n    \"dataset ScandiQA\",\n    huggingface_id=\"ScandEval/scandiqa-sv-mini\",\n    task=RC,\n    languages=[SV],\n    prompt_prefix=\"Nedan f\u00f6ljer texter med tillh\u00f6rande fr\u00e5gor och svar.\",\n    prompt_template=\"Text: {text}\\nFr\u00e5ga: {question}\\nSvar p\u00e5 max 3 ord: {label}\",\n    instruction_prompt=\"Text: {text}\\n\\nBesvara f\u00f6ljande fr\u00e5ga om texten ovan med \"\n    \"h\u00f6gst 3 ord.\\n\\nFr\u00e5ga: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nNQII_CONFIG = DatasetConfig(\n    name=\"nqii\",\n    pretty_name=\"the truncated version of the Icelandic question answering dataset \"\n    \"Natural Questions in Icelandic\",\n    huggingface_id=\"ScandEval/nqii-mini\",\n    task=RC,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru textar me\u00f0 tilheyrandi spurningum og sv\u00f6rum.\",\n    prompt_template=\"Texti: {text}\\nSpurning: {question}\\nSvara\u00f0u me\u00f0 a\u00f0 h\u00e1marki 3 \"\n    \"or\u00f0um: {label}\",\n    instruction_prompt=\"Texti: {text}\\n\\nSvara\u00f0u eftirfarandi spurningu um textann a\u00f0 \"\n    \"h\u00e1marki \u00ed 3 or\u00f0um.\\n\\nSpurning: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nFOQA_CONFIG = DatasetConfig(\n    name=\"foqa\",\n    pretty_name=\"the Faroese question answering dataset FoQA\",\n    huggingface_id=\"ScandEval/foqa\",\n    task=RC,\n    languages=[FO],\n    prompt_prefix=\"Hetta eru tekstir saman vi\u00f0 spurningum og svar.\",\n    prompt_template=\"{text}\\nSpurningur: {question}\\nSvara vi\u00f0 \u00ed mesta lagi trimum \"\n    \"or\u00f0um: {label}\",\n    instruction_prompt=\"Tekstur: {text}\\n\\nSvara hesum spurninginum um tekstin \"\n    \"uppiyvir vi\u00f0 \u00ed mesta lagi trimum or\u00f0um.\\n\\nSpurningur: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nGERMANQUAD_CONFIG = DatasetConfig(\n    name=\"germanquad\",\n    pretty_name=\"the truncated version of the German question answering dataset \"\n    \"GermanQuAD\",\n    huggingface_id=\"ScandEval/germanquad-mini\",\n    task=RC,\n    languages=[DE],\n    prompt_prefix=\"Im Folgenden finden Sie Texte mit den dazugeh\u00f6rigen Fragen und \"\n    \"Antworten.\",\n    prompt_template=\"Text: {text}\\nFragen: {question}\\nFragen Antwort in maximal 3 \"\n    \"W\u00f6rtern: {label}\",\n    instruction_prompt=\"Text: {text}\\n\\nBeantworten Sie die folgende Frage zum obigen \"\n    \"Text in h\u00f6chstens 3 W\u00f6rtern.\\n\\nFrage: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nSQUAD_CONFIG = DatasetConfig(\n    name=\"squad\",\n    pretty_name=\"the truncated version of the English question answering \"\n    \"dataset SQuAD\",\n    huggingface_id=\"ScandEval/squad-mini\",\n    task=RC,\n    languages=[EN],\n    prompt_prefix=\"The following are texts with accompanying questions and answers.\",\n    prompt_template=\"Text: {text}\\nQuestion: {question}\\nAnswer in max 3 words: \"\n    \"{label}\",\n    instruction_prompt=\"Text: {text}\\n\\nAnswer the following question about the \"\n    \"above text in at most 3 words.\\n\\nQuestion: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nSQUAD_NL_CONFIG = DatasetConfig(\n    name=\"squad-nl\",\n    pretty_name=\"the truncated version of the Dutch question answering dataset \"\n    \"SQuAD-nl, translated from the English SQuAD dataset\",\n    huggingface_id=\"ScandEval/squad-nl-mini\",\n    task=RC,\n    languages=[NL],\n    prompt_prefix=\"Hieronder volgen teksten met bijbehorende vragen en antwoorden.\",\n    prompt_template=\"Tekst: {text}\\nVraag: {question}\\nAntwoord in max 3 woorden: \"\n    \"{label}\",\n    instruction_prompt=\"Tekst: {text}\\n\\nBeantwoord de volgende vraag over de \"\n    \"bovenstaande tekst in maximaal 3 woorden.\\n\\nVraag: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nICELANDIC_QA_CONFIG = DatasetConfig(\n    name=\"icelandic-qa\",\n    pretty_name=\"the Icelandic question answering dataset about Icelandic culture and \"\n    \"history\",\n    huggingface_id=\"ScandEval/icelandic-qa\",\n    task=RC,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru textar me\u00f0 tilheyrandi spurningum og sv\u00f6rum.\",\n    prompt_template=\"Texti: {text}\\nSpurning: {question}\\nSvara\u00f0u me\u00f0 a\u00f0 h\u00e1marki 3 \"\n    \"or\u00f0um: {label}\",\n    instruction_prompt=\"Texti: {text}\\n\\nSvara\u00f0u eftirfarandi spurningu um textann a\u00f0 \"\n    \"h\u00e1marki \u00ed 3 or\u00f0um.\\n\\nSpurning: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n    unofficial=True,\n)\n\n### SUMMARIZATION DATASETS ###\n\nNORDJYLLAND_NEWS_CONFIG = DatasetConfig(\n    name=\"nordjylland-news\",\n    pretty_name=\"the truncated version of the Danish summarisation dataset \"\n    \"Nordjylland News\",\n    huggingface_id=\"ScandEval/nordjylland-news-mini\",\n    task=SUMM,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er nyhedsartikler med tilh\u00f8rende resum\u00e9er.\",\n    prompt_template=\"Nyhedsartikel: {text}\\nResum\u00e9: {target_text}\",\n    instruction_prompt=\"Nyhedsartikel: {text}\\n\\nSkriv et resum\u00e9 af ovenst\u00e5ende \"\n    \"artikel.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nMLSUM_CONFIG = DatasetConfig(\n    name=\"mlsum\",\n    pretty_name=\"the truncated version of the German summarisation dataset MLSum\",\n    huggingface_id=\"ScandEval/mlsum-mini\",\n    task=SUMM,\n    languages=[DE],\n    prompt_prefix=\"Im Folgenden finden Sie Nachrichtenartikel mit den dazugeh\u00f6rigen \"\n    \"Zusammenfassungen.\",\n    prompt_template=\"Nachrichtenartikel: {text}\\nZusammenfassung: {target_text}\",\n    instruction_prompt=\"Nachrichtenartikel: {text}\\n\\nSchreiben Sie eine \"\n    \"Zusammenfassung des obigen Artikels.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nRRN_CONFIG = DatasetConfig(\n    name=\"rrn\",\n    pretty_name=\"the truncated version of the Icelandic summarisation dataset \"\n    \"R\u00daV Radio News\",\n    huggingface_id=\"ScandEval/rrn-mini\",\n    task=SUMM,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru fr\u00e9ttagreinar me\u00f0 tilheyrandi samantektum.\",\n    prompt_template=\"Fr\u00e9ttagrein: {text}\\nSamantekt: {target_text}\",\n    instruction_prompt=\"Fr\u00e9ttagrein: {text}\\n\\nSkrifa\u00f0u samantekt um ofangreindu \"\n    \"grein.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nICESUM_CONFIG = DatasetConfig(\n    name=\"icesum\",\n    pretty_name=\"the Icelandic summarisation dataset IceSum\",\n    huggingface_id=\"ScandEval/icesum\",\n    task=SUMM,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru fr\u00e9ttagreinar me\u00f0 tilheyrandi samantektum.\",\n    prompt_template=\"Fr\u00e9ttagrein: {text}\\nSamantekt: {target_text}\",\n    instruction_prompt=\"Fr\u00e9ttagrein: {text}\\n\\nSkrifa\u00f0u samantekt um ofangreindu \"\n    \"grein.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n    unofficial=True,\n)\n\nNO_SAMMENDRAG_CONFIG = DatasetConfig(\n    name=\"no-sammendrag\",\n    pretty_name=\"the truncated version of the Norwegian summarisation dataset \"\n    \"Norske Sammendrag\",\n    huggingface_id=\"ScandEval/no-sammendrag-mini\",\n    task=SUMM,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"Her f\u00f8lger nyhetsartikler med tilh\u00f8rende sammendrag.\",\n    prompt_template=\"Nyhetsartikkel: {text}\\nSammendrag: {target_text}\",\n    instruction_prompt=\"Nyhetsartikkel: {text}\\n\\nSkriv et sammendrag av den \"\n    \"ovennevnte artikkelen.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nNORGLM_MULTI_SUM = DatasetConfig(\n    name=\"norglm-multi-sum\",\n    pretty_name=\"the summarisation part of the Norwegian NorGLM multi-task human \"\n    \"annotated dataset NO-Multi-QA-Sum\",\n    huggingface_id=\"ScandEval/norglm-multi-sum\",\n    task=SUMM,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"Her f\u00f8lger nyhetsartikler med tilh\u00f8rende sammendrag.\",\n    prompt_template=\"Nyhetsartikkel: {text}\\nSammendrag: {target_text}\",\n    instruction_prompt=\"Nyhetsartikkel: {text}\\n\\nSkriv et sammendrag av den \"\n    \"ovennevnte artikkelen.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n    unofficial=True,\n)\n\nWIKI_LINGUA_NL_CONFIG = DatasetConfig(\n    name=\"wiki-lingua-nl\",\n    pretty_name=\"the Dutch part of the truncated version of the summarisation dataset \"\n    \"WikiLingua\",\n    huggingface_id=\"ScandEval/wiki-lingua-nl-mini\",\n    task=SUMM,\n    languages=[NL],\n    prompt_prefix=\"Hieronder volgen artikelen met bijbehorende samenvattingen.\",\n    prompt_template=\"Artikel: {text}\\nSamenvatting: {target_text}\",\n    instruction_prompt=\"Artikel: {text}\\n\\nSchrijf een samenvatting van het \"\n    \"bovenstaande artikel.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nSWEDN_CONFIG = DatasetConfig(\n    name=\"swedn\",\n    pretty_name=\"the truncated version of the Swedish summarisation dataset SweDN\",\n    huggingface_id=\"ScandEval/swedn-mini\",\n    task=SUMM,\n    languages=[SV],\n    prompt_prefix=\"Nedan f\u00f6ljer artiklar med tillh\u00f6rande sammanfattningar.\",\n    prompt_template=\"Artikel: {text}\\nSammanfattning: {target_text}\",\n    instruction_prompt=\"Artikel: {text}\\n\\nSkriv en sammanfattning av artikeln ovan.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nCNN_DAILYMAIL_CONFIG = DatasetConfig(\n    name=\"cnn-dailymail\",\n    pretty_name=\"the truncated version of the English summarisation dataset \"\n    \"CNN-DailyMail\",\n    huggingface_id=\"ScandEval/cnn-dailymail-mini\",\n    task=SUMM,\n    languages=[EN],\n    prompt_prefix=\"The following are articles with accompanying summaries.\",\n    prompt_template=\"News article: {text}\\nSummary: {target_text}\",\n    instruction_prompt=\"News article: {text}\\n\\nWrite a summary of the above \"\n    \"article.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nSCHIBSTED_SV = DatasetConfig(\n    name=\"schibsted-sv\",\n    pretty_name=\"article summaries from Schibsted Media Swedish newsrooms.\",\n    huggingface_id=\"ScandEval/schibsted-article-summaries-sv\",\n    task=SUMM,\n    languages=[SV],\n    prompt_prefix=\"Nedan f\u00f6ljer artiklar med tillh\u00f6rande sammanfattningar.\",\n    prompt_template=\"Artikel: {text}\\nSammanfattning: {target_text}\",\n    instruction_prompt=\"Artikel: {text}\\n\\nSkriv en sammanfattning av artikeln ovan.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n    unofficial=True,\n)\n\nSCHIBSTED_NO = DatasetConfig(\n    name=\"schibsted-no\",\n    pretty_name=\"article summaries from Schibsted Medias Norwegian newsrooms.\",\n    huggingface_id=\"ScandEval/schibsted-article-summaries-no\",\n    task=SUMM,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"Her f\u00f8lger nyhetsartikler med tilh\u00f8rende sammendrag.\",\n    prompt_template=\"Nyhetsartikkel: {text}\\nSammendrag: {target_text}\",\n    instruction_prompt=\"Nyhetsartikkel: {text}\\n\\nSkriv et sammendrag av den \"\n    \"ovennevnte artikkelen.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n    unofficial=True,\n)\n\n# TODO: Faroese summarization\n\n\n### KNOWLEDGE DATASETS ###\n\nDANSKE_TALEMAADER_CONFIG = DatasetConfig(\n    name=\"danske-talemaader\",\n    pretty_name=\"the truncated version of the Danish knowledge dataset Danske \"\n    \"Talem\u00e5der\",\n    huggingface_id=\"ScandEval/danske-talemaader-mini\",\n    task=KNOW,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\",\n    prompt_template=\"Hvad er betydningen af f\u00f8lgende talem\u00e5de: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rgsm\u00e5l: {text}\\n\\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at \"\n    \"svare med 'a', 'b', 'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nDANISH_CITIZEN_TESTS_CONFIG = DatasetConfig(\n    name=\"danish-citizen-tests\",\n    pretty_name=\"the Danish knowledge dataset Danish Citizen Tests\",\n    huggingface_id=\"ScandEval/danish-citizen-tests\",\n    task=KNOW,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rgsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rgsm\u00e5l: {text}\\n\\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at \"\n    \"svare med 'a', 'b', 'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nMMLU_NO_CONFIG = DatasetConfig(\n    name=\"mmlu-no\",\n    pretty_name=\"the truncated version of the Norwegian knowledge dataset MMLU-no, \"\n    \"translated from the English MMLU dataset\",\n    huggingface_id=\"ScandEval/mmlu-no-mini\",\n    task=KNOW,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"F\u00f8lgende er flervalgssp\u00f8rsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rsm\u00e5l: {text}\\n\\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', \"\n    \"'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nMMLU_SV_CONFIG = DatasetConfig(\n    name=\"mmlu-sv\",\n    pretty_name=\"the truncated version of the Swedish knowledge dataset MMLU-sv, \"\n    \"translated from the English MMLU dataset\",\n    huggingface_id=\"ScandEval/mmlu-sv-mini\",\n    task=KNOW,\n    languages=[SV],\n    prompt_prefix=\"F\u00f6ljande \u00e4r flervalsfr\u00e5gor (med svar).\",\n    prompt_template=\"Fr\u00e5ga: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Fr\u00e5ga: {text}\\n\\nBesvara f\u00f6ljande fr\u00e5ga med 'a', 'b', 'c' \"\n    \"eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nMMLU_IS_CONFIG = DatasetConfig(\n    name=\"mmlu-is\",\n    pretty_name=\"the truncated version of the Icelandic knowledge dataset MMLU-is, \"\n    \"translated from the English MMLU dataset\",\n    huggingface_id=\"ScandEval/mmlu-is-mini\",\n    task=KNOW,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\",\n    prompt_template=\"Spurningar: {text}\\nSvara: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Spurningar: {text}\\n\\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', \"\n    \"'b', 'c' e\u00f0a 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nMMLU_DE_CONFIG = DatasetConfig(\n    name=\"mmlu-de\",\n    pretty_name=\"the truncated version of the German knowledge dataset MMLU-de, \"\n    \"translated from the English MMLU dataset\",\n    huggingface_id=\"ScandEval/mmlu-de-mini\",\n    task=KNOW,\n    languages=[DE],\n    prompt_prefix=\"Die folgenden Fragen sind Multiple-Choice-Fragen (mit Antworten).\",\n    prompt_template=\"Frage: {text}\\nAntwort: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Frage: {text}\\n\\nBeantworten Sie die obige Frage mit 'a', 'b', \"\n    \"'c' oder 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nMMLU_NL_CONFIG = DatasetConfig(\n    name=\"mmlu-nl\",\n    pretty_name=\"the truncated version of the Dutch knowledge dataset MMLU-nl, \"\n    \"translated from the English MMLU dataset\",\n    huggingface_id=\"ScandEval/mmlu-nl-mini\",\n    task=KNOW,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan meerkeuzevragen (met antwoorden).\",\n    prompt_template=\"Vraag: {text}\\nAntwoord: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Vraag: {text}\\n\\nBeantwoord de bovenstaande vraag met 'a', 'b', \"\n    \"'c' of 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nMMLU_CONFIG = DatasetConfig(\n    name=\"mmlu\",\n    pretty_name=\"the truncated version of the English knowledge dataset MMLU\",\n    huggingface_id=\"ScandEval/mmlu-mini\",\n    task=KNOW,\n    languages=[EN],\n    prompt_prefix=\"The following are multiple choice questions (with answers).\",\n    prompt_template=\"Question: {text}\\nAnswer: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Question: {text}\\n\\nAnswer the above question by \"\n    \"replying with 'a', 'b', 'c' or 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nMMLU_DA_CONFIG = DatasetConfig(\n    name=\"mmlu-da\",\n    pretty_name=\"the truncated version of the Danish knowledge dataset MMLU-da, \"\n    \"translated from the English MMLU dataset\",\n    huggingface_id=\"ScandEval/mmlu-da-mini\",\n    task=KNOW,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rgsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rgsm\u00e5l: {text}\\n\\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at \"\n    \"svare med 'a', 'b', 'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nARC_DA_CONFIG = DatasetConfig(\n    name=\"arc-da\",\n    pretty_name=\"the truncated version of the Danish knowledge dataset ARC-da, \"\n    \"translated from the English ARC dataset\",\n    huggingface_id=\"ScandEval/arc-da-mini\",\n    task=KNOW,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rgsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rgsm\u00e5l: {text}\\n\\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at \"\n    \"svare med 'a', 'b', 'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nARC_NO_CONFIG = DatasetConfig(\n    name=\"arc-no\",\n    pretty_name=\"the truncated version of the Norwegian knowledge dataset ARC-no, \"\n    \"translated from the English ARC dataset\",\n    huggingface_id=\"ScandEval/arc-no-mini\",\n    task=KNOW,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"F\u00f8lgende er flervalgssp\u00f8rsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rsm\u00e5l: {text}\\n\\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', \"\n    \"'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nARC_SV_CONFIG = DatasetConfig(\n    name=\"arc-sv\",\n    pretty_name=\"the truncated version of the Swedish knowledge dataset ARC-sv, \"\n    \"translated from the English ARC dataset\",\n    huggingface_id=\"ScandEval/arc-sv-mini\",\n    task=KNOW,\n    languages=[SV],\n    prompt_prefix=\"F\u00f6ljande \u00e4r flervalsfr\u00e5gor (med svar).\",\n    prompt_template=\"Fr\u00e5ga: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Fr\u00e5ga: {text}\\n\\nBesvara f\u00f6ljande fr\u00e5ga med 'a', 'b', 'c' \"\n    \"eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nARC_IS_CONFIG = DatasetConfig(\n    name=\"arc-is\",\n    pretty_name=\"the truncated version of the Icelandic knowledge dataset ARC-is, \"\n    \"translated from the English ARC dataset\",\n    huggingface_id=\"ScandEval/arc-is-mini\",\n    task=KNOW,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\",\n    prompt_template=\"Spurningar: {text}\\nSvara: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Spurningar: {text}\\n\\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', \"\n    \"'b', 'c' e\u00f0a 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nARC_DE_CONFIG = DatasetConfig(\n    name=\"arc-de\",\n    pretty_name=\"the truncated version of the German knowledge dataset ARC-de, \"\n    \"translated from the English ARC dataset\",\n    huggingface_id=\"ScandEval/arc-de-mini\",\n    task=KNOW,\n    languages=[DE],\n    prompt_prefix=\"Die folgenden Fragen sind Multiple-Choice-Fragen (mit Antworten).\",\n    prompt_template=\"Frage: {text}\\nAntwort: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Frage: {text}\\n\\nBeantworten Sie die obige Frage mit 'a', 'b', \"\n    \"'c' oder 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nARC_NL_CONFIG = DatasetConfig(\n    name=\"arc-nl\",\n    pretty_name=\"the truncated version of the Dutch knowledge dataset ARC-nl, \"\n    \"translated from the English ARC dataset\",\n    huggingface_id=\"ScandEval/arc-nl-mini\",\n    task=KNOW,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan meerkeuzevragen (met antwoorden).\",\n    prompt_template=\"Vraag: {text}\\nAntwoord: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Vraag: {text}\\n\\nBeantwoord de bovenstaande vraag met 'a', \"\n    \"'b', 'c' of 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nARC_CONFIG = DatasetConfig(\n    name=\"arc\",\n    pretty_name=\"the truncated version of the English knowledge dataset ARC\",\n    huggingface_id=\"ScandEval/arc-mini\",\n    task=KNOW,\n    languages=[EN],\n    prompt_prefix=\"The following are multiple choice questions (with answers).\",\n    prompt_template=\"Question: {text}\\nAnswer: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Question: {text}\\n\\nAnswer the above question by \"\n    \"replying with 'a', 'b', 'c' or 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\n# TODO: Faroese knowledge\n\n\n### COMMON SENSE REASONING DATASETS ###\n\nHELLASWAG_DA_CONFIG = DatasetConfig(\n    name=\"hellaswag-da\",\n    pretty_name=\"the truncated version of the Danish common-sense reasoning dataset \"\n    \"HellaSwag-da, translated from the English HellaSwag dataset\",\n    huggingface_id=\"ScandEval/hellaswag-da-mini\",\n    task=COMMON_SENSE,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rgsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rgsm\u00e5l: {text}\\n\\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at \"\n    \"svare med 'a', 'b', 'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nHELLASWAG_NO_CONFIG = DatasetConfig(\n    name=\"hellaswag-no\",\n    pretty_name=\"the truncated version of the Norwegian common-sense reasoning dataset \"\n    \"HellaSwag-no, translated from the English HellaSwag dataset\",\n    huggingface_id=\"ScandEval/hellaswag-no-mini\",\n    task=COMMON_SENSE,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"F\u00f8lgende er flervalgssp\u00f8rsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rsm\u00e5l: {text}\\n\\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', \"\n    \"'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nHELLASWAG_SV_CONFIG = DatasetConfig(\n    name=\"hellaswag-sv\",\n    pretty_name=\"the truncated version of the Swedish common-sense reasoning dataset \"\n    \"HellaSwag-sv, translated from the English HellaSwag dataset\",\n    huggingface_id=\"ScandEval/hellaswag-sv-mini\",\n    task=COMMON_SENSE,\n    languages=[SV],\n    prompt_prefix=\"F\u00f6ljande \u00e4r flervalsfr\u00e5gor (med svar).\",\n    prompt_template=\"Fr\u00e5ga: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Fr\u00e5ga: {text}\\n\\nBesvara f\u00f6ljande fr\u00e5ga med 'a', 'b', 'c' \"\n    \"eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nHELLASWAG_IS_CONFIG = DatasetConfig(\n    name=\"hellaswag-is\",\n    pretty_name=\"the truncated version of the Icelandic common-sense reasoning dataset \"\n    \"HellaSwag-is, translated from the English HellaSwag dataset\",\n    huggingface_id=\"ScandEval/hellaswag-is-mini\",\n    task=COMMON_SENSE,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\",\n    prompt_template=\"Spurningar: {text}\\nSvara: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Spurningar: {text}\\n\\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', \"\n    \"'b', 'c' e\u00f0a 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nWINOGRANDE_IS = DatasetConfig(\n    name=\"winogrande-is\",\n    pretty_name=\"the Icelandic common-sense reasoning dataset \"\n    \"Winogrande-is, manually translated from the English Winogrande dataset\",\n    huggingface_id=\"ScandEval/winogrande-is\",\n    task=COMMON_SENSE,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\",\n    prompt_template=\"Spurningar: {text}\\nSvara: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Spurningar: {text}\\n\\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', \"\n    \"'b', 'c' e\u00f0a 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nHELLASWAG_DE_CONFIG = DatasetConfig(\n    name=\"hellaswag-de\",\n    pretty_name=\"the truncated version of the German common-sense reasoning dataset \"\n    \"HellaSwag-de, translated from the English HellaSwag dataset\",\n    huggingface_id=\"ScandEval/hellaswag-de-mini\",\n    task=COMMON_SENSE,\n    languages=[DE],\n    prompt_prefix=\"Die folgenden Fragen sind Multiple-Choice-Fragen (mit Antworten).\",\n    prompt_template=\"Frage: {text}\\nAntwort: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Frage: {text}\\n\\nBeantworten Sie die obige Frage mit 'a', 'b', \"\n    \"'c' oder 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nHELLASWAG_NL_CONFIG = DatasetConfig(\n    name=\"hellaswag-nl\",\n    pretty_name=\"the truncated version of the Dutch common-sense reasoning dataset \"\n    \"HellaSwag-nl, translated from the English HellaSwag dataset\",\n    huggingface_id=\"ScandEval/hellaswag-nl-mini\",\n    task=COMMON_SENSE,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan meerkeuzevragen (met antwoorden).\",\n    prompt_template=\"Vraag: {text}\\nAntwoord: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Vraag: {text}\\n\\nBeantwoord de bovenstaande vraag met 'a', 'b', \"\n    \"'c' of 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nHELLASWAG_CONFIG = DatasetConfig(\n    name=\"hellaswag\",\n    pretty_name=\"the truncated version of the English common-sense reasoning \"\n    \"dataset HellaSwag\",\n    huggingface_id=\"ScandEval/hellaswag-mini\",\n    task=COMMON_SENSE,\n    languages=[EN],\n    prompt_prefix=\"The following are multiple choice questions (with answers).\",\n    prompt_template=\"Question: {text}\\nAnswer: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Question: {text}\\n\\nAnswer the above question by \"\n    \"replying with 'a', 'b', 'c' or 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\n# TODO: Faroese common sense reasoning\n\n\n### MULTIPLE CHOICE READING COMPREHENSION DATASETS ###\n\nBELEBELE_DA_CONFIG = DatasetConfig(\n    name=\"belebele-da\",\n    pretty_name=\"the Danish multiple choice reading comprehension dataset BeleBele-da, \"\n    \"translated from the English BeleBele dataset\",\n    huggingface_id=\"ScandEval/belebele-da-mini\",\n    task=MCRC,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er tekster med tilh\u00f8rende multiple choice sp\u00f8rgsm\u00e5l og \"\n    \"svar.\",\n    prompt_template=\"{text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rgsm\u00e5l: {text}\\n\\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at \"\n    \"svare med 'a', 'b', 'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nBELEBELE_SV_CONFIG = DatasetConfig(\n    name=\"belebele-sv\",\n    pretty_name=\"the Swedish multiple choice reading comprehension dataset \"\n    \"BeleBele-sv, translated from the English BeleBele dataset\",\n    huggingface_id=\"ScandEval/belebele-sv-mini\",\n    task=MCRC,\n    languages=[SV],\n    prompt_prefix=\"Nedan f\u00f6ljer texter med tillh\u00f6rande multiple choice fr\u00e5gor och \"\n    \"svar.\",\n    prompt_template=\"{text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Fr\u00e5ga: {text}\\n\\nBesvara f\u00f6ljande fr\u00e5ga med 'a', 'b', 'c' \"\n    \"eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nBELEBELE_NO_CONFIG = DatasetConfig(\n    name=\"belebele-no\",\n    pretty_name=\"the Norwegian multiple choice reading comprehension dataset \"\n    \"BeleBele-no, translated from the English BeleBele dataset\",\n    huggingface_id=\"ScandEval/belebele-no-mini\",\n    task=MCRC,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"Her f\u00f8lger tekster med tilh\u00f8rende multiple choice sp\u00f8rsm\u00e5l og svar.\",\n    prompt_template=\"{text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rsm\u00e5l: {text}\\n\\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', \"\n    \"'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nBELEBELE_IS_CONFIG = DatasetConfig(\n    name=\"belebele-is\",\n    pretty_name=\"the Icelandic multiple choice reading comprehension dataset \"\n    \"BeleBele-is, translated from the English BeleBele dataset\",\n    huggingface_id=\"ScandEval/belebele-is-mini\",\n    task=MCRC,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru textar me\u00f0 tilheyrandi fj\u00f6lvalsspurningum og \"\n    \"sv\u00f6rum.\",\n    prompt_template=\"{text}\\nSvara: {label}\",\n    instruction_prompt=\"Spurningar: {text}\\n\\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', \"\n    \"'b', 'c' e\u00f0a 'd'.\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nBELEBELE_DE_CONFIG = DatasetConfig(\n    name=\"belebele-de\",\n    pretty_name=\"the German multiple choice reading comprehension dataset BeleBele-de, \"\n    \"translated from the English BeleBele dataset\",\n    huggingface_id=\"ScandEval/belebele-de-mini\",\n    task=MCRC,\n    languages=[DE],\n    prompt_prefix=\"Die folgenden Texte sind mit dazugeh\u00f6rigen Multiple-Choice-Fragen \"\n    \"und Antworten.\",\n    prompt_template=\"{text}\\nAntwort: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Frage: {text}\\n\\nBeantworten Sie die obige Frage mit 'a', 'b', \"\n    \"'c' oder 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nBELEBELE_NL_CONFIG = DatasetConfig(\n    name=\"belebele-nl\",\n    pretty_name=\"the Dutch multiple choice reading comprehension dataset BeleBele-nl, \"\n    \"translated from the English BeleBele dataset\",\n    huggingface_id=\"ScandEval/belebele-nl-mini\",\n    task=MCRC,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan teksten met bijbehorende multiple choice vragen en \"\n    \"antwoorden.\",\n    prompt_template=\"{text}\\nAntwoord: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Vraag: {text}\\n\\nBeantwoord de bovenstaande vraag met 'a', \"\n    \"'b', 'c' of 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nBELEBELE_CONFIG = DatasetConfig(\n    name=\"belebele\",\n    pretty_name=\"the English multiple choice reading comprehension dataset BeleBele\",\n    huggingface_id=\"ScandEval/belebele-mini\",\n    task=MCRC,\n    languages=[EN],\n    prompt_prefix=\"The following are texts with accompanying multiple choice questions \"\n    \"and answers.\",\n    prompt_template=\"{text}\\nAnswer: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Question: {text}\\n\\nAnswer the above question by \"\n    \"replying with 'a', 'b', 'c' or 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\n\n### SPEED ESTIMATION DATASETS ###\n\nSPEED_CONFIG = DatasetConfig(\n    name=\"speed\",\n    pretty_name=\"the speed estimation benchmark\",\n    huggingface_id=\"\",\n    task=SPEED,\n    languages=list(get_all_languages().values()),\n    prompt_prefix=\"\",\n    prompt_template=\"\",\n    instruction_prompt=\"\",\n    num_few_shot_examples=0,\n    max_generated_tokens=5,\n)\n</code></pre>"},{"location":"api/scandeval/data_loading/","title":"scandeval.data_loading","text":"scandeval.data_loading<p> source module scandeval.data_loading </p> <p>Functions related to the loading of the data.</p> <p> Functions </p> <ul> <li> <p>load_data \u2014 Load the raw bootstrapped datasets.</p> </li> </ul> <p> source load_data(rng: Generator, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 list[DatasetDict] </p> <p>Load the raw bootstrapped datasets.</p> <p> Parameters </p> <ul> <li> <p>rng :  Generator \u2014</p> <p>The random number generator to use.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration for the dataset.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The configuration for the benchmark.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[DatasetDict] \u2014 A list of bootstrapped datasets, one for each iteration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul>"},{"location":"src/scandeval/data_loading/","title":"scandeval.data_loading","text":"scandeval.data_loading<p> docs module scandeval.data_loading </p> <pre><code>\"\"\"Functions related to the loading of the data.\"\"\"\n\nimport logging\nimport sys\nimport time\n\nfrom datasets import Dataset, DatasetDict, load_dataset\nfrom datasets.exceptions import DatasetsError\nfrom huggingface_hub.utils import HfHubHTTPError\nfrom numpy.random import Generator\n\nfrom .data_models import BenchmarkConfig, DatasetConfig\nfrom .exceptions import InvalidBenchmark\nfrom .utils import unscramble\n\nlogger = logging.getLogger(\"scandeval\")\n\n\ndef load_data(docs\n    rng: Generator, dataset_config: \"DatasetConfig\", benchmark_config: \"BenchmarkConfig\"\n) -&gt; list[DatasetDict]:\n    \"\"\"Load the raw bootstrapped datasets.\n\n    Args:\n        rng:\n            The random number generator to use.\n        dataset_config:\n            The configuration for the dataset.\n        benchmark_config:\n            The configuration for the benchmark.\n\n    Returns:\n        A list of bootstrapped datasets, one for each iteration.\n    \"\"\"\n    num_attempts = 5\n    for _ in range(num_attempts):\n        try:\n            dataset = load_dataset(\n                path=dataset_config.huggingface_id,\n                cache_dir=benchmark_config.cache_dir,\n                token=unscramble(\"HjccJFhIozVymqXDVqTUTXKvYhZMTbfIjMxG_\"),\n            )\n            break\n        except (FileNotFoundError, DatasetsError):\n            logger.warning(\n                f\"Failed to load dataset {dataset_config.huggingface_id!r}. Retrying...\"\n            )\n            time.sleep(1)\n            continue\n        except HfHubHTTPError:\n            raise InvalidBenchmark(\"The Hugging Face Hub seems to be down.\")\n    else:\n        raise InvalidBenchmark(\n            f\"Failed to load dataset {dataset_config.huggingface_id!r} after \"\n            f\"{num_attempts} attempts.\"\n        )\n\n    assert isinstance(dataset, DatasetDict)  # type: ignore[used-before-def]\n\n    dataset = DatasetDict({key: dataset[key] for key in [\"train\", \"val\", \"test\"]})\n\n    if not benchmark_config.evaluate_test_split:\n        dataset[\"test\"] = dataset[\"val\"]\n\n    # Remove empty examples from the datasets\n    for text_feature in [\"tokens\", \"text\"]:\n        if text_feature in dataset[\"train\"].features:\n            dataset = dataset.filter(lambda x: len(x[text_feature]) &gt; 0)\n\n    # If we are testing then truncate the test set\n    if hasattr(sys, \"_called_from_test\"):\n        dataset[\"test\"] = dataset[\"test\"].select(range(1))\n\n    # Bootstrap the splits\n    bootstrapped_splits: dict[str, list[Dataset]] = dict()\n    for split in [\"train\", \"val\", \"test\"]:\n        bootstrap_indices = rng.integers(\n            0,\n            len(dataset[split]),\n            size=(benchmark_config.num_iterations, len(dataset[split])),\n        )\n        bootstrapped_splits[split] = [\n            dataset[split].select(bootstrap_indices[idx])\n            for idx in range(benchmark_config.num_iterations)\n        ]\n\n    datasets = [\n        DatasetDict(\n            {\n                split: bootstrapped_splits[split][idx]\n                for split in [\"train\", \"val\", \"test\"]\n            }\n        )\n        for idx in range(benchmark_config.num_iterations)\n    ]\n    return datasets\n</code></pre>"},{"location":"api/scandeval/data_models/","title":"scandeval.data_models","text":"scandeval.data_models<p> source module scandeval.data_models </p> <p>Data models used in ScandEval.</p> <p> Classes </p> <ul> <li> <p>MetricConfig \u2014 Configuration for a metric.</p> </li> <li> <p>Task \u2014 A dataset task.</p> </li> <li> <p>Language \u2014 A benchmarkable language.</p> </li> <li> <p>BenchmarkConfig \u2014 General benchmarking configuration, across datasets and models.</p> </li> <li> <p>BenchmarkConfigParams \u2014 The parameters for the benchmark configuration.</p> </li> <li> <p>BenchmarkResult \u2014 A benchmark result.</p> </li> <li> <p>DatasetConfig \u2014 Configuration for a dataset.</p> </li> <li> <p>ModelConfig \u2014 Configuration for a model.</p> </li> <li> <p>PreparedModelInputs \u2014 The inputs to a model.</p> </li> <li> <p>GenerativeModelOutput \u2014 The output of a generative model.</p> </li> <li> <p>SingleGenerativeModelOutput \u2014 A single output of a generative model.</p> </li> <li> <p>HFModelInfo \u2014 Information about a Hugging Face model.</p> </li> </ul> <p> source dataclass MetricConfig(name: str, pretty_name: str, huggingface_id: str, results_key: str, compute_kwargs: dict[str, t.Any] = field(default_factory=dict), postprocessing_fn: c.Callable[[float], tuple[float, str]] = field(default_factory=lambda: lambda raw_score: (100 * raw_score, f'{raw_score:.2%}'))) </p> <p>Configuration for a metric.</p> <p> Attributes </p> <ul> <li> <p>name :  str \u2014</p> <p>The name of the metric.</p> </li> <li> <p>pretty_name :  str \u2014</p> <p>A longer prettier name for the metric, which allows cases and spaces. Used for logging.</p> </li> <li> <p>huggingface_id :  str \u2014</p> <p>The Hugging Face ID of the metric.</p> </li> <li> <p>results_key :  str \u2014</p> <p>The name of the key used to extract the metric scores from the results dictionary.</p> </li> <li> <p>compute_kwargs :  dict[str, t.Any] \u2014</p> <p>Keyword arguments to pass to the metric's compute function. Defaults to an empty dictionary.</p> </li> <li> <p>postprocessing_fn :  c.Callable[[float], tuple[float, str]] \u2014</p> <p>A function to apply to the metric scores after they are computed, taking the score to the postprocessed score along with its string representation. Defaults to x -&gt; (100 * x, f\"{x:.2%}\").</p> </li> </ul> <p> source dataclass Task(name: str, supertask: str, metrics: list[MetricConfig], labels: list[str]) </p> <p>A dataset task.</p> <p> Attributes </p> <ul> <li> <p>name :  str \u2014</p> <p>The name of the task.</p> </li> <li> <p>supertask :  str \u2014</p> <p>The supertask of the task, describing the overall type of task.</p> </li> <li> <p>metrics :  list[MetricConfig] \u2014</p> <p>The metrics used to evaluate the task.</p> </li> <li> <p>labels :  list[str] \u2014</p> <p>The labels used in the task.</p> </li> </ul> <p> source dataclass Language(code: str, name: str) </p> <p>A benchmarkable language.</p> <p> Attributes </p> <ul> <li> <p>code :  str \u2014</p> <p>The ISO 639-1 language code of the language.</p> </li> <li> <p>name :  str \u2014</p> <p>The name of the language.</p> </li> </ul> <p> source dataclass BenchmarkConfig(model_languages: list[Language], dataset_languages: list[Language], tasks: list[Task], datasets: list[str], framework: Framework | None, batch_size: int, raise_errors: bool, cache_dir: str, api_key: str | None, force: bool, progress_bar: bool, save_results: bool, device: torch.device, verbose: bool, trust_remote_code: bool, load_in_4bit: bool | None, use_flash_attention: bool | None, clear_model_cache: bool, evaluate_test_split: bool, few_shot: bool, num_iterations: int, api_base: str | None, api_version: str | None, debug: bool, run_with_cli: bool) </p> <p>General benchmarking configuration, across datasets and models.</p> <p> Attributes </p> <ul> <li> <p>model_languages :  list[Language] \u2014</p> <p>The languages of the models to benchmark.</p> </li> <li> <p>dataset_languages :  list[Language] \u2014</p> <p>The languages of the datasets in the benchmark.</p> </li> <li> <p>tasks :  list[Task] \u2014</p> <p>The tasks benchmark the model(s) on.</p> </li> <li> <p>datasets :  list[str] \u2014</p> <p>The datasets to benchmark on.</p> </li> <li> <p>framework :  Framework | None \u2014</p> <p>The framework of the models to benchmark. If None then the framework will be inferred.</p> </li> <li> <p>batch_size :  int \u2014</p> <p>The batch size to use.</p> </li> <li> <p>raise_errors :  bool \u2014</p> <p>Whether to raise errors instead of skipping them.</p> </li> <li> <p>cache_dir :  str \u2014</p> <p>Directory to store cached models and datasets.</p> </li> <li> <p>api_key :  str | None \u2014</p> <p>The API key to use for a given inference API.</p> </li> <li> <p>force :  bool \u2014</p> <p>Whether to force the benchmark to run even if the results are already cached.</p> </li> <li> <p>progress_bar :  bool \u2014</p> <p>Whether to show a progress bar.</p> </li> <li> <p>save_results :  bool \u2014</p> <p>Whether to save the benchmark results to 'scandeval_benchmark_results.json'.</p> </li> <li> <p>device :  torch.device \u2014</p> <p>The device to use for benchmarking.</p> </li> <li> <p>verbose :  bool \u2014</p> <p>Whether to print verbose output.</p> </li> <li> <p>trust_remote_code :  bool \u2014</p> <p>Whether to trust remote code when loading models from the Hugging Face Hub.</p> </li> <li> <p>load_in_4bit :  bool | None \u2014</p> <p>Whether to load models in 4-bit precision. If None then this will be done if CUDA is available and the model is a decoder model.</p> </li> <li> <p>use_flash_attention :  bool | None \u2014</p> <p>Whether to use Flash Attention. If None then this will be used for generative models.</p> </li> <li> <p>clear_model_cache :  bool \u2014</p> <p>Whether to clear the model cache after benchmarking each model.</p> </li> <li> <p>evaluate_test_split :  bool \u2014</p> <p>Whether to evaluate on the test split.</p> </li> <li> <p>few_shot :  bool \u2014</p> <p>Whether to only evaluate the model using few-shot evaluation. Only relevant if the model is generative.</p> </li> <li> <p>num_iterations :  int \u2014</p> <p>The number of iterations each model should be evaluated for.</p> </li> <li> <p>api_base :  str | None \u2014</p> <p>The base URL for a given inference API. Only relevant if <code>model</code> refers to a model on an inference API.</p> </li> <li> <p>api_version :  str | None \u2014</p> <p>The version of the API to use. Only relevant if <code>model</code> refers to a model on an inference API.</p> </li> <li> <p>debug :  bool \u2014</p> <p>Whether to run the benchmark in debug mode.</p> </li> <li> <p>run_with_cli :  bool \u2014</p> <p>Whether the benchmark is being run with the CLI.</p> </li> </ul> <p> source class BenchmarkConfigParams() </p> <p><p>Bases : pydantic.BaseModel</p></p> <p>The parameters for the benchmark configuration.</p> <p> Attributes </p> <ul> <li> <p>model_extra :  dict[str, Any] | None \u2014 Get extra fields set during validation.</p> </li> <li> <p>model_fields_set :  set[str] \u2014 Returns the set of fields that have been explicitly set on this model instance.</p> </li> </ul> <p> source class BenchmarkResult() </p> <p><p>Bases : pydantic.BaseModel</p></p> <p>A benchmark result.</p> <p> Attributes </p> <ul> <li> <p>model_config :  ClassVar[ConfigDict] \u2014 Configuration for the model, should be a dictionary conforming to [<code>ConfigDict</code>][pydantic.config.ConfigDict].</p> </li> <li> <p>model_extra :  dict[str, Any] | None \u2014 Get extra fields set during validation.</p> </li> <li> <p>model_fields_set :  set[str] \u2014 Returns the set of fields that have been explicitly set on this model instance.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>from_dict \u2014 Create a benchmark result from a dictionary.</p> </li> <li> <p>append_to_results \u2014 Append the benchmark result to the results file.</p> </li> </ul> <p> source classmethod BenchmarkResult.from_dict(config: dict) \u2192 BenchmarkResult </p> <p>Create a benchmark result from a dictionary.</p> <p> Parameters </p> <ul> <li> <p>config :  dict \u2014</p> <p>The configuration dictionary.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BenchmarkResult \u2014 The benchmark result.</p> </li> </ul> <p> source method BenchmarkResult.append_to_results(results_path: pathlib.Path) \u2192 None </p> <p>Append the benchmark result to the results file.</p> <p> Parameters </p> <ul> <li> <p>results_path :  pathlib.Path \u2014</p> <p>The path to the results file.</p> </li> </ul> <p> source dataclass DatasetConfig(name: str, pretty_name: str, huggingface_id: str, task: Task, languages: list[Language], prompt_template: str, max_generated_tokens: int, prompt_prefix: str, num_few_shot_examples: int, instruction_prompt: str, prompt_label_mapping: dict[str, str] = field(default_factory=dict), unofficial: bool = False) </p> <p>Configuration for a dataset.</p> <p> Attributes </p> <ul> <li> <p>name :  str \u2014</p> <p>The name of the dataset. Must be lower case with no spaces.</p> </li> <li> <p>pretty_name :  str \u2014</p> <p>A longer prettier name for the dataset, which allows cases and spaces. Used for logging.</p> </li> <li> <p>huggingface_id :  str \u2014</p> <p>The Hugging Face ID of the dataset.</p> </li> <li> <p>task :  Task \u2014</p> <p>The task of the dataset.</p> </li> <li> <p>languages :  list[Language] \u2014</p> <p>The ISO 639-1 language codes of the entries in the dataset.</p> </li> <li> <p>id2label :  dict[int, str] \u2014</p> <p>The mapping from ID to label.</p> </li> <li> <p>label2id :  dict[str, int] \u2014</p> <p>The mapping from label to ID.</p> </li> <li> <p>num_labels :  int \u2014</p> <p>The number of labels in the dataset.</p> </li> <li> <p>prompt_template :  str \u2014</p> <p>The template for the prompt to use when benchmarking the dataset using few-shot evaluation.</p> </li> <li> <p>max_generated_tokens :  int \u2014</p> <p>The maximum number of tokens to generate when benchmarking the dataset using few-shot evaluation.</p> </li> <li> <p>prompt_prefix :  str \u2014</p> <p>The prefix to use in the few-shot prompt.</p> </li> <li> <p>num_few_shot_examples :  int \u2014</p> <p>The number of examples to use when benchmarking the dataset using few-shot evaluation. For a classification task, these will be drawn evenly from each label.</p> </li> <li> <p>instruction_prompt :  str \u2014</p> <p>The prompt to use when benchmarking the dataset using instruction-based evaluation.</p> </li> <li> <p>prompt_label_mapping :  optional \u2014</p> <p>A mapping from the labels to another phrase which is used as a substitute for the label in few-shot evaluation. Defaults to an empty dictionary.</p> </li> <li> <p>unofficial :  optional \u2014</p> <p>Whether the dataset is unofficial. Defaults to False.</p> </li> </ul> <p> source property DatasetConfig.id2label: dict[int, str] </p> <p>The mapping from ID to label.</p> <p> source property DatasetConfig.label2id: dict[str, int] </p> <p>The mapping from label to ID.</p> <p> source property DatasetConfig.num_labels: int </p> <p>The number of labels in the dataset.</p> <p> source dataclass ModelConfig(model_id: str, revision: str, framework: Framework, task: str, languages: list[Language], model_type: ModelType, model_cache_dir: str, adapter_base_model_id: str | None) </p> <p>Configuration for a model.</p> <p> Attributes </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The ID of the model.</p> </li> <li> <p>revision :  str \u2014</p> <p>The revision of the model.</p> </li> <li> <p>framework :  Framework \u2014</p> <p>The framework of the model.</p> </li> <li> <p>task :  str \u2014</p> <p>The task that the model was trained on.</p> </li> <li> <p>languages :  list[Language] \u2014</p> <p>The languages of the model.</p> </li> <li> <p>model_type :  ModelType \u2014</p> <p>The type of the model.</p> </li> <li> <p>model_cache_dir :  str \u2014</p> <p>The directory to cache the model in.</p> </li> <li> <p>adapter_base_model_id :  str | None \u2014</p> <p>The model ID of the base model if the model is an adapter model. Can be None if the model is not an adapter model.</p> </li> </ul> <p> source dataclass PreparedModelInputs(texts: list[str] | None = None, input_ids: torch.Tensor | None = None, attention_mask: torch.Tensor | None = None) </p> <p>The inputs to a model.</p> <p> Attributes </p> <ul> <li> <p>texts :  list[str] | None \u2014</p> <p>The texts to input to the model. Can be None if the input IDs and attention mask are provided instead.</p> </li> <li> <p>input_ids :  torch.Tensor | None \u2014</p> <p>The input IDs of the texts. Can be None if the texts are provided instead.</p> </li> <li> <p>attention_mask :  torch.Tensor | None \u2014</p> <p>The attention mask of the texts. Can be None if the texts are provided instead.</p> </li> </ul> <p> source dataclass GenerativeModelOutput(sequences: list[str], scores: list[list[list[tuple[str, float]]]] | None = None) </p> <p>The output of a generative model.</p> <p> Attributes </p> <ul> <li> <p>sequences :  list[str] \u2014</p> <p>The generated sequences.</p> </li> <li> <p>scores :  list[list[list[tuple[str, float]]]] | None \u2014</p> <p>The scores of the sequences. This is an array of shape (batch_size, num_tokens, num_logprobs, 2), where the last dimension contains the token and its logprob. Can be None if the scores are not available.</p> </li> </ul> <p> source dataclass SingleGenerativeModelOutput(sequence: str, scores: list[list[tuple[str, float]]] | None = None) </p> <p>A single output of a generative model.</p> <p> Attributes </p> <ul> <li> <p>sequence :  str \u2014</p> <p>The generated sequence.</p> </li> <li> <p>scores :  list[list[tuple[str, float]]] | None \u2014</p> <p>The scores of the sequence. This is an array of shape (num_tokens, num_logprobs, 2), where the last dimension contains the token and its logprob. Can be None if the scores are not available.</p> </li> </ul> <p> source dataclass HFModelInfo(pipeline_tag: str, tags: list[str], adapter_base_model_id: str | None) </p> <p>Information about a Hugging Face model.</p> <p> Attributes </p> <ul> <li> <p>pipeline_tag :  str \u2014</p> <p>The pipeline tag of the model.</p> </li> <li> <p>tags :  list[str] \u2014</p> <p>The other tags of the model.</p> </li> <li> <p>adapter_base_model_id :  str | None \u2014</p> <p>The model ID of the base model if the model is an adapter model. Can be None if the model is not an adapter model.</p> </li> </ul>"},{"location":"src/scandeval/data_models/","title":"scandeval.data_models","text":"scandeval.data_models<p> docs module scandeval.data_models </p> <pre><code>\"\"\"Data models used in ScandEval.\"\"\"\n\nimport collections.abc as c\nimport importlib.metadata\nimport json\nimport pathlib\nimport re\nimport typing as t\nfrom dataclasses import dataclass, field\n\nimport pydantic\nimport torch\n\nfrom .enums import Device, Framework, ModelType\nfrom .types import ScoreDict\n\n\n@dataclass\nclass MetricConfig:docs\n    \"\"\"Configuration for a metric.\n\n    Attributes:\n        name:\n            The name of the metric.\n        pretty_name:\n            A longer prettier name for the metric, which allows cases and spaces. Used\n            for logging.\n        huggingface_id:\n            The Hugging Face ID of the metric.\n        results_key:\n            The name of the key used to extract the metric scores from the results\n            dictionary.\n        compute_kwargs:\n            Keyword arguments to pass to the metric's compute function. Defaults to\n            an empty dictionary.\n        postprocessing_fn:\n            A function to apply to the metric scores after they are computed, taking\n            the score to the postprocessed score along with its string representation.\n            Defaults to x -&gt; (100 * x, f\"{x:.2%}\").\n    \"\"\"\n\n    name: str\n    pretty_name: str\n    huggingface_id: str\n    results_key: str\n    compute_kwargs: dict[str, t.Any] = field(default_factory=dict)\n    postprocessing_fn: c.Callable[[float], tuple[float, str]] = field(\n        default_factory=lambda: lambda raw_score: (100 * raw_score, f\"{raw_score:.2%}\")\n    )\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hash of the metric configuration.\"\"\"\n        return hash(self.name)\n\n\n@dataclass\nclass Task:docs\n    \"\"\"A dataset task.\n\n    Attributes:\n        name:\n            The name of the task.\n        supertask:\n            The supertask of the task, describing the overall type of task.\n        metrics:\n            The metrics used to evaluate the task.\n        labels:\n            The labels used in the task.\n    \"\"\"\n\n    name: str\n    supertask: str\n    metrics: list[MetricConfig]\n    labels: list[str]\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hash of the task.\"\"\"\n        return hash(self.name)\n\n\n@dataclass\nclass Language:docs\n    \"\"\"A benchmarkable language.\n\n    Attributes:\n        code:\n            The ISO 639-1 language code of the language.\n        name:\n            The name of the language.\n    \"\"\"\n\n    code: str\n    name: str\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hash of the language.\"\"\"\n        return hash(self.code)\n\n\n@dataclass\nclass BenchmarkConfig:docs\n    \"\"\"General benchmarking configuration, across datasets and models.\n\n    Attributes:\n        model_languages:\n            The languages of the models to benchmark.\n        dataset_languages:\n            The languages of the datasets in the benchmark.\n        tasks:\n            The tasks benchmark the model(s) on.\n        datasets:\n            The datasets to benchmark on.\n        framework:\n            The framework of the models to benchmark. If None then the framework will be\n            inferred.\n        batch_size:\n            The batch size to use.\n        raise_errors:\n            Whether to raise errors instead of skipping them.\n        cache_dir:\n            Directory to store cached models and datasets.\n        api_key:\n            The API key to use for a given inference API.\n        force:\n            Whether to force the benchmark to run even if the results are already\n            cached.\n        progress_bar:\n            Whether to show a progress bar.\n        save_results:\n            Whether to save the benchmark results to 'scandeval_benchmark_results.json'.\n        device:\n            The device to use for benchmarking.\n        verbose:\n            Whether to print verbose output.\n        trust_remote_code:\n            Whether to trust remote code when loading models from the Hugging Face Hub.\n        load_in_4bit:\n            Whether to load models in 4-bit precision. If None then this will be done\n            if CUDA is available and the model is a decoder model.\n        use_flash_attention:\n            Whether to use Flash Attention. If None then this will be used for\n            generative models.\n        clear_model_cache:\n            Whether to clear the model cache after benchmarking each model.\n        evaluate_test_split:\n            Whether to evaluate on the test split.\n        few_shot:\n            Whether to only evaluate the model using few-shot evaluation. Only relevant\n            if the model is generative.\n        num_iterations:\n            The number of iterations each model should be evaluated for.\n        api_base:\n            The base URL for a given inference API. Only relevant if `model` refers to a\n            model on an inference API.\n        api_version:\n            The version of the API to use. Only relevant if `model` refers to a model on\n            an inference API.\n        debug:\n            Whether to run the benchmark in debug mode.\n        run_with_cli:\n            Whether the benchmark is being run with the CLI.\n    \"\"\"\n\n    model_languages: list[Language]\n    dataset_languages: list[Language]\n    tasks: list[Task]\n    datasets: list[str]\n    framework: Framework | None\n    batch_size: int\n    raise_errors: bool\n    cache_dir: str\n    api_key: str | None\n    force: bool\n    progress_bar: bool\n    save_results: bool\n    device: torch.device\n    verbose: bool\n    trust_remote_code: bool\n    load_in_4bit: bool | None\n    use_flash_attention: bool | None\n    clear_model_cache: bool\n    evaluate_test_split: bool\n    few_shot: bool\n    num_iterations: int\n    api_base: str | None\n    api_version: str | None\n    debug: bool\n    run_with_cli: bool\n\n\nclass BenchmarkConfigParams(pydantic.BaseModel):docs\n    \"\"\"The parameters for the benchmark configuration.\"\"\"\n\n    model_config = pydantic.ConfigDict(protected_namespaces=())\n\n    progress_bar: bool\n    save_results: bool\n    task: str | list[str] | None\n    dataset: str | list[str] | None\n    language: str | list[str]\n    model_language: str | list[str] | None\n    dataset_language: str | list[str] | None\n    framework: Framework | str | None\n    device: Device | None\n    batch_size: int\n    raise_errors: bool\n    cache_dir: str\n    api_key: str | None\n    force: bool\n    verbose: bool\n    trust_remote_code: bool\n    load_in_4bit: bool | None\n    use_flash_attention: bool | None\n    clear_model_cache: bool\n    evaluate_test_split: bool\n    few_shot: bool\n    num_iterations: int\n    api_base: str | None\n    api_version: str | None\n    debug: bool\n    run_with_cli: bool\n\n\nclass BenchmarkResult(pydantic.BaseModel):docs\n    \"\"\"A benchmark result.\"\"\"\n\n    dataset: str\n    task: str\n    dataset_languages: list[str]\n    model: str\n    results: ScoreDict\n    num_model_parameters: int\n    max_sequence_length: int\n    vocabulary_size: int\n    generative: bool\n    few_shot: bool\n    validation_split: bool\n    scandeval_version: str = importlib.metadata.version(\"scandeval\")\n\n    @classmethod\n    def from_dict(cls, config: dict) -&gt; \"BenchmarkResult\":docs\n        \"\"\"Create a benchmark result from a dictionary.\n\n        Args:\n            config:\n                The configuration dictionary.\n\n        Returns:\n            The benchmark result.\n        \"\"\"\n        # To be backwards compatible, we accept old results which changed the model\n        # name with parameters rather than adding them as explicit parameters\n        val_matches = re.search(r\"\\(.*val.*\\)$\", config[\"model\"])\n        few_shot_matches = re.search(r\"\\(.*few-shot.*\\)$\", config[\"model\"])\n        config[\"model\"] = re.sub(\n            r\"\\(.*(few-shot|val).*\\)$\", \"\", config[\"model\"]\n        ).strip()\n\n        # The default value for `few_shot` is True. It won't do anything if the model\n        # is not generative, so this is fine\n        if \"generative\" not in config:\n            config[\"generative\"] = few_shot_matches is not None\n        if \"few_shot\" not in config:\n            config[\"few_shot\"] = True\n\n        if \"validation_split\" not in config:\n            config[\"validation_split\"] = val_matches is not None\n\n        return cls(**config)\n\n    def append_to_results(self, results_path: pathlib.Path) -&gt; None:docs\n        \"\"\"Append the benchmark result to the results file.\n\n        Args:\n            results_path:\n                The path to the results file.\n        \"\"\"\n        json_str = json.dumps(self.model_dump())\n        with results_path.open(\"a\") as f:\n            f.write(\"\\n\" + json_str)\n\n\n@dataclass\nclass DatasetConfig:docs\n    \"\"\"Configuration for a dataset.\n\n    Attributes:\n        name:\n            The name of the dataset. Must be lower case with no spaces.\n        pretty_name:\n            A longer prettier name for the dataset, which allows cases and spaces. Used\n            for logging.\n        huggingface_id:\n            The Hugging Face ID of the dataset.\n        task:\n            The task of the dataset.\n        languages:\n            The ISO 639-1 language codes of the entries in the dataset.\n        id2label:\n            The mapping from ID to label.\n        label2id:\n            The mapping from label to ID.\n        num_labels:\n            The number of labels in the dataset.\n        prompt_template:\n            The template for the prompt to use when benchmarking the dataset using\n            few-shot evaluation.\n        max_generated_tokens:\n            The maximum number of tokens to generate when benchmarking the dataset\n            using few-shot evaluation.\n        prompt_prefix:\n            The prefix to use in the few-shot prompt.\n        num_few_shot_examples:\n            The number of examples to use when benchmarking the dataset using few-shot\n            evaluation. For a classification task, these will be drawn evenly from\n            each label.\n        instruction_prompt:\n            The prompt to use when benchmarking the dataset using instruction-based\n            evaluation.\n        prompt_label_mapping (optional):\n            A mapping from the labels to another phrase which is used as a substitute\n            for the label in few-shot evaluation. Defaults to an empty dictionary.\n        unofficial (optional):\n            Whether the dataset is unofficial. Defaults to False.\n    \"\"\"\n\n    name: str\n    pretty_name: str\n    huggingface_id: str\n    task: Task\n    languages: list[Language]\n    prompt_template: str\n    max_generated_tokens: int\n    prompt_prefix: str\n    num_few_shot_examples: int\n    instruction_prompt: str\n    prompt_label_mapping: dict[str, str] = field(default_factory=dict)\n    unofficial: bool = False\n\n    @property\n    def id2label(self) -&gt; dict[int, str]:docs\n        \"\"\"The mapping from ID to label.\"\"\"\n        return {idx: label for idx, label in enumerate(self.task.labels)}\n\n    @property\n    def label2id(self) -&gt; dict[str, int]:docs\n        \"\"\"The mapping from label to ID.\"\"\"\n        return {label: i for i, label in enumerate(self.task.labels)}\n\n    @property\n    def num_labels(self) -&gt; int:docs\n        \"\"\"The number of labels in the dataset.\"\"\"\n        return len(self.task.labels)\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hash of the dataset configuration.\"\"\"\n        return hash(self.name)\n\n\n@dataclass\nclass ModelConfig:docs\n    \"\"\"Configuration for a model.\n\n    Attributes:\n        model_id:\n            The ID of the model.\n        revision:\n            The revision of the model.\n        framework:\n            The framework of the model.\n        task:\n            The task that the model was trained on.\n        languages:\n            The languages of the model.\n        model_type:\n            The type of the model.\n        model_cache_dir:\n            The directory to cache the model in.\n        adapter_base_model_id:\n            The model ID of the base model if the model is an adapter model. Can be None\n            if the model is not an adapter model.\n    \"\"\"\n\n    model_id: str\n    revision: str\n    framework: Framework\n    task: str\n    languages: list[Language]\n    model_type: ModelType\n    model_cache_dir: str\n    adapter_base_model_id: str | None\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hash of the model configuration.\"\"\"\n        return hash(self.model_id)\n\n\n@dataclass\nclass PreparedModelInputs:docs\n    \"\"\"The inputs to a model.\n\n    Attributes:\n        texts:\n            The texts to input to the model. Can be None if the input IDs and attention\n            mask are provided instead.\n        input_ids:\n            The input IDs of the texts. Can be None if the texts are provided instead.\n        attention_mask:\n            The attention mask of the texts. Can be None if the texts are provided\n            instead.\n    \"\"\"\n\n    texts: list[str] | None = None\n    input_ids: torch.Tensor | None = None\n    attention_mask: torch.Tensor | None = None\n\n\n@dataclass\nclass GenerativeModelOutput:docs\n    \"\"\"The output of a generative model.\n\n    Attributes:\n        sequences:\n            The generated sequences.\n        scores:\n            The scores of the sequences. This is an array of shape (batch_size,\n            num_tokens, num_logprobs, 2), where the last dimension contains the\n            token and its logprob. Can be None if the scores are not available.\n    \"\"\"\n\n    sequences: list[str]\n    scores: list[list[list[tuple[str, float]]]] | None = None\n\n\n@dataclass\nclass SingleGenerativeModelOutput:docs\n    \"\"\"A single output of a generative model.\n\n    Attributes:\n        sequence:\n            The generated sequence.\n        scores:\n            The scores of the sequence. This is an array of shape (num_tokens,\n            num_logprobs, 2), where the last dimension contains the token and its\n            logprob. Can be None if the scores are not available.\n    \"\"\"\n\n    sequence: str\n    scores: list[list[tuple[str, float]]] | None = None\n\n\n@dataclass\nclass HFModelInfo:docs\n    \"\"\"Information about a Hugging Face model.\n\n    Attributes:\n        pipeline_tag:\n            The pipeline tag of the model.\n        tags:\n            The other tags of the model.\n        adapter_base_model_id:\n            The model ID of the base model if the model is an adapter model. Can be None\n            if the model is not an adapter model.\n    \"\"\"\n\n    pipeline_tag: str\n    tags: list[str]\n    adapter_base_model_id: str | None\n</code></pre>"},{"location":"api/scandeval/enums/","title":"scandeval.enums","text":"scandeval.enums<p> source module scandeval.enums </p> <p>Enums used in the project.</p> <p> Classes </p> <ul> <li> <p>AutoStrEnum \u2014 StrEnum where auto() returns the field name in lower case.</p> </li> <li> <p>Device \u2014 The compute device to use for the evaluation.</p> </li> <li> <p>Framework \u2014 The framework of a model.</p> </li> <li> <p>ModelType \u2014 The type of a model.</p> </li> <li> <p>DataType \u2014 The data type of the model weights.</p> </li> <li> <p>BatchingPreference \u2014 The preference for batching.</p> </li> </ul> <p> source enum AutoStrEnum() </p> <p><p>Bases : str, Enum</p></p> <p>StrEnum where auto() returns the field name in lower case.</p> <p> source enum Device() </p> <p><p>Bases : AutoStrEnum</p></p> <p>The compute device to use for the evaluation.</p> <p> Attributes </p> <ul> <li> <p>CPU \u2014</p> <p>CPU device.</p> </li> <li> <p>MPS \u2014</p> <p>MPS GPU, used in M-series MacBooks.</p> </li> <li> <p>CUDA \u2014</p> <p>CUDA GPU, used with NVIDIA GPUs.</p> </li> </ul> <p> source enum Framework() </p> <p><p>Bases : AutoStrEnum</p></p> <p>The framework of a model.</p> <p> Attributes </p> <ul> <li> <p>PYTORCH \u2014</p> <p>PyTorch framework.</p> </li> <li> <p>JAX \u2014</p> <p>JAX framework.</p> </li> <li> <p>API \u2014</p> <p>Accessible via an API.</p> </li> <li> <p>HUMAN \u2014</p> <p>Human evaluator.</p> </li> </ul> <p> source enum ModelType() </p> <p><p>Bases : AutoStrEnum</p></p> <p>The type of a model.</p> <p> Attributes </p> <ul> <li> <p>FRESH \u2014</p> <p>Randomly initialised Hugging Face model.</p> </li> <li> <p>HF_HUB_ENCODER \u2014</p> <p>Hugging Face encoder model from the Hub.</p> </li> <li> <p>HF_HUB_GENERATIVE \u2014</p> <p>Hugging Face generative model from the Hub.</p> </li> <li> <p>API \u2014</p> <p>Model accessed through an API.</p> </li> <li> <p>HUMAN \u2014</p> <p>Human evaluator.</p> </li> </ul> <p> source enum DataType() </p> <p><p>Bases : AutoStrEnum</p></p> <p>The data type of the model weights.</p> <p> Attributes </p> <ul> <li> <p>FP32 \u2014</p> <p>32-bit floating point.</p> </li> <li> <p>FP16 \u2014</p> <p>16-bit floating point.</p> </li> <li> <p>BF16 \u2014</p> <p>16-bit bfloat.</p> </li> </ul> <p> source enum BatchingPreference() </p> <p><p>Bases : AutoStrEnum</p></p> <p>The preference for batching.</p> <p> Attributes </p> <ul> <li> <p>NO_PREFERENCE \u2014</p> <p>No preference for batching.</p> </li> <li> <p>SINGLE_SAMPLE \u2014</p> <p>Single sample batching.</p> </li> <li> <p>ALL_AT_ONCE \u2014</p> <p>All samples at once batching.</p> </li> </ul>"},{"location":"src/scandeval/enums/","title":"scandeval.enums","text":"scandeval.enums<p> docs module scandeval.enums </p> <pre><code>\"\"\"Enums used in the project.\"\"\"\n\nfrom enum import Enum, auto\n\n\nclass AutoStrEnum(str, Enum):docs\n    \"\"\"StrEnum where auto() returns the field name in lower case.\"\"\"\n\n    @staticmethod\n    def _generate_next_value_(\n        name: str, start: int, count: int, last_values: list\n    ) -&gt; str:\n        return name.lower()\n\n\nclass Device(AutoStrEnum):docs\n    \"\"\"The compute device to use for the evaluation.\n\n    Attributes:\n        CPU:\n            CPU device.\n        MPS:\n            MPS GPU, used in M-series MacBooks.\n        CUDA:\n            CUDA GPU, used with NVIDIA GPUs.\n    \"\"\"\n\n    CPU = auto()\n    MPS = auto()\n    CUDA = auto()\n\n\nclass Framework(AutoStrEnum):docs\n    \"\"\"The framework of a model.\n\n    Attributes:\n        PYTORCH:\n            PyTorch framework.\n        JAX:\n            JAX framework.\n        API:\n            Accessible via an API.\n        HUMAN:\n            Human evaluator.\n    \"\"\"\n\n    PYTORCH = auto()\n    JAX = auto()\n    API = auto()\n    HUMAN = auto()\n\n\nclass ModelType(AutoStrEnum):docs\n    \"\"\"The type of a model.\n\n    Attributes:\n        FRESH:\n            Randomly initialised Hugging Face model.\n        HF_HUB_ENCODER:\n            Hugging Face encoder model from the Hub.\n        HF_HUB_GENERATIVE:\n            Hugging Face generative model from the Hub.\n        API:\n            Model accessed through an API.\n        HUMAN:\n            Human evaluator.\n    \"\"\"\n\n    FRESH = auto()\n    HF_HUB_ENCODER = auto()\n    HF_HUB_GENERATIVE = auto()\n    API = auto()\n    HUMAN = auto()\n\n\nclass DataType(AutoStrEnum):docs\n    \"\"\"The data type of the model weights.\n\n    Attributes:\n        FP32:\n            32-bit floating point.\n        FP16:\n            16-bit floating point.\n        BF16:\n            16-bit bfloat.\n    \"\"\"\n\n    FP32 = auto()\n    FP16 = auto()\n    BF16 = auto()\n\n\nclass BatchingPreference(AutoStrEnum):docs\n    \"\"\"The preference for batching.\n\n    Attributes:\n        NO_PREFERENCE:\n            No preference for batching.\n        SINGLE_SAMPLE:\n            Single sample batching.\n        ALL_AT_ONCE:\n            All samples at once batching.\n    \"\"\"\n\n    NO_PREFERENCE = auto()\n    SINGLE_SAMPLE = auto()\n    ALL_AT_ONCE = auto()\n</code></pre>"},{"location":"api/scandeval/exceptions/","title":"scandeval.exceptions","text":"scandeval.exceptions<p> source module scandeval.exceptions </p> <p>Exceptions to used by other functions.</p> <p> Classes </p> <ul> <li> <p>InvalidBenchmark \u2014 The (model, dataset) combination cannot be benchmarked.</p> </li> <li> <p>InvalidModel \u2014 The model cannot be benchmarked on any datasets.</p> </li> <li> <p>HuggingFaceHubDown \u2014 The Hugging Face Hub seems to be down.</p> </li> <li> <p>NoInternetConnection \u2014 There seems to be no internet connection.</p> </li> <li> <p>NaNValueInModelOutput \u2014 There is a NaN value in the model output.</p> </li> <li> <p>FlashAttentionNotInstalled \u2014 The <code>flash-attn</code> package has not been installed.</p> </li> <li> <p>NeedsExtraInstalled \u2014 The evaluation requires extra to be installed.</p> </li> <li> <p>NeedsManualDependency \u2014 The evaluation requires a dependency to be manually installed.</p> </li> <li> <p>NeedsAdditionalArgument \u2014 The evaluation requires additional arguments to the <code>scandeval</code> command.</p> </li> <li> <p>NeedsEnvironmentVariable \u2014 The evaluation requires an environment variable to be set.</p> </li> </ul> <p> source class InvalidBenchmark() </p> <p><p>Bases : Exception</p></p> <p>The (model, dataset) combination cannot be benchmarked.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to display.</p> </li> </ul> <p> source class InvalidModel() </p> <p><p>Bases : Exception</p></p> <p>The model cannot be benchmarked on any datasets.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to display.</p> </li> </ul> <p> source class HuggingFaceHubDown() </p> <p><p>Bases : Exception</p></p> <p>The Hugging Face Hub seems to be down.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to display.</p> </li> </ul> <p> source class NoInternetConnection() </p> <p><p>Bases : Exception</p></p> <p>There seems to be no internet connection.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to display.</p> </li> </ul> <p> source class NaNValueInModelOutput() </p> <p><p>Bases : Exception</p></p> <p>There is a NaN value in the model output.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to display.</p> </li> </ul> <p> source class FlashAttentionNotInstalled() </p> <p><p>Bases : Exception</p></p> <p>The <code>flash-attn</code> package has not been installed.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to display.</p> </li> </ul> <p> source class NeedsExtraInstalled() </p> <p><p>Bases : InvalidModel</p></p> <p>The evaluation requires extra to be installed.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>extra :  str \u2014</p> <p>The extra that needs to be installed.</p> </li> </ul> <p> source class NeedsManualDependency() </p> <p><p>Bases : InvalidModel</p></p> <p>The evaluation requires a dependency to be manually installed.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>package :  str \u2014</p> <p>The package that needs to be manually installed.</p> </li> </ul> <p> source class NeedsAdditionalArgument(script_argument: str, run_with_cli: bool) </p> <p><p>Bases : InvalidModel</p></p> <p>The evaluation requires additional arguments to the <code>scandeval</code> command.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>cli_argument :  str \u2014</p> <p>The argument that needs to be passed to the <code>scandeval</code> command.</p> </li> <li> <p>script_argument :  str \u2014</p> <p>The argument that needs to be passed to the <code>Benchmarker</code> class.</p> </li> <li> <p>run_with_cli :  bool \u2014</p> <p>Whether the benchmark is being run with the CLI.</p> </li> </ul> <p> source class NeedsEnvironmentVariable() </p> <p><p>Bases : InvalidModel</p></p> <p>The evaluation requires an environment variable to be set.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>env_var :  str \u2014</p> <p>The environment variable that needs to be set.</p> </li> </ul>"},{"location":"src/scandeval/exceptions/","title":"scandeval.exceptions","text":"scandeval.exceptions<p> docs module scandeval.exceptions </p> <pre><code>\"\"\"Exceptions to used by other functions.\"\"\"\n\n\nclass InvalidBenchmark(Exception):docs\n    \"\"\"The (model, dataset) combination cannot be benchmarked.\"\"\"\n\n    def __init__(\n        self, message: str = \"This model cannot be benchmarked on the given dataset.\"\n    ):\n        \"\"\"Initialize the exception.\n\n        Args:\n            message:\n                The message to display.\n        \"\"\"\n        self.message = message\n        super().__init__(self.message)\n\n\nclass InvalidModel(Exception):docs\n    \"\"\"The model cannot be benchmarked on any datasets.\"\"\"\n\n    def __init__(\n        self, message: str = \"The model cannot be benchmarked on any datasets.\"\n    ):\n        \"\"\"Initialize the exception.\n\n        Args:\n            message:\n                The message to display.\n        \"\"\"\n        self.message = message\n        super().__init__(self.message)\n\n\nclass HuggingFaceHubDown(Exception):docs\n    \"\"\"The Hugging Face Hub seems to be down.\"\"\"\n\n    def __init__(self, message: str = \"The Hugging Face Hub is currently down.\"):\n        \"\"\"Initialize the exception.\n\n        Args:\n            message:\n                The message to display.\n        \"\"\"\n        self.message = message\n        super().__init__(self.message)\n\n\nclass NoInternetConnection(Exception):docs\n    \"\"\"There seems to be no internet connection.\"\"\"\n\n    def __init__(self, message: str = \"There is currently no internet connection.\"):\n        \"\"\"Initialize the exception.\n\n        Args:\n            message:\n                The message to display.\n        \"\"\"\n        self.message = message\n        super().__init__(self.message)\n\n\nclass NaNValueInModelOutput(Exception):docs\n    \"\"\"There is a NaN value in the model output.\"\"\"\n\n    def __init__(self, message: str = \"There is a NaN value in the model output.\"):\n        \"\"\"Initialize the exception.\n\n        Args:\n            message:\n                The message to display.\n        \"\"\"\n        self.message = message\n        super().__init__(self.message)\n\n\nclass FlashAttentionNotInstalled(Exception):docs\n    \"\"\"The `flash-attn` package has not been installed.\"\"\"\n\n    def __init__(\n        self,\n        message: str = (\n            \"The model you are trying to load requires Flash Attention. To use Flash \"\n            \"Attention, please install the `flash-attn` package, which can be done by \"\n            \"running `pip install -U wheel &amp;&amp; FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE \"\n            \"pip install flash-attn --no-build-isolation`.\"\n        ),\n    ):\n        \"\"\"Initialize the exception.\n\n        Args:\n            message:\n                The message to display.\n        \"\"\"\n        self.message = message\n        super().__init__(self.message)\n\n\nclass NeedsExtraInstalled(InvalidModel):docs\n    \"\"\"The evaluation requires extra to be installed.\"\"\"\n\n    def __init__(self, extra: str):\n        \"\"\"Initialize the exception.\n\n        Args:\n            extra:\n                The extra that needs to be installed.\n        \"\"\"\n        self.extra = extra\n        self.message = (\n            f\"The model you are trying to load requires the `{extra}` extra to be \"\n            f\"installed. To install the `{extra}` extra, please run `pip install \"\n            f\"scandeval[{extra}]` or `pip install scandeval[all]`.\"\n        )\n        super().__init__(self.message)\n\n\nclass NeedsManualDependency(InvalidModel):docs\n    \"\"\"The evaluation requires a dependency to be manually installed.\"\"\"\n\n    def __init__(self, package: str):\n        \"\"\"Initialize the exception.\n\n        Args:\n            package:\n                The package that needs to be manually installed.\n        \"\"\"\n        self.package = package\n        self.message = (\n            f\"The model you are trying to load requires the `{package}` package to be \"\n            f\"installed - please run `pip install {package}` and try again.\"\n        )\n        super().__init__(self.message)\n\n\nclass NeedsAdditionalArgument(InvalidModel):docs\n    \"\"\"The evaluation requires additional arguments to the `scandeval` command.\"\"\"\n\n    def __init__(self, cli_argument: str, script_argument: str, run_with_cli: bool):\n        \"\"\"Initialize the exception.\n\n        Args:\n            cli_argument:\n                The argument that needs to be passed to the `scandeval` command.\n            script_argument:\n                The argument that needs to be passed to the `Benchmarker` class.\n            run_with_cli:\n                Whether the benchmark is being run with the CLI.\n        \"\"\"\n        self.cli_argument = cli_argument\n        self.script_argument = script_argument\n        if run_with_cli:\n            self.message = (\n                f\"The model you are trying to load requires the `{cli_argument}` \"\n                \"argument to be passed to the `scandeval` command. Please pass the \"\n                \"argument and try again.\"\n            )\n        else:\n            self.message = (\n                f\"The model you are trying to load requires the `{script_argument}` \"\n                \"argument  to be passed to the `Benchmarker` class. Please pass the \"\n                \"argument and try again.\"\n            )\n        super().__init__(self.message)\n\n\nclass NeedsEnvironmentVariable(InvalidModel):docs\n    \"\"\"The evaluation requires an environment variable to be set.\"\"\"\n\n    def __init__(self, env_var: str):\n        \"\"\"Initialize the exception.\n\n        Args:\n            env_var:\n                The environment variable that needs to be set.\n        \"\"\"\n        self.env_var = env_var\n        self.message = (\n            f\"The model you are trying to load requires the `{env_var}` environment \"\n            \"variable to be set. Please set the environment variable and try again.\"\n        )\n        super().__init__(self.message)\n</code></pre>"},{"location":"api/scandeval/finetuning/","title":"scandeval.finetuning","text":"scandeval.finetuning<p> source module scandeval.finetuning </p> <p>Functions related to the finetuning of models.</p> <p> Functions </p> <ul> <li> <p>finetune \u2014 Evaluate a model on a dataset through finetuning.</p> </li> <li> <p>finetune_single_iteration \u2014 Run a single iteration of a benchmark.</p> </li> <li> <p>get_training_args \u2014 Get the training arguments for the current iteration.</p> </li> </ul> <p> source finetune(model: BenchmarkModule, datasets: list[DatasetDict], model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 list[dict[str, float]] </p> <p>Evaluate a model on a dataset through finetuning.</p> <p> Parameters </p> <ul> <li> <p>model :  BenchmarkModule \u2014</p> <p>The model to evaluate.</p> </li> <li> <p>datasets :  list[DatasetDict] \u2014</p> <p>The datasets to use for training and evaluation.</p> </li> <li> <p>model_config :  ModelConfig \u2014</p> <p>The configuration of the model.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[dict[str, float]] \u2014 A list of dicts containing the scores for each metric for each iteration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source finetune_single_iteration(model: BenchmarkModule | None, dataset: DatasetDict, iteration_idx: int, training_args: TrainingArguments, model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 dict[str, float] </p> <p>Run a single iteration of a benchmark.</p> <p> Parameters </p> <ul> <li> <p>model :  BenchmarkModule | None \u2014</p> <p>The model to use in the benchmark. If None then a new model will be loaded.</p> </li> <li> <p>dataset :  DatasetDict \u2014</p> <p>The dataset to use for training and evaluation.</p> </li> <li> <p>iteration_idx :  int \u2014</p> <p>The index of the iteration.</p> </li> <li> <p>training_args :  TrainingArguments \u2014</p> <p>The training arguments.</p> </li> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] \u2014 The scores for the test dataset.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>e</p> </li> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source get_training_args(benchmark_config: BenchmarkConfig, model_config: ModelConfig, iteration_idx: int, dtype: DataType, batch_size: int | None = None) \u2192 TrainingArguments </p> <p>Get the training arguments for the current iteration.</p> <p> Parameters </p> <ul> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>iteration_idx :  int \u2014</p> <p>The index of the current iteration. This is only used to generate a unique random seed for the current iteration.</p> </li> <li> <p>dtype :  DataType \u2014</p> <p>The data type to use for the model weights.</p> </li> <li> <p>batch_size :  int | None \u2014</p> <p>The batch size to use for the current iteration, or None if the batch size in the benchmark config should be used.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>TrainingArguments \u2014 The training arguments for the current iteration.</p> </li> </ul>"},{"location":"src/scandeval/finetuning/","title":"scandeval.finetuning","text":"scandeval.finetuning<p> docs module scandeval.finetuning </p> <pre><code>\"\"\"Functions related to the finetuning of models.\"\"\"\n\nimport importlib.util\nimport logging\nimport sys\nimport typing as t\nfrom functools import partial\n\nimport torch\nfrom datasets import DatasetDict\nfrom tqdm.auto import tqdm\nfrom transformers import (\n    EarlyStoppingCallback,\n    IntervalStrategy,\n    PrinterCallback,\n    ProgressCallback,\n    TrainingArguments,\n)\nfrom transformers.trainer import OptimizerNames\n\nfrom .benchmark_modules import BenchmarkModule\nfrom .callbacks import NeverLeaveProgressCallback\nfrom .enums import DataType\nfrom .exceptions import InvalidBenchmark, NaNValueInModelOutput\nfrom .model_loading import load_model\nfrom .utils import block_terminal_output, clear_memory, enforce_reproducibility\n\nif t.TYPE_CHECKING:\n    from .data_models import BenchmarkConfig, DatasetConfig, ModelConfig\n\nlogger = logging.getLogger(\"scandeval\")\n\n\ndef finetune(docs\n    model: BenchmarkModule,\n    datasets: list[DatasetDict],\n    model_config: \"ModelConfig\",\n    dataset_config: \"DatasetConfig\",\n    benchmark_config: \"BenchmarkConfig\",\n) -&gt; list[dict[str, float]]:\n    \"\"\"Evaluate a model on a dataset through finetuning.\n\n    Args:\n        model:\n            The model to evaluate.\n        datasets:\n            The datasets to use for training and evaluation.\n        model_config:\n            The configuration of the model.\n        dataset_config:\n            The dataset configuration.\n        benchmark_config:\n            The benchmark configuration.\n\n    Returns:\n        A list of dicts containing the scores for each metric for each iteration.\n    \"\"\"\n    # Set the data type to use for the model weights\n    using_cuda = benchmark_config.device == torch.device(\"cuda\")\n    if using_cuda and torch.cuda.is_bf16_supported():\n        dtype = DataType.BF16\n    elif using_cuda:\n        dtype = DataType.FP16\n    else:\n        dtype = DataType.FP32\n\n    bs: int = benchmark_config.batch_size\n    scores: list[dict[str, float]] = list()\n    for idx in tqdm(\n        iterable=range(benchmark_config.num_iterations),\n        desc=\"Benchmarking\",\n        disable=not benchmark_config.progress_bar,\n    ):\n        # Set variable that tracks whether we need to initialize new models in\n        # the single iteration call\n        model_already_initialized = idx == 0\n\n        # Run a loop here to deal with automatic reduction of batch size\n        while True:\n            # Clear GPU memory\n            if not model_already_initialized:\n                try:\n                    del model\n                except UnboundLocalError:\n                    pass\n                clear_memory()\n\n            try:\n                # Re-block terminal output, as it gets unblocked by the `transformers`\n                # package before training\n                block_terminal_output()\n\n                training_args = get_training_args(\n                    benchmark_config=benchmark_config,\n                    model_config=model_config,\n                    iteration_idx=idx,\n                    dtype=dtype,\n                    batch_size=bs,\n                )\n\n                itr_scores = finetune_single_iteration(\n                    model=model if model_already_initialized else None,\n                    dataset=datasets[idx],\n                    iteration_idx=idx,\n                    training_args=training_args,\n                    model_config=model_config,\n                    dataset_config=dataset_config,\n                    benchmark_config=benchmark_config,\n                )\n\n                scores.append(itr_scores)\n                logger.debug(f\"Test scores for iteration {idx}: {itr_scores}\")\n\n                break\n\n            # NaN values can appear in the model output when using mixed precision, as\n            # the hidden states get overflowed. In this case we try to disable mixed\n            # precision and try again.\n            except NaNValueInModelOutput:\n                if dtype != DataType.FP32:\n                    dtype = DataType.FP32\n                    model_already_initialized = False\n                    logger.debug(\n                        \"NaN value detected in model outputs while using mixed \"\n                        \"precision. Retrying with full fp32 precision.\"\n                    )\n                else:\n                    raise InvalidBenchmark(\n                        \"NaN value detected in model outputs, even with mixed \"\n                        \"precision disabled.\"\n                    )\n\n            except Exception as e:\n                if \"CUDA\" not in str(e) and \"out of memory\" not in str(e):\n                    raise InvalidBenchmark(str(e))\n\n                if bs &lt;= 1:\n                    msg = \"Could not benchmark the model, even with a batch size of 1!\"\n                    if \"MPS\" in str(e):\n                        msg += (\n                            \" As you are using MPS, you can try running the evaluation \"\n                            \"with the `PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0` \"\n                            \"environment variable set, as this removes the upper bound \"\n                            \"on the memory usage.\"\n                        )\n                    raise InvalidBenchmark(msg)\n\n                model_already_initialized = False\n\n                bs //= 2\n                logger.debug(f\"Reduced batch size to {bs}\")\n\n    return scores\n\n\ndef finetune_single_iteration(docs\n    model: BenchmarkModule | None,\n    dataset: DatasetDict,\n    iteration_idx: int,\n    training_args: TrainingArguments,\n    model_config: \"ModelConfig\",\n    dataset_config: \"DatasetConfig\",\n    benchmark_config: \"BenchmarkConfig\",\n) -&gt; dict[str, float]:\n    \"\"\"Run a single iteration of a benchmark.\n\n    Args:\n        model:\n            The model to use in the benchmark. If None then a new model will be loaded.\n        dataset:\n            The dataset to use for training and evaluation.\n        iteration_idx:\n            The index of the iteration.\n        training_args:\n            The training arguments.\n        model_config:\n            The model configuration.\n        dataset_config:\n            The dataset configuration.\n        benchmark_config:\n            The benchmark configuration.\n\n    Returns:\n        The scores for the test dataset.\n    \"\"\"\n    # Set random seeds to enforce reproducibility of the randomly initialised\n    # weights\n    seed = 4242 + iteration_idx\n    enforce_reproducibility(framework=model_config.framework, seed=seed)\n\n    if model is None:\n        model = load_model(\n            model_config=model_config,\n            dataset_config=dataset_config,\n            benchmark_config=benchmark_config,\n        )\n\n    trainer = model.trainer_class(\n        model=model.get_pytorch_module(),\n        tokenizer=model.get_tokenizer(),\n        args=training_args,\n        train_dataset=dataset[\"train\"],\n        eval_dataset=dataset[\"val\"],\n        compute_metrics=partial(\n            model.compute_metrics, id2label=dataset_config.id2label\n        ),\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n        data_collator=model.data_collator,\n    )\n\n    if not benchmark_config.verbose:\n\n        def no_logging(logs: dict[str, float]) -&gt; None:\n            return\n\n        trainer.log = no_logging\n\n    # Re-block terminal output, as it gets unblocked by the `transformers`\n    # package before training\n    block_terminal_output()\n\n    # Sort out callbacks. We remove the callbacks that are producing unnecessary\n    # output, to avoid cluttering the terminal output\n    if not benchmark_config.verbose:\n        trainer.remove_callback(PrinterCallback)\n    trainer.remove_callback(ProgressCallback)\n    if benchmark_config.progress_bar:\n        trainer.add_callback(NeverLeaveProgressCallback)\n\n    try:\n        trainer.train()\n        with torch.inference_mode():\n            try:\n                test_scores = trainer.evaluate(\n                    eval_dataset=dataset[\"test\"],\n                    orig_eval_dataset=dataset[\"original_test\"],\n                    metric_key_prefix=\"test\",\n                )\n            except TypeError:\n                test_scores = trainer.evaluate(\n                    eval_dataset=dataset[\"test\"], metric_key_prefix=\"test\"\n                )\n        return test_scores\n\n    except NaNValueInModelOutput as e:\n        del trainer\n        del model\n        clear_memory()\n        raise e\n\n    except (RuntimeError, ValueError, IndexError) as e:\n        raise InvalidBenchmark(str(e))\n\n\ndef get_training_args(docs\n    benchmark_config: \"BenchmarkConfig\",\n    model_config: \"ModelConfig\",\n    iteration_idx: int,\n    dtype: DataType,\n    batch_size: int | None = None,\n) -&gt; TrainingArguments:\n    \"\"\"Get the training arguments for the current iteration.\n\n    Args:\n        benchmark_config:\n            The benchmark configuration.\n        model_config:\n            The model configuration.\n        iteration_idx:\n            The index of the current iteration. This is only used to generate a\n            unique random seed for the current iteration.\n        dtype:\n            The data type to use for the model weights.\n        batch_size:\n            The batch size to use for the current iteration, or None if the batch size\n            in the benchmark config should be used.\n\n    Returns:\n        The training arguments for the current iteration.\n    \"\"\"\n    # Set the logging strategy\n    if benchmark_config.verbose:\n        logging_strategy = IntervalStrategy.STEPS\n    else:\n        logging_strategy = IntervalStrategy.NO\n\n    # Set seed variable\n    seed = 4242 + iteration_idx\n\n    if batch_size is None:\n        batch_size = benchmark_config.batch_size\n\n    if (\n        benchmark_config.device == torch.device(\"cuda\")\n        and importlib.util.find_spec(\"bitsandbytes\") is not None\n    ):\n        optimizer = OptimizerNames.ADAMW_8BIT\n    else:\n        optimizer = OptimizerNames.ADAMW_TORCH\n\n    training_args = TrainingArguments(\n        output_dir=model_config.model_cache_dir,\n        evaluation_strategy=IntervalStrategy.STEPS,\n        logging_strategy=logging_strategy,\n        save_strategy=IntervalStrategy.STEPS,\n        eval_steps=30,\n        logging_steps=30,\n        save_steps=30,\n        max_steps=1 if hasattr(sys, \"_called_from_test\") else 10_000,\n        use_cpu=benchmark_config.device == torch.device(\"cpu\"),\n        report_to=[],\n        save_total_limit=1,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        learning_rate=2e-5,\n        warmup_ratio=0.01,\n        gradient_accumulation_steps=32 // batch_size,\n        load_best_model_at_end=True,\n        optim=optimizer,\n        seed=seed,\n        fp16=dtype == DataType.FP16,\n        bf16=dtype == DataType.BF16,\n        disable_tqdm=not benchmark_config.progress_bar,\n        ddp_find_unused_parameters=False,\n        save_safetensors=False,\n    )\n\n    # TEMP: Use only 1 GPU for now for finetuning\n    if benchmark_config.device == torch.device(\"cuda\"):\n        training_args._n_gpu = 1\n\n    return training_args\n</code></pre>"},{"location":"api/scandeval/generation/","title":"scandeval.generation","text":"scandeval.generation<p> source module scandeval.generation </p> <p>Functions related to text generation of models.</p> <p> Functions </p> <ul> <li> <p>generate \u2014 Evaluate a model on a dataset through generation.</p> </li> <li> <p>generate_single_iteration \u2014 Evaluate a model on a dataset in a single iteration through generation.</p> </li> <li> <p>debug_log \u2014 Log inputs and outputs for debugging purposes.</p> </li> </ul> <p> source generate(model: BenchmarkModule, datasets: list[DatasetDict], model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 list[dict[str, float]] </p> <p>Evaluate a model on a dataset through generation.</p> <p> Parameters </p> <ul> <li> <p>model :  BenchmarkModule \u2014</p> <p>The model to evaluate.</p> </li> <li> <p>datasets :  list[DatasetDict] \u2014</p> <p>The datasets to evaluate on.</p> </li> <li> <p>model_config :  ModelConfig \u2014</p> <p>The configuration of the model.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The configuration of the benchmark.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[dict[str, float]] \u2014 A list of dictionaries containing the test scores.</p> </li> </ul> <p> source generate_single_iteration(dataset: Dataset, model: BenchmarkModule, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig, cache: ModelCache) \u2192 dict[str, float] </p> <p>Evaluate a model on a dataset in a single iteration through generation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  Dataset \u2014</p> <p>The dataset to evaluate on.</p> </li> <li> <p>model :  BenchmarkModule \u2014</p> <p>The model to evaluate.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The configuration of the benchmark.</p> </li> <li> <p>cache :  ModelCache \u2014</p> <p>The model output cache.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] \u2014 A list of dictionaries containing the scores for each metric.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError</p> </li> </ul> <p> source debug_log(batch: dict[str, t.Any], extracted_labels: list[dict | str | list[str]], dataset_config: DatasetConfig) \u2192 None </p> <p>Log inputs and outputs for debugging purposes.</p> <p> Parameters </p> <ul> <li> <p>batch :  dict[str, t.Any] \u2014</p> <p>The batch of examples to evaluate on.</p> </li> <li> <p>extracted_labels :  list[dict | str | list[str]] \u2014</p> <p>The extracted labels from the model output.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul>"},{"location":"src/scandeval/generation/","title":"scandeval.generation","text":"scandeval.generation<p> docs module scandeval.generation </p> <pre><code>\"\"\"Functions related to text generation of models.\"\"\"\n\nimport logging\nimport sys\nimport typing as t\nfrom pathlib import Path\n\nimport more_itertools as mit\nfrom datasets import Dataset, DatasetDict\nfrom tqdm.auto import tqdm\n\nfrom .benchmark_modules import BenchmarkModule\nfrom .enums import BatchingPreference\nfrom .exceptions import InvalidBenchmark\nfrom .model_cache import (\n    ModelCache,\n    load_cached_model_outputs,\n    split_dataset_into_cached_and_non_cached,\n)\nfrom .utils import clear_memory\n\nif t.TYPE_CHECKING:\n    from .data_models import BenchmarkConfig, DatasetConfig, ModelConfig\n\nlogger = logging.getLogger(\"scandeval\")\n\n\ndef generate(docs\n    model: \"BenchmarkModule\",\n    datasets: list[DatasetDict],\n    model_config: \"ModelConfig\",\n    dataset_config: \"DatasetConfig\",\n    benchmark_config: \"BenchmarkConfig\",\n) -&gt; list[dict[str, float]]:\n    \"\"\"Evaluate a model on a dataset through generation.\n\n    Args:\n        model:\n            The model to evaluate.\n        datasets:\n            The datasets to evaluate on.\n        model_config:\n            The configuration of the model.\n        benchmark_config:\n            The configuration of the benchmark.\n        dataset_config:\n            The configuration of the dataset.\n\n    Returns:\n        A list of dictionaries containing the test scores.\n    \"\"\"\n    # Set up the name of the model output cache. If we are testing then we save the\n    # model outputs to a different cache and ensure that that cache is deleted before\n    # the next test, to ensure that the tests are independent of each other\n    if benchmark_config.debug:\n        model_cache_dir = Path.cwd()\n    else:\n        model_cache_dir = Path(model_config.model_cache_dir)\n    if hasattr(sys, \"_called_from_test\"):\n        cache_name = f\"{dataset_config.name}-model-outputs-test.json\"\n        (model_cache_dir / cache_name).unlink(missing_ok=True)\n    elif benchmark_config.debug:\n        cache_name = f\"{model_config.model_id}-{dataset_config.name}-model-outputs.json\"\n    else:\n        cache_name = f\"{dataset_config.name}-model-outputs.json\"\n\n    cache = ModelCache(\n        model_cache_dir=model_cache_dir,\n        cache_name=cache_name,\n        max_generated_tokens=dataset_config.max_generated_tokens,\n    )\n\n    scores: list[dict[str, float]] = list()\n    for idx in tqdm(\n        iterable=range(benchmark_config.num_iterations),\n        desc=\"Benchmarking\",\n        disable=not benchmark_config.progress_bar,\n    ):\n        test_scores = generate_single_iteration(\n            model=model,\n            dataset=datasets[idx][\"test\"],\n            cache=cache,\n            dataset_config=dataset_config,\n            benchmark_config=benchmark_config,\n        )\n\n        logger.debug(f\"Test scores for iteration {idx}: {test_scores}\")\n        scores.append(test_scores)\n        clear_memory()\n\n    if not benchmark_config.debug:\n        cache.remove()\n\n    return scores\n\n\ndef generate_single_iteration(docs\n    dataset: Dataset,\n    model: \"BenchmarkModule\",\n    dataset_config: \"DatasetConfig\",\n    benchmark_config: \"BenchmarkConfig\",\n    cache: ModelCache,\n) -&gt; dict[str, float]:\n    \"\"\"Evaluate a model on a dataset in a single iteration through generation.\n\n    Args:\n        dataset:\n            The dataset to evaluate on.\n        model:\n            The model to evaluate.\n        dataset_config:\n            The configuration of the dataset.\n        benchmark_config:\n            The configuration of the benchmark.\n        cache:\n            The model output cache.\n\n    Returns:\n        A list of dictionaries containing the scores for each metric.\n    \"\"\"\n    cache.load()\n\n    # Split up the dataset into a cached and non-cached part\n    cached_dataset, non_cached_dataset = split_dataset_into_cached_and_non_cached(\n        dataset=dataset, cache=cache\n    )\n\n    all_preds: list[str] = list()\n\n    if len(non_cached_dataset) &gt; 0:\n        match model.batching_preference:\n            case BatchingPreference.SINGLE_SAMPLE:\n                itr = tqdm(iterable=non_cached_dataset, leave=False)\n            case BatchingPreference.ALL_AT_ONCE:\n                itr = [non_cached_dataset[:]]\n            case _:\n                num_batches = len(non_cached_dataset) // benchmark_config.batch_size\n                if len(non_cached_dataset) % benchmark_config.batch_size != 0:\n                    num_batches += 1\n                itr = tqdm(\n                    iterable=mit.batched(\n                        iterable=non_cached_dataset, n=benchmark_config.batch_size\n                    ),\n                    total=len(non_cached_dataset) // benchmark_config.batch_size,\n                )\n\n        # Generate the completions for the non-cached examples\n        for batch in itr:\n            assert isinstance(batch, dict)\n\n            single_sample_batch = (\n                \"text\" in batch and isinstance(batch[\"text\"], str)\n            ) or (\"messages\" in batch and isinstance(batch[\"messages\"][0], dict))\n            if single_sample_batch:\n                batch = {key: [value] for key, value in batch.items()}\n\n            model_output = model.generate(inputs=batch)\n            extracted_labels = model.extract_labels_from_generation(\n                input_batch=batch, model_output=model_output\n            )\n\n            # Extended logging if we are running in debug mode\n            if benchmark_config.debug:\n                debug_log(\n                    batch=batch,\n                    extracted_labels=extracted_labels,  # type: ignore[arg-type]\n                    dataset_config=dataset_config,\n                )\n\n            cache.add_to_cache(model_inputs=batch, model_output=model_output)\n            all_preds.extend(extracted_labels)\n\n            # If we are debugging then we save the cache often, but since this makes\n            # evaluation slower, we do not do this by default\n            if benchmark_config.debug:\n                cache.save()\n\n        if isinstance(itr, tqdm):\n            itr.close()\n\n        # Store the cache to disk\n        cache.save()\n\n    # Fetch the cached predictions for the cached examples\n    if len(cached_dataset) &gt; 0:\n        model_output = load_cached_model_outputs(\n            cached_dataset=cached_dataset, cache=cache\n        )\n        extracted_labels = model.extract_labels_from_generation(\n            input_batch=cached_dataset[:], model_output=model_output\n        )\n        all_preds.extend(extracted_labels)\n\n    if \"label\" in non_cached_dataset.column_names:\n        ground_truth = [\n            label.lower() if isinstance(label, str) else label\n            for label in non_cached_dataset[\"label\"] + cached_dataset[\"label\"]\n        ]\n    elif \"labels\" in non_cached_dataset.column_names:\n        ground_truth = [\n            [label.lower() if isinstance(label, str) else label for label in label_list]\n            for label_list in non_cached_dataset[\"labels\"] + cached_dataset[\"labels\"]\n        ]\n    elif \"target_text\" in non_cached_dataset.column_names:\n        ground_truth = non_cached_dataset[\"target_text\"] + cached_dataset[\"target_text\"]\n    else:\n        raise ValueError(\n            \"The dataset must have either a 'label', 'labels', or 'target_text' column\"\n        )\n\n    itr_scores: dict[str, float] = model.compute_metrics(\n        model_outputs_and_labels=(all_preds, ground_truth),\n        id2label=dataset_config.id2label,\n    )\n\n    return itr_scores\n\n\ndef debug_log(docs\n    batch: dict[str, t.Any],\n    extracted_labels: list[dict | str | list[str]],\n    dataset_config: \"DatasetConfig\",\n) -&gt; None:\n    \"\"\"Log inputs and outputs for debugging purposes.\n\n    Args:\n        batch:\n            The batch of examples to evaluate on.\n        extracted_labels:\n            The extracted labels from the model output.\n        dataset_config:\n            The configuration of the dataset.\n    \"\"\"\n    match dataset_config.task.supertask:\n        case \"token-classification\":\n            log_msgs = [\"\"]\n            for tokens, predictions, labels in zip(\n                batch[\"tokens\"], extracted_labels, batch[\"labels\"]\n            ):\n                predictions = [tag.upper() for tag in predictions]\n                sample = list(zip(tokens, predictions, labels))\n                log_batches = [\n                    [(\"Tokens: \", \"Predictions: \", \"Labels: \")] + sample[i : i + 10]\n                    for i in range(0, len(sample), 10)\n                ]\n                for log_batch in log_batches:\n                    lengths = [len(max(triple, key=len)) for triple in log_batch]\n                    log_batch = [\n                        [f\"{x:&lt;{length}}\" for x in triple]\n                        for triple, length in zip(log_batch, lengths)\n                    ]\n                    tokens = [triple[0] for triple in log_batch]\n                    predictions = [triple[1] for triple in log_batch]\n                    labels = [triple[2] for triple in log_batch]\n                    log_msgs.append(\n                        \"\\t\".join(tokens)\n                        + \"\\n\"\n                        + \"\\t\".join(predictions)\n                        + \"\\n\"\n                        + \"\\t\".join(labels)\n                    )\n            logger.info(\"\\n\\n\".join(log_msgs))\n            return\n\n        case \"sequence-classification\":\n            labels = [\n                dataset_config.prompt_label_mapping.get(label, label).lower()\n                for label in batch[\"label\"]\n            ]\n\n        case \"question-answering\":\n            extracted_labels = [\n                prediction[\"prediction_text\"]\n                for prediction in extracted_labels\n                if isinstance(prediction, dict)\n            ]\n            labels = [label[\"answers\"][\"text\"][0] for label in batch[\"label\"]]\n\n        case \"text-to-text\":\n            labels = batch[\"target_text\"]\n\n        case _:\n            raise InvalidBenchmark(\n                f\"The supertask '{dataset_config.task.supertask}' is not supported.\"\n            )\n\n    if \"messages\" in batch:\n        input_texts = [messages[-1][\"content\"] for messages in batch[\"messages\"]]\n    else:\n        input_texts = batch[\"text\"]\n\n    for input_text, prediction, label in zip(input_texts, extracted_labels, labels):\n        logger.info(\n            f\"Input: {input_text!r}\\nPrediction: {prediction!r}\\nLabel: {label!r}\"\n        )\n</code></pre>"},{"location":"api/scandeval/human_evaluation/","title":"scandeval.human_evaluation","text":"scandeval.human_evaluation<p> source module scandeval.human_evaluation </p> <p>Gradio app for conducting human evaluation of the tasks.</p> <p> Classes </p> <ul> <li> <p>HumanEvaluator \u2014 An app for evaluating human performance on the ScandEval benchmark.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>main \u2014 Start the Gradio app for human evaluation.</p> </li> </ul> <p> source class HumanEvaluator(title: str, description: str, dummy_model_id: str = 'mistralai/Mistral-7B-v0.1') </p> <p>An app for evaluating human performance on the ScandEval benchmark.</p> <p>Initialize the HumanEvaluator.</p> <p> Parameters </p> <ul> <li> <p>annotator_id :  int \u2014</p> <p>The annotator ID for the evaluation.</p> </li> <li> <p>title :  str \u2014</p> <p>The title of the app.</p> </li> <li> <p>description :  str \u2014</p> <p>The description of the app.</p> </li> <li> <p>dummy_model_id :  str \u2014</p> <p>The model ID to use for generating prompts.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>create_app \u2014 Create the Gradio app for human evaluation.</p> </li> <li> <p>update_dataset_choices \u2014 Update the dataset choices based on the selected language and task.</p> </li> <li> <p>update_dataset \u2014 Update the dataset based on a selected dataset name.</p> </li> <li> <p>add_entity_to_answer \u2014 Add an entity to the answer.</p> </li> <li> <p>reset_entities \u2014 Reset the entities in the answer.</p> </li> <li> <p>submit_answer \u2014 Submit an answer to the dataset.</p> </li> <li> <p>example_to_markdown \u2014 Convert an example to a Markdown string.</p> </li> <li> <p>compute_and_log_scores \u2014 Computes and logs the scores for the dataset.</p> </li> </ul> <p> source method HumanEvaluator.create_app() \u2192 gr.Blocks </p> <p>Create the Gradio app for human evaluation.</p> <p> Returns </p> <ul> <li> <p>gr.Blocks \u2014 The Gradio app for human evaluation.</p> </li> </ul> <p> source method HumanEvaluator.update_dataset_choices(language: str | None, task: str | None) \u2192 gr.Dropdown </p> <p>Update the dataset choices based on the selected language and task.</p> <p> Parameters </p> <ul> <li> <p>language :  str | None \u2014</p> <p>The language selected by the user.</p> </li> <li> <p>task :  str | None \u2014</p> <p>The task selected by the user.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>gr.Dropdown \u2014 A list of dataset names that match the selected language and task.</p> </li> </ul> <p> source method HumanEvaluator.update_dataset(dataset_name: str, iteration: int) \u2192 tuple[gr.Markdown, gr.Markdown, gr.Dropdown, gr.Textbox, gr.Button, gr.Button, gr.Textbox, gr.Button] </p> <p>Update the dataset based on a selected dataset name.</p> <p> Parameters </p> <ul> <li> <p>dataset_name :  str \u2014</p> <p>The dataset name selected by the user.</p> </li> <li> <p>iteration :  int \u2014</p> <p>The iteration index of the datasets to evaluate.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[gr.Markdown, gr.Markdown, gr.Dropdown, gr.Textbox, gr.Button, gr.Button, gr.Textbox, gr.Button] \u2014 A tuple (task_examples, question, entity_type, entity, entity_add_button, entity_reset_button, answer, submit_button) for the selected dataset.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method HumanEvaluator.add_entity_to_answer(question: str, entity_type: str, entity: str, answer: str) \u2192 tuple[gr.Textbox, gr.Textbox] </p> <p>Add an entity to the answer.</p> <p> Parameters </p> <ul> <li> <p>question :  str \u2014</p> <p>The current question.</p> </li> <li> <p>entity_type :  str \u2014</p> <p>The entity type selected by the user.</p> </li> <li> <p>entity :  str \u2014</p> <p>The entity provided by the user.</p> </li> <li> <p>answer :  str \u2014</p> <p>The current answer.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[gr.Textbox, gr.Textbox] \u2014 A tuple (entity, answer) with a (blank) entity and answer.</p> </li> </ul> <p> source method HumanEvaluator.reset_entities() \u2192 gr.Textbox </p> <p>Reset the entities in the answer.</p> <p> Returns </p> <ul> <li> <p>gr.Textbox \u2014 A blank answer.</p> </li> </ul> <p> source method HumanEvaluator.submit_answer(dataset_name: str, question: str, answer: str, annotator_id: int) \u2192 tuple[str, str] </p> <p>Submit an answer to the dataset.</p> <p> Parameters </p> <ul> <li> <p>dataset_name :  str \u2014</p> <p>The name of the dataset.</p> </li> <li> <p>question :  str \u2014</p> <p>The question for the dataset.</p> </li> <li> <p>answer :  str \u2014</p> <p>The answer to the question.</p> </li> <li> <p>annotator_id :  int \u2014</p> <p>The annotator ID for the evaluation.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[str, str] \u2014 A tuple (question, answer), with <code>question</code> being the next question, and <code>answer</code> being an empty string.</p> </li> </ul> <p> source method HumanEvaluator.example_to_markdown(example: dict) \u2192 tuple[str, str] </p> <p>Convert an example to a Markdown string.</p> <p> Parameters </p> <ul> <li> <p>example :  dict \u2014</p> <p>The example to convert.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[str, str] \u2014 A tuple (task_examples, question) for the example.</p> </li> </ul> <p> source method HumanEvaluator.compute_and_log_scores() \u2192 None </p> <p>Computes and logs the scores for the dataset.</p> <p> source main(annotator_id: int) \u2192 None </p> <p>Start the Gradio app for human evaluation.</p> <p> Raises </p> <ul> <li> <p>NeedsExtraInstalled</p> </li> </ul>"},{"location":"src/scandeval/human_evaluation/","title":"scandeval.human_evaluation","text":"scandeval.human_evaluation<p> docs module scandeval.human_evaluation </p> <pre><code>\"\"\"Gradio app for conducting human evaluation of the tasks.\"\"\"\n\nimport importlib.util\nimport json\nimport logging\nfrom collections import defaultdict\nfrom functools import partial\nfrom pathlib import Path\n\nimport click\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\n\nfrom .benchmark_config_factory import build_benchmark_config\nfrom .benchmarker import BenchmarkResult\nfrom .data_loading import load_data\nfrom .data_models import GenerativeModelOutput\nfrom .dataset_configs import SPEED_CONFIG, get_all_dataset_configs\nfrom .enums import Framework\nfrom .exceptions import NeedsExtraInstalled\nfrom .scores import aggregate_scores\nfrom .task_utils import (\n    question_answering,\n    sequence_classification,\n    text_to_text,\n    token_classification,\n)\nfrom .tasks import NER\nfrom .types import ComputeMetricsFunction, ExtractLabelsFunction, ScoreDict\nfrom .utils import enforce_reproducibility\n\nif importlib.util.find_spec(\"gradio\") is not None:\n    import gradio as gr\n\n\nlogger = logging.getLogger(\"scandeval\")\n\n\nclass HumanEvaluator:docs\n    \"\"\"An app for evaluating human performance on the ScandEval benchmark.\"\"\"\n\n    def __init__(\n        self,\n        annotator_id: int,\n        title: str,\n        description: str,\n        dummy_model_id: str = \"mistralai/Mistral-7B-v0.1\",\n    ) -&gt; None:\n        \"\"\"Initialize the HumanEvaluator.\n\n        Args:\n            annotator_id:\n                The annotator ID for the evaluation.\n            title:\n                The title of the app.\n            description:\n                The description of the app.\n            dummy_model_id:\n                The model ID to use for generating prompts.\n        \"\"\"\n        self.annotator_id = annotator_id\n        self.title = title\n        self.description = description\n        self.dummy_model_id = dummy_model_id\n\n        self.sample_idx: int\n        self.active_dataset: Dataset\n\n        self.dataset_configs = {\n            name: cfg\n            for name, cfg in get_all_dataset_configs().items()\n            if not cfg.unofficial\n        }\n        self.tasks = sorted(\n            {\n                cfg.task.name.replace(\"-\", \" \").title()\n                for cfg in self.dataset_configs.values()\n                if cfg != SPEED_CONFIG\n            }\n        )\n        self.languages = sorted(\n            {\n                language.name\n                for cfg in self.dataset_configs.values()\n                if cfg != SPEED_CONFIG\n                for language in cfg.languages\n                if language.name not in {\"Norwegian Bokm\u00e5l\", \"Norwegian Nynorsk\"}\n            }\n        )\n\n        self.extract_labels_from_generation: ExtractLabelsFunction\n        self.compute_metrics: ComputeMetricsFunction\n\n    def create_app(self) -&gt; \"gr.Blocks\":docs\n        \"\"\"Create the Gradio app for human evaluation.\n\n        Returns:\n            The Gradio app for human evaluation.\n        \"\"\"\n        with gr.Blocks(title=self.title, theme=gr.themes.Monochrome()) as app:\n            gr.components.HTML(f\"&lt;center&gt;&lt;h1&gt;{self.title}&lt;/h1&gt;&lt;/center&gt;\")\n            gr.components.Markdown(self.description)\n            with gr.Row(variant=\"panel\"):\n                language_dropdown = gr.Dropdown(\n                    label=\"Language\", choices=self.languages\n                )\n                task_dropdown = gr.Dropdown(label=\"Task\", choices=self.tasks)\n                dataset_dropdown = gr.Dropdown(label=\"Dataset\", choices=[\"\"])\n            with gr.Row(variant=\"panel\"):\n                with gr.Column():\n                    task_examples = gr.Markdown(\"Task Examples\", visible=False)\n                with gr.Column():\n                    question = gr.Markdown(label=\"Question\", visible=False)\n                    with gr.Row():\n                        ner_tag_dropdown = gr.Dropdown(\n                            label=\"Entity type\",\n                            choices=[\"\"],\n                            interactive=True,\n                            visible=False,\n                            scale=0.5,\n                        )\n                        ner_tag_answer = gr.Textbox(\n                            label=\"Entity\", interactive=True, visible=False, scale=1\n                        )\n                        with gr.Column(scale=0.2):\n                            ner_tag_add_button = gr.Button(\"Add entity\", visible=False)\n                            ner_tag_reset_button = gr.Button(\n                                \"Reset entities\", visible=False\n                            )\n                    answer = gr.Textbox(label=\"Answer\", visible=False)\n                    submit_button = gr.Button(\"Submit\", visible=False)\n\n            language_dropdown.change(\n                fn=self.update_dataset_choices,\n                inputs=[language_dropdown, task_dropdown],\n                outputs=dataset_dropdown,\n            )\n            task_dropdown.change(\n                fn=self.update_dataset_choices,\n                inputs=[language_dropdown, task_dropdown],\n                outputs=dataset_dropdown,\n            )\n            dataset_dropdown.change(\n                fn=partial(self.update_dataset, iteration=self.annotator_id),\n                inputs=dataset_dropdown,\n                outputs=[\n                    task_examples,\n                    question,\n                    ner_tag_dropdown,\n                    ner_tag_answer,\n                    ner_tag_add_button,\n                    ner_tag_reset_button,\n                    answer,\n                    submit_button,\n                ],\n            )\n            ner_tag_add_button.click(\n                fn=self.add_entity_to_answer,\n                inputs=[question, ner_tag_dropdown, ner_tag_answer, answer],\n                outputs=[ner_tag_answer, answer],\n            )\n            ner_tag_answer.submit(\n                fn=self.add_entity_to_answer,\n                inputs=[question, ner_tag_dropdown, ner_tag_answer, answer],\n                outputs=[ner_tag_answer, answer],\n            )\n            ner_tag_reset_button.click(fn=self.reset_entities, outputs=answer)\n            submit_button.click(\n                fn=partial(self.submit_answer, annotator_id=self.annotator_id),\n                inputs=[dataset_dropdown, question, answer],\n                outputs=[question, answer],\n            )\n            answer.submit(\n                fn=partial(self.submit_answer, annotator_id=self.annotator_id),\n                inputs=[dataset_dropdown, question, answer],\n                outputs=[question, answer],\n            )\n        return app\n\n    def update_dataset_choices(docs\n        self, language: str | None, task: str | None\n    ) -&gt; \"gr.Dropdown\":\n        \"\"\"Update the dataset choices based on the selected language and task.\n\n        Args:\n            language:\n                The language selected by the user.\n            task:\n                The task selected by the user.\n\n        Returns:\n            A list of dataset names that match the selected language and task.\n        \"\"\"\n        if language is None or task is None:\n            return gr.Dropdown(choices=[])\n\n        dataset_configs = [\n            cfg\n            for cfg in get_all_dataset_configs().values()\n            if language in {language.name for language in cfg.languages}\n            and task.lower().replace(\" \", \"-\") == cfg.task.name\n            and not cfg.unofficial\n        ]\n        assert len(dataset_configs) &gt; 0\n\n        choices = sorted([cfg.name for cfg in dataset_configs])\n\n        logger.info(\n            f\"User selected {language} and {task}, which resulted in the datasets \"\n            f\"{choices}, with {choices[0]!r} being chosen by default.\"\n        )\n\n        return gr.Dropdown(choices=choices, value=choices[0])\n\n    def update_dataset(docs\n        self, dataset_name: str, iteration: int\n    ) -&gt; \"tuple[gr.Markdown, gr.Markdown, gr.Dropdown, gr.Textbox, gr.Button, gr.Button, gr.Textbox, gr.Button]\":\n        \"\"\"Update the dataset based on a selected dataset name.\n\n        Args:\n            dataset_name:\n                The dataset name selected by the user.\n            iteration:\n                The iteration index of the datasets to evaluate.\n\n        Returns:\n            A tuple (task_examples, question, entity_type, entity, entity_add_button,\n            entity_reset_button, answer, submit_button) for the selected dataset.\n        \"\"\"\n        blank_answer = (\n            gr.Markdown(\"\", visible=False),\n            gr.Markdown(\"\", visible=False),\n            gr.Dropdown(visible=False),\n            gr.Textbox(visible=False),\n            gr.Button(visible=False),\n            gr.Button(visible=False),\n            gr.Textbox(\"\", visible=False),\n            gr.Button(visible=False),\n        )\n\n        if not dataset_name:\n            return blank_answer\n\n        logger.info(f\"User selected dataset {dataset_name} - loading dataset...\")\n        gr.Info(f\"Loading dataset {dataset_name}...\")\n\n        benchmark_config = build_benchmark_config(\n            progress_bar=False,\n            save_results=True,\n            task=None,\n            dataset=None,\n            language=[\n                language.code\n                for cfg in get_all_dataset_configs().values()\n                for language in cfg.languages\n                if not cfg.unofficial\n            ],\n            model_language=None,\n            dataset_language=None,\n            framework=None,\n            device=None,\n            batch_size=1,\n            raise_errors=False,\n            cache_dir=\".scandeval_cache\",\n            api_key=None,\n            force=False,\n            verbose=False,\n            trust_remote_code=False,\n            load_in_4bit=None,\n            use_flash_attention=None,\n            clear_model_cache=False,\n            evaluate_test_split=False,\n            few_shot=True,\n            num_iterations=iteration + 1,\n            api_base=None,\n            api_version=None,\n            debug=False,\n            run_with_cli=True,\n        )\n        self.dataset_config = get_all_dataset_configs()[dataset_name]\n\n        # TODO: Is this needed?\n        # model_id = f\"human-{iteration}\"\n        # model_config = ModelConfig(\n        #     model_id=model_id,\n        #     revision=\"main\",\n        #     framework=Framework.HUMAN,\n        #     task=\"text-generation\",\n        #     languages=dataset_config.languages,\n        #     model_type=ModelType.HUMAN,\n        #     model_cache_dir=create_model_cache_dir(\n        #         cache_dir=benchmark_config.cache_dir, model_id=model_id\n        #     ),\n        #     adapter_base_model_id=None,\n        # )\n\n        self.sample_idx = 0\n\n        dataset_path = (\n            Path(\".scandeval_cache\")\n            / \"human-evaluation\"\n            / dataset_name\n            / f\"human-{iteration}.csv\"\n        )\n        if dataset_path.exists():\n            active_dataset = Dataset.from_csv(str(dataset_path))\n            assert isinstance(active_dataset, Dataset)\n            self.active_dataset = active_dataset\n            try:\n                while self.active_dataset[\"answer\"][self.sample_idx] is not None:\n                    self.sample_idx += 1\n            except IndexError:\n                self.compute_and_log_scores()\n                return blank_answer\n        else:\n            rng = enforce_reproducibility(framework=Framework.PYTORCH)\n            datasets = load_data(\n                rng=rng,\n                dataset_config=self.dataset_config,\n                benchmark_config=benchmark_config,\n            )\n            # TODO: Prepare data?\n            self.active_dataset = (\n                datasets[iteration][\"test\"]\n                .remove_columns(\n                    column_names=[\"input_ids\", \"attention_mask\"],\n                    new_fingerprint=datasets[iteration][\"test\"]._fingerprint,\n                )\n                .add_column(\n                    name=\"answer\",\n                    column=[None] * len(datasets[iteration][\"test\"]),\n                    new_fingerprint=datasets[iteration][\"test\"]._fingerprint,\n                )\n            )\n            if self.dataset_config.task == NER:\n                labels_in_train: set[str] = {\n                    tag\n                    for tag_list in self.active_dataset[\"labels\"]\n                    for tag in tag_list\n                }\n                self.has_misc_tags = (\n                    \"B-MISC\" in labels_in_train or \"I-MISC\" in labels_in_train\n                )\n\n        match self.dataset_config.task.supertask:\n            case \"sequence-classification\":\n                self.compute_metrics = partial(\n                    sequence_classification.compute_metrics,\n                    id2label=self.dataset_config.id2label,\n                    dataset_config=self.dataset_config,\n                    benchmark_config=benchmark_config,\n                )\n                self.extract_labels_from_generation = partial(\n                    sequence_classification.extract_labels_from_generation,\n                    dataset_config=self.dataset_config,\n                )\n            case \"text-to-text\":\n                self.compute_metrics = partial(\n                    text_to_text.compute_metrics,\n                    dataset_config=self.dataset_config,\n                    benchmark_config=benchmark_config,\n                )\n                self.extract_labels_from_generation = (\n                    text_to_text.extract_labels_from_generation\n                )\n            case \"token-classification\":\n                self.compute_metrics = partial(\n                    token_classification.compute_metrics,\n                    has_misc_tags=self.has_misc_tags,\n                    dataset_config=self.dataset_config,\n                    benchmark_config=benchmark_config,\n                )\n                self.extract_labels_from_generation = partial(\n                    token_classification.extract_labels_from_generation,\n                    dataset_config=self.dataset_config,\n                )\n            case \"question-answering\":\n                self.compute_metrics = partial(\n                    question_answering.compute_metrics,\n                    dataset_config=self.dataset_config,\n                    benchmark_config=benchmark_config,\n                )\n                self.extract_labels_from_generation = (\n                    question_answering.extract_labels_from_generation\n                )\n            case _:\n                raise NotImplementedError(\n                    f\"Supertask {self.dataset_config.task.supertask} is not supported.\"\n                )\n\n        task_examples, question = self.example_to_markdown(\n            example=self.active_dataset[self.sample_idx]\n        )\n\n        logger.info(\n            f\"Loaded dataset {dataset_name}, with the following task examples:\\n\\n\"\n            f\"{task_examples}\"\n        )\n\n        if self.dataset_config.task == NER:\n            ner_tags = list()\n            for ner_tag in self.dataset_config.prompt_label_mapping.values():\n                if ner_tag not in ner_tags:\n                    ner_tags.append(ner_tag)\n            return (\n                gr.Markdown(task_examples, visible=True),\n                gr.Markdown(question, visible=True),\n                gr.Dropdown(\n                    label=\"Entity type\",\n                    choices=ner_tags,\n                    value=ner_tags[0],\n                    visible=True,\n                ),\n                gr.Textbox(label=\"Entity\", interactive=True, visible=True),\n                gr.Button(\"Add entity\", visible=True),\n                gr.Button(\"Reset entities\", visible=True),\n                gr.Textbox(\n                    json.dumps({ner_tag: [] for ner_tag in ner_tags}),\n                    interactive=False,\n                    visible=True,\n                ),\n                gr.Button(\"Submit\", visible=True),\n            )\n        else:\n            return (\n                gr.Markdown(task_examples, visible=True),\n                gr.Markdown(question, visible=True),\n                gr.Dropdown(label=\"Entity type\", choices=[], visible=False),\n                gr.Textbox(label=\"Entity\", interactive=True, visible=False),\n                gr.Button(\"Add entity\", visible=False),\n                gr.Button(\"Reset entities\", visible=False),\n                gr.Textbox(\"\", interactive=True, visible=True),\n                gr.Button(\"Submit\", visible=True),\n            )\n\n    def add_entity_to_answer(docs\n        self, question: str, entity_type: str, entity: str, answer: str\n    ) -&gt; \"tuple[gr.Textbox, gr.Textbox]\":\n        \"\"\"Add an entity to the answer.\n\n        Args:\n            question:\n                The current question.\n            entity_type:\n                The entity type selected by the user.\n            entity:\n                The entity provided by the user.\n            answer:\n                The current answer.\n\n        Returns:\n            A tuple (entity, answer) with a (blank) entity and answer.\n        \"\"\"\n        if not entity_type or not entity:\n            return gr.Textbox(\"\"), gr.Textbox(\"\")\n\n        if entity not in question:\n            gr.Warning(\n                f\"The entity {entity!r} is not present in the question. Please \"\n                \"write it *exactly* as it appears in the question.\"\n            )\n            return gr.Textbox(entity), gr.Textbox(answer)\n\n        current_answer_obj = json.loads(answer)\n        if entity not in current_answer_obj[entity_type]:\n            current_answer_obj[entity_type].append(entity)\n\n        answer = json.dumps(current_answer_obj)\n        return gr.Textbox(\"\"), gr.Textbox(answer)\n\n    def reset_entities(self) -&gt; \"gr.Textbox\":docs\n        \"\"\"Reset the entities in the answer.\n\n        Returns:\n            A blank answer.\n        \"\"\"\n        ner_tags = list()\n        for ner_tag in self.dataset_config.prompt_label_mapping.values():\n            if ner_tag not in ner_tags:\n                ner_tags.append(ner_tag)\n        return gr.Textbox(json.dumps({ner_tag: [] for ner_tag in ner_tags}))\n\n    def submit_answer(docs\n        self, dataset_name: str, question: str, answer: str, annotator_id: int\n    ) -&gt; tuple[str, str]:\n        \"\"\"Submit an answer to the dataset.\n\n        Args:\n            dataset_name:\n                The name of the dataset.\n            question:\n                The question for the dataset.\n            answer:\n                The answer to the question.\n            annotator_id:\n                The annotator ID for the evaluation.\n\n        Returns:\n            A tuple (question, answer), with `question` being the next question, and\n            `answer` being an empty string.\n        \"\"\"\n        if not answer:\n            gr.Warning(\"Please provide an answer before submitting.\")\n            logger.info(\"User tried to submit without providing an answer.\")\n            return question, answer\n\n        # Custom NER validation\n        if self.dataset_config.task == NER:\n            try:\n                json.loads(answer)\n            except json.JSONDecodeError:\n                gr.Warning(\"Please provide a valid JSON object as an answer.\")\n                logger.info(\"User tried to submit an invalid JSON object as an answer.\")\n                return question, answer\n\n            if not isinstance(json.loads(answer), dict):\n                gr.Warning(\n                    \"Please provide a JSON object with a dictionary as an answer.\"\n                )\n                logger.info(\n                    \"User tried to submit a JSON object without a dictionary as an answer.\"\n                )\n                return question, answer\n\n            ner_tags = list(self.dataset_config.prompt_label_mapping.values())\n            for ner_tag in ner_tags:\n                if ner_tag not in json.loads(answer).keys():\n                    gr.Warning(\n                        f\"Please provide a JSON object with the key {ner_tag!r}.\"\n                    )\n                    logger.info(\n                        \"User tried to submit a JSON object without the key \"\n                        f\"{ner_tag!r}.\"\n                    )\n                    return question, answer\n\n        samples_left = len(self.active_dataset) - self.sample_idx - 1\n        if samples_left:\n            gr.Info(f\"Submitted - {samples_left} to go!\")\n\n        # Store the user's answer\n        answers = self.active_dataset[\"answer\"]\n        answers[self.sample_idx] = answer\n        self.active_dataset = self.active_dataset.remove_columns(\n            column_names=[\"answer\"], new_fingerprint=self.active_dataset._fingerprint\n        ).add_column(\n            name=\"answer\",\n            column=answers,\n            new_fingerprint=self.active_dataset._fingerprint,\n        )\n        logger.info(\n            f\"User submitted the answer {answer!r} to the question {question!r}, with \"\n            f\"sample index {self.sample_idx}.\"\n        )\n\n        dataset_path = (\n            Path(\".scandeval_cache\")\n            / \"human-evaluation\"\n            / dataset_name\n            / f\"human-{annotator_id}.csv\"\n        )\n        dataset_path.parent.mkdir(parents=True, exist_ok=True)\n        self.active_dataset.to_csv(dataset_path)\n\n        # Attempt to get the next question\n        try:\n            self.sample_idx += 1\n            _, question = self.example_to_markdown(\n                example=self.active_dataset[self.sample_idx]\n            )\n\n            if self.dataset_config.task == NER:\n                ner_tags = list()\n                for ner_tag in self.dataset_config.prompt_label_mapping.values():\n                    if ner_tag not in ner_tags:\n                        ner_tags.append(ner_tag)\n                answer = json.dumps({ner_tag: [] for ner_tag in ner_tags})\n            else:\n                answer = \"\"\n\n        # If we fail to get the next question it means that the user has finished\n        # annotating the dataset, so we compute and log the scores\n        except IndexError:\n            self.compute_and_log_scores()\n            question = \"\"\n            answer = \"\"\n\n        return question, answer\n\n    def example_to_markdown(self, example: dict) -&gt; tuple[str, str]:docs\n        \"\"\"Convert an example to a Markdown string.\n\n        Args:\n            example:\n                The example to convert.\n\n        Returns:\n            A tuple (task_examples, question) for the example.\n        \"\"\"\n        task_examples: str | list[str] = [\n            sample.replace(\"\\n\", \"\\n\\n\")\n            for sample in example[\"text\"].split(\"\\n\\n\")[:-1]\n        ]\n        task_examples = \"\\n\\n**Example**\\n\\n\".join(task_examples)\n\n        question = \"**Question**\\n\\n\"\n        question += \"\\n\\n\".join(example[\"text\"].split(\"\\n\\n\")[-1].split(\"\\n\")[:-1])\n        question += \"\\n\\n\" + example[\"text\"].split(\"\\n\\n\")[-1].split(\"\\n\")[-1]\n\n        return task_examples, question\n\n    def compute_and_log_scores(self) -&gt; None:docs\n        \"\"\"Computes and logs the scores for the dataset.\"\"\"\n        tokenizer = AutoTokenizer.from_pretrained(self.dummy_model_id)\n        tokenizer.pad_token = tokenizer.eos_token\n        sequences = tokenizer(\n            self.active_dataset[\"answer\"],\n            add_special_tokens=False,\n            padding=True,\n            return_tensors=\"pt\",\n        ).input_ids\n        model_output = GenerativeModelOutput(sequences=sequences)\n\n        active_dataset_dict = self.active_dataset.to_dict()\n        assert isinstance(active_dataset_dict, dict)\n\n        all_preds = self.extract_labels_from_generation(\n            input_batch=active_dataset_dict, model_output=model_output\n        )\n        ground_truth = self.active_dataset[\"label\"]\n        itr_scores: dict[str, float] = self.compute_metrics(\n            model_outputs_and_labels=(all_preds, ground_truth),\n            id2label=self.dataset_config.id2label,\n        )\n\n        # We reverse the order, as the Info messages are printed in reverse order\n        scores = list(itr_scores.items())\n        scores.reverse()\n        gr.Info(\n            \"If you want to evaluate another dataset then please select a new \"\n            \"one from the menus.\"\n        )\n        for metric_name, score in scores:\n            gr.Info(f\"\\n\\n{metric_name}: {score:.2%}\")\n        gr.Info(\"You have completed this dataset! Here are your scores:\")\n        logger.info(\n            f\"User completed the dataset {self.dataset_config.name!r}\"\n            f\", with the following scores: {itr_scores}\"\n        )\n\n        # Load previous human results, if any. We do this since the human evaluation is\n        # only a single iteration, so the results from the current annotation should be\n        # added to the previous results.\n        results_path = Path.cwd() / \"scandeval_benchmark_results.jsonl\"\n        results: ScoreDict = defaultdict(list)\n        if results_path.exists():\n            all_results = [\n                json.loads(line.strip())\n                for line in results_path.read_text().strip().split(\"\\n\")\n                if line.strip()\n            ]\n            human_result_candidates = [\n                result\n                for result in all_results\n                if result[\"model\"] == \"human\"\n                and result[\"dataset\"] == self.dataset_config.name\n            ]\n            if human_result_candidates:\n                results = human_result_candidates[0][\"results\"]\n\n        # Append to results\n        results[\"raw\"].append(  # type: ignore[union-attr]\n            {f\"test_{metric_name}\": score for metric_name, score in itr_scores.items()}\n        )\n\n        # Aggregate scores\n        total_dict: dict[str, float] = dict()\n        for metric_cfg in self.dataset_config.task.metrics:\n            test_score, test_se = aggregate_scores(\n                scores=results[\"raw\"],  # type: ignore[arg-type]\n                metric_config=metric_cfg,\n            )\n            test_score, _ = metric_cfg.postprocessing_fn(test_score)\n            test_se, _ = metric_cfg.postprocessing_fn(test_se)\n            total_dict[f\"test_{metric_cfg.name}\"] = test_score\n            total_dict[f\"test_{metric_cfg.name}_se\"] = test_se\n        results[\"total\"] = total_dict\n\n        benchmark_result = BenchmarkResult(\n            dataset=self.dataset_config.name,\n            task=self.dataset_config.task.name,\n            dataset_languages=[\n                language.code for language in self.dataset_config.languages\n            ],\n            model=\"human\",\n            results=results,\n            num_model_parameters=-1,\n            max_sequence_length=-1,\n            vocabulary_size=-1,\n            generative=True,\n            few_shot=True,\n            validation_split=True,\n        )\n        benchmark_result.append_to_results(results_path=results_path)\n\n\n@click.command()\n@click.option(\n    \"--annotator-id\",\n    \"-id\",\n    type=int,\n    required=True,\n    help=\"\"\"The annotator ID to use for the evaluation. Needs to be between 0 and 10,\n    inclusive.\"\"\",\n)\ndef main(annotator_id: int) -&gt; None:docs\n    \"\"\"Start the Gradio app for human evaluation.\"\"\"\n    if importlib.util.find_spec(\"gradio\") is None:\n        raise NeedsExtraInstalled(extra=\"human_evaluation\")\n\n    evaluator = HumanEvaluator(\n        annotator_id=annotator_id,\n        title=\"ScandEval Human Evaluation\",\n        description=\"\"\"\n        In this app we will evaluate your performance on a variety of tasks, with the\n        goal of comparing human performance to language model performance.\n\n        When you select a language and a task then you will be given a brief\n        description of the task, as well as examples of how to solve it. Please read\n        through these examples before proceeding with the task.\n\n        Please do not use any additional aids (such as search engines) when completing\n        these tasks.\n\n        Note that several examples appear more than once - this is intentional, as it\n        allows us to compare your performance across multiple examples.\n\n        Note that the Enter key will also submit your answer!\n        \"\"\",\n    )\n    evaluator.create_app().queue().launch()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"api/scandeval/languages/","title":"scandeval.languages","text":"scandeval.languages<p> source module scandeval.languages </p> <p>List of languages and their ISO 639-1 codes.</p> <p>Taken from https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes.</p> <p>Last updated 19 June 2022.</p> <p> Functions </p> <ul> <li> <p>get_all_languages \u2014 Get a list of all the languages.</p> </li> </ul> <p> source get_all_languages() \u2192 dict[str, Language] </p> <p>Get a list of all the languages.</p> <p> Returns </p> <ul> <li> <p>dict[str, Language] \u2014 A mapping between language codes and their configurations.</p> </li> </ul>"},{"location":"src/scandeval/languages/","title":"scandeval.languages","text":"scandeval.languages<p> docs module scandeval.languages </p> <pre><code>\"\"\"List of languages and their ISO 639-1 codes.\n\nTaken from https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes.\n\nLast updated 19 June 2022.\n\"\"\"\n\nfrom .data_models import Language\n\n\ndef get_all_languages() -&gt; dict[str, Language]:docs\n    \"\"\"Get a list of all the languages.\n\n    Returns:\n        A mapping between language codes and their configurations.\n    \"\"\"\n    return {cfg.code: cfg for cfg in globals().values() if isinstance(cfg, Language)}\n\n\nAB = Language(code=\"ab\", name=\"Abkhazian\")\nAA = Language(code=\"aa\", name=\"Afar\")\nAF = Language(code=\"af\", name=\"Afrikaans\")\nSQ = Language(code=\"sq\", name=\"Albanian\")\nAM = Language(code=\"am\", name=\"Amharic\")\nAR = Language(code=\"ar\", name=\"Arabic\")\nAN = Language(code=\"an\", name=\"Aragonese\")\nHY = Language(code=\"hy\", name=\"Armenian\")\nAS = Language(code=\"as\", name=\"Assamese\")\nAV = Language(code=\"av\", name=\"Avaric\")\nAE = Language(code=\"ae\", name=\"Avestan\")\nAY = Language(code=\"ay\", name=\"Aymara\")\nAZ = Language(code=\"az\", name=\"Azerbaijani\")\nBM = Language(code=\"bm\", name=\"Bambara\")\nBA = Language(code=\"ba\", name=\"Bashkir\")\nEU = Language(code=\"eu\", name=\"Basque\")\nBE = Language(code=\"be\", name=\"Belarusian\")\nBN = Language(code=\"bn\", name=\"Bengali\")\nBI = Language(code=\"bi\", name=\"Bislama\")\nBS = Language(code=\"bs\", name=\"Bosnian\")\nBR = Language(code=\"br\", name=\"Breton\")\nBG = Language(code=\"bg\", name=\"Bulgarian\")\nMY = Language(code=\"my\", name=\"Burmese\")\nCA = Language(code=\"ca\", name=\"Catalan\")\nCH = Language(code=\"ch\", name=\"Chamorro\")\nCE = Language(code=\"ce\", name=\"Chechen\")\nNY = Language(code=\"ny\", name=\"Chichewa\")\nZH = Language(code=\"zh\", name=\"Chinese\")\nCU = Language(code=\"cu\", name=\"Church Slavic\")\nCV = Language(code=\"cv\", name=\"Chuvash\")\nKW = Language(code=\"kw\", name=\"Cornish\")\nCO = Language(code=\"co\", name=\"Corsican\")\nCR = Language(code=\"cr\", name=\"Cree\")\nHR = Language(code=\"hr\", name=\"Croatian\")\nCS = Language(code=\"cs\", name=\"Czech\")\nDA = Language(code=\"da\", name=\"Danish\")\nDV = Language(code=\"dv\", name=\"Divehi\")\nNL = Language(code=\"nl\", name=\"Dutch\")\nDZ = Language(code=\"dz\", name=\"Dzongkha\")\nEN = Language(code=\"en\", name=\"English\")\nEO = Language(code=\"eo\", name=\"Esperanto\")\nET = Language(code=\"et\", name=\"Estonian\")\nEE = Language(code=\"ee\", name=\"Ewe\")\nFO = Language(code=\"fo\", name=\"Faroese\")\nFJ = Language(code=\"fj\", name=\"Fijian\")\nFI = Language(code=\"fi\", name=\"Finnish\")\nFR = Language(code=\"fr\", name=\"French\")\nFY = Language(code=\"fy\", name=\"Western Frisian\")\nFF = Language(code=\"ff\", name=\"Fulah\")\nGD = Language(code=\"gd\", name=\"Gaelic\")\nGL = Language(code=\"gl\", name=\"Galician\")\nLG = Language(code=\"lg\", name=\"Ganda\")\nKA = Language(code=\"ka\", name=\"Georgian\")\nDE = Language(code=\"de\", name=\"German\")\nEL = Language(code=\"el\", name=\"Greek\")\nKL = Language(code=\"kl\", name=\"Greenlandic\")\nGN = Language(code=\"gn\", name=\"Guarani\")\nGU = Language(code=\"gu\", name=\"Gujarati\")\nHT = Language(code=\"ht\", name=\"Haitian\")\nHA = Language(code=\"ha\", name=\"Hausa\")\nHE = Language(code=\"he\", name=\"Hebrew\")\nHZ = Language(code=\"hz\", name=\"Herero\")\nHI = Language(code=\"hi\", name=\"Hindi\")\nHO = Language(code=\"ho\", name=\"Hiri Motu\")\nHU = Language(code=\"hu\", name=\"Hungarian\")\nIS = Language(code=\"is\", name=\"Icelandic\")\nIO = Language(code=\"io\", name=\"Ido\")\nIG = Language(code=\"ig\", name=\"Igbo\")\nID = Language(code=\"id\", name=\"Indonesian\")\nIA = Language(code=\"ia\", name=\"Interlingua\")\nIE = Language(code=\"ie\", name=\"Interlingue\")\nIU = Language(code=\"iu\", name=\"Inuktitut\")\nIK = Language(code=\"ik\", name=\"Inupiaq\")\nGA = Language(code=\"ga\", name=\"Irish\")\nIT = Language(code=\"it\", name=\"Italian\")\nJA = Language(code=\"ja\", name=\"Japanese\")\nKN = Language(code=\"kn\", name=\"Kannada\")\nKR = Language(code=\"kr\", name=\"Kanuri\")\nKS = Language(code=\"ks\", name=\"Kashmiri\")\nKK = Language(code=\"kk\", name=\"Kazakh\")\nKM = Language(code=\"km\", name=\"Central Khmer\")\nKI = Language(code=\"ki\", name=\"Kikuyu\")\nRW = Language(code=\"rw\", name=\"Kinyarwanda\")\nKY = Language(code=\"ky\", name=\"Kirghiz\")\nKV = Language(code=\"kv\", name=\"Komi\")\nKG = Language(code=\"kg\", name=\"Kongo\")\nKO = Language(code=\"ko\", name=\"Korean\")\nKJ = Language(code=\"kj\", name=\"Kuanyama\")\nKU = Language(code=\"ku\", name=\"Kurdish\")\nLO = Language(code=\"lo\", name=\"Lao\")\nLA = Language(code=\"la\", name=\"Latin\")\nLV = Language(code=\"lv\", name=\"Latvian\")\nLI = Language(code=\"li\", name=\"Limburgan\")\nLN = Language(code=\"ln\", name=\"Lingala\")\nLT = Language(code=\"lt\", name=\"Lithuanian\")\nLU = Language(code=\"lu\", name=\"Luba-Katanga\")\nLB = Language(code=\"lb\", name=\"Luxembourgish\")\nMK = Language(code=\"mk\", name=\"Macedonian\")\nMG = Language(code=\"mg\", name=\"Malagasy\")\nMS = Language(code=\"ms\", name=\"Malay\")\nML = Language(code=\"ml\", name=\"Malayalam\")\nMT = Language(code=\"mt\", name=\"Maltese\")\nGV = Language(code=\"gv\", name=\"Manx\")\nMI = Language(code=\"mi\", name=\"Maori\")\nMR = Language(code=\"mr\", name=\"Marathi\")\nMH = Language(code=\"mh\", name=\"Marshallese\")\nMN = Language(code=\"mn\", name=\"Mongolian\")\nNA = Language(code=\"na\", name=\"Nauru\")\nNV = Language(code=\"nv\", name=\"Navajo\")\nND = Language(code=\"nd\", name=\"Northern Ndebele\")\nNR = Language(code=\"nr\", name=\"South Ndebele\")\nNG = Language(code=\"ng\", name=\"Ndonga\")\nNE = Language(code=\"ne\", name=\"Nepali\")\nNO = Language(code=\"no\", name=\"Norwegian\")\nNB = Language(code=\"nb\", name=\"Norwegian Bokm\u00e5l\")\nNN = Language(code=\"nn\", name=\"Norwegian Nynorsk\")\nII = Language(code=\"ii\", name=\"Sichuan Yi\")\nOC = Language(code=\"oc\", name=\"Occitan\")\nOJ = Language(code=\"oj\", name=\"Ojibwa\")\nOR = Language(code=\"or\", name=\"Oriya\")\nOM = Language(code=\"om\", name=\"Oromo\")\nOS = Language(code=\"os\", name=\"Ossetian\")\nPI = Language(code=\"pi\", name=\"Pali\")\nPS = Language(code=\"ps\", name=\"Pashto\")\nFA = Language(code=\"fa\", name=\"Persian\")\nPL = Language(code=\"pl\", name=\"Polish\")\nPT = Language(code=\"pt\", name=\"Portuguese\")\nPA = Language(code=\"pa\", name=\"Punjabi\")\nQU = Language(code=\"qu\", name=\"Quechua\")\nRO = Language(code=\"ro\", name=\"Romanian\")\nRM = Language(code=\"rm\", name=\"Romansh\")\nRN = Language(code=\"rn\", name=\"Rundi\")\nRU = Language(code=\"ru\", name=\"Russian\")\nSE = Language(code=\"se\", name=\"Northern Sami\")\nSM = Language(code=\"sm\", name=\"Samoan\")\nSG = Language(code=\"sg\", name=\"Sango\")\nSA = Language(code=\"sa\", name=\"Sanskrit\")\nSC = Language(code=\"sc\", name=\"Sardinian\")\nSR = Language(code=\"sr\", name=\"Serbian\")\nSN = Language(code=\"sn\", name=\"Shona\")\nSD = Language(code=\"sd\", name=\"Sindhi\")\nSI = Language(code=\"si\", name=\"Sinhala\")\nSK = Language(code=\"sk\", name=\"Slovak\")\nSL = Language(code=\"sl\", name=\"Slovenian\")\nSO = Language(code=\"so\", name=\"Somali\")\nST = Language(code=\"st\", name=\"Sotho\")\nES = Language(code=\"es\", name=\"Spanish\")\nSU = Language(code=\"su\", name=\"Sundanese\")\nSW = Language(code=\"sw\", name=\"Swahili\")\nSS = Language(code=\"ss\", name=\"Swati\")\nSV = Language(code=\"sv\", name=\"Swedish\")\nTL = Language(code=\"tl\", name=\"Tagalog\")\nTY = Language(code=\"ty\", name=\"Tahitian\")\nTG = Language(code=\"tg\", name=\"Tajik\")\nTA = Language(code=\"ta\", name=\"Tamil\")\nTT = Language(code=\"tt\", name=\"Tatar\")\nTE = Language(code=\"te\", name=\"Telugu\")\nTH = Language(code=\"th\", name=\"Thai\")\nBO = Language(code=\"bo\", name=\"Tibetan\")\nTI = Language(code=\"ti\", name=\"Tigrinya\")\nTO = Language(code=\"to\", name=\"Tonga\")\nTS = Language(code=\"ts\", name=\"Tsonga\")\nTN = Language(code=\"tn\", name=\"Tswana\")\nTR = Language(code=\"tr\", name=\"Turkish\")\nTK = Language(code=\"tk\", name=\"Turkmen\")\nTW = Language(code=\"tw\", name=\"Twi\")\nUG = Language(code=\"ug\", name=\"Uighur\")\nUK = Language(code=\"uk\", name=\"Ukrainian\")\nUR = Language(code=\"ur\", name=\"Urdu\")\nUZ = Language(code=\"uz\", name=\"Uzbek\")\nVE = Language(code=\"ve\", name=\"Venda\")\nVI = Language(code=\"vi\", name=\"Vietnamese\")\nVO = Language(code=\"vo\", name=\"Volap\u00fck\")\nWA = Language(code=\"wa\", name=\"Walloon\")\nCY = Language(code=\"cy\", name=\"Welsh\")\nWO = Language(code=\"wo\", name=\"Wolof\")\nXH = Language(code=\"xh\", name=\"Xhosa\")\nYI = Language(code=\"yi\", name=\"Yiddish\")\nYO = Language(code=\"yo\", name=\"Yoruba\")\nZA = Language(code=\"za\", name=\"Zhuang\")\nZU = Language(code=\"zu\", name=\"Zulu\")\n</code></pre>"},{"location":"api/scandeval/model_cache/","title":"scandeval.model_cache","text":"scandeval.model_cache<p> source module scandeval.model_cache </p> <p>ModelCache class for caching model outputs.</p> <p> Classes </p> <ul> <li> <p>ModelCache \u2014 A cache for model outputs.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>split_dataset_into_cached_and_non_cached \u2014 Split a dataset into a cached and non-cached part.</p> </li> <li> <p>load_cached_model_outputs \u2014 Load the cached model outputs.</p> </li> </ul> <p> source class ModelCache(cache_name: str, max_generated_tokens: int) </p> <p>A cache for model outputs.</p> <p>Initialize the model output cache.</p> <p> Attributes </p> <ul> <li> <p>model_cache_dir \u2014</p> <p>The directory to store the cache in.</p> </li> <li> <p>cache_path \u2014</p> <p>The path to the cache file.</p> </li> <li> <p>cache \u2014</p> <p>The model output cache.</p> </li> <li> <p>max_generated_tokens \u2014</p> <p>The maximum number of tokens to generate for each example.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>model_cache_dir :  Path \u2014</p> <p>The directory to store the cache in.</p> </li> <li> <p>cache_name :  str \u2014</p> <p>The name of the cache file.</p> </li> <li> <p>max_generated_tokens :  int \u2014</p> <p>The maximum number of tokens to generate for each example.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>load \u2014 Load the model output cache.</p> </li> <li> <p>save \u2014 Save the model output cache to disk.</p> </li> <li> <p>remove \u2014 Remove the cache from memory and delete it from disk.</p> </li> <li> <p>cached_texts \u2014 Return the text inputs indexed in the cache.</p> </li> <li> <p>add_to_cache \u2014 Add the model input/output to the cache.</p> </li> </ul> <p> source method ModelCache.load() \u2192 None </p> <p>Load the model output cache.</p> <p> source method ModelCache.save() \u2192 None </p> <p>Save the model output cache to disk.</p> <p> source method ModelCache.remove() \u2192 None </p> <p>Remove the cache from memory and delete it from disk.</p> <p> source method ModelCache.cached_texts() \u2192 list[str] </p> <p>Return the text inputs indexed in the cache.</p> <p> source method ModelCache.add_to_cache(model_inputs: dict, model_output: GenerativeModelOutput) \u2192 None </p> <p>Add the model input/output to the cache.</p> <p> Parameters </p> <ul> <li> <p>model_inputs :  dict \u2014</p> <p>The model inputs.</p> </li> <li> <p>model_output :  GenerativeModelOutput \u2014</p> <p>The model output.</p> </li> </ul> <p> source split_dataset_into_cached_and_non_cached(dataset: Dataset, cache: ModelCache) \u2192 tuple[Dataset, Dataset] </p> <p>Split a dataset into a cached and non-cached part.</p> <p> Parameters </p> <ul> <li> <p>dataset :  Dataset \u2014</p> <p>The dataset to split.</p> </li> <li> <p>cache :  ModelCache \u2014</p> <p>The model output cache.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[Dataset, Dataset] \u2014 The cached and non-cached parts of the dataset.</p> </li> </ul> <p> source load_cached_model_outputs(cached_dataset: Dataset, cache: ModelCache) \u2192 GenerativeModelOutput </p> <p>Load the cached model outputs.</p> <p> Parameters </p> <ul> <li> <p>cached_dataset :  Dataset \u2014</p> <p>The dataset containing the cached examples.</p> </li> <li> <p>cache :  ModelCache \u2014</p> <p>The model output cache.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>GenerativeModelOutput \u2014 The model output containing the cached sequences.</p> </li> </ul>"},{"location":"src/scandeval/model_cache/","title":"scandeval.model_cache","text":"scandeval.model_cache<p> docs module scandeval.model_cache </p> <pre><code>\"\"\"ModelCache class for caching model outputs.\"\"\"\n\nimport hashlib\nimport json\nimport logging\nimport sys\nimport typing as t\nfrom collections import defaultdict\nfrom dataclasses import asdict\n\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nfrom .data_models import GenerativeModelOutput, SingleGenerativeModelOutput\n\nif t.TYPE_CHECKING:\n    from pathlib import Path\n\n    from datasets import Dataset\n\n\nlogger = logging.getLogger(\"scandeval\")\n\n\nclass ModelCache:docs\n    \"\"\"A cache for model outputs.\n\n    Attributes:\n        model_cache_dir:\n            The directory to store the cache in.\n        cache_path:\n            The path to the cache file.\n        cache:\n            The model output cache.\n        max_generated_tokens:\n            The maximum number of tokens to generate for each example.\n    \"\"\"\n\n    def __init__(\n        self, model_cache_dir: \"Path\", cache_name: str, max_generated_tokens: int\n    ):\n        \"\"\"Initialize the model output cache.\n\n        Args:\n            model_cache_dir:\n                The directory to store the cache in.\n            cache_name:\n                The name of the cache file.\n            max_generated_tokens:\n                The maximum number of tokens to generate for each example.\n        \"\"\"\n        self.model_cache_dir = model_cache_dir\n        self.model_cache_dir.mkdir(parents=True, exist_ok=True)\n        self.cache_path = self.model_cache_dir / cache_name.replace(\"/\", \"--\")\n        self.max_generated_tokens = max_generated_tokens\n\n    def load(self) -&gt; None:docs\n        \"\"\"Load the model output cache.\"\"\"\n        if not self.cache_path.exists():\n            with self.cache_path.open(\"w\") as f:\n                json.dump(dict(), f)\n\n        with self.cache_path.open() as f:\n            json_cache = json.load(f)\n\n        cache: dict[str, SingleGenerativeModelOutput] = dict()\n        for key in json_cache:\n            cache[key] = SingleGenerativeModelOutput(**json_cache[key])\n\n        self.cache = cache\n\n    def save(self) -&gt; None:docs\n        \"\"\"Save the model output cache to disk.\"\"\"\n        dumpable_cache: dict[str, dict] = defaultdict(dict)\n        for key, value in self.cache.items():\n            dumpable_cache[key] = asdict(value)\n\n        with self.cache_path.open(\"w\") as f:\n            json.dump(dumpable_cache, f)\n\n    def _hash_key(self, key: str | list[dict[str, str]]) -&gt; str:\n        \"\"\"Hash the key to use as an index in the cache.\n\n        Args:\n            key:\n                The key to hash.\n\n        Returns:\n            The hashed key.\n        \"\"\"\n        return hashlib.md5(string=str(key).encode()).hexdigest()\n\n    def __getitem__(\n        self, key: str | list[dict[str, str]]\n    ) -&gt; SingleGenerativeModelOutput:\n        \"\"\"Get an item from the cache.\n\n        Args:\n            key:\n                The key to use to index the cache.\n\n        Returns:\n            The model output.\n        \"\"\"\n        hashed_key = self._hash_key(key=key)\n        return self.cache[hashed_key]\n\n    def __setitem__(self, key: t.Any, value: SingleGenerativeModelOutput) -&gt; None:\n        \"\"\"Set an item in the cache.\n\n        Args:\n            key:\n                The key to use to index the cache.\n            value:\n                The value to set in the cache.\n        \"\"\"\n        hashed_key = self._hash_key(key=key)\n        self.cache[hashed_key] = value\n\n    def remove(self) -&gt; None:docs\n        \"\"\"Remove the cache from memory and delete it from disk.\"\"\"\n        self.cache_path.unlink()\n        del self.cache\n\n    def cached_texts(self) -&gt; list[str]:docs\n        \"\"\"Return the text inputs indexed in the cache.\"\"\"\n        return [key for key in self.cache.keys()]\n\n    def add_to_cache(docs\n        self, model_inputs: dict, model_output: GenerativeModelOutput\n    ) -&gt; None:\n        \"\"\"Add the model input/output to the cache.\n\n        Args:\n            model_inputs:\n                The model inputs.\n            model_output:\n                The model output.\n        \"\"\"\n        input_column = \"messages\" if \"messages\" in model_inputs else \"text\"\n        model_inputs = model_inputs[input_column]\n\n        # Store the generated sequences in the cache, one by one\n        with tqdm(\n            iterable=model_inputs,\n            desc=\"Caching model outputs\",\n            leave=False,\n            disable=hasattr(sys, \"_called_from_test\"),\n        ) as pbar:\n            for sample_idx, model_input in enumerate(pbar):\n                # Extract the scores from the model output, to be cached. We only store\n                # the indices of the top scores, to save space. Further, we only store\n                # the scores if the generated sequence is shorter than the maximum\n                # length\n                if model_output.scores is not None and self.max_generated_tokens &lt; 8:\n                    assert model_output.scores is not None\n                    scores = model_output.scores[sample_idx]\n                else:\n                    scores = None\n                self[model_input] = SingleGenerativeModelOutput(\n                    sequence=model_output.sequences[sample_idx], scores=scores\n                )\n\n\ndef split_dataset_into_cached_and_non_cached(docs\n    dataset: \"Dataset\", cache: ModelCache\n) -&gt; tuple[\"Dataset\", \"Dataset\"]:\n    \"\"\"Split a dataset into a cached and non-cached part.\n\n    Args:\n        dataset:\n            The dataset to split.\n        cache:\n            The model output cache.\n\n    Returns:\n        The cached and non-cached parts of the dataset.\n    \"\"\"\n    # Get the sample indices of the non-cached examples, which are unique with respect\n    # to the \"text\" column.\n    input_column = \"messages\" if \"messages\" in dataset.column_names else \"text\"\n    dataset_texts = pd.Series(dataset[input_column])\n    dataset_texts.drop_duplicates(inplace=True)\n    unique_non_cached_ids = set(\n        dataset_texts[~dataset_texts.isin(cache.cached_texts())].index.tolist()\n    )\n\n    # The cached examples are the ones that are not in the non-cached examples. This\n    # means that if the dataset has duplicates, only a single copy of the duplicate\n    # will be put in the non-cached part, and the rest in the cached part.\n    cached_ids = set(range(len(dataset))) - unique_non_cached_ids\n\n    cached = dataset.select(cached_ids)\n    non_cached = dataset.select(unique_non_cached_ids)\n    return cached, non_cached\n\n\ndef load_cached_model_outputs(docs\n    cached_dataset: \"Dataset\", cache: ModelCache\n) -&gt; GenerativeModelOutput:\n    \"\"\"Load the cached model outputs.\n\n    Args:\n        cached_dataset:\n            The dataset containing the cached examples.\n        cache:\n            The model output cache.\n\n    Returns:\n        The model output containing the cached sequences.\n    \"\"\"\n    input_column = \"messages\" if \"messages\" in cached_dataset.column_names else \"text\"\n    cached_model_outputs: list[SingleGenerativeModelOutput] = [\n        cache[prompt] for prompt in cached_dataset[input_column]\n    ]\n\n    cached_sequences = [model_output.sequence for model_output in cached_model_outputs]\n\n    if cached_model_outputs[0].scores is None:\n        return GenerativeModelOutput(sequences=cached_sequences)\n\n    cached_scores = [model_output.scores or [] for model_output in cached_model_outputs]\n    return GenerativeModelOutput(sequences=cached_sequences, scores=cached_scores)\n</code></pre>"},{"location":"api/scandeval/model_config/","title":"scandeval.model_config","text":"scandeval.model_config<p> source module scandeval.model_config </p> <p>Functions related to getting the model configuration.</p> <p> Functions </p> <ul> <li> <p>get_model_config \u2014 Fetches configuration for a model.</p> </li> </ul> <p> source get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetches configuration for a model.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The configuration of the benchmark.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel \u2014</p> <p>If all model setups can handle the model, but the model does not exist.</p> </li> <li> <p>NeedsExtraInstalled</p> </li> </ul>"},{"location":"src/scandeval/model_config/","title":"scandeval.model_config","text":"scandeval.model_config<p> docs module scandeval.model_config </p> <pre><code>\"\"\"Functions related to getting the model configuration.\"\"\"\n\nimport importlib.util\nimport typing as t\n\nfrom . import benchmark_modules\nfrom .enums import Framework\nfrom .exceptions import InvalidModel, NeedsEnvironmentVariable, NeedsExtraInstalled\n\nif t.TYPE_CHECKING:\n    from .data_models import BenchmarkConfig, ModelConfig\n\n\ndef get_model_config(docs\n    model_id: str, benchmark_config: \"BenchmarkConfig\"\n) -&gt; \"ModelConfig\":\n    \"\"\"Fetches configuration for a model.\n\n    Args:\n        model_id:\n            The model ID.\n        benchmark_config:\n            The configuration of the benchmark.\n\n    Returns:\n        The model configuration.\n\n    Raises:\n        InvalidModel:\n            If all model setups can handle the model, but the model does not exist.\n    \"\"\"\n    all_benchmark_modules = [\n        cls\n        for cls in benchmark_modules.__dict__.values()\n        if isinstance(cls, type)\n        and issubclass(cls, benchmark_modules.BenchmarkModule)\n        and cls is not benchmark_modules.BenchmarkModule\n    ]\n\n    needs_extras: list[str] = list()\n    needs_env_vars: list[str] = list()\n    for benchmark_module in all_benchmark_modules:\n        exists_or_err = benchmark_module.model_exists(\n            model_id=model_id, benchmark_config=benchmark_config\n        )\n        if isinstance(exists_or_err, NeedsExtraInstalled):\n            needs_extras.append(exists_or_err.extra)\n        elif isinstance(exists_or_err, NeedsEnvironmentVariable):\n            needs_env_vars.append(exists_or_err.env_var)\n        elif exists_or_err is True:\n            model_config = benchmark_module.get_model_config(\n                model_id=model_id, benchmark_config=benchmark_config\n            )\n            if (\n                model_config.framework == Framework.JAX\n                and importlib.util.find_spec(\"jax\") is None\n            ):\n                raise NeedsExtraInstalled(extra=\"jax\")\n            return model_config\n    else:\n        msg = f\"Model {model_id} not found.\"\n        if needs_extras:\n            msg += (\n                \" However, it is possible that the model exists, but a package \"\n                \"needs to be installed to check if it exists. Please try running \"\n                f\"`pip install scandeval[{','.join(needs_extras)}]` or `pip install \"\n                \"scandeval[all]`, and try again.\"\n            )\n        elif needs_env_vars:\n            msg += (\n                \" However, it is possible that the model exists, but an environment \"\n                \"variable needs to be set to check if it exists. Please set the \"\n                f\"environment variables {','.join(needs_env_vars)} and try again.\"\n            )\n        raise InvalidModel(msg)\n</code></pre>"},{"location":"api/scandeval/model_loading/","title":"scandeval.model_loading","text":"scandeval.model_loading<p> source module scandeval.model_loading </p> <p>Functions related to the loading of models.</p> <p> Functions </p> <ul> <li> <p>load_model \u2014 Load a model.</p> </li> </ul> <p> source load_model(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 BenchmarkModule </p> <p>Load a model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BenchmarkModule \u2014 The model.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul>"},{"location":"src/scandeval/model_loading/","title":"scandeval.model_loading","text":"scandeval.model_loading<p> docs module scandeval.model_loading </p> <pre><code>\"\"\"Functions related to the loading of models.\"\"\"\n\nimport typing as t\n\nfrom .benchmark_modules import (\n    FreshEncoderModel,\n    HuggingFaceEncoderModel,\n    LiteLLMModel,\n    VLLMModel,\n)\nfrom .constants import GENERATIVE_DATASET_SUPERTASKS, GENERATIVE_DATASET_TASKS\nfrom .enums import ModelType\nfrom .exceptions import InvalidBenchmark\n\nif t.TYPE_CHECKING:\n    from .benchmark_modules import BenchmarkModule\n    from .data_models import BenchmarkConfig, DatasetConfig, ModelConfig\n\n\ndef load_model(docs\n    model_config: \"ModelConfig\",\n    dataset_config: \"DatasetConfig\",\n    benchmark_config: \"BenchmarkConfig\",\n) -&gt; \"BenchmarkModule\":\n    \"\"\"Load a model.\n\n    Args:\n        model_config:\n            The model configuration.\n        dataset_config:\n            The dataset configuration.\n        benchmark_config:\n            The benchmark configuration.\n\n    Returns:\n        The model.\n    \"\"\"\n    # The order matters; the first model type that matches will be used. For this\n    # reason, they have been ordered in terms of the most common model types.\n    model_type_to_module_mapping: dict[ModelType, t.Type[BenchmarkModule]] = {\n        ModelType.HF_HUB_GENERATIVE: VLLMModel,\n        ModelType.HF_HUB_ENCODER: HuggingFaceEncoderModel,\n        ModelType.API: LiteLLMModel,\n        ModelType.FRESH: FreshEncoderModel,\n    }\n    model_class = model_type_to_module_mapping[model_config.model_type]\n\n    model = model_class(\n        model_config=model_config,\n        dataset_config=dataset_config,\n        benchmark_config=benchmark_config,\n    )\n\n    # Refuse to benchmark non-generative models on generative tasks\n    if (\n        (\n            dataset_config.task.supertask in GENERATIVE_DATASET_SUPERTASKS\n            or dataset_config.task.name in GENERATIVE_DATASET_TASKS\n        )\n        and model is not None\n        and not model.is_generative\n    ):\n        raise InvalidBenchmark(\n            f\"Cannot benchmark non-generative model {model_config.model_id!r} on \"\n            f\"generative task {dataset_config.task.name!r}.\"\n        )\n\n    return model\n</code></pre>"},{"location":"api/scandeval/scores/","title":"scandeval.scores","text":"scandeval.scores<p> source module scandeval.scores </p> <p>Aggregation of raw scores into the mean and a confidence interval.</p> <p> Functions </p> <ul> <li> <p>log_scores \u2014 Log the scores.</p> </li> <li> <p>aggregate_scores \u2014 Helper function to compute the mean with confidence intervals.</p> </li> </ul> <p> source log_scores(dataset_name: str, metric_configs: list[MetricConfig], scores: list[dict[str, float]], model_id: str) \u2192 ScoreDict </p> <p>Log the scores.</p> <p> Parameters </p> <ul> <li> <p>dataset_name :  str \u2014</p> <p>Name of the dataset.</p> </li> <li> <p>metric_configs :  list[MetricConfig] \u2014</p> <p>List of metrics to log.</p> </li> <li> <p>scores :  list[dict[str, float]] \u2014</p> <p>The scores that are to be logged. This is a list of dictionaries full of scores.</p> </li> <li> <p>model_id :  str \u2014</p> <p>The full Hugging Face Hub path to the pretrained transformer model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ScoreDict \u2014 A dictionary with keys 'raw_scores' and 'total', with 'raw_scores' being identical to <code>scores</code> and 'total' being a dictionary with the aggregated scores (means and standard errors).</p> </li> </ul> <p> source aggregate_scores(scores: list[dict[str, float]], metric_config: MetricConfig) \u2192 tuple[float, float] </p> <p>Helper function to compute the mean with confidence intervals.</p> <p> Parameters </p> <ul> <li> <p>scores :  list[dict[str, float]] \u2014</p> <p>Dictionary with the names of the metrics as keys, of the form \"_\", such as \"val_f1\", and values the metric values. <li> <p>metric_config :  MetricConfig \u2014</p> <p>The configuration of the metric, which is used to collect the correct metric from <code>scores</code>.</p> </li> <p> Returns </p> <ul> <li> <p>tuple[float, float] \u2014 A pair of floats, containing the score and the radius of its 95% confidence interval.</p> </li> </ul>"},{"location":"src/scandeval/scores/","title":"scandeval.scores","text":"scandeval.scores<p> docs module scandeval.scores </p> <pre><code>\"\"\"Aggregation of raw scores into the mean and a confidence interval.\"\"\"\n\nimport logging\nimport typing as t\nimport warnings\n\nimport numpy as np\n\nif t.TYPE_CHECKING:\n    from .data_models import MetricConfig\n    from .types import ScoreDict\n\nlogger = logging.getLogger(\"scandeval\")\n\n\ndef log_scores(docs\n    dataset_name: str,\n    metric_configs: list[\"MetricConfig\"],\n    scores: list[dict[str, float]],\n    model_id: str,\n) -&gt; \"ScoreDict\":\n    \"\"\"Log the scores.\n\n    Args:\n        dataset_name:\n            Name of the dataset.\n        metric_configs:\n            List of metrics to log.\n        scores:\n            The scores that are to be logged. This is a list of dictionaries full of\n            scores.\n        model_id:\n            The full Hugging Face Hub path to the pretrained transformer model.\n\n    Returns:\n        A dictionary with keys 'raw_scores' and 'total', with 'raw_scores' being\n        identical to `scores` and 'total' being a dictionary with the aggregated scores\n        (means and standard errors).\n    \"\"\"\n    logger.info(f\"Finished evaluation of {model_id} on {dataset_name}.\")\n\n    total_dict: dict[str, float] = dict()\n    for metric_cfg in metric_configs:\n        test_score, test_se = aggregate_scores(scores=scores, metric_config=metric_cfg)\n        test_score, test_score_str = metric_cfg.postprocessing_fn(test_score)\n        test_se, test_se_str = metric_cfg.postprocessing_fn(test_se)\n        total_dict[f\"test_{metric_cfg.name}\"] = test_score\n        total_dict[f\"test_{metric_cfg.name}_se\"] = test_se\n        logger.info(f\"{metric_cfg.pretty_name}: {test_score_str} \u00b1 {test_se_str}\")\n\n    return dict(raw=scores, total=total_dict)\n\n\ndef aggregate_scores(docs\n    scores: list[dict[str, float]], metric_config: \"MetricConfig\"\n) -&gt; tuple[float, float]:\n    \"\"\"Helper function to compute the mean with confidence intervals.\n\n    Args:\n        scores:\n            Dictionary with the names of the metrics as keys, of the form\n            \"&lt;split&gt;_&lt;metric_name&gt;\", such as \"val_f1\", and values the metric values.\n        metric_config:\n            The configuration of the metric, which is used to collect the correct\n            metric from `scores`.\n\n    Returns:\n        A pair of floats, containing the score and the radius of its 95% confidence\n        interval.\n    \"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n\n        test_scores = [\n            (\n                dct[metric_config.name]\n                if metric_config.name in dct\n                else dct[f\"test_{metric_config.name}\"]\n            )\n            for dct in scores\n        ]\n        test_score = np.mean(test_scores).item()\n\n        if len(test_scores) &gt; 1:\n            sample_std = np.std(test_scores, ddof=1)\n            test_se = sample_std / np.sqrt(len(test_scores))\n        else:\n            test_se = np.nan\n\n        return (test_score, 1.96 * test_se)\n</code></pre>"},{"location":"api/scandeval/speed_benchmark/","title":"scandeval.speed_benchmark","text":"scandeval.speed_benchmark<p> source module scandeval.speed_benchmark </p> <p>Benchmarking model inference speed.</p> <p> Functions </p> <ul> <li> <p>benchmark_speed \u2014 Benchmark model inference speed.</p> </li> <li> <p>benchmark_speed_single_iteration \u2014 Run a single iteration of the speed benchmark.</p> </li> </ul> <p> source benchmark_speed(model: BenchmarkModule, benchmark_config: BenchmarkConfig) \u2192 list[dict[str, float]] </p> <p>Benchmark model inference speed.</p> <p> Parameters </p> <ul> <li> <p>model :  BenchmarkModule \u2014</p> <p>Model to use.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>Configuration for the benchmark.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[dict[str, float]] \u2014 Dictionary of scores.</p> </li> </ul> <p> source benchmark_speed_single_iteration(model: BenchmarkModule, itr_idx: int) \u2192 dict[str, float] </p> <p>Run a single iteration of the speed benchmark.</p> <p> Parameters </p> <ul> <li> <p>model :  BenchmarkModule \u2014</p> <p>The model to use in the benchmark.</p> </li> <li> <p>itr_idx :  int \u2014</p> <p>The index of the iteration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] \u2014 A dictionary containing the scores for the current iteration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul>"},{"location":"src/scandeval/speed_benchmark/","title":"scandeval.speed_benchmark","text":"scandeval.speed_benchmark<p> docs module scandeval.speed_benchmark </p> <pre><code>\"\"\"Benchmarking model inference speed.\"\"\"\n\nimport collections.abc as c\nimport logging\nimport typing as t\n\nimport pyinfer\nfrom tqdm.auto import tqdm\nfrom transformers import AutoTokenizer\n\nfrom .benchmark_modules import (\n    BenchmarkModule,\n    HuggingFaceEncoderModel,\n    LiteLLMModel,\n    VLLMModel,\n)\nfrom .data_models import BenchmarkConfig\nfrom .exceptions import InvalidBenchmark\nfrom .utils import clear_memory\n\nlogger = logging.getLogger(\"scandeval\")\n\n\ndef benchmark_speed(docs\n    model: \"BenchmarkModule\", benchmark_config: \"BenchmarkConfig\"\n) -&gt; list[dict[str, float]]:\n    \"\"\"Benchmark model inference speed.\n\n    Args:\n        model:\n            Model to use.\n        benchmark_config:\n            Configuration for the benchmark.\n\n    Returns:\n        Dictionary of scores.\n    \"\"\"\n    scores: list[dict[str, float]] = list()\n    for idx in tqdm(\n        iterable=range(benchmark_config.num_iterations),\n        desc=\"Benchmarking\",\n        disable=not benchmark_config.progress_bar,\n    ):\n        itr_scores = benchmark_speed_single_iteration(model=model, itr_idx=idx)\n        clear_memory()\n        scores.append(itr_scores)\n        logger.debug(f\"Scores for iteration {idx}: {itr_scores}\")\n    return scores\n\n\ndef benchmark_speed_single_iteration(docs\n    model: \"BenchmarkModule\", itr_idx: int\n) -&gt; dict[str, float]:\n    \"\"\"Run a single iteration of the speed benchmark.\n\n    Args:\n        model:\n            The model to use in the benchmark.\n        itr_idx:\n            The index of the iteration.\n\n    Returns:\n        A dictionary containing the scores for the current iteration.\n    \"\"\"\n    gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n    base_doc = \"Document which contains roughly 10 tokens. \"\n    multiplier = 10 * (1 + itr_idx)\n    doc = base_doc * multiplier\n    short_multiplier = 1.25 * (1 + itr_idx)\n    short_doc = base_doc * round(short_multiplier)\n\n    def generate_messages_predict(doc: str) -&gt; None:\n        model.generate(inputs=dict(messages=[[dict(role=\"user\", content=doc)]]))\n\n    def generate_prompt_predict(doc: str) -&gt; None:\n        model.generate(inputs=dict(prompt=[doc]))\n\n    def encoder_predict(doc: str) -&gt; None:\n        tokenizer = model.get_tokenizer()\n        pytorch_model = model.get_pytorch_module()\n        inputs = {\n            key: tensor.to(pytorch_model.device)\n            for key, tensor in tokenizer(\n                text=[doc], truncation=True, return_tensors=\"pt\"\n            ).items()\n        }\n        pytorch_model(**inputs)\n\n    predict_fn_mapping: dict[t.Type[BenchmarkModule], c.Callable[[str], None]] = {\n        HuggingFaceEncoderModel: encoder_predict,\n        LiteLLMModel: generate_messages_predict,\n        VLLMModel: generate_prompt_predict,\n    }\n    predict = predict_fn_mapping[type(model)]\n\n    try:\n        # Do a warmup run, as the first run is always slower\n        pyinfer.InferenceReport(model=predict, inputs=base_doc, n_seconds=1).run(\n            print_report=False\n        )\n\n        speed_scores = pyinfer.InferenceReport(\n            model=predict, inputs=doc, n_seconds=3\n        ).run(print_report=False)\n        num_gpt2_tokens = len(gpt2_tokenizer([doc], truncation=True)[\"input_ids\"][0])\n        gpt2_tokens_per_second = speed_scores[\"Infer(p/sec)\"] * num_gpt2_tokens\n\n        speed_scores_short = pyinfer.InferenceReport(\n            model=predict, inputs=short_doc, n_seconds=3\n        ).run(print_report=False)\n        num_gpt2_tokens_short = len(\n            gpt2_tokenizer([short_doc], truncation=True)[\"input_ids\"][0]\n        )\n        gpt2_tokens_per_second_short = (\n            speed_scores_short[\"Infer(p/sec)\"] * num_gpt2_tokens_short\n        )\n\n    except (RuntimeError, ValueError, IndexError) as e:\n        raise InvalidBenchmark(f\"Speed benchmark failed with error: {e!r}\")\n\n    return dict(\n        test_speed=gpt2_tokens_per_second, test_speed_short=gpt2_tokens_per_second_short\n    )\n</code></pre>"},{"location":"api/scandeval/structured_generation_utils/","title":"scandeval.structured_generation_utils","text":"scandeval.structured_generation_utils<p> source module scandeval.structured_generation_utils </p> <p>Utility functions related to structured generation.</p> <p> Functions </p> <ul> <li> <p>get_ner_schema \u2014 Get the schema for the NER answer format, used for structured generation.</p> </li> <li> <p>get_ner_logits_processors \u2014 Get the logits processors for the NER task, used in vLLM.</p> </li> </ul> <p> source get_ner_schema(ner_tag_names: list[str]) \u2192 type[BaseModel] </p> <p>Get the schema for the NER answer format, used for structured generation.</p> <p> Parameters </p> <ul> <li> <p>ner_tag_names :  list[str] \u2014</p> <p>The NER tag names.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>type[BaseModel] \u2014 The schema for the NER answer format.</p> </li> </ul> <p> source get_ner_logits_processors(ner_tag_names: list[str], llm: LLM) \u2192 list[c.Callable[[list[int], torch.Tensor], torch.Tensor]] </p> <p>Get the logits processors for the NER task, used in vLLM.</p> <p> Parameters </p> <ul> <li> <p>ner_tag_names :  list[str] \u2014</p> <p>The NER tag names.</p> </li> <li> <p>llm :  LLM \u2014</p> <p>The vLLM model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[c.Callable[[list[int], torch.Tensor], torch.Tensor]] \u2014 The logit processors for the NER task.</p> </li> </ul>"},{"location":"src/scandeval/structured_generation_utils/","title":"scandeval.structured_generation_utils","text":"scandeval.structured_generation_utils<p> docs module scandeval.structured_generation_utils </p> <pre><code>\"\"\"Utility functions related to structured generation.\"\"\"\n\nimport collections.abc as c\nimport importlib.util\nimport typing as t\n\nimport torch\nfrom pydantic import BaseModel, conlist, create_model\n\nif importlib.util.find_spec(\"outlines\") is not None:\n    from outlines.integrations.vllm import JSONLogitsProcessor\n\nif t.TYPE_CHECKING:\n    from vllm import LLM\n\n\ndef get_ner_schema(ner_tag_names: list[str]) -&gt; type[BaseModel]:docs\n    \"\"\"Get the schema for the NER answer format, used for structured generation.\n\n    Args:\n        ner_tag_names:\n            The NER tag names.\n\n    Returns:\n        The schema for the NER answer format.\n    \"\"\"\n    keys_and_their_types: dict[str, t.Any] = {\n        tag_name: (conlist(str, max_length=5), ...) for tag_name in ner_tag_names\n    }\n    schema = create_model(\"AnswerFormat\", **keys_and_their_types)\n    return schema\n\n\ndef get_ner_logits_processors(docs\n    ner_tag_names: list[str], llm: \"LLM\"\n) -&gt; list[c.Callable[[list[int], torch.Tensor], torch.Tensor]]:\n    \"\"\"Get the logits processors for the NER task, used in vLLM.\n\n    Args:\n        ner_tag_names:\n            The NER tag names.\n        llm:\n            The vLLM model.\n\n    Returns:\n        The logit processors for the NER task.\n    \"\"\"\n    logits_processor = JSONLogitsProcessor(\n        schema=get_ner_schema(ner_tag_names=ner_tag_names),\n        llm=llm,\n        whitespace_pattern=r\" ?\",\n    )\n    return [logits_processor]\n</code></pre>"},{"location":"api/scandeval/tasks/","title":"scandeval.tasks","text":"scandeval.tasks<p> source module scandeval.tasks </p> <p>All benchmarks tasks used in ScandEval.</p> <p> Functions </p> <ul> <li> <p>get_all_tasks \u2014 Get a list of all the dataset tasks.</p> </li> </ul> <p> source get_all_tasks() \u2192 dict[str, Task] </p> <p>Get a list of all the dataset tasks.</p> <p> Returns </p> <ul> <li> <p>dict[str, Task] \u2014 A mapping between names of dataset tasks and their configurations.</p> </li> </ul>"},{"location":"src/scandeval/tasks/","title":"scandeval.tasks","text":"scandeval.tasks<p> docs module scandeval.tasks </p> <pre><code>\"\"\"All benchmarks tasks used in ScandEval.\"\"\"\n\nfrom .data_models import MetricConfig, Task\n\n\ndef get_all_tasks() -&gt; dict[str, Task]:docs\n    \"\"\"Get a list of all the dataset tasks.\n\n    Returns:\n        A mapping between names of dataset tasks and their configurations.\n    \"\"\"\n    return {cfg.name: cfg for cfg in globals().values() if isinstance(cfg, Task)}\n\n\nLA = Task(\n    name=\"linguistic-acceptability\",\n    supertask=\"sequence-classification\",\n    metrics=[\n        MetricConfig(\n            name=\"mcc\",\n            pretty_name=\"Matthew's Correlation Coefficient\",\n            huggingface_id=\"matthews_correlation\",\n            results_key=\"matthews_correlation\",\n        ),\n        MetricConfig(\n            name=\"macro_f1\",\n            pretty_name=\"Macro-average F1-score\",\n            huggingface_id=\"f1\",\n            results_key=\"f1\",\n            compute_kwargs=dict(average=\"macro\"),\n        ),\n    ],\n    labels=[\"incorrect\", \"correct\"],\n)\n\n\nNER = Task(\n    name=\"named-entity-recognition\",\n    supertask=\"token-classification\",\n    metrics=[\n        MetricConfig(\n            name=\"micro_f1_no_misc\",\n            pretty_name=\"Micro-average F1-score without MISC tags\",\n            huggingface_id=\"seqeval\",\n            results_key=\"overall_f1\",\n        ),\n        MetricConfig(\n            name=\"micro_f1\",\n            pretty_name=\"Micro-average F1-score with MISC tags\",\n            huggingface_id=\"seqeval\",\n            results_key=\"overall_f1\",\n        ),\n    ],\n    labels=[\n        \"o\",\n        \"b-loc\",\n        \"i-loc\",\n        \"b-org\",\n        \"i-org\",\n        \"b-per\",\n        \"i-per\",\n        \"b-misc\",\n        \"i-misc\",\n    ],\n)\n\n\nRC = Task(\n    name=\"reading-comprehension\",\n    supertask=\"question-answering\",\n    metrics=[\n        MetricConfig(\n            name=\"em\",\n            pretty_name=\"Exact Match\",\n            huggingface_id=\"squad_v2\",\n            results_key=\"exact\",\n            postprocessing_fn=lambda raw_score: (raw_score, f\"{raw_score:.2f}%\"),\n        ),\n        MetricConfig(\n            name=\"f1\",\n            pretty_name=\"F1-score\",\n            huggingface_id=\"squad_v2\",\n            results_key=\"f1\",\n            postprocessing_fn=lambda raw_score: (raw_score, f\"{raw_score:.2f}%\"),\n        ),\n    ],\n    labels=[\"start_positions\", \"end_positions\"],\n)\n\n\nSENT = Task(\n    name=\"sentiment-classification\",\n    supertask=\"sequence-classification\",\n    metrics=[\n        MetricConfig(\n            name=\"mcc\",\n            pretty_name=\"Matthew's Correlation Coefficient\",\n            huggingface_id=\"matthews_correlation\",\n            results_key=\"matthews_correlation\",\n        ),\n        MetricConfig(\n            name=\"macro_f1\",\n            pretty_name=\"Macro-average F1-score\",\n            huggingface_id=\"f1\",\n            results_key=\"f1\",\n            compute_kwargs=dict(average=\"macro\"),\n        ),\n    ],\n    labels=[\"negative\", \"neutral\", \"positive\"],\n)\n\n\nSUMM = Task(\n    name=\"summarization\",\n    supertask=\"text-to-text\",\n    metrics=[\n        MetricConfig(\n            name=\"bertscore\",\n            pretty_name=\"BERTScore\",\n            huggingface_id=\"bertscore\",\n            results_key=\"f1\",\n            compute_kwargs=dict(\n                model_type=\"microsoft/mdeberta-v3-base\", device=\"auto\", batch_size=32\n            ),\n        ),\n        MetricConfig(\n            name=\"rouge_l\",\n            pretty_name=\"ROUGE-L\",\n            huggingface_id=\"rouge\",\n            results_key=\"rougeL\",\n        ),\n    ],\n    labels=[],\n)\n\n\nKNOW = Task(\n    name=\"knowledge\",\n    supertask=\"sequence-classification\",\n    metrics=[\n        MetricConfig(\n            name=\"mcc\",\n            pretty_name=\"Matthew's Correlation Coefficient\",\n            huggingface_id=\"matthews_correlation\",\n            results_key=\"matthews_correlation\",\n        ),\n        MetricConfig(\n            name=\"accuracy\",\n            pretty_name=\"Accuracy\",\n            huggingface_id=\"accuracy\",\n            results_key=\"accuracy\",\n        ),\n    ],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n)\n\n\nMCRC = Task(\n    name=\"multiple-choice-reading-comprehension\",\n    supertask=\"sequence-classification\",\n    metrics=[\n        MetricConfig(\n            name=\"mcc\",\n            pretty_name=\"Matthew's Correlation Coefficient\",\n            huggingface_id=\"matthews_correlation\",\n            results_key=\"matthews_correlation\",\n        ),\n        MetricConfig(\n            name=\"accuracy\",\n            pretty_name=\"Accuracy\",\n            huggingface_id=\"accuracy\",\n            results_key=\"accuracy\",\n        ),\n    ],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n)\n\n\nCOMMON_SENSE = Task(\n    name=\"common-sense-reasoning\",\n    supertask=\"sequence-classification\",\n    metrics=[\n        MetricConfig(\n            name=\"mcc\",\n            pretty_name=\"Matthew's Correlation Coefficient\",\n            huggingface_id=\"matthews_correlation\",\n            results_key=\"matthews_correlation\",\n        ),\n        MetricConfig(\n            name=\"accuracy\",\n            pretty_name=\"Accuracy\",\n            huggingface_id=\"accuracy\",\n            results_key=\"accuracy\",\n        ),\n    ],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n)\n\n\nTEXT_MODELLING = Task(\n    name=\"text-modelling\",\n    supertask=\"text-modelling\",\n    metrics=[\n        MetricConfig(\n            name=\"perplexity\",\n            pretty_name=\"Perplexity\",\n            huggingface_id=\"perplexity\",\n            results_key=\"mean_perplexity\",\n        )\n    ],\n    labels=[],\n)\n\n\nSPEED = Task(\n    name=\"speed\",\n    supertask=\"sequence-classification\",\n    metrics=[\n        MetricConfig(\n            name=\"speed\",\n            pretty_name=\"Tokens per second\",\n            huggingface_id=\"\",\n            results_key=\"speed\",\n            postprocessing_fn=lambda raw_score: (raw_score, f\"{raw_score:,.0f}\"),\n        ),\n        MetricConfig(\n            name=\"speed_short\",\n            pretty_name=\"Tokens per second on short documents\",\n            huggingface_id=\"\",\n            results_key=\"speed\",\n            postprocessing_fn=lambda raw_score: (raw_score, f\"{raw_score:,.0f}\"),\n        ),\n    ],\n    labels=[],\n)\n</code></pre>"},{"location":"api/scandeval/types/","title":"scandeval.types","text":"scandeval.types<p> source module scandeval.types </p> <p>Types used throughout the project.</p> <p> Classes </p> <ul> <li> <p>ComputeMetricsFunction \u2014 A function used to compute the metrics.</p> </li> <li> <p>ExtractLabelsFunction \u2014 A function used to extract the labels from the generated output.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>is_list_of_int \u2014 Check if an object is a list of integers.</p> </li> <li> <p>is_list_of_list_of_int \u2014 Check if an object is a list of list of integers.</p> </li> <li> <p>is_list_of_str \u2014 Check if an object is a list of integers.</p> </li> </ul> <p> source class ComputeMetricsFunction() </p> <p><p>Bases : t.Protocol</p></p> <p>A function used to compute the metrics.</p> <p> source class ExtractLabelsFunction() </p> <p><p>Bases : t.Protocol</p></p> <p>A function used to extract the labels from the generated output.</p> <p> source is_list_of_int(x: t.Any) \u2192 t.TypeGuard[list[int]] </p> <p>Check if an object is a list of integers.</p> <p> Parameters </p> <ul> <li> <p>x :  t.Any \u2014</p> <p>The object to check.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>t.TypeGuard[list[int]] \u2014 Whether the object is a list of integers.</p> </li> </ul> <p> source is_list_of_list_of_int(x: t.Any) \u2192 t.TypeGuard[list[list[int]]] </p> <p>Check if an object is a list of list of integers.</p> <p> Parameters </p> <ul> <li> <p>x :  t.Any \u2014</p> <p>The object to check.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>t.TypeGuard[list[list[int]]] \u2014 Whether the object is a list of list of integers.</p> </li> </ul> <p> source is_list_of_str(x: t.Any) \u2192 t.TypeGuard[list[str]] </p> <p>Check if an object is a list of integers.</p> <p> Parameters </p> <ul> <li> <p>x :  t.Any \u2014</p> <p>The object to check.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>t.TypeGuard[list[str]] \u2014 Whether the object is a list of strings.</p> </li> </ul>"},{"location":"src/scandeval/types/","title":"scandeval.types","text":"scandeval.types<p> docs module scandeval.types </p> <pre><code>\"\"\"Types used throughout the project.\"\"\"\n\nimport typing as t\n\nfrom numpy.typing import NDArray\n\nif t.TYPE_CHECKING:\n    from .data_models import GenerativeModelOutput\n\n\nScoreDict = dict[str, dict[str, float] | list[dict[str, float]]]\nPredictions = NDArray | list[str] | list[list[str]]\nLabels = NDArray | list[str] | list[list[str]]\n\n\nclass ComputeMetricsFunction(t.Protocol):docs\n    \"\"\"A function used to compute the metrics.\"\"\"\n\n    def __call__(\n        self,\n        model_outputs_and_labels: tuple[\n            NDArray | list[str] | list[list[str]], NDArray | list[str] | list[list[str]]\n        ],\n        id2label: dict[int, str],\n    ) -&gt; dict[str, float]:\n        \"\"\"Compute the metrics.\n\n        Args:\n            model_outputs_and_labels:\n                The model outputs and labels.\n            id2label:\n                The mapping from label IDs to labels.\n\n        Returns:\n            The computed metrics.\n        \"\"\"\n        ...\n\n\nclass ExtractLabelsFunction(t.Protocol):docs\n    \"\"\"A function used to extract the labels from the generated output.\"\"\"\n\n    def __call__(\n        self, input_batch: dict[str, list], model_output: \"GenerativeModelOutput\"\n    ) -&gt; list[str]:\n        \"\"\"Extract the labels from the generated output.\n\n        Args:\n            input_batch:\n                The input batch.\n            model_output:\n                The model output.\n\n        Returns:\n            The extracted labels.\n        \"\"\"\n        ...\n\n\ndef is_list_of_int(x: t.Any) -&gt; t.TypeGuard[list[int]]:docs\n    \"\"\"Check if an object is a list of integers.\n\n    Args:\n        x:\n            The object to check.\n\n    Returns:\n        Whether the object is a list of integers.\n    \"\"\"\n    return isinstance(x, list) and all(isinstance(i, int) for i in x)\n\n\ndef is_list_of_list_of_int(x: t.Any) -&gt; t.TypeGuard[list[list[int]]]:docs\n    \"\"\"Check if an object is a list of list of integers.\n\n    Args:\n        x:\n            The object to check.\n\n    Returns:\n        Whether the object is a list of list of integers.\n    \"\"\"\n    return (\n        isinstance(x, list)\n        and all(isinstance(i, list) for i in x)\n        and all(isinstance(j, int) for i in x for j in i)\n    )\n\n\ndef is_list_of_str(x: t.Any) -&gt; t.TypeGuard[list[str]]:docs\n    \"\"\"Check if an object is a list of integers.\n\n    Args:\n        x:\n            The object to check.\n\n    Returns:\n        Whether the object is a list of strings.\n    \"\"\"\n    return isinstance(x, list) and all(isinstance(i, str) for i in x)\n</code></pre>"},{"location":"api/scandeval/utils/","title":"scandeval.utils","text":"scandeval.utils<p> source module scandeval.utils </p> <p>Utility functions to be used in other scripts.</p> <p> Classes </p> <ul> <li> <p>HiddenPrints \u2014 Context manager which removes all terminal output.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>create_model_cache_dir \u2014 Create cache directory for a model.</p> </li> <li> <p>clear_memory \u2014 Clears the memory of unused items.</p> </li> <li> <p>enforce_reproducibility \u2014 Ensures reproducibility of experiments.</p> </li> <li> <p>is_module_installed \u2014 Check if a module is installed.</p> </li> <li> <p>block_terminal_output \u2014 Blocks libraries from writing output to the terminal.</p> </li> <li> <p>get_class_by_name \u2014 Get a class by its name.</p> </li> <li> <p>kebab_to_pascal \u2014 Converts a kebab-case string to PascalCase.</p> </li> <li> <p>internet_connection_available \u2014 Checks if internet connection is available by pinging google.com.</p> </li> <li> <p>get_special_token_metadata \u2014 Get the special token metadata for a tokenizer.</p> </li> <li> <p>raise_if_model_output_contains_nan_values \u2014 Raise an exception if the model output contains NaN values.</p> </li> <li> <p>should_prompts_be_stripped \u2014 Determine if we should strip the prompts for few-shot evaluation.</p> </li> <li> <p>should_prefix_space_be_added_to_labels \u2014 Determine if we should add a prefix space to the labels.</p> </li> <li> <p>get_end_of_chat_token_ids \u2014 Get the end token ID for chat models.</p> </li> <li> <p>scramble \u2014 Scramble a string in a bijective manner.</p> </li> <li> <p>unscramble \u2014 Unscramble a string in a bijective manner.</p> </li> <li> <p>log_once \u2014 Log a message once.</p> </li> </ul> <p> source create_model_cache_dir(cache_dir: str, model_id: str) \u2192 str </p> <p>Create cache directory for a model.</p> <p> Parameters </p> <ul> <li> <p>cache_dir :  str \u2014</p> <p>The cache directory.</p> </li> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The path to the cache directory.</p> </li> </ul> <p> source clear_memory() </p> <p>Clears the memory of unused items.</p> <p> source enforce_reproducibility(framework: Framework | str, seed: int = 4242) </p> <p>Ensures reproducibility of experiments.</p> <p> Parameters </p> <ul> <li> <p>framework :  Framework | str \u2014</p> <p>The framework used for the benchmarking.</p> </li> <li> <p>seed :  int \u2014</p> <p>Seed for the random number generator.</p> </li> </ul> <p> source is_module_installed(module: str) \u2192 bool </p> <p>Check if a module is installed.</p> <p>This is used when dealing with spaCy models, as these are installed as separate Python packages.</p> <p> Parameters </p> <ul> <li> <p>module :  str \u2014</p> <p>The name of the module.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool \u2014 Whether the module is installed or not.</p> </li> </ul> <p> source block_terminal_output() </p> <p>Blocks libraries from writing output to the terminal.</p> <p>This filters warnings from some libraries, sets the logging level to ERROR for some libraries, disabled tokeniser progress bars when using Hugging Face tokenisers, and disables most of the logging from the <code>transformers</code> library.</p> <p> source get_class_by_name(class_name: str | list[str], module_name: str | None = None) \u2192 t.Type | None </p> <p>Get a class by its name.</p> <p> Parameters </p> <ul> <li> <p>class_name :  str | list[str] \u2014</p> <p>The name of the class, written in kebab-case. The corresponding class name must be the same, but written in PascalCase, and lying in a module with the same name, but written in snake_case. If a list of strings is passed, the first class that is found is returned.</p> </li> <li> <p>module_name :  str | None \u2014</p> <p>The name of the module where the class is located. If None then the module name is assumed to be the same as the class name, but written in snake_case. Defaults to None.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>t.Type | None \u2014 The class. If the class is not found, None is returned.</p> </li> </ul> <p> source kebab_to_pascal(kebab_string: str) \u2192 str </p> <p>Converts a kebab-case string to PascalCase.</p> <p> Parameters </p> <ul> <li> <p>kebab_string :  str \u2014</p> <p>The kebab-case string.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The PascalCase string.</p> </li> </ul> <p> source internet_connection_available() \u2192 bool </p> <p>Checks if internet connection is available by pinging google.com.</p> <p> Returns </p> <ul> <li> <p>bool \u2014 Whether or not internet connection is available.</p> </li> </ul> <p> source get_special_token_metadata(tokenizer: PreTrainedTokenizer) \u2192 dict </p> <p>Get the special token metadata for a tokenizer.</p> <p> Parameters </p> <ul> <li> <p>tokenizer :  PreTrainedTokenizer \u2014</p> <p>The tokenizer.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict \u2014 The special token metadata.</p> </li> </ul> <p> source class HiddenPrints() </p> <p>Context manager which removes all terminal output.</p> <p> source raise_if_model_output_contains_nan_values(model_output: Predictions) \u2192 None </p> <p>Raise an exception if the model output contains NaN values.</p> <p> Parameters </p> <ul> <li> <p>model_output :  Predictions \u2014</p> <p>The model output to check.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>If the model output contains NaN values.</p> </li> <li> <p>NaNValueInModelOutput</p> </li> </ul> <p> source should_prompts_be_stripped(labels_to_be_generated: list[str], tokenizer: PreTrainedTokenizer) \u2192 bool </p> <p>Determine if we should strip the prompts for few-shot evaluation.</p> <p>This is the case if the tokenizer needs to include the space as part of the label token. The strategy is thus to tokenize a label with a preceeding colon (as in the prompts), i.e., \": positive\", and check if the tokenization starts with the tokens of \": \". If this is the case, then we should not strip the prompts, since the tokenizer produces the whitespace token separately.</p> <p> Parameters </p> <ul> <li> <p>labels_to_be_generated :  list[str] \u2014</p> <p>The labels that are to be generated.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer \u2014</p> <p>The tokenizer used to tokenize the labels.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool \u2014 Whether we should strip the prompts.</p> </li> </ul> <p> source should_prefix_space_be_added_to_labels(labels_to_be_generated: list[str], tokenizer: PreTrainedTokenizer) \u2192 bool </p> <p>Determine if we should add a prefix space to the labels.</p> <p>This is the case if the prompts are stripped and the tokenizer doesn't automatically add prefix whitespaces to the labels.</p> <p> Parameters </p> <ul> <li> <p>labels_to_be_generated :  list[str] \u2014</p> <p>The labels that are to be generated.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer \u2014</p> <p>The tokenizer used to tokenize the labels.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool \u2014 Whether we should add a prefix space to the labels.</p> </li> </ul> <p> source get_end_of_chat_token_ids(tokenizer: PreTrainedTokenizer) \u2192 list[int] | None </p> <p>Get the end token ID for chat models.</p> <p>This is only relevant for tokenizers with a chat template.</p> <p> Parameters </p> <ul> <li> <p>tokenizer :  PreTrainedTokenizer \u2014</p> <p>The tokenizer.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[int] | None \u2014 The token IDs used to end chats, or None if the tokenizer does not have a chat template.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014</p> <p>If the end-of-chat token could not be located.</p> </li> </ul> <p> source scramble(text: str) \u2192 str </p> <p>Scramble a string in a bijective manner.</p> <p> Parameters </p> <ul> <li> <p>text :  str \u2014</p> <p>The string to scramble.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The scrambled string.</p> </li> </ul> <p> source unscramble(scrambled_text: str) \u2192 str </p> <p>Unscramble a string in a bijective manner.</p> <p> Parameters </p> <ul> <li> <p>scrambled_text :  str \u2014</p> <p>The scrambled string to unscramble.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The unscrambled string.</p> </li> </ul> <p> source log_once(message: str, level: int = logging.INFO) \u2192 None </p> <p>Log a message once.</p> <p>This is ensured by caching the input/output pairs of this function, using the <code>functools.cache</code> decorator.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to log.</p> </li> <li> <p>level :  int \u2014</p> <p>The logging level. Defaults to logging.INFO.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError</p> </li> </ul>"},{"location":"src/scandeval/utils/","title":"scandeval.utils","text":"scandeval.utils<p> docs module scandeval.utils </p> <pre><code>\"\"\"Utility functions to be used in other scripts.\"\"\"\n\nimport gc\nimport importlib\nimport importlib.util\nimport logging\nimport os\nimport random\nimport re\nimport sys\nimport typing as t\nimport warnings\nfrom functools import cache\nfrom pathlib import Path\n\nimport litellm\nimport numpy as np\nimport pkg_resources\nimport requests\nimport torch\nfrom datasets.utils import disable_progress_bar\nfrom requests.exceptions import RequestException\nfrom transformers import PreTrainedTokenizer\nfrom transformers import logging as tf_logging\n\nfrom .enums import Framework\nfrom .exceptions import NaNValueInModelOutput\n\nif importlib.util.find_spec(\"ray\") is not None:\n    import ray\n\nif t.TYPE_CHECKING:\n    from .types import Predictions\n\n\nlogger = logging.getLogger(\"scandeval\")\n\n\ndef create_model_cache_dir(cache_dir: str, model_id: str) -&gt; str:docs\n    \"\"\"Create cache directory for a model.\n\n    Args:\n        cache_dir:\n            The cache directory.\n        model_id:\n            The model ID.\n\n    Returns:\n        The path to the cache directory.\n    \"\"\"\n    # to avoid nesting due to models name containing '/'\n    _model_id = model_id.replace(\"/\", \"--\")\n    cache_dir_path = Path(cache_dir) / \"model_cache\" / _model_id\n    return str(cache_dir_path)\n\n\ndef clear_memory():docs\n    \"\"\"Clears the memory of unused items.\"\"\"\n    for gc_generation in range(3):\n        gc.collect(generation=gc_generation)\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    if torch.backends.mps.is_available():\n        torch.mps.empty_cache()\n\ndocs\ndef enforce_reproducibility(framework: Framework | str, seed: int = 4242):\n    \"\"\"Ensures reproducibility of experiments.\n\n    Args:\n        framework:\n            The framework used for the benchmarking.\n        seed:\n            Seed for the random number generator.\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    rng = np.random.default_rng(seed)\n    if framework in (Framework.PYTORCH, Framework.JAX):\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cudnn.deterministic = True\n        torch.use_deterministic_algorithms(True, warn_only=True)\n    return rng\n\n\ndef is_module_installed(module: str) -&gt; bool:docs\n    \"\"\"Check if a module is installed.\n\n    This is used when dealing with spaCy models, as these are installed as separate\n    Python packages.\n\n    Args:\n        module:\n            The name of the module.\n\n    Returns:\n        Whether the module is installed or not.\n    \"\"\"\n    # Get list of all modules, including their versions\n    installed_modules_with_versions = list(pkg_resources.working_set)\n\n    # Strip the module versions from the list of modules. Also make the modules lower\n    # case and replace dashes with underscores\n    installed_modules = [\n        re.sub(\"[0-9. ]\", \"\", str(module)).lower().replace(\"-\", \"_\")\n        for module in installed_modules_with_versions\n    ]\n\n    # Check if the module is installed by checking if the module name is in the list\n    return module.lower() in installed_modules\n\n\ndef block_terminal_output():docs\n    \"\"\"Blocks libraries from writing output to the terminal.\n\n    This filters warnings from some libraries, sets the logging level to ERROR for some\n    libraries, disabled tokeniser progress bars when using Hugging Face tokenisers, and\n    disables most of the logging from the `transformers` library.\n    \"\"\"\n    # Ignore miscellaneous warnings\n    warnings.filterwarnings(\"ignore\", category=UserWarning)\n    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n    warnings.filterwarnings(\n        \"ignore\",\n        module=\"torch.nn.parallel*\",\n        message=\"Was asked to gather along dimension 0, but all input tensors were \"\n        \"scalars; will instead unsqueeze and return a vector.\",\n    )\n    warnings.filterwarnings(\"ignore\", module=\"seqeval*\")\n\n    # Up the logging level, to disable outputs\n    logging.getLogger(\"filelock\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"absl\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"datasets\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"openai\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"torch.distributed.distributed_c10d\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"torch.nn.parallel.distributed\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"vllm\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"vllm.engine.llm_engine\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"vllm.transformers_utils.tokenizer\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"vllm.core.scheduler\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"vllm.model_executor.weight_utils\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"httpx\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"ray._private.worker\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"matplotlib.font_manager\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"accelerate\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"LiteLLM\").setLevel(logging.CRITICAL)\n\n    # This suppresses vLLM logging\n    os.environ[\"LOG_LEVEL\"] = \"CRITICAL\"\n    os.environ[\"VLLM_CONFIGURE_LOGGING\"] = \"0\"\n\n    if importlib.util.find_spec(\"ray\") is not None:\n        ray._private.worker._worker_logs_enabled = False\n\n    # Disable the tokeniser progress bars\n    disable_progress_bar()\n\n    # Disable most of the `transformers` logging\n    tf_logging._default_log_level = logging.CRITICAL\n    tf_logging.set_verbosity(logging.CRITICAL)\n    logging.getLogger(\"transformers.trainer\").setLevel(logging.CRITICAL)\n\n    # Disable logging from `litellm`\n    litellm.suppress_debug_info = True\n\n\ndef get_class_by_name(docs\n    class_name: str | list[str], module_name: str | None = None\n) -&gt; t.Type | None:\n    \"\"\"Get a class by its name.\n\n    Args:\n        class_name:\n            The name of the class, written in kebab-case. The corresponding class name\n            must be the same, but written in PascalCase, and lying in a module with the\n            same name, but written in snake_case. If a list of strings is passed, the\n            first class that is found is returned.\n        module_name:\n            The name of the module where the class is located. If None then the module\n            name is assumed to be the same as the class name, but written in\n            snake_case. Defaults to None.\n\n    Returns:\n        The class. If the class is not found, None is returned.\n    \"\"\"\n    if isinstance(class_name, str):\n        class_name = [class_name]\n\n    error_messages = list()\n    for name in class_name:\n        name_snake = name.replace(\"-\", \"_\")\n        name_pascal = kebab_to_pascal(kebab_string=name)\n\n        module_names = (\n            [module_name]\n            if module_name is not None\n            else [\n                f\"scandeval.{name_snake}\",\n                f\"scandeval.benchmark_datasets.{name_snake}\",\n                f\"scandeval.benchmark_modules.{name_snake}\",\n            ]\n        )\n        for m_name in module_names:\n            try:\n                module = importlib.import_module(name=m_name)\n                class_: t.Type = getattr(module, name_pascal)\n                return class_\n            except (ModuleNotFoundError, AttributeError) as e:\n                error_messages.append(str(e))\n\n    if error_messages:\n        errors = \"\\n- \" + \"\\n- \".join(error_messages)\n        logger.debug(\n            f\"Could not find the class with the name(s) {', '.join(class_name)}. The \"\n            f\"following error messages were raised: {errors}\"\n        )\n\n    # If the class could not be found, return None\n    return None\n\n\ndef kebab_to_pascal(kebab_string: str) -&gt; str:docs\n    \"\"\"Converts a kebab-case string to PascalCase.\n\n    Args:\n        kebab_string:\n            The kebab-case string.\n\n    Returns:\n        The PascalCase string.\n    \"\"\"\n    return \"\".join(word.title() for word in kebab_string.split(\"-\"))\n\n\ndef internet_connection_available() -&gt; bool:docs\n    \"\"\"Checks if internet connection is available by pinging google.com.\n\n    Returns:\n        Whether or not internet connection is available.\n    \"\"\"\n    try:\n        requests.get(\"https://www.google.com\")\n        return True\n    except RequestException:\n        return False\n\ndocs\ndef get_special_token_metadata(tokenizer: \"PreTrainedTokenizer\") -&gt; dict:\n    \"\"\"Get the special token metadata for a tokenizer.\n\n    Args:\n        tokenizer:\n            The tokenizer.\n\n    Returns:\n        The special token metadata.\n    \"\"\"\n    # Create some test input IDs, to check if the tokenizer is adding special tokens\n    test_input_ids = tokenizer(\"Test\").input_ids\n\n    # Extract the CLS token IDs from the tokenizer, if it's using them\n    has_cls_token = True\n    if tokenizer.cls_token_id in test_input_ids:\n        cls_token_id = tokenizer.cls_token_id\n        cls_token = tokenizer.cls_token\n    elif tokenizer.bos_token_id in test_input_ids:\n        cls_token_id = tokenizer.bos_token_id\n        cls_token = tokenizer.bos_token\n    elif tokenizer.cls_token is not None:\n        cls_token_id = tokenizer.cls_token_id\n        cls_token = tokenizer.cls_token\n        has_cls_token = False\n    else:\n        cls_token_id = tokenizer.bos_token_id\n        cls_token = tokenizer.bos_token\n        has_cls_token = False\n\n    # Extract the SEP token IDs from the tokenizer, if it's using them\n    has_sep_token = True\n    if tokenizer.sep_token_id in test_input_ids:\n        sep_token = tokenizer.sep_token\n    elif tokenizer.eos_token_id in test_input_ids:\n        sep_token = tokenizer.eos_token\n    elif tokenizer.sep_token is not None:\n        sep_token = tokenizer.sep_token\n        has_sep_token = False\n    else:\n        sep_token = tokenizer.eos_token\n        has_sep_token = False\n\n    return dict(\n        cls_token_id=cls_token_id,\n        cls_token=cls_token,\n        sep_token=sep_token,\n        has_cls_token=has_cls_token,\n        has_sep_token=has_sep_token,\n    )\n\n\nclass HiddenPrints:docs\n    \"\"\"Context manager which removes all terminal output.\"\"\"\n\n    def __enter__(self):\n        \"\"\"Enter the context manager.\"\"\"\n        self._original_stdout = sys.stdout\n        self._original_stderr = sys.stderr\n        sys.stdout = open(os.devnull, \"w\")\n        sys.stderr = open(os.devnull, \"w\")\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Exit the context manager.\"\"\"\n        sys.stdout.close()\n        sys.stderr.close()\n        sys.stdout = self._original_stdout\n        sys.stderr = self._original_stderr\n\ndocs\ndef raise_if_model_output_contains_nan_values(model_output: \"Predictions\") -&gt; None:\n    \"\"\"Raise an exception if the model output contains NaN values.\n\n    Args:\n        model_output:\n            The model output to check.\n\n    Raises:\n        If the model output contains NaN values.\n    \"\"\"\n    if isinstance(model_output, np.ndarray):\n        if model_output.dtype == np.float32 and np.isnan(model_output).any():\n            raise NaNValueInModelOutput()\n    elif len(model_output) &gt; 0:\n        if isinstance(model_output[0], str):\n            if any(x != x for x in model_output):\n                raise NaNValueInModelOutput()\n        elif len(model_output[0]) &gt; 0:\n            if any(x != x for sublist in model_output for x in sublist):\n                raise NaNValueInModelOutput()\n\n\ndef should_prompts_be_stripped(docs\n    labels_to_be_generated: list[str], tokenizer: \"PreTrainedTokenizer\"\n) -&gt; bool:\n    \"\"\"Determine if we should strip the prompts for few-shot evaluation.\n\n    This is the case if the tokenizer needs to include the space as part of the label\n    token. The strategy is thus to tokenize a label with a preceeding colon (as in the\n    prompts), i.e., \": positive\", and check if the tokenization starts with the tokens\n    of \": \". If this is the case, then we should not strip the prompts, since the\n    tokenizer produces the whitespace token separately.\n\n    Args:\n        labels_to_be_generated:\n            The labels that are to be generated.\n        tokenizer:\n            The tokenizer used to tokenize the labels.\n\n    Returns:\n        Whether we should strip the prompts.\n    \"\"\"\n    strip_prompts = True\n    for label in labels_to_be_generated:\n        colon_tokens = tokenizer(\": \", add_special_tokens=False).input_ids\n        label_tokens = tokenizer(\": \" + label, add_special_tokens=False).input_ids\n\n        if isinstance(colon_tokens, torch.Tensor):\n            colon_tokens = list(colon_tokens.squeeze(0))\n        if isinstance(label_tokens, torch.Tensor):\n            label_tokens = list(label_tokens.squeeze(0))\n\n        label_tokens_start_with_colon_tokens = (\n            label_tokens[: len(colon_tokens)] == colon_tokens\n        )\n        if label_tokens_start_with_colon_tokens:\n            strip_prompts = False\n\n    return strip_prompts\n\n\n# TODO: This is currently not used - maybe remove.\ndef should_prefix_space_be_added_to_labels(docs\n    labels_to_be_generated: list[str], tokenizer: \"PreTrainedTokenizer\"\n) -&gt; bool:\n    \"\"\"Determine if we should add a prefix space to the labels.\n\n    This is the case if the prompts are stripped and the tokenizer doesn't\n    automatically add prefix whitespaces to the labels.\n\n    Args:\n        labels_to_be_generated:\n            The labels that are to be generated.\n        tokenizer:\n            The tokenizer used to tokenize the labels.\n\n    Returns:\n        Whether we should add a prefix space to the labels.\n    \"\"\"\n    if not should_prompts_be_stripped(\n        labels_to_be_generated=labels_to_be_generated, tokenizer=tokenizer\n    ):\n        return False\n\n    whitespace_token = tokenizer.convert_ids_to_tokens(\n        ids=tokenizer(\" \", add_special_tokens=False).input_ids[0]\n    )[0]\n\n    add_prefix_space = True\n    for label in labels_to_be_generated:\n        label_tokens = tokenizer(label, add_special_tokens=False).input_ids\n        if isinstance(label_tokens, torch.Tensor):\n            label_tokens = list(label_tokens.squeeze(0))\n        first_label_token: int = int(label_tokens[0])\n        first_character_of_label = tokenizer.convert_ids_to_tokens(first_label_token)[0]\n        has_prefix_space = first_character_of_label == whitespace_token\n        if has_prefix_space:\n            add_prefix_space = False\n            break\n\n    return add_prefix_space\n\ndocs\ndef get_end_of_chat_token_ids(tokenizer: \"PreTrainedTokenizer\") -&gt; list[int] | None:\n    \"\"\"Get the end token ID for chat models.\n\n    This is only relevant for tokenizers with a chat template.\n\n    Args:\n        tokenizer:\n            The tokenizer.\n\n    Returns:\n        The token IDs used to end chats, or None if the tokenizer does not have a chat\n        template.\n\n    Raises:\n        ValueError:\n            If the end-of-chat token could not be located.\n    \"\"\"\n    if tokenizer.chat_template is None:\n        return None\n\n    user_message: dict[t.Literal[\"role\", \"content\"], str] = dict()\n    user_message[\"role\"] = \"user\"\n    user_message[\"content\"] = \"X\"\n    token_ids = tokenizer.apply_chat_template(conversation=[user_message])\n    assert isinstance(token_ids, list)\n\n    for idx, token in enumerate(tokenizer.convert_ids_to_tokens(token_ids)):\n        token_id = tokenizer.convert_tokens_to_ids(token)\n        assert isinstance(token_id, int)\n        token = tokenizer.decode([token_id])\n        if \"X\" in token:\n            x_token_index = idx\n            break\n    else:\n        raise ValueError(\"Could not locate the end-of-chat token for the model.\")\n\n    end_of_chat_tokens = token_ids[x_token_index + 1 :]\n    if len(end_of_chat_tokens) == 0:\n        return None\n    return end_of_chat_tokens\n\n\ndef scramble(text: str) -&gt; str:docs\n    \"\"\"Scramble a string in a bijective manner.\n\n    Args:\n        text:\n            The string to scramble.\n\n    Returns:\n        The scrambled string.\n    \"\"\"\n    rng = np.random.default_rng(seed=4242)\n    permutation = rng.permutation(x=len(text))\n    scrambled = \"\".join(text[i] for i in permutation)\n    return scrambled\n\n\ndef unscramble(scrambled_text: str) -&gt; str:docs\n    \"\"\"Unscramble a string in a bijective manner.\n\n    Args:\n        scrambled_text:\n            The scrambled string to unscramble.\n\n    Returns:\n        The unscrambled string.\n    \"\"\"\n    rng = np.random.default_rng(seed=4242)\n    permutation = rng.permutation(x=len(scrambled_text))\n    inverse_permutation = np.argsort(permutation)\n    unscrambled = \"\".join(scrambled_text[i] for i in inverse_permutation)\n    return unscrambled\n\n\n@cache\ndef log_once(message: str, level: int = logging.INFO) -&gt; None:docs\n    \"\"\"Log a message once.\n\n    This is ensured by caching the input/output pairs of this function, using the\n    `functools.cache` decorator.\n\n    Args:\n        message:\n            The message to log.\n        level:\n            The logging level. Defaults to logging.INFO.\n    \"\"\"\n    match level:\n        case logging.DEBUG:\n            logger.debug(message)\n        case logging.INFO:\n            logger.info(message)\n        case logging.WARNING:\n            logger.warning(message)\n        case logging.ERROR:\n            logger.error(message)\n        case logging.CRITICAL:\n            logger.critical(message)\n        case _:\n            raise ValueError(f\"Invalid logging level: {level}\")\n</code></pre>"}]}
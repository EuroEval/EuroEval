{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#evaluation-of-pretrained-language-models-on-mono-or-multilingual-language-tasks","title":"Evaluation of pretrained language models on mono- or multilingual language tasks.","text":""},{"location":"#maintainers","title":"Maintainers","text":"<ul> <li>Dan Saattrup Nielsen (@saattrupdan,   dan.nielsen@alexandra.dk)</li> <li>Kenneth Enevoldsen (@KennethEnevoldsen,   kenneth.enevoldsen@cas.au.dk)</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To install the package simply write the following command in your favorite terminal: <pre><code>$ pip install scandeval[all]\n</code></pre></p> <p>This will install the ScandEval package with all extras. You can also install the minimal version by leaving out the <code>[all]</code>, in which case the package will let you know when an evaluation requires a certain extra dependency, and how you install it.</p>"},{"location":"#quickstart","title":"Quickstart","text":""},{"location":"#benchmarking-from-the-command-line","title":"Benchmarking from the Command Line","text":"<p>The easiest way to benchmark pretrained models is via the command line interface. After having installed the package, you can benchmark your favorite model like so: <pre><code>$ scandeval --model &lt;model-id&gt;\n</code></pre></p> <p>Here <code>model</code> is the HuggingFace model ID, which can be found on the HuggingFace Hub. By default this will benchmark the model on all the tasks available. If you want to benchmark on a particular task, then use the <code>--task</code> argument: <pre><code>$ scandeval --model &lt;model-id&gt; --task sentiment-classification\n</code></pre></p> <p>We can also narrow down which languages we would like to benchmark on. This can be done by setting the <code>--language</code> argument. Here we thus benchmark the model on the Danish sentiment classification task: <pre><code>$ scandeval --model &lt;model-id&gt; --task sentiment-classification --language da\n</code></pre></p> <p>Multiple models, datasets and/or languages can be specified by just attaching multiple arguments. Here is an example with two models: <pre><code>$ scandeval --model &lt;model-id1&gt; --model &lt;model-id2&gt;\n</code></pre></p> <p>The specific model version/revision to use can also be added after the suffix '@': <pre><code>$ scandeval --model &lt;model-id&gt;@&lt;commit&gt;\n</code></pre></p> <p>This can be a branch name, a tag name, or a commit id. It defaults to 'main' for latest.</p> <p>See all the arguments and options available for the <code>scandeval</code> command by typing <pre><code>$ scandeval --help\n</code></pre></p>"},{"location":"#benchmarking-from-a-script","title":"Benchmarking from a Script","text":"<p>In a script, the syntax is similar to the command line interface. You simply initialise an object of the <code>Benchmarker</code> class, and call this benchmark object with your favorite model: <pre><code>&gt;&gt;&gt; from scandeval import Benchmarker\n&gt;&gt;&gt; benchmark = Benchmarker()\n&gt;&gt;&gt; benchmark(model=\"&lt;model&gt;\")\n</code></pre></p> <p>To benchmark on a specific task and/or language, you simply specify the <code>task</code> or <code>language</code> arguments, shown here with same example as above: <pre><code>&gt;&gt;&gt; benchmark(model=\"&lt;model&gt;\", task=\"sentiment-classification\", language=\"da\")\n</code></pre></p> <p>If you want to benchmark a subset of all the models on the Hugging Face Hub, you can simply leave out the <code>model</code> argument. In this example, we're benchmarking all Danish models on the Danish sentiment classification task: <pre><code>&gt;&gt;&gt; benchmark(task=\"sentiment-classification\", language=\"da\")\n</code></pre></p>"},{"location":"#benchmarking-from-docker","title":"Benchmarking from Docker","text":"<p>A Dockerfile is provided in the repo, which can be downloaded and run, without needing to clone the repo and installing from source. This can be fetched programmatically by running the following: <pre><code>$ wget https://raw.githubusercontent.com/ScandEval/ScandEval/main/Dockerfile.cuda\n</code></pre></p> <p>Next, to be able to build the Docker image, first ensure that the NVIDIA Container Toolkit is installed and configured. Ensure that the the CUDA version stated at the top of the Dockerfile matches the CUDA version installed (which you can check using <code>nvidia-smi</code>). After that, we build the image as follows: <pre><code>$ docker build --pull -t scandeval -f Dockerfile.cuda .\n</code></pre></p> <p>With the Docker image built, we can now evaluate any model as follows: <pre><code>$ docker run -e args=\"&lt;scandeval-arguments&gt;\" --gpus 1 --name scandeval --rm scandeval\n</code></pre></p> <p>Here <code>&lt;scandeval-arguments&gt;</code> consists of the arguments added to the <code>scandeval</code> CLI argument. This could for instance be <code>--model &lt;model-id&gt; --task sentiment-classification</code>.</p>"},{"location":"#special-thanks-pray","title":"Special Thanks :pray:","text":"<ul> <li>Thanks to OpenAI for sponsoring OpenAI credits as part of their   Researcher Access Program.</li> <li>Thanks to UWV and KU   Leuven for sponsoring the Azure OpenAI   credits used to evaluate GPT-4-turbo in Dutch.</li> <li>Thanks to Mi\u00f0eind for sponsoring the OpenAI   credits used to evaluate GPT-4-turbo in Icelandic and Faroese.</li> <li>Thanks to CHC for sponsoring the OpenAI credits used to   evaluate GPT-4-turbo in German.</li> </ul>"},{"location":"#citing-scandeval","title":"Citing ScandEval","text":"<p>If you want to cite the framework then feel free to use this:</p> <pre><code>@article{nielsen2024encoder,\n  title={Encoder vs Decoder: Comparative Analysis of Encoder and Decoder Language Models on Multilingual NLU Tasks},\n  author={Nielsen, Dan Saattrup and Enevoldsen, Kenneth and Schneider-Kamp, Peter},\n  journal={arXiv preprint arXiv:2406.13469},\n  year={2024}\n}\n@inproceedings{nielsen2023scandeval,\n  author = {Nielsen, Dan Saattrup},\n  booktitle = {Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  month = may,\n  pages = {185--201},\n  title = {{ScandEval: A Benchmark for Scandinavian Natural Language Processing}},\n  year = {2023}\n}\n</code></pre>"},{"location":"#remarks","title":"Remarks","text":"<p>The image used in the logo has been created by the amazing Scandinavia and the World team. Go check them out!</p>"},{"location":"api/scandeval/","title":"scandeval","text":"scandeval<p> source package scandeval </p> <p>ScandEval - A benchmarking framework for language models.</p> <p> Classes </p> <ul> <li> <p>Benchmarker \u2014 Benchmarking all the Scandinavian language models.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>block_terminal_output \u2014 Blocks libraries from writing output to the terminal.</p> </li> </ul> <p> source class Benchmarker(save_results: bool = True, task: str | list[str] | None = None, dataset: list[str] | str | None = None, language: str | list[str] = 'all', model_language: str | list[str] | None = None, dataset_language: str | list[str] | None = None, framework: Framework | str | None = None, device: Device | None = None, batch_size: int = 32, evaluate_train: bool = False, raise_errors: bool = False, cache_dir: str = '.scandeval_cache', token: bool | str = True, openai_api_key: str | None = None, prefer_azure: bool = False, azure_openai_api_key: str | None = None, azure_openai_endpoint: str | None = None, azure_openai_api_version: str | None = None, force: bool = False, verbose: bool = False, trust_remote_code: bool = False, load_in_4bit: bool | None = None, use_flash_attention: bool | None = None, clear_model_cache: bool = False, only_validation_split: bool = False, few_shot: bool = True, num_iterations: int = 10, debug: bool = False, run_with_cli: bool = False) </p> <p>Benchmarking all the Scandinavian language models.</p> <p>Initialise the benchmarker.</p> <p> Attributes </p> <ul> <li> <p>benchmark_config_default_params \u2014</p> <p>The default parameters for the benchmark configuration.</p> </li> <li> <p>benchmark_config \u2014</p> <p>The benchmark configuration.</p> </li> <li> <p>force \u2014</p> <p>Whether to force evaluations of models, even if they have been benchmarked already.</p> </li> <li> <p>dataset_factory \u2014</p> <p>The factory for creating datasets.</p> </li> <li> <p>results_path \u2014</p> <p>The path to the results file.</p> </li> <li> <p>benchmark_results :  list[BenchmarkResult] \u2014</p> <p>The benchmark results.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>progress_bar :  bool \u2014</p> <p>Whether progress bars should be shown. Defaults to True.</p> </li> <li> <p>save_results :  bool \u2014</p> <p>Whether to save the benchmark results to 'scandeval_benchmark_results.jsonl'. Defaults to True.</p> </li> <li> <p>task :  str | list[str] | None \u2014</p> <p>The tasks benchmark the model(s) on. Mutually exclusive with <code>dataset</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked.</p> </li> <li> <p>dataset :  list[str] | str | None \u2014</p> <p>The datasets to benchmark on. Mutually exclusive with <code>task</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked.</p> </li> <li> <p>language :  str | list[str] \u2014</p> <p>The language codes of the languages to include, both for models and datasets. Set this to 'all' if all languages should be considered. Defaults to \"all\".</p> </li> <li> <p>model_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for models. If specified then this overrides the <code>language</code> parameter for model languages. Defaults to None.</p> </li> <li> <p>dataset_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for datasets. If specified then this overrides the <code>language</code> parameter for dataset languages. Defaults to None.</p> </li> <li> <p>framework :  Framework | str | None \u2014</p> <p>The model framework to use. Only relevant if <code>model-id</code> refers to a local path. Otherwise, the framework will be set automatically. Defaults to None.</p> </li> <li> <p>device :  Device | None \u2014</p> <p>The device to use for benchmarking. Defaults to None.</p> </li> <li> <p>batch_size :  int \u2014</p> <p>The batch size to use. Defaults to 32.</p> </li> <li> <p>evaluate_train :  bool \u2014</p> <p>Whether to evaluate the training set as well. Defaults to False.</p> </li> <li> <p>raise_errors :  bool \u2014</p> <p>Whether to raise errors instead of skipping the model evaluation. Defaults to False.</p> </li> <li> <p>cache_dir :  str \u2014</p> <p>Directory to store cached models. Defaults to '.scandeval_cache'.</p> </li> <li> <p>token :  bool | str \u2014</p> <p>The authentication token for the Hugging Face Hub. If a boolean value is specified then the token will be fetched from the Hugging Face CLI, where the user has logged in through <code>huggingface-cli login</code>. If a string is specified then it will be used as the token. Defaults to True.</p> </li> <li> <p>openai_api_key :  str | None \u2014</p> <p>The OpenAI API key to use for authentication. If None, then this will be loaded from the environment variable <code>OPENAI_API_KEY</code>. Defaults to None.</p> </li> <li> <p>prefer_azure :  bool \u2014</p> <p>In the case where both OpenAI and Azure OpenAI models are available, whether to use the Azure OpenAI models. Defaults to False.</p> </li> <li> <p>azure_openai_api_key :  str | None \u2014</p> <p>The Azure OpenAI API key to use for authentication. If None, then this will be loaded from the environment variable <code>AZURE_OPENAI_API_KEY</code>. Defaults to None.</p> </li> <li> <p>azure_openai_endpoint :  str | None \u2014</p> <p>The Azure OpenAI endpoint to use for authentication. If None, then this will be loaded from the environment variable <code>AZURE_OPENAI_ENDPOINT</code>. Defaults to None.</p> </li> <li> <p>azure_openai_api_version :  str | None \u2014</p> <p>The Azure OpenAI API version to use for authentication. If None, then this will be loaded from the environment variable <code>AZURE_OPENAI_API_VERSION</code>. Defaults to None.</p> </li> <li> <p>force :  bool \u2014</p> <p>Whether to force evaluations of models, even if they have been benchmarked already. Defaults to False.</p> </li> <li> <p>verbose :  bool \u2014</p> <p>Whether to output additional output. Defaults to False.</p> </li> <li> <p>trust_remote_code :  bool \u2014</p> <p>Whether to trust remote code when loading models. Defaults to False.</p> </li> <li> <p>load_in_4bit :  bool | None \u2014</p> <p>Whether to load models in 4-bit precision. If None then this will be done if CUDA is available and the model is a decoder model. Defaults to None.</p> </li> <li> <p>use_flash_attention :  bool | None \u2014</p> <p>Whether to use Flash Attention. If None then it will be used if it is installed and the model is a decoder model. Defaults to None.</p> </li> <li> <p>clear_model_cache :  bool \u2014</p> <p>Whether to clear the model cache after benchmarking each model. Defaults to False.</p> </li> <li> <p>only_validation_split :  bool \u2014</p> <p>Whether to only evaluate the validation split of the datasets. Defaults to False.</p> </li> <li> <p>few_shot :  bool \u2014</p> <p>Whether to only evaluate the model using few-shot evaluation. Only relevant if the model is generative. Defaults to True.</p> </li> <li> <p>num_iterations :  int \u2014</p> <p>The number of times each model should be evaluated. This is only meant to be used for power users, and scores will not be allowed on the leaderboards if this is changed. Defaults to 10.</p> </li> <li> <p>debug :  bool \u2014</p> <p>Whether to output debug information. Defaults to False.</p> </li> <li> <p>run_with_cli :  bool \u2014</p> <p>Whether the benchmarker is being run from the command-line interface. Defaults to False.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014</p> <p>If both <code>task</code> and <code>dataset</code> are specified.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>benchmark \u2014 Benchmarks models on datasets.</p> </li> </ul> <p> source property Benchmarker.benchmark_results: list[BenchmarkResult] </p> <p>The benchmark results.</p> <p> source method Benchmarker.benchmark(model: list[str] | str | None = None, task: str | list[str] | None = None, dataset: list[str] | str | None = None, progress_bar: bool | None = None, save_results: bool | None = None, language: str | list[str] | None = None, model_language: str | list[str] | None = None, dataset_language: str | list[str] | None = None, framework: Framework | str | None = None, device: Device | None = None, batch_size: int | None = None, evaluate_train: bool | None = None, raise_errors: bool | None = None, cache_dir: str | None = None, token: bool | str | None = None, openai_api_key: str | None = None, azure_openai_api_key: str | None = None, azure_openai_endpoint: str | None = None, azure_openai_api_version: str | None = None, force: bool | None = None, verbose: bool | None = None, trust_remote_code: bool | None = None, load_in_4bit: bool | None = None, use_flash_attention: bool | None = None, clear_model_cache: bool | None = None, only_validation_split: bool | None = None, few_shot: bool | None = None, num_iterations: int | None = None) \u2192 list[BenchmarkResult] </p> <p>Benchmarks models on datasets.</p> <p> Parameters </p> <ul> <li> <p>model :  list[str] | str | None \u2014</p> <p>The full Hugging Face Hub path(s) to the pretrained transformer model. The specific model version to use can be added after the suffix '@': \"model@v1.0.0\". It can be a branch name, a tag name, or a commit id, and defaults to the latest version if not specified. If None then all relevant model IDs will be benchmarked. Defaults to None.</p> </li> <li> <p>task :  str | list[str] | None \u2014</p> <p>The tasks benchmark the model(s) on. Mutually exclusive with <code>dataset</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked. Defaults to None.</p> </li> <li> <p>dataset :  list[str] | str | None \u2014</p> <p>The datasets to benchmark on. Mutually exclusive with <code>task</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked. Defaults to None.</p> </li> <li> <p>progress_bar :  bool | None \u2014</p> <p>Whether progress bars should be shown. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>save_results :  bool | None \u2014</p> <p>Whether to save the benchmark results to 'scandeval_benchmark_results.jsonl'. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include, both for models and datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this to 'all' if all languages (also non-Scandinavian) should be considered. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>model_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for models. If specified then this overrides the <code>language</code> parameter for model languages. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>dataset_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for datasets. If specified then this overrides the <code>language</code> parameter for dataset languages. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>framework :  Framework | str | None \u2014</p> <p>The model framework to use. Only relevant if <code>model-id</code> refers to a local path. Otherwise, the framework will be set automatically. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>device :  Device | None \u2014</p> <p>The device to use for benchmarking. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>batch_size :  int | None \u2014</p> <p>The batch size to use. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>evaluate_train :  bool | None \u2014</p> <p>Whether to evaluate the training set as well. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>raise_errors :  bool | None \u2014</p> <p>Whether to raise errors instead of skipping the model evaluation.</p> </li> <li> <p>cache_dir :  str | None \u2014</p> <p>Directory to store cached models. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>token :  bool | str | None \u2014</p> <p>The authentication token for the Hugging Face Hub. If a boolean value is specified then the token will be fetched from the Hugging Face CLI, where the user has logged in through <code>huggingface-cli login</code>. If a string is specified then it will be used as the token. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>openai_api_key :  str | None \u2014</p> <p>The OpenAI API key to use for authentication. If None, then this will be loaded from the environment variable <code>OPENAI_API_KEY</code>. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>azure_openai_api_key :  str | None \u2014</p> <p>The Azure OpenAI API key to use for authentication. If None, then this will be loaded from the environment variable <code>AZURE_OPENAI_API_KEY</code>. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>azure_openai_endpoint :  str | None \u2014</p> <p>The Azure OpenAI endpoint to use for authentication. If None, then this will be loaded from the environment variable <code>AZURE_OPENAI_ENDPOINT</code>. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>azure_openai_api_version :  str | None \u2014</p> <p>The api version for the Azure OpenAI API, e.g. \"2023-12-01-preview\". If None then the environment varaible <code>AZURE_OPENAI_API_VERSION</code> will be used. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>force :  bool | None \u2014</p> <p>Whether to force evaluations of models, even if they have been benchmarked already. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>verbose :  bool | None \u2014</p> <p>Whether to output additional output. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>trust_remote_code :  bool | None \u2014</p> <p>Whether to trust remote code when loading models. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>load_in_4bit :  bool | None \u2014</p> <p>Whether to load models in 4-bit precision. If None then this will be done if CUDA is available and the model is a decoder model. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>use_flash_attention :  bool | None \u2014</p> <p>Whether to use Flash Attention. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>clear_model_cache :  bool | None \u2014</p> <p>Whether to clear the model cache after benchmarking each model. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>only_validation_split :  bool | None \u2014</p> <p>Whether to only evaluate the validation split of the datasets. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>few_shot :  bool | None \u2014</p> <p>Whether to only evaluate the model using few-shot evaluation. Only relevant if the model is generative. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>num_iterations :  int | None \u2014</p> <p>The number of times each model should be evaluated. This is only meant to be used for power users, and scores will not be allowed on the leaderboards if this is changed. Defaults to the value specified when initialising the benchmarker.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[BenchmarkResult] \u2014 A list of benchmark results.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014</p> <p>If both <code>task</code> and <code>dataset</code> are specified.</p> </li> <li> <p>e</p> </li> </ul> <p> source block_terminal_output() </p> <p>Blocks libraries from writing output to the terminal.</p> <p>This filters warnings from some libraries, sets the logging level to ERROR for some libraries, disabled tokeniser progress bars when using Hugging Face tokenisers, and disables most of the logging from the <code>transformers</code> library.</p>"},{"location":"src/scandeval/","title":"scandeval","text":"scandeval<p> docs package scandeval </p> <pre><code>\"\"\"ScandEval - A benchmarking framework for language models.\"\"\"\n\nimport importlib.metadata\nimport logging\nimport os\nimport sys\n\nfrom dotenv import load_dotenv\nfrom termcolor import colored\n\nfrom .benchmarker import Benchmarker\nfrom .utils import block_terminal_output\n\n# Fetches the version of the package as defined in pyproject.toml\n__version__ = importlib.metadata.version(__package__)\n\n\n# Block unwanted terminal outputs\nblock_terminal_output()\n\n\n# Loads environment variables\nload_dotenv()\n\n\n# Set up logging\nfmt = colored(\"%(asctime)s\", \"light_blue\") + \" \u22c5 \" + colored(\"%(message)s\", \"green\")\nlogging.basicConfig(\n    level=logging.CRITICAL if hasattr(sys, \"_called_from_test\") else logging.INFO,\n    format=fmt,\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n)\n\n\n# Disable parallelisation when tokenizing, as that can lead to errors\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n\n# Enable MPS fallback\nos.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n\n\n# Set amount of threads per GPU - this is the default and is only set to prevent a\n# warning from showing\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\n</code></pre>"},{"location":"api/scandeval/model_setups/","title":"scandeval.model_setups","text":"scandeval.model_setups<p> source package scandeval.model_setups </p> <p>The different types of models that can be benchmarked.</p> <p> Classes </p> <ul> <li> <p>FreshModelSetup \u2014 Model setup for fresh models.</p> </li> <li> <p>HFModelSetup \u2014 Model setup for Hugging Face Hub models.</p> </li> <li> <p>LocalModelSetup \u2014 Model setup for local Hugging Face Hub models.</p> </li> <li> <p>OpenAIModelSetup \u2014 Model setup for OpenAI models.</p> </li> </ul> <p> source class FreshModelSetup() </p> <p>Model setup for fresh models.</p> <p>Initialize the FreshModelSetup class.</p> <p> Attributes </p> <ul> <li> <p>benchmark_config \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>model_exists \u2014 Check if a model ID denotes a fresh model.</p> </li> <li> <p>get_model_config \u2014 Fetches configuration for a fresh model.</p> </li> <li> <p>load_model \u2014 Load a fresh model.</p> </li> </ul> <p> source method FreshModelSetup.model_exists(model_id: str) \u2192 bool | dict[str, str] </p> <p>Check if a model ID denotes a fresh model.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | dict[str, str] \u2014 Whether the model exist, or a dictionary explaining why we cannot check whether the model exists.</p> </li> </ul> <p> source method FreshModelSetup.get_model_config(model_id: str) \u2192 ModelConfig </p> <p>Fetches configuration for a fresh model.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> source method FreshModelSetup.load_model(model_config: ModelConfig, dataset_config: DatasetConfig) \u2192 tuple[PreTrainedModel | GenerativeModel, Tokenizer] </p> <p>Load a fresh model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[PreTrainedModel | GenerativeModel, Tokenizer] \u2014 The tokenizer and model.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source class HFModelSetup() </p> <p>Model setup for Hugging Face Hub models.</p> <p>Initialize the model setup.</p> <p> Parameters </p> <ul> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> <p>The benchmark configuration.</p> </li> </ul> <p> Attributes </p> <ul> <li> <p>benchmark_config \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>model_exists \u2014 Check if a model ID denotes a model on the Hugging Face Hub.</p> </li> <li> <p>get_model_config \u2014 Fetches configuration for an OpenAI model.</p> </li> <li> <p>load_model \u2014 Load an OpenAI model.</p> </li> </ul> <p> source method HFModelSetup.model_exists(model_id: str) \u2192 bool | dict[str, str] </p> <p>Check if a model ID denotes a model on the Hugging Face Hub.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | dict[str, str] \u2014 Whether the model exist, or a dictionary explaining why we cannot check whether the model exists.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NeedsAdditionalArgument</p> </li> <li> <p>HuggingFaceHubDown</p> </li> <li> <p>NoInternetConnection</p> </li> <li> <p>MissingHuggingFaceToken</p> </li> </ul> <p> source method HFModelSetup.get_model_config(model_id: str) \u2192 ModelConfig </p> <p>Fetches configuration for an OpenAI model.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID of the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> <li> <p>NeedsManualDependency</p> </li> <li> <p>HuggingFaceHubDown</p> </li> <li> <p>NoInternetConnection</p> </li> </ul> <p> source method HFModelSetup.load_model(model_config: ModelConfig, dataset_config: DatasetConfig) \u2192 tuple[PreTrainedModel | GenerativeModel, Tokenizer] </p> <p>Load an OpenAI model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[PreTrainedModel | GenerativeModel, Tokenizer] \u2014 The tokenizer and model.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NeedsManualDependency</p> </li> <li> <p>NeedsExtraInstalled</p> </li> <li> <p>InvalidModel</p> </li> <li> <p>e</p> </li> <li> <p>InvalidBenchmark</p> </li> <li> <p>FlashAttentionNotInstalled</p> </li> </ul> <p> source class LocalModelSetup() </p> <p>Model setup for local Hugging Face Hub models.</p> <p>Initialize the LocalModelSetup class.</p> <p> Attributes </p> <ul> <li> <p>benchmark_config \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>model_exists \u2014 Check if a model exists locally.</p> </li> <li> <p>get_model_config \u2014 Fetches configuration for a local Hugging Face model.</p> </li> <li> <p>load_model \u2014 Load a local Hugging Face model.</p> </li> </ul> <p> source method LocalModelSetup.model_exists(model_id: str) \u2192 bool | dict[str, str] </p> <p>Check if a model exists locally.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | dict[str, str] \u2014 Whether the model exist, or a dictionary explaining why we cannot check whether the model exists.</p> </li> </ul> <p> source method LocalModelSetup.get_model_config(model_id: str) \u2192 ModelConfig </p> <p>Fetches configuration for a local Hugging Face model.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID of the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NeedsManualDependency</p> </li> <li> <p>e</p> </li> <li> <p>InvalidModel</p> </li> </ul> <p> source method LocalModelSetup.load_model(model_config: ModelConfig, dataset_config: DatasetConfig) \u2192 tuple[PreTrainedModel | GenerativeModel, Tokenizer] </p> <p>Load a local Hugging Face model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[PreTrainedModel | GenerativeModel, Tokenizer] \u2014 The tokenizer and model.</p> </li> </ul> <p> source class OpenAIModelSetup() </p> <p>Model setup for OpenAI models.</p> <p>Initialize the model setup.</p> <p> Attributes </p> <ul> <li> <p>benchmark_config \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>model_exists \u2014 Check if a model ID denotes an OpenAI model.</p> </li> <li> <p>get_model_config \u2014 Fetches configuration for an OpenAI model.</p> </li> <li> <p>load_model \u2014 Load an OpenAI model.</p> </li> </ul> <p> source method OpenAIModelSetup.model_exists(model_id: str) \u2192 bool | dict[str, str] </p> <p>Check if a model ID denotes an OpenAI model.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | dict[str, str] \u2014 Whether the model exists, or a dictionary explaining why we cannot check whether the model exists.</p> </li> </ul> <p> source method OpenAIModelSetup.get_model_config(model_id: str) \u2192 ModelConfig </p> <p>Fetches configuration for an OpenAI model.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID of the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> source method OpenAIModelSetup.load_model(model_config: ModelConfig, dataset_config: DatasetConfig) \u2192 tuple[PreTrainedModel | GenerativeModel, Tokenizer] </p> <p>Load an OpenAI model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[PreTrainedModel | GenerativeModel, Tokenizer] \u2014 The tokenizer and model.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError</p> </li> </ul>"},{"location":"src/scandeval/model_setups/","title":"scandeval.model_setups","text":"scandeval.model_setups<p> docs package scandeval.model_setups </p> <pre><code>\"\"\"The different types of models that can be benchmarked.\"\"\"\n\nfrom typing import TYPE_CHECKING, Type\n\nfrom .fresh import FreshModelSetup\nfrom .hf import HFModelSetup\nfrom .local import LocalModelSetup\nfrom .openai import OpenAIModelSetup\n\nif TYPE_CHECKING:\n    from ..protocols import ModelSetup\n\n\n# Note that the order of the model setup classes is important, as the first model setup\n# that can load a model will be used\nMODEL_SETUP_CLASSES: list[Type[\"ModelSetup\"]] = [\n    FreshModelSetup,\n    LocalModelSetup,\n    HFModelSetup,\n    OpenAIModelSetup,\n]\n</code></pre>"},{"location":"api/scandeval/model_setups/fresh/","title":"scandeval.model_setups.fresh","text":"scandeval.model_setups.fresh<p> source module scandeval.model_setups.fresh </p> <p>Model setup for fresh models.</p> <p> Classes </p> <ul> <li> <p>FreshModelSetup \u2014 Model setup for fresh models.</p> </li> </ul> <p> source class FreshModelSetup() </p> <p>Model setup for fresh models.</p> <p>Initialize the FreshModelSetup class.</p> <p> Attributes </p> <ul> <li> <p>benchmark_config \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>model_exists \u2014 Check if a model ID denotes a fresh model.</p> </li> <li> <p>get_model_config \u2014 Fetches configuration for a fresh model.</p> </li> <li> <p>load_model \u2014 Load a fresh model.</p> </li> </ul> <p> source method FreshModelSetup.model_exists(model_id: str) \u2192 bool | dict[str, str] </p> <p>Check if a model ID denotes a fresh model.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | dict[str, str] \u2014 Whether the model exist, or a dictionary explaining why we cannot check whether the model exists.</p> </li> </ul> <p> source method FreshModelSetup.get_model_config(model_id: str) \u2192 ModelConfig </p> <p>Fetches configuration for a fresh model.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> source method FreshModelSetup.load_model(model_config: ModelConfig, dataset_config: DatasetConfig) \u2192 tuple[PreTrainedModel | GenerativeModel, Tokenizer] </p> <p>Load a fresh model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[PreTrainedModel | GenerativeModel, Tokenizer] \u2014 The tokenizer and model.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> <li> <p>InvalidBenchmark</p> </li> </ul>"},{"location":"src/scandeval/model_setups/fresh/","title":"scandeval.model_setups.fresh","text":"scandeval.model_setups.fresh<p> docs module scandeval.model_setups.fresh </p> <pre><code>\"\"\"Model setup for fresh models.\"\"\"\n\nimport re\nfrom json import JSONDecodeError\nfrom typing import TYPE_CHECKING\n\nfrom transformers import (\n    AutoConfig,\n    AutoTokenizer,\n    ElectraForQuestionAnswering,\n    ElectraForSequenceClassification,\n    ElectraForTokenClassification,\n    XLMRobertaForQuestionAnswering,\n    XLMRobertaForSequenceClassification,\n    XLMRobertaForTokenClassification,\n)\n\nfrom ..config import ModelConfig\nfrom ..enums import Framework, ModelType\nfrom ..exceptions import InvalidBenchmark, InvalidModel\nfrom ..utils import block_terminal_output, create_model_cache_dir, model_is_generative\nfrom .utils import align_model_and_tokenizer, setup_model_for_question_answering\n\nif TYPE_CHECKING:\n    from transformers import PretrainedConfig, PreTrainedModel, PreTrainedTokenizerBase\n\n    from ..config import BenchmarkConfig, DatasetConfig\n    from ..protocols import GenerativeModel, Tokenizer\n\n\nFRESH_MODELS: list[str] = [\"electra-small\", \"xlm-roberta-base\"]\n\n\nclass FreshModelSetup:docs\n    \"\"\"Model setup for fresh models.\n\n    Attributes:\n        benchmark_config:\n            The benchmark configuration.\n    \"\"\"\n\n    def __init__(self, benchmark_config: \"BenchmarkConfig\") -&gt; None:\n        \"\"\"Initialize the FreshModelSetup class.\n\n        Args:\n            benchmark_config:\n                The benchmark configuration.\n        \"\"\"\n        self.benchmark_config = benchmark_config\n\n    @staticmethod\n    def _strip_model_id(model_id: str) -&gt; str:\n        return re.sub(\"(@.*$|^fresh-)\", \"\", model_id)\n\n    def model_exists(self, model_id: str) -&gt; bool | dict[str, str]:docs\n        \"\"\"Check if a model ID denotes a fresh model.\n\n        Args:\n            model_id:\n                The model ID.\n\n        Returns:\n            Whether the model exist, or a dictionary explaining why we cannot check\n            whether the model exists.\n        \"\"\"\n        return self._strip_model_id(model_id=model_id) in FRESH_MODELS\n\n    def get_model_config(self, model_id: str) -&gt; ModelConfig:docs\n        \"\"\"Fetches configuration for a fresh model.\n\n        Args:\n            model_id:\n                The model ID.\n\n        Returns:\n            The model configuration.\n        \"\"\"\n        return ModelConfig(\n            model_id=self._strip_model_id(model_id=model_id),\n            framework=Framework.PYTORCH,\n            task=\"fill-mask\",\n            languages=list(),\n            revision=\"main\",\n            model_type=ModelType.FRESH,\n            model_cache_dir=create_model_cache_dir(\n                cache_dir=self.benchmark_config.cache_dir, model_id=model_id\n            ),\n            adapter_base_model_id=None,\n        )\n\n    def load_model(docs\n        self, model_config: ModelConfig, dataset_config: \"DatasetConfig\"\n    ) -&gt; tuple[\"PreTrainedModel | GenerativeModel\", \"Tokenizer\"]:\n        \"\"\"Load a fresh model.\n\n        Args:\n            model_config:\n                The model configuration.\n            dataset_config:\n                The dataset configuration.\n\n        Returns:\n            The tokenizer and model.\n        \"\"\"\n        config: \"PretrainedConfig\"\n        block_terminal_output()\n\n        model_id = model_config.model_id\n        supertask = dataset_config.task.supertask\n\n        if model_config.model_id == \"xlm-roberta-base\":\n            if supertask == \"sequence-classification\":\n                model_cls = XLMRobertaForSequenceClassification\n            elif supertask == \"token-classification\":\n                model_cls = XLMRobertaForTokenClassification\n            elif supertask == \"question-answering\":\n                model_cls = XLMRobertaForQuestionAnswering\n            else:\n                raise InvalidBenchmark(\n                    f\"Supertask {supertask} is not supported for model {model_id}\"\n                )\n\n        elif model_id == \"electra-small\":\n            model_id = \"google/electra-small-discriminator\"\n            if supertask == \"sequence-classification\":\n                model_cls = ElectraForSequenceClassification\n            elif supertask == \"token-classification\":\n                model_cls = ElectraForTokenClassification\n            elif supertask == \"question-answering\":\n                model_cls = ElectraForQuestionAnswering\n            else:\n                raise InvalidBenchmark(\n                    f\"Supertask {supertask} is not supported for model {model_id}\"\n                )\n\n        else:\n            raise InvalidModel(f\"Model {model_id} is not supported as a fresh class.\")\n\n        config = AutoConfig.from_pretrained(\n            model_id,\n            token=self.benchmark_config.token,\n            num_labels=dataset_config.num_labels,\n            id2label=dataset_config.id2label,\n            label2id=dataset_config.label2id,\n            cache_dir=model_config.model_cache_dir,\n        )\n        model = model_cls(config)\n\n        if supertask == \"question-answering\":\n            model = setup_model_for_question_answering(model=model)\n\n        # Load the tokenizer. If the model is a subclass of a RoBERTa model then we\n        # have to add a prefix space to the tokens, by the way the model is constructed\n        prefix_models = [\"Roberta\", \"GPT\", \"Deberta\"]\n        prefix = any(model_type in type(model).__name__ for model_type in prefix_models)\n        try:\n            tokenizer: \"PreTrainedTokenizerBase\" = AutoTokenizer.from_pretrained(\n                model_id,\n                revision=model_config.revision,\n                token=self.benchmark_config.token,\n                add_prefix_space=prefix,\n                cache_dir=model_config.model_cache_dir,\n                use_fast=True,\n                verbose=False,\n            )\n        except (JSONDecodeError, OSError):\n            raise InvalidModel(f\"Could not load tokenizer for model {model_id!r}.\")\n\n        model, tokenizer = align_model_and_tokenizer(\n            model=model,\n            tokenizer=tokenizer,\n            generation_length=dataset_config.max_generated_tokens,\n            raise_errors=self.benchmark_config.raise_errors,\n            generative_model=model_is_generative(model=model),\n        )\n\n        return model, tokenizer\n</code></pre>"},{"location":"api/scandeval/model_setups/hf/","title":"scandeval.model_setups.hf","text":"scandeval.model_setups.hf<p> source module scandeval.model_setups.hf </p> <p>Model setup for Hugging Face Hub models.</p> <p> Classes </p> <ul> <li> <p>HFModelSetup \u2014 Model setup for Hugging Face Hub models.</p> </li> </ul> <p> source class HFModelSetup() </p> <p>Model setup for Hugging Face Hub models.</p> <p>Initialize the model setup.</p> <p> Parameters </p> <ul> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The benchmark configuration.</p> <p>The benchmark configuration.</p> </li> </ul> <p> Attributes </p> <ul> <li> <p>benchmark_config \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>model_exists \u2014 Check if a model ID denotes a model on the Hugging Face Hub.</p> </li> <li> <p>get_model_config \u2014 Fetches configuration for an OpenAI model.</p> </li> <li> <p>load_model \u2014 Load an OpenAI model.</p> </li> </ul> <p> source method HFModelSetup.model_exists(model_id: str) \u2192 bool | dict[str, str] </p> <p>Check if a model ID denotes a model on the Hugging Face Hub.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | dict[str, str] \u2014 Whether the model exist, or a dictionary explaining why we cannot check whether the model exists.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NeedsAdditionalArgument</p> </li> <li> <p>HuggingFaceHubDown</p> </li> <li> <p>NoInternetConnection</p> </li> <li> <p>MissingHuggingFaceToken</p> </li> </ul> <p> source method HFModelSetup.get_model_config(model_id: str) \u2192 ModelConfig </p> <p>Fetches configuration for an OpenAI model.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID of the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> <li> <p>NeedsManualDependency</p> </li> <li> <p>HuggingFaceHubDown</p> </li> <li> <p>NoInternetConnection</p> </li> </ul> <p> source method HFModelSetup.load_model(model_config: ModelConfig, dataset_config: DatasetConfig) \u2192 tuple[PreTrainedModel | GenerativeModel, Tokenizer] </p> <p>Load an OpenAI model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[PreTrainedModel | GenerativeModel, Tokenizer] \u2014 The tokenizer and model.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NeedsManualDependency</p> </li> <li> <p>NeedsExtraInstalled</p> </li> <li> <p>InvalidModel</p> </li> <li> <p>e</p> </li> <li> <p>InvalidBenchmark</p> </li> <li> <p>FlashAttentionNotInstalled</p> </li> </ul>"},{"location":"src/scandeval/model_setups/hf/","title":"scandeval.model_setups.hf","text":"scandeval.model_setups.hf<p> docs module scandeval.model_setups.hf </p> <pre><code>\"\"\"Model setup for Hugging Face Hub models.\"\"\"\n\nimport importlib.util\nimport logging\nimport os\nfrom json import JSONDecodeError\nfrom time import sleep\nfrom typing import TYPE_CHECKING, Type\n\nimport torch\nfrom huggingface_hub import HfApi, HfFileSystem\nfrom huggingface_hub import whoami as hf_whoami\nfrom huggingface_hub.hf_api import ModelInfo, RepositoryNotFoundError\nfrom huggingface_hub.utils import (\n    GatedRepoError,\n    HFValidationError,\n    LocalTokenNotFoundError,\n)\nfrom requests.exceptions import RequestException\nfrom transformers import AutoConfig, AutoTokenizer, BitsAndBytesConfig, PreTrainedModel\nfrom urllib3.exceptions import RequestError\n\nfrom ..config import ModelConfig\nfrom ..enums import Framework, ModelType\nfrom ..exceptions import (\n    FlashAttentionNotInstalled,\n    HuggingFaceHubDown,\n    InvalidBenchmark,\n    InvalidModel,\n    MissingHuggingFaceToken,\n    NeedsAdditionalArgument,\n    NeedsExtraInstalled,\n    NeedsManualDependency,\n    NoInternetConnection,\n)\nfrom ..languages import get_all_languages\nfrom ..protocols import GenerativeModel\nfrom ..utils import (\n    GENERATIVE_DATASET_SUPERTASKS,\n    GENERATIVE_DATASET_TASKS,\n    GENERATIVE_MODEL_TASKS,\n    block_terminal_output,\n    create_model_cache_dir,\n    get_class_by_name,\n    internet_connection_available,\n    model_is_generative,\n)\nfrom ..vllm_models import VLLMModel\nfrom .utils import align_model_and_tokenizer, setup_model_for_question_answering\n\nif TYPE_CHECKING:\n    from peft.config import PeftConfig\n    from transformers import PretrainedConfig, PreTrainedModel\n\n    from ..config import BenchmarkConfig, DatasetConfig\n    from ..protocols import Tokenizer\n\nif importlib.util.find_spec(\"peft\") is not None:\n    from peft.config import PeftConfig\n\n\nlogger = logging.getLogger(__package__)\n\n\nclass HFModelSetup:docs\n    \"\"\"Model setup for Hugging Face Hub models.\n\n    Args:\n        benchmark_config:\n            The benchmark configuration.\n\n    Attributes:\n        benchmark_config:\n            The benchmark configuration.\n    \"\"\"\n\n    def __init__(self, benchmark_config: \"BenchmarkConfig\") -&gt; None:\n        \"\"\"Initialize the model setup.\n\n        Args:\n            benchmark_config:\n                The benchmark configuration.\n        \"\"\"\n        self.benchmark_config = benchmark_config\n\n    def model_exists(self, model_id: str) -&gt; bool | dict[str, str]:docs\n        \"\"\"Check if a model ID denotes a model on the Hugging Face Hub.\n\n        Args:\n            model_id:\n                The model ID.\n\n        Returns:\n            Whether the model exist, or a dictionary explaining why we cannot check\n            whether the model exists.\n        \"\"\"\n        # Extract the revision from the model_id, if present\n        model_id, revision = (\n            model_id.split(\"@\") if \"@\" in model_id else (model_id, \"main\")\n        )\n\n        # Connect to the Hugging Face Hub API\n        hf_api = HfApi()\n\n        # Get the model info, and return it\n        try:\n            hf_api.model_info(\n                repo_id=model_id, revision=revision, token=self.benchmark_config.token\n            )\n            return True\n\n        except (GatedRepoError, LocalTokenNotFoundError):\n            try:\n                hf_whoami()\n                raise NeedsAdditionalArgument(\n                    cli_argument=\"--use-token\",\n                    script_argument=\"token=True\",\n                    run_with_cli=self.benchmark_config.run_with_cli,\n                )\n            except LocalTokenNotFoundError:\n                raise MissingHuggingFaceToken(\n                    run_with_cli=self.benchmark_config.run_with_cli\n                )\n\n        except (RepositoryNotFoundError, HFValidationError):\n            return False\n\n        # If fetching from the Hugging Face Hub failed in a different way then throw a\n        # reasonable exception\n        except OSError:\n            if internet_connection_available():\n                raise HuggingFaceHubDown()\n            else:\n                raise NoInternetConnection()\n\n    def get_model_config(self, model_id: str) -&gt; ModelConfig:docs\n        \"\"\"Fetches configuration for an OpenAI model.\n\n        Args:\n            model_id:\n                The model ID of the model.\n\n        Returns:\n            The model configuration.\n        \"\"\"\n        # Extract the revision from the model IDs, if they are specified\n        if \"@\" in model_id:\n            model_id_without_revision, revision = model_id.split(\"@\", 1)\n        else:\n            model_id_without_revision = model_id\n            revision = \"main\"\n\n        # Extract the author and model name from the model ID\n        author: str | None\n        if \"/\" in model_id_without_revision:\n            author, model_name = model_id_without_revision.split(\"/\")\n        else:\n            author = None\n            model_name = model_id_without_revision\n\n        # Attempt to fetch model data from the Hugging Face Hub\n        try:\n            api: HfApi = HfApi()\n            fs = HfFileSystem()\n\n            # Fetch the model metadata\n            models = api.list_models(\n                author=author, model_name=model_name, token=self.benchmark_config.token\n            )\n\n            # Filter the models to only keep the one with the specified model ID\n            models = [\n                model for model in models if model.id == model_id_without_revision\n            ]\n\n            # Check that the model exists. If it does not then raise an error\n            if len(models) == 0:\n                raise InvalidModel(\n                    f\"The model {model_id} does not exist on the Hugging Face Hub.\"\n                )\n\n            model: ModelInfo = models[0]\n            tags: list[str] = model.tags or list()\n\n            framework = Framework.PYTORCH\n            if \"pytorch\" in tags:\n                pass\n            elif \"jax\" in tags:\n                framework = Framework.JAX\n            elif \"spacy\" in tags:\n                raise InvalidModel(\"SpaCy models are not supported.\")\n            elif \"tf\" in tags or \"tensorflow\" in tags or \"keras\" in tags:\n                raise InvalidModel(\"TensorFlow/Keras models are not supported.\")\n\n            base_model_ids: list[str] = list()\n\n            # If the model is a finetuned model then we fetch the base model ID\n            has_base_model_tag = any(\n                tag.startswith(\"base_model:\") and tag.count(\":\") == 1 for tag in tags\n            )\n            if has_base_model_tag:\n                base_model_id = [\n                    tag.split(\":\")[1]\n                    for tag in tags\n                    if tag.startswith(\"base_model:\") and tag.count(\":\") == 1\n                ][0]\n                base_model_ids.append(base_model_id)\n\n            # If the model is an adapter model then we fetch the base model ID\n            is_adapter = \"adapter_config.json\" in [\n                path.split(\"/\")[-1]\n                for path in fs.ls(path=model_id, detail=False)\n                if isinstance(path, str)\n            ]\n            adapter_base_model_id: str | None = None\n            if is_adapter:\n                if importlib.util.find_spec(\"peft\") is None:\n                    raise NeedsManualDependency(package=\"peft\")\n                peft_config = PeftConfig.from_pretrained(model_id)\n                adapter_base_model_id = peft_config.base_model_name_or_path\n                if adapter_base_model_id is not None:\n                    base_model_ids.append(adapter_base_model_id)\n\n            # Add tags from the base models\n            for base_model_id in base_model_ids:\n                base_model_author, base_model_name = base_model_id.split(\"/\")\n                base_models = [\n                    model\n                    for model in api.list_models(\n                        author=base_model_author,\n                        model_name=base_model_name,\n                        token=self.benchmark_config.token,\n                    )\n                    if model.id == base_model_id\n                ]\n                if len(base_models) &gt; 0:\n                    tags += base_models[0].tags or list()\n                    tags = list(set(tags))\n\n            model_task: str | None = model.pipeline_tag\n            if model_task is None:\n                generative_tags = [\n                    \"trl\",\n                    \"mistral\",\n                    \"text-generation-inference\",\n                    \"unsloth\",\n                    \"text-generation\",\n                    \"llama\",\n                ]\n                if any(tag in tags for tag in generative_tags):\n                    model_task = \"text-generation\"\n                else:\n                    model_task = \"fill-mask\"\n\n            language_mapping = get_all_languages()\n            language_codes = list(language_mapping.keys())\n\n            model_config = ModelConfig(\n                model_id=model_id,\n                framework=framework,\n                task=model_task,\n                languages=[\n                    language_mapping[tag] for tag in tags if tag in language_codes\n                ],\n                revision=revision,\n                model_type=ModelType.HF,\n                model_cache_dir=create_model_cache_dir(\n                    cache_dir=self.benchmark_config.cache_dir, model_id=model_id\n                ),\n                adapter_base_model_id=adapter_base_model_id,\n            )\n\n        # If fetching from the Hugging Face Hub failed then throw a reasonable\n        # exception\n        except RequestException:\n            if internet_connection_available():\n                raise HuggingFaceHubDown()\n            else:\n                raise NoInternetConnection()\n\n        return model_config\n\n    def load_model(docs\n        self, model_config: ModelConfig, dataset_config: \"DatasetConfig\"\n    ) -&gt; tuple[\"PreTrainedModel | GenerativeModel\", \"Tokenizer\"]:\n        \"\"\"Load an OpenAI model.\n\n        Args:\n            model_config:\n                The model configuration.\n            dataset_config:\n                The dataset configuration.\n\n        Returns:\n            The tokenizer and model.\n        \"\"\"\n        config: \"PretrainedConfig\"\n        block_terminal_output()\n\n        model_id = model_config.model_id\n        supertask = dataset_config.task.supertask\n        from_flax = model_config.framework == Framework.JAX\n        ignore_mismatched_sizes = False\n\n        config = self._load_hf_model_config(\n            model_id=model_config.adapter_base_model_id or model_id,\n            num_labels=dataset_config.num_labels,\n            id2label=dataset_config.id2label,\n            label2id=dataset_config.label2id,\n            revision=model_config.revision,\n            model_cache_dir=model_config.model_cache_dir,\n        )\n\n        quantization = None\n        if hasattr(config, \"quantization_config\"):\n            quantization = config.quantization_config.get(\"quant_method\", None)\n        if quantization == \"gptq\" and importlib.util.find_spec(\"auto_gptq\") is None:\n            raise NeedsManualDependency(package=\"auto-gptq\")\n        if quantization == \"gptq\" and importlib.util.find_spec(\"optimum\") is None:\n            raise NeedsManualDependency(package=\"optimum\")\n        if quantization == \"awq\" and importlib.util.find_spec(\"awq\") is None:\n            raise NeedsManualDependency(package=\"autoawq\")\n\n        if self.benchmark_config.load_in_4bit is not None:\n            load_in_4bit = self.benchmark_config.load_in_4bit\n        else:\n            load_in_4bit = (\n                model_config.task in GENERATIVE_MODEL_TASKS\n                and self.benchmark_config.device == torch.device(\"cuda\")\n                and (\n                    not hasattr(config, \"quantization_config\")\n                    or config.quantization_config is None\n                )\n            )\n\n        if load_in_4bit and importlib.util.find_spec(\"bitsandbytes\") is None:\n            raise NeedsExtraInstalled(extra=\"generative\")\n\n        use_bf16 = (\n            self.benchmark_config.device == torch.device(\"cuda\")\n            and torch.cuda.is_bf16_supported()\n            and config.to_dict().get(\"torch_dtype\") == \"bfloat16\"\n        )\n        bnb_config = (\n            BitsAndBytesConfig(\n                load_in_4bit=load_in_4bit,\n                bnb_4bit_use_double_quant=True,\n                bnb_4bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n            )\n            if load_in_4bit\n            else None\n        )\n\n        use_vllm = (\n            model_config.task in GENERATIVE_MODEL_TASKS\n            and self.benchmark_config.device == torch.device(\"cuda\")\n            and os.getenv(\"USE_VLLM\", \"1\") == \"1\"\n        )\n\n        if use_vllm and importlib.util.find_spec(\"vllm\") is None:\n            raise NeedsExtraInstalled(extra=\"generative\")\n\n        model: \"PreTrainedModel | GenerativeModel | None\" = None\n        if use_vllm:\n            try:\n                model = VLLMModel(\n                    model_config=model_config,\n                    hf_model_config=config,\n                    model_cache_dir=model_config.model_cache_dir,\n                    trust_remote_code=self.benchmark_config.trust_remote_code,\n                    adapter_base_model_id=model_config.adapter_base_model_id,\n                )\n            except ValueError as e:\n                # If the model is too large to fit on the GPU then we simply throw an\n                # informative error message\n                oom_error_message = \"No available memory for the cache blocks\"\n                if oom_error_message in str(e):\n                    raise InvalidModel(\"The model is too large to load on the GPU.\")\n\n                if self.benchmark_config.raise_errors:\n                    raise e\n\n                # Otherwise some other error occurred, and we log it and try to load\n                # the model with Hugging Face instead\n                use_vllm = False\n                logger.info(\n                    \"Failed to benchmark with vLLM - trying with the Hugging Face \"\n                    f\"implementation instead. The error raised was {e!r}\"\n                )\n\n        if not use_vllm:\n            if self.benchmark_config.use_flash_attention is None:\n                flash_attention = model_config.task in GENERATIVE_MODEL_TASKS\n            else:\n                flash_attention = self.benchmark_config.use_flash_attention\n\n            model_kwargs = dict(\n                config=config,\n                from_flax=from_flax,\n                ignore_mismatched_sizes=ignore_mismatched_sizes,\n                revision=model_config.revision,\n                token=self.benchmark_config.token,\n                cache_dir=model_config.model_cache_dir,\n                trust_remote_code=self.benchmark_config.trust_remote_code,\n                quantization_config=bnb_config,\n                torch_dtype=self._get_torch_dtype(config=config),\n                attn_implementation=\"flash_attention_2\" if flash_attention else None,\n                device_map=(\n                    \"cuda:0\"\n                    if (\n                        hasattr(config, \"quantization_config\")\n                        and config.quantization_config.get(\"quant_method\") == \"gptq\"\n                    )\n                    else None\n                ),\n            )\n\n            # These are used when a timeout occurs\n            attempts_left = 5\n\n            while True:\n                try:\n                    # Get the model class associated with the supertask\n                    if model_config.task in [\"text-generation\", \"conversational\"]:\n                        model_cls_supertask = \"causal-l-m\"\n                    elif model_config.task == \"text2text-generation\":\n                        model_cls_supertask = \"seq-2-seq-l-m\"\n                    elif (\n                        dataset_config.task.name in GENERATIVE_DATASET_TASKS\n                        or supertask in GENERATIVE_DATASET_SUPERTASKS\n                    ):\n                        raise InvalidBenchmark(\n                            f\"The {dataset_config.task.name!r} task is not supported \"\n                            f\"for the model {model_id!r}.\"\n                        )\n                    else:\n                        model_cls_supertask = supertask\n                    model_cls_or_none: Type[\"PreTrainedModel\"] | None = (\n                        get_class_by_name(\n                            class_name=f\"auto-model-for-{model_cls_supertask}\",\n                            module_name=\"transformers\",\n                        )\n                    )\n\n                    # If the model class could not be found then raise an error\n                    if not model_cls_or_none:\n                        raise InvalidBenchmark(\n                            f\"The supertask {supertask!r} does not correspond to a \"\n                            \"Hugging Face AutoModel type (such as \"\n                            \"`AutoModelForSequenceClassification`).\"\n                        )\n\n                    # If the model is a DeBERTaV2 model then we ensure that\n                    # `pooler_hidden_size` is the same size as `hidden_size`\n                    if config.model_type == \"deberta-v2\":\n                        config.pooler_hidden_size = config.hidden_size\n\n                    try:\n                        model_or_tuple = model_cls_or_none.from_pretrained(\n                            model_config.model_id, **model_kwargs\n                        )\n                    except ImportError as e:\n                        if \"flash attention\" in str(e).lower():\n                            raise FlashAttentionNotInstalled()\n                        else:\n                            raise e\n                    except (KeyError, RuntimeError) as e:\n                        if not model_kwargs[\"ignore_mismatched_sizes\"]:\n                            logger.debug(\n                                f\"{type(e).__name__} occurred during the loading \"\n                                f\"of the {model_id!r} model. Retrying with \"\n                                \"`ignore_mismatched_sizes` set to True.\"\n                            )\n                            model_kwargs[\"ignore_mismatched_sizes\"] = True\n                            continue\n                        else:\n                            raise InvalidModel(str(e))\n                    except (TimeoutError, RequestError):\n                        attempts_left -= 1\n                        if attempts_left == 0:\n                            raise InvalidModel(\n                                \"The model could not be loaded after 5 attempts.\"\n                            )\n                        logger.info(f\"Couldn't load the model {model_id!r}. Retrying.\")\n                        sleep(5)\n                        continue\n                    except ValueError as e:\n                        if \"already quantized\" in str(e):\n                            model_kwargs[\"quantization_config\"] = None\n                            model_or_tuple = model_cls_or_none.from_pretrained(\n                                model_config.model_id, **model_kwargs\n                            )\n                        elif \"does not support Flash Attention\" in str(e):\n                            model_kwargs[\"attn_implementation\"] = None\n                            continue\n                        else:\n                            raise e\n\n                    if isinstance(model_or_tuple, tuple):\n                        model = model_or_tuple[0]\n                    else:\n                        model = model_or_tuple\n                    break\n\n                except (OSError, ValueError) as e:\n                    # If `from_flax` is False but only Flax models are available then\n                    # try again with `from_flax` set to True\n                    if (\n                        not from_flax\n                        and \"Use `from_flax=True` to load this model\" in str(e)\n                    ):\n                        from_flax = True\n                        continue\n\n                    self._handle_loading_exception(exception=e, model_id=model_id)\n\n        assert isinstance(model, (PreTrainedModel, GenerativeModel))\n        model.eval()\n        if not load_in_4bit:\n            model.to(self.benchmark_config.device)\n\n        generative_model = model_is_generative(model=model)\n\n        if isinstance(model, PreTrainedModel) and supertask == \"question-answering\":\n            model = setup_model_for_question_answering(model=model)\n\n        tokenizer = self._load_tokenizer(\n            model=model,\n            model_id=model_config.adapter_base_model_id or model_id,\n            generative_model=generative_model,\n        )\n\n        if use_vllm and isinstance(model, VLLMModel):\n            model.set_tokenizer(tokenizer=tokenizer)\n\n        model, tokenizer = align_model_and_tokenizer(\n            model=model,\n            tokenizer=tokenizer,\n            generative_model=generative_model,\n            generation_length=dataset_config.max_generated_tokens,\n            raise_errors=self.benchmark_config.raise_errors,\n        )\n\n        return model, tokenizer\n\n    def _get_torch_dtype(self, config: \"PretrainedConfig\") -&gt; str | torch.dtype:\n        \"\"\"Get the torch dtype, used for loading the model.\n\n        Args:\n            config:\n                The Hugging Face model configuration.\n\n        Returns:\n            The torch dtype.\n        \"\"\"\n        using_cuda = self.benchmark_config.device == torch.device(\"cuda\")\n        torch_dtype_is_set = config.to_dict().get(\"torch_dtype\") is not None\n        bf16_available = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n        if using_cuda and torch_dtype_is_set:\n            return \"auto\"\n        elif using_cuda and bf16_available:\n            return torch.bfloat16\n        elif using_cuda:\n            return torch.float16\n        return torch.float32\n\n    def _load_hf_model_config(\n        self,\n        model_id: str,\n        num_labels: int,\n        id2label: dict[int, str],\n        label2id: dict[str, int],\n        revision: str,\n        model_cache_dir: str | None,\n    ) -&gt; \"PretrainedConfig\":\n        \"\"\"Load the Hugging Face model configuration.\n\n        Args:\n            model_id:\n                The Hugging Face model ID.\n            num_labels:\n                The number of labels in the dataset.\n            id2label:\n                The mapping from label IDs to labels.\n            label2id:\n                The mapping from labels to label IDs.\n            revision:\n                The revision of the model.\n            model_cache_dir:\n                The directory to cache the model in.\n\n        Returns:\n            The Hugging Face model configuration.\n        \"\"\"\n        while True:\n            try:\n                config = AutoConfig.from_pretrained(\n                    model_id,\n                    num_labels=num_labels,\n                    id2label=id2label,\n                    label2id=label2id,\n                    revision=revision,\n                    token=self.benchmark_config.token,\n                    trust_remote_code=self.benchmark_config.trust_remote_code,\n                    cache_dir=model_cache_dir,\n                )\n                if config.eos_token_id is not None and config.pad_token_id is None:\n                    if isinstance(config.eos_token_id, list):\n                        config.pad_token_id = config.eos_token_id[0]\n                    else:\n                        config.pad_token_id = config.eos_token_id\n                return config\n            except KeyError as e:\n                key = e.args[0]\n                raise InvalidModel(\n                    f\"The model config for the model {model_id!r} could not be \"\n                    f\"loaded, as the key {key!r} was not found in the config.\"\n                )\n            except OSError as e:\n                # TEMP: When the model is gated then we cannot set cache dir, for some\n                # reason (transformers==4.38.2). This should be included back in when\n                # this is fixed.\n                if \"gated repo\" in str(e):\n                    model_cache_dir = None\n                    continue\n                raise InvalidModel(\n                    f\"Couldn't load model config for {model_id!r}. The error was \"\n                    f\"{e!r}. Skipping\"\n                )\n            except (TimeoutError, RequestError):\n                logger.info(f\"Couldn't load model config for {model_id!r}. Retrying.\")\n                sleep(5)\n                continue\n            except ValueError as e:\n                requires_trust_remote_code = \"trust_remote_code\" in str(e)\n                if requires_trust_remote_code:\n                    raise NeedsAdditionalArgument(\n                        cli_argument=\"--trust-remote-code\",\n                        script_argument=\"trust_remote_code=True\",\n                        run_with_cli=self.benchmark_config.run_with_cli,\n                    )\n                raise e\n\n    def _load_tokenizer(\n        self,\n        model: \"PreTrainedModel | GenerativeModel\",\n        model_id: str,\n        generative_model: bool,\n    ) -&gt; \"Tokenizer\":\n        \"\"\"Load the tokenizer.\n\n        Args:\n            model:\n                The model, which is used to determine whether to add a prefix space to\n                the tokens.\n            model_id:\n                The model identifier. Used for logging.\n            generative_model:\n                Whether the model is a generative model.\n\n        Returns:\n            The loaded tokenizer.\n        \"\"\"\n        loading_kwargs: dict[str, bool | str] = dict(\n            use_fast=True,\n            verbose=False,\n            trust_remote_code=self.benchmark_config.trust_remote_code,\n        )\n\n        # If the model is a subclass of a certain model types then we have to add a\n        # prefix space to the tokens, by the way the model is constructed.\n        prefix_models = [\"Roberta\", \"GPT\", \"Deberta\"]\n        add_prefix = any(\n            model_type in type(model).__name__ for model_type in prefix_models\n        )\n        if add_prefix:\n            loading_kwargs[\"add_prefix_space\"] = True\n\n        padding_side = \"left\" if generative_model else \"right\"\n        loading_kwargs[\"padding_side\"] = padding_side\n        loading_kwargs[\"truncation_side\"] = padding_side\n\n        while True:\n            try:\n                return AutoTokenizer.from_pretrained(model_id, **loading_kwargs)\n            except (JSONDecodeError, OSError, TypeError):\n                raise InvalidModel(f\"Could not load tokenizer for model {model_id!r}.\")\n            except (TimeoutError, RequestError):\n                logger.info(f\"Couldn't load tokenizer for {model_id!r}. Retrying.\")\n                sleep(5)\n                continue\n\n    @staticmethod\n    def _handle_loading_exception(exception: Exception, model_id: str) -&gt; None:\n        if \"checkpoint seems to be incorrect\" in str(exception):\n            raise InvalidModel(f\"The model {model_id!r} has an incorrect checkpoint.\")\n        if \"trust_remote_code\" in str(exception):\n            raise InvalidModel(\n                f\"Loading the model {model_id!r} needs to trust remote code. \"\n                \"If you trust the suppliers of this model, then you can enable \"\n                \"this by setting the `--trust-remote-code` flag.\"\n            )\n        raise InvalidModel(\n            f\"The model {model_id} either does not exist on the Hugging Face \"\n            \"Hub, or it has no frameworks registered, or it is a private \"\n            \"model. If it *does* exist on the Hub and is a public model then \"\n            \"please ensure that it has a framework registered. If it is a \"\n            \"private model then enable the `--use-token` flag and make \"\n            \"sure that you are logged in to the Hub via the \"\n            f\"`huggingface-cli login` command. The error raised was {exception!r}\"\n        )\n</code></pre>"},{"location":"api/scandeval/model_setups/local/","title":"scandeval.model_setups.local","text":"scandeval.model_setups.local<p> source module scandeval.model_setups.local </p> <p>Model setup for local Hugging Face Hub models.</p> <p> Classes </p> <ul> <li> <p>LocalModelSetup \u2014 Model setup for local Hugging Face Hub models.</p> </li> </ul> <p> source class LocalModelSetup() </p> <p>Model setup for local Hugging Face Hub models.</p> <p>Initialize the LocalModelSetup class.</p> <p> Attributes </p> <ul> <li> <p>benchmark_config \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>model_exists \u2014 Check if a model exists locally.</p> </li> <li> <p>get_model_config \u2014 Fetches configuration for a local Hugging Face model.</p> </li> <li> <p>load_model \u2014 Load a local Hugging Face model.</p> </li> </ul> <p> source method LocalModelSetup.model_exists(model_id: str) \u2192 bool | dict[str, str] </p> <p>Check if a model exists locally.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | dict[str, str] \u2014 Whether the model exist, or a dictionary explaining why we cannot check whether the model exists.</p> </li> </ul> <p> source method LocalModelSetup.get_model_config(model_id: str) \u2192 ModelConfig </p> <p>Fetches configuration for a local Hugging Face model.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID of the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NeedsManualDependency</p> </li> <li> <p>e</p> </li> <li> <p>InvalidModel</p> </li> </ul> <p> source method LocalModelSetup.load_model(model_config: ModelConfig, dataset_config: DatasetConfig) \u2192 tuple[PreTrainedModel | GenerativeModel, Tokenizer] </p> <p>Load a local Hugging Face model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[PreTrainedModel | GenerativeModel, Tokenizer] \u2014 The tokenizer and model.</p> </li> </ul>"},{"location":"src/scandeval/model_setups/local/","title":"scandeval.model_setups.local","text":"scandeval.model_setups.local<p> docs module scandeval.model_setups.local </p> <pre><code>\"\"\"Model setup for local Hugging Face Hub models.\"\"\"\n\nimport importlib.util\nimport logging\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING\n\nfrom transformers import AutoConfig\nfrom transformers.models.auto.modeling_auto import (\n    MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,\n    MODEL_FOR_MASKED_LM_MAPPING_NAMES,\n    MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES,\n)\n\nfrom ..config import ModelConfig\nfrom ..enums import Framework, ModelType\nfrom ..exceptions import InvalidModel, NeedsManualDependency\nfrom ..utils import create_model_cache_dir\nfrom .hf import HFModelSetup\n\nif TYPE_CHECKING:\n    from peft.config import PeftConfig\n    from transformers import PreTrainedModel\n\n    from ..config import BenchmarkConfig, DatasetConfig\n    from ..protocols import GenerativeModel, Tokenizer\n\nif importlib.util.find_spec(\"peft\") is not None:\n    from peft.config import PeftConfig\n\n\nlogger = logging.getLogger(__package__)\n\n\nclass LocalModelSetup:docs\n    \"\"\"Model setup for local Hugging Face Hub models.\n\n    Attributes:\n        benchmark_config:\n            The benchmark configuration.\n    \"\"\"\n\n    def __init__(self, benchmark_config: \"BenchmarkConfig\") -&gt; None:\n        \"\"\"Initialize the LocalModelSetup class.\n\n        Args:\n            benchmark_config:\n                The benchmark configuration.\n        \"\"\"\n        self.benchmark_config = benchmark_config\n\n    def model_exists(self, model_id: str) -&gt; bool | dict[str, str]:docs\n        \"\"\"Check if a model exists locally.\n\n        Args:\n            model_id:\n                The model ID.\n\n        Returns:\n            Whether the model exist, or a dictionary explaining why we cannot check\n            whether the model exists.\n        \"\"\"\n        model_dir = Path(model_id)\n\n        if not model_dir.exists():\n            return False\n\n        try:\n            AutoConfig.from_pretrained(model_id)\n        except OSError:\n            return False\n\n        return (\n            model_dir.glob(\"*.bin\") is not None\n            or model_dir.glob(\"*.pt\") is not None\n            or model_dir.glob(\"*.msgpack\") is not None\n            or model_dir.glob(\"*.safetensors\") is not None\n        )\n\n    def get_model_config(self, model_id: str) -&gt; ModelConfig:docs\n        \"\"\"Fetches configuration for a local Hugging Face model.\n\n        Args:\n            model_id:\n                The model ID of the model.\n\n        Returns:\n            The model configuration.\n        \"\"\"\n        framework = self.benchmark_config.framework\n        if framework is None:\n            try:\n                exts = {f.suffix for f in Path(model_id).iterdir()}\n                if \".bin\" in exts:\n                    framework = Framework.PYTORCH\n                elif \".msgpack\" in exts:\n                    framework = Framework.JAX\n                elif \".whl\" in exts:\n                    raise InvalidModel(\"SpaCy models are not supported.\")\n                elif \".h5\" in exts:\n                    raise InvalidModel(\"TensorFlow/Keras models are not supported.\")\n            except OSError as e:\n                logger.info(f\"Cannot list files for local model `{model_id}`!\")\n                if self.benchmark_config.raise_errors:\n                    raise e\n\n        if framework is None:\n            msg = f\"Assuming 'pytorch' as the framework for local model `{model_id}`. \"\n            if self.benchmark_config.run_with_cli:\n                msg += (\n                    \"If this is not the case then please use the --framework argument \"\n                    \"to override.\"\n                )\n            else:\n                msg += (\n                    \"If this is not the case then please use the `framework` argument \"\n                    \"in the `Benchmarker` class to override.\"\n                )\n            logger.info(msg)\n            framework = Framework.PYTORCH\n\n        hf_model_config = AutoConfig.from_pretrained(model_id)\n        model_type = hf_model_config.model_type.lower()\n        if model_type in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES:\n            task = \"text-generation\"\n        elif model_type in MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES:\n            task = \"text2text-generation\"\n        elif model_type in MODEL_FOR_MASKED_LM_MAPPING_NAMES:\n            task = \"fill-mask\"\n        else:\n            task = \"unknown\"\n\n        is_adapter = \"adapter_config.json\" in [\n            path.name for path in Path(model_id).glob(\"*.json\")\n        ]\n        adapter_base_model_id: str | None = None\n        if is_adapter:\n            if importlib.util.find_spec(\"peft\") is None:\n                raise NeedsManualDependency(package=\"peft\")\n            peft_config = PeftConfig.from_pretrained(model_id)\n            adapter_base_model_id = peft_config.base_model_name_or_path\n\n        model_config = ModelConfig(\n            model_id=model_id,\n            revision=\"main\",\n            framework=framework,\n            task=task,\n            languages=list(),\n            model_type=ModelType.LOCAL,\n            model_cache_dir=create_model_cache_dir(\n                cache_dir=self.benchmark_config.cache_dir, model_id=model_id\n            ),\n            adapter_base_model_id=adapter_base_model_id,\n        )\n        return model_config\n\n    def load_model(docs\n        self, model_config: ModelConfig, dataset_config: \"DatasetConfig\"\n    ) -&gt; tuple[\"PreTrainedModel | GenerativeModel\", \"Tokenizer\"]:\n        \"\"\"Load a local Hugging Face model.\n\n        Args:\n            model_config:\n                The model configuration.\n            dataset_config:\n                The dataset configuration.\n\n        Returns:\n            The tokenizer and model.\n        \"\"\"\n        hf_model_setup = HFModelSetup(benchmark_config=self.benchmark_config)\n        return hf_model_setup.load_model(\n            model_config=model_config, dataset_config=dataset_config\n        )\n</code></pre>"},{"location":"api/scandeval/model_setups/openai/","title":"scandeval.model_setups.openai","text":"scandeval.model_setups.openai<p> source module scandeval.model_setups.openai </p> <p>Model setup for OpenAI models.</p> <p> Classes </p> <ul> <li> <p>OpenAIModelSetup \u2014 Model setup for OpenAI models.</p> </li> </ul> <p> source class OpenAIModelSetup() </p> <p>Model setup for OpenAI models.</p> <p>Initialize the model setup.</p> <p> Attributes </p> <ul> <li> <p>benchmark_config \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>model_exists \u2014 Check if a model ID denotes an OpenAI model.</p> </li> <li> <p>get_model_config \u2014 Fetches configuration for an OpenAI model.</p> </li> <li> <p>load_model \u2014 Load an OpenAI model.</p> </li> </ul> <p> source method OpenAIModelSetup.model_exists(model_id: str) \u2192 bool | dict[str, str] </p> <p>Check if a model ID denotes an OpenAI model.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | dict[str, str] \u2014 Whether the model exists, or a dictionary explaining why we cannot check whether the model exists.</p> </li> </ul> <p> source method OpenAIModelSetup.get_model_config(model_id: str) \u2192 ModelConfig </p> <p>Fetches configuration for an OpenAI model.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID of the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> source method OpenAIModelSetup.load_model(model_config: ModelConfig, dataset_config: DatasetConfig) \u2192 tuple[PreTrainedModel | GenerativeModel, Tokenizer] </p> <p>Load an OpenAI model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[PreTrainedModel | GenerativeModel, Tokenizer] \u2014 The tokenizer and model.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError</p> </li> </ul>"},{"location":"src/scandeval/model_setups/openai/","title":"scandeval.model_setups.openai","text":"scandeval.model_setups.openai<p> docs module scandeval.model_setups.openai </p> <pre><code>\"\"\"Model setup for OpenAI models.\"\"\"\n\nimport importlib.util\nimport logging\nimport re\nfrom typing import TYPE_CHECKING\n\nfrom transformers import PretrainedConfig\n\nfrom ..config import ModelConfig\nfrom ..enums import Framework, ModelType\nfrom ..openai_models import OpenAIModel, OpenAITokenizer\nfrom ..utils import create_model_cache_dir\n\nif TYPE_CHECKING:\n    from transformers import PreTrainedModel\n\n    from ..config import BenchmarkConfig, DatasetConfig\n    from ..protocols import GenerativeModel, Tokenizer\n\nif importlib.util.find_spec(\"openai\") is not None:\n    import openai\n\n    # Older versions of `openai` doesn't have the `models` module, so we need to check\n    # that, as it will cause errors later otherwise\n    openai.models\n\n\nlogger = logging.getLogger(__package__)\n\n\n# This is a list of the major models that OpenAI has released\nCACHED_OPENAI_MODEL_IDS: list[str] = [\n    \"ada|babbage|curie|davinci\",\n    \"(code|text)-(ada|babbage|curie|davinci)-[0-9]{3}\",\n    \"gpt-3.5-turbo(-16k|-instruct)?(-[0-9]{4})?(-preview)?\",\n    \"gpt-4(-[0-9]{4})?(-preview)?\",\n    \"gpt-4-turbo(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\",\n    \"gpt-4-32k(-[0-9]{4})?(-preview)?\",\n    \"gpt-4o(-mini)?(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\",\n]\n\n\nVOCAB_SIZE_MAPPING = {\n    \"(text-)?(ada|babbage|curie|davinci)(-001)?\": 50_257,\n    \"(code|text)-davinci-00[2-9]\": 50_281,\n    \"gpt-3.5-turbo(-16k)?(-[0-9]{4})?\": 100_256,\n    \"gpt-4-(32k)?(-[0-9]{4})?\": 100_256,\n    \"gpt-4-[0-9]{4}-preview\": 100_256,\n    \"gpt-4-turbo(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": 100_256,\n    \"gpt-4-(vision|turbo)(-preview)?\": 100_256,\n    \"gpt-3.5-turbo-instruct(-[0-9]{4})?\": 100_256,\n    \"gpt-4o(-mini)?(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": 200_019,\n}\n\n\nMODEL_MAX_LENGTH_MAPPING = {\n    \"(text-)?(ada|babbage|curie|davinci)(-001)?\": 2_050,\n    \"text-davinci-00[2-9]\": 4_098,\n    \"code-davinci-00[1-9]\": 8_002,\n    \"gpt-3.5-turbo-0613\": 4_096,\n    \"gpt-3.5-turbo(-[0-9]{4})?\": 16_385,\n    \"gpt-3.5-turbo-16k(-[0-9]{4})?\": 16_384,\n    \"gpt-4(-[0-9]{4})?\": 8_191,\n    \"gpt-4-32k(-[0-9]{4})?\": 32_767,\n    \"gpt-4-[0-9]{4}-preview\": 128_000,\n    \"gpt-4-turbo(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": 128_000,\n    \"gpt-4-(vision|turbo)(-preview)?\": 128_000,\n    \"gpt-3.5-turbo-instruct(-[0-9]{4})?\": 4_095,\n    \"gpt-4o(-mini)?(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": 128_000,\n}\n\n\nNUM_PARAMS_MAPPING = {\n    \"(text-)?ada(-001)?\": 350_000_000,\n    \"(text-)?babbage(-001)?\": 3_000_000_000,\n    \"(text-)?curie(-001)?\": 13_000_000_000,\n    \"((text|code)-)?davinci(-00[1-9])?\": 175_000_000_000,\n    \"gpt-(3.5|4)-turbo-((16|32)k)?(-[0-9]{4})?\": -1,\n    \"gpt-4-[0-9]{4}-preview\": -1,\n    \"gpt-4-turbo(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": -1,\n    \"gpt-4-(vision|turbo)(-preview)?\": -1,\n    \"gpt-3.5-turbo-instruct(-[0-9]{4})?\": -1,\n    \"gpt-4o(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": -1,\n    \"gpt-4o-mini(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": -1,\n}\n\n\nclass OpenAIModelSetup:docs\n    \"\"\"Model setup for OpenAI models.\n\n    Attributes:\n        benchmark_config:\n            The benchmark configuration.\n    \"\"\"\n\n    def __init__(self, benchmark_config: \"BenchmarkConfig\") -&gt; None:\n        \"\"\"Initialize the model setup.\n\n        Args:\n            benchmark_config:\n                The benchmark configuration.\n        \"\"\"\n        self.benchmark_config = benchmark_config\n\n    def model_exists(self, model_id: str) -&gt; bool | dict[str, str]:docs\n        \"\"\"Check if a model ID denotes an OpenAI model.\n\n        Args:\n            model_id:\n                The model ID.\n\n        Returns:\n            Whether the model exists, or a dictionary explaining why we cannot check\n            whether the model exists.\n        \"\"\"\n        if importlib.util.find_spec(\"openai\") is None:\n            return dict(missing_extra=\"openai\")\n\n        # The model ID for the Azure OpenAI API is the deployment name and therefore\n        # different from the model ID used in the OpenAI API. We'll just assume that\n        # the model exists in this case.\n        if self.benchmark_config.azure_openai_api_key is not None:\n            return True\n\n        all_models: list[openai.types.model.Model] = list()\n        try:\n            all_models = list(openai.models.list())\n            return model_id in [model.id for model in all_models]\n        except openai.OpenAIError as e:\n            model_exists = any(\n                [\n                    re.match(pattern=model_pattern, string=model_id) is not None\n                    for model_pattern in CACHED_OPENAI_MODEL_IDS\n                ]\n            )\n            if not model_exists:\n                if \"OPENAI_API_KEY\" in str(e):\n                    return dict(missing_env_var=\"OPENAI_API_KEY\")\n                elif \"AZURE_OPENAI_API_KEY\" in str(e):\n                    return dict(missing_env_var=\"AZURE_OPENAI_API_KEY\")\n                elif \"AZURE_OPENAI_ENDPOINT\" in str(e):\n                    return dict(missing_env_var=\"AZURE_OPENAI_ENDPOINT\")\n            return model_exists\n\n    def get_model_config(self, model_id: str) -&gt; ModelConfig:docs\n        \"\"\"Fetches configuration for an OpenAI model.\n\n        Args:\n            model_id:\n                The model ID of the model.\n\n        Returns:\n            The model configuration.\n        \"\"\"\n        return ModelConfig(\n            model_id=model_id,\n            revision=\"main\",\n            framework=Framework.API,\n            task=\"text-generation\",\n            languages=list(),\n            model_type=ModelType.OPENAI,\n            model_cache_dir=create_model_cache_dir(\n                cache_dir=self.benchmark_config.cache_dir, model_id=model_id\n            ),\n            adapter_base_model_id=None,\n        )\n\n    def load_model(docs\n        self, model_config: ModelConfig, dataset_config: \"DatasetConfig\"\n    ) -&gt; tuple[\"PreTrainedModel | GenerativeModel\", \"Tokenizer\"]:\n        \"\"\"Load an OpenAI model.\n\n        Args:\n            model_config:\n                The model configuration.\n            dataset_config:\n                The dataset configuration.\n\n        Returns:\n            The tokenizer and model.\n        \"\"\"\n        hf_model_config = PretrainedConfig.from_pretrained(\"gpt2\")\n\n        vocab_sizes = [\n            vocab_size\n            for pattern, vocab_size in VOCAB_SIZE_MAPPING.items()\n            if re.match(pattern=pattern, string=model_config.model_id)\n        ]\n        hf_model_config.vocab_size = vocab_sizes[0] if vocab_sizes else 100_256\n\n        model_lengths = [\n            model_length\n            for pattern, model_length in MODEL_MAX_LENGTH_MAPPING.items()\n            if re.match(pattern=f\"^{pattern}$\", string=model_config.model_id)\n        ]\n        hf_model_config.model_max_length = model_lengths[0] if model_lengths else -1\n\n        num_params = [\n            num_param\n            for pattern, num_param in NUM_PARAMS_MAPPING.items()\n            if re.match(pattern=pattern, string=model_config.model_id)\n        ]\n        hf_model_config.num_params = num_params[0] if num_params else -1\n\n        hf_model_config.id2label = dataset_config.id2label\n        hf_model_config.label2id = dataset_config.label2id\n        hf_model_config.eos_token_id = hf_model_config.vocab_size - 1\n        hf_model_config.bos_token_id = hf_model_config.vocab_size - 1\n        hf_model_config.pad_token_id = hf_model_config.vocab_size - 1\n\n        # Check if the vocab size is correct, and if not then correct it\n        tok = OpenAITokenizer(\n            model_config=model_config, hf_model_config=hf_model_config\n        )\n        for idx in range(hf_model_config.vocab_size - 1, 0, -1):\n            try:\n                tok.decode([idx])\n                hf_model_config.vocab_size = idx + 1\n                break\n            except Exception:\n                pass\n        else:\n            raise ValueError(\n                f\"Couldn't find vocab size for the model {model_config.model_id!r}\"\n            )\n\n        tokenizer = OpenAITokenizer(\n            model_config=model_config, hf_model_config=hf_model_config\n        )\n        model = OpenAIModel(\n            model_config=model_config,\n            hf_model_config=hf_model_config,\n            dataset_config=dataset_config,\n            benchmark_config=self.benchmark_config,\n            tokenizer=tokenizer,\n        )\n\n        # If the model is a chat model then we need to reduce the maximum context\n        # length by 7 tokens, as these are used in the chat prompt\n        if model.is_chat_model:\n            hf_model_config.model_max_length -= 7\n            tokenizer.hf_model_config = hf_model_config\n            model.config = hf_model_config\n\n        return model, tokenizer\n</code></pre>"},{"location":"api/scandeval/model_setups/utils/","title":"scandeval.model_setups.utils","text":"scandeval.model_setups.utils<p> source module scandeval.model_setups.utils </p> <p>Utility functions related to setting up models.</p> <p> Functions </p> <ul> <li> <p>get_children_of_module \u2014 Get the children of a module.</p> </li> <li> <p>setup_model_for_question_answering \u2014 Setup a model for question answering.</p> </li> <li> <p>align_model_and_tokenizer \u2014 Aligns the model and the tokenizer.</p> </li> </ul> <p> source get_children_of_module(name: str, module: nn.Module) \u2192 nn.Module | dict[str, Any] | None </p> <p>Get the children of a module.</p> <p> Parameters </p> <ul> <li> <p>name :  str \u2014</p> <p>The name of the module.</p> </li> <li> <p>module :  nn.Module \u2014</p> <p>The module to get the children of.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>nn.Module | dict[str, Any] | None \u2014 The children of the module, or None if the module has no children.</p> </li> </ul> <p> source setup_model_for_question_answering(model: PreTrainedModel) \u2192 PreTrainedModel </p> <p>Setup a model for question answering.</p> <p> Parameters </p> <ul> <li> <p>model :  PreTrainedModel \u2014</p> <p>The model to setup.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>PreTrainedModel \u2014 The setup model.</p> </li> </ul> <p> source align_model_and_tokenizer(model: PreTrainedModel | GenerativeModel, tokenizer: Tokenizer, generative_model: bool, generation_length: int, raise_errors: bool = False) \u2192 tuple[PreTrainedModel | GenerativeModel, Tokenizer] </p> <p>Aligns the model and the tokenizer.</p> <p> Parameters </p> <ul> <li> <p>model :  PreTrainedModel | GenerativeModel \u2014</p> <p>The model to fix.</p> </li> <li> <p>tokenizer :  Tokenizer \u2014</p> <p>The tokenizer to fix.</p> </li> <li> <p>generative_model :  bool \u2014</p> <p>Whether the model is a generative model.</p> </li> <li> <p>generation_length :  int \u2014</p> <p>The length of the generation, which depends on the benchmark dataset. Only relevant if the model is a generative model.</p> </li> <li> <p>raise_errors :  bool \u2014</p> <p>Whether to raise errors instead of trying to fix them silently.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[PreTrainedModel | GenerativeModel, Tokenizer] \u2014 The fixed model and tokenizer.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> <li> <p>e</p> </li> </ul>"},{"location":"src/scandeval/model_setups/utils/","title":"scandeval.model_setups.utils","text":"scandeval.model_setups.utils<p> docs module scandeval.model_setups.utils </p> <pre><code>\"\"\"Utility functions related to setting up models.\"\"\"\n\nfrom typing import TYPE_CHECKING, Any\n\nimport torch\nimport torch.nn as nn\n\nfrom ..exceptions import InvalidModel\nfrom ..protocols import GenerativeModel, Tokenizer\nfrom ..utils import DUMMY_FILL_VALUE, get_model_max_length\nfrom ..vllm_models import VLLMModel\n\nif TYPE_CHECKING:\n    from transformers import PreTrainedModel\n\n\ndef get_children_of_module(docs\n    name: str, module: nn.Module\n) -&gt; nn.Module | dict[str, Any] | None:\n    \"\"\"Get the children of a module.\n\n    Args:\n        name:\n            The name of the module.\n        module:\n            The module to get the children of.\n\n    Returns:\n        The children of the module, or None if the module has no children.\n    \"\"\"\n    if len(list(module.children())) == 0:\n        if name == \"token_type_embeddings\":\n            return module\n        else:\n            return None\n    else:\n        submodules = dict()\n        for subname, submodule in module.named_children():\n            children = get_children_of_module(name=subname, module=submodule)\n            if children:\n                submodules[subname] = children\n        return submodules\n\ndocs\ndef setup_model_for_question_answering(model: \"PreTrainedModel\") -&gt; \"PreTrainedModel\":\n    \"\"\"Setup a model for question answering.\n\n    Args:\n        model:\n            The model to setup.\n\n    Returns:\n        The setup model.\n    \"\"\"\n    # Get the models' token type embedding children, if they exist\n    children = get_children_of_module(name=\"model\", module=model)\n\n    # If the model has token type embeddings then get them\n    if children:\n        # Get the list of attributes that are token type embeddings\n        attribute_list = list()\n        done = False\n        while not done:\n            for key, value in children.items():\n                attribute_list.append(key)\n                if isinstance(value, dict):\n                    children = value\n                else:\n                    done = True\n                break\n\n        # Get the token type embeddings\n        token_type_embeddings = model\n        for attribute in attribute_list:\n            token_type_embeddings = getattr(token_type_embeddings, attribute)\n\n        # If the token type embeddings has shape (1, ...) then set the shape to\n        # (2, ...) by randomly initializing the second token type embedding\n        if token_type_embeddings.weight.data.shape[0] == 1:\n            token_type_embeddings.weight.data = torch.cat(\n                (\n                    token_type_embeddings.weight.data,\n                    torch.rand_like(token_type_embeddings.weight.data),\n                ),\n                dim=0,\n            )\n            token_type_embeddings.num_embeddings = 2\n\n        # Set the model config to use the new type vocab size\n        model.config.type_vocab_size = 2\n\n    return model\n\n\ndef align_model_and_tokenizer(docs\n    model: \"PreTrainedModel | GenerativeModel\",\n    tokenizer: \"Tokenizer\",\n    generative_model: bool,\n    generation_length: int,\n    raise_errors: bool = False,\n) -&gt; tuple[\"PreTrainedModel | GenerativeModel\", \"Tokenizer\"]:\n    \"\"\"Aligns the model and the tokenizer.\n\n    Args:\n        model:\n            The model to fix.\n        tokenizer:\n            The tokenizer to fix.\n        generative_model:\n            Whether the model is a generative model.\n        generation_length:\n            The length of the generation, which depends on the benchmark dataset. Only\n            relevant if the model is a generative model.\n        raise_errors:\n            Whether to raise errors instead of trying to fix them silently.\n\n    Returns:\n        The fixed model and tokenizer.\n    \"\"\"\n    model_max_length = get_model_max_length(model=model, tokenizer=tokenizer)\n\n    # If the model is a generative model then we need to subtract the generation length\n    # from the maximum length, to allow it to keep generating\n    if generative_model:\n        model_max_length -= generation_length\n\n    # Ensure that the model max length is at least 5,000, to avoid OOM errors\n    model_max_length = min(model_max_length, 5_000)\n\n    if model_max_length &gt; 0:\n        tokenizer.model_max_length = model_max_length\n    elif generative_model:\n        tokenizer.model_max_length = 5_000\n    else:\n        tokenizer.model_max_length = 512\n\n    # If we're not dealing with a generative model then we move it to CPU to avoid OOM\n    # errors\n    device: torch.device = model.device if generative_model else torch.device(\"cpu\")\n    model_device = model.device\n    model.to(device)\n\n    # Manually check that this model max length is valid for the model, and adjust\n    # otherwise\n    initial_max_length = tokenizer.model_max_length\n    for max_length in range(initial_max_length, 0, -1):\n        tokenizer.model_max_length = max_length\n        dummy_inputs = torch.full(\n            size=(1, max_length),\n            fill_value=DUMMY_FILL_VALUE,\n            dtype=torch.long,\n            device=device,\n        )\n\n        with torch.inference_mode():\n            try:\n                model(dummy_inputs)\n                break\n\n            # This handles the case where the model is a sequence-to-sequence model, as\n            # they require text labels to be passed in\n            except ValueError as e:\n                if \"decoder_input_ids\" not in str(e) or isinstance(model, VLLMModel):\n                    raise e\n                model(input_ids=dummy_inputs, labels=torch.zeros(1, 1).long())\n                break\n\n            # This happens if `max_length` is too large\n            except IndexError:\n                continue\n\n    # If there is a mismatch between the vocab size according to the tokenizer and\n    # the vocab size according to the model, we raise an error\n    if hasattr(model.config, \"vocab_size\") and hasattr(tokenizer, \"vocab_size\"):\n        if model.config.vocab_size &lt; tokenizer.vocab_size:\n            if raise_errors:\n                raise InvalidModel(\n                    \"The vocab size of the tokenizer is larger than the vocab size of \"\n                    \"the model. As the --raise-errors option was specified, the \"\n                    \"embeddings of the model will not be automatically adjusted.\"\n                )\n            if isinstance(model, VLLMModel):\n                raise InvalidModel(\n                    \"The vocab size of the tokenizer is larger than the vocab size of \"\n                    \"the model.\"\n                )\n            if hasattr(model, \"resize_token_embeddings\"):\n                model.resize_token_embeddings(new_num_tokens=tokenizer.vocab_size + 1)\n\n    # For generative models, the `transformers` package requires the pad token to be\n    # identical to the eos token, if the latter exists. Otherwise, if both the pad and\n    # eos token are not defined, then we attempt to set the padding token to the sep\n    # token. If a sep token doesn't exist either, we raise an error.\n    if generative_model:\n        tokenizer.padding_side = \"left\"\n        if tokenizer.eos_token is not None:\n            tokenizer.pad_token = tokenizer.eos_token\n        elif tokenizer.pad_token is None:\n            if tokenizer.sep_token is not None:\n                tokenizer.pad_token = tokenizer.sep_token\n            else:\n                raise InvalidModel(\n                    \"The tokenizer does not have a padding token and does not have a \"\n                    \"SEP token or EOS token to use as a padding token.\"\n                )\n        model.config.pad_token_id = tokenizer.pad_token_id\n\n    if tokenizer.bos_token is None and tokenizer.eos_token is not None:\n        tokenizer.bos_token = tokenizer.eos_token\n        tokenizer.bos_token_id = tokenizer.eos_token_id\n\n    model.to(model_device)\n\n    return model, tokenizer\n</code></pre>"},{"location":"api/scandeval/benchmarker/","title":"scandeval.benchmarker","text":"scandeval.benchmarker<p> source module scandeval.benchmarker </p> <p>Class that benchmarks Scandinavian language models.</p> <p> Classes </p> <ul> <li> <p>BenchmarkConfigParams \u2014 The parameters for the benchmark configuration.</p> </li> <li> <p>BenchmarkResult \u2014 A benchmark result.</p> </li> <li> <p>Benchmarker \u2014 Benchmarking all the Scandinavian language models.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>model_has_been_benchmarked \u2014 Checks whether a model has already been benchmarked on a dataset.</p> </li> <li> <p>adjust_logging_level \u2014 Adjust the logging level based on verbosity.</p> </li> <li> <p>clear_model_cache_fn \u2014 Clear the model cache.</p> </li> <li> <p>prepare_dataset_configs \u2014 Prepare the dataset configuration(s) to be benchmarked.</p> </li> </ul> <p> source class BenchmarkConfigParams() </p> <p><p>Bases : BaseModel</p></p> <p>The parameters for the benchmark configuration.</p> <p> Attributes </p> <ul> <li> <p>model_extra :  dict[str, Any] | None \u2014 Get extra fields set during validation.</p> </li> <li> <p>model_fields_set :  set[str] \u2014 Returns the set of fields that have been explicitly set on this model instance.</p> </li> </ul> <p> source class BenchmarkResult() </p> <p><p>Bases : BaseModel</p></p> <p>A benchmark result.</p> <p> Attributes </p> <ul> <li> <p>model_config :  ClassVar[ConfigDict] \u2014 Configuration for the model, should be a dictionary conforming to [<code>ConfigDict</code>][pydantic.config.ConfigDict].</p> </li> <li> <p>model_extra :  dict[str, Any] | None \u2014 Get extra fields set during validation.</p> </li> <li> <p>model_fields_set :  set[str] \u2014 Returns the set of fields that have been explicitly set on this model instance.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>from_dict \u2014 Create a benchmark result from a dictionary.</p> </li> <li> <p>append_to_results \u2014 Append the benchmark result to the results file.</p> </li> </ul> <p> source classmethod BenchmarkResult.from_dict(config: dict) \u2192 BenchmarkResult </p> <p>Create a benchmark result from a dictionary.</p> <p> Parameters </p> <ul> <li> <p>config :  dict \u2014</p> <p>The configuration dictionary.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BenchmarkResult \u2014 The benchmark result.</p> </li> </ul> <p> source method BenchmarkResult.append_to_results(results_path: Path) \u2192 None </p> <p>Append the benchmark result to the results file.</p> <p> Parameters </p> <ul> <li> <p>results_path :  Path \u2014</p> <p>The path to the results file.</p> </li> </ul> <p> source class Benchmarker(save_results: bool = True, task: str | list[str] | None = None, dataset: list[str] | str | None = None, language: str | list[str] = 'all', model_language: str | list[str] | None = None, dataset_language: str | list[str] | None = None, framework: Framework | str | None = None, device: Device | None = None, batch_size: int = 32, evaluate_train: bool = False, raise_errors: bool = False, cache_dir: str = '.scandeval_cache', token: bool | str = True, openai_api_key: str | None = None, prefer_azure: bool = False, azure_openai_api_key: str | None = None, azure_openai_endpoint: str | None = None, azure_openai_api_version: str | None = None, force: bool = False, verbose: bool = False, trust_remote_code: bool = False, load_in_4bit: bool | None = None, use_flash_attention: bool | None = None, clear_model_cache: bool = False, only_validation_split: bool = False, few_shot: bool = True, num_iterations: int = 10, debug: bool = False, run_with_cli: bool = False) </p> <p>Benchmarking all the Scandinavian language models.</p> <p>Initialise the benchmarker.</p> <p> Attributes </p> <ul> <li> <p>benchmark_config_default_params \u2014</p> <p>The default parameters for the benchmark configuration.</p> </li> <li> <p>benchmark_config \u2014</p> <p>The benchmark configuration.</p> </li> <li> <p>force \u2014</p> <p>Whether to force evaluations of models, even if they have been benchmarked already.</p> </li> <li> <p>dataset_factory \u2014</p> <p>The factory for creating datasets.</p> </li> <li> <p>results_path \u2014</p> <p>The path to the results file.</p> </li> <li> <p>benchmark_results :  list[BenchmarkResult] \u2014</p> <p>The benchmark results.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>progress_bar :  bool \u2014</p> <p>Whether progress bars should be shown. Defaults to True.</p> </li> <li> <p>save_results :  bool \u2014</p> <p>Whether to save the benchmark results to 'scandeval_benchmark_results.jsonl'. Defaults to True.</p> </li> <li> <p>task :  str | list[str] | None \u2014</p> <p>The tasks benchmark the model(s) on. Mutually exclusive with <code>dataset</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked.</p> </li> <li> <p>dataset :  list[str] | str | None \u2014</p> <p>The datasets to benchmark on. Mutually exclusive with <code>task</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked.</p> </li> <li> <p>language :  str | list[str] \u2014</p> <p>The language codes of the languages to include, both for models and datasets. Set this to 'all' if all languages should be considered. Defaults to \"all\".</p> </li> <li> <p>model_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for models. If specified then this overrides the <code>language</code> parameter for model languages. Defaults to None.</p> </li> <li> <p>dataset_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for datasets. If specified then this overrides the <code>language</code> parameter for dataset languages. Defaults to None.</p> </li> <li> <p>framework :  Framework | str | None \u2014</p> <p>The model framework to use. Only relevant if <code>model-id</code> refers to a local path. Otherwise, the framework will be set automatically. Defaults to None.</p> </li> <li> <p>device :  Device | None \u2014</p> <p>The device to use for benchmarking. Defaults to None.</p> </li> <li> <p>batch_size :  int \u2014</p> <p>The batch size to use. Defaults to 32.</p> </li> <li> <p>evaluate_train :  bool \u2014</p> <p>Whether to evaluate the training set as well. Defaults to False.</p> </li> <li> <p>raise_errors :  bool \u2014</p> <p>Whether to raise errors instead of skipping the model evaluation. Defaults to False.</p> </li> <li> <p>cache_dir :  str \u2014</p> <p>Directory to store cached models. Defaults to '.scandeval_cache'.</p> </li> <li> <p>token :  bool | str \u2014</p> <p>The authentication token for the Hugging Face Hub. If a boolean value is specified then the token will be fetched from the Hugging Face CLI, where the user has logged in through <code>huggingface-cli login</code>. If a string is specified then it will be used as the token. Defaults to True.</p> </li> <li> <p>openai_api_key :  str | None \u2014</p> <p>The OpenAI API key to use for authentication. If None, then this will be loaded from the environment variable <code>OPENAI_API_KEY</code>. Defaults to None.</p> </li> <li> <p>prefer_azure :  bool \u2014</p> <p>In the case where both OpenAI and Azure OpenAI models are available, whether to use the Azure OpenAI models. Defaults to False.</p> </li> <li> <p>azure_openai_api_key :  str | None \u2014</p> <p>The Azure OpenAI API key to use for authentication. If None, then this will be loaded from the environment variable <code>AZURE_OPENAI_API_KEY</code>. Defaults to None.</p> </li> <li> <p>azure_openai_endpoint :  str | None \u2014</p> <p>The Azure OpenAI endpoint to use for authentication. If None, then this will be loaded from the environment variable <code>AZURE_OPENAI_ENDPOINT</code>. Defaults to None.</p> </li> <li> <p>azure_openai_api_version :  str | None \u2014</p> <p>The Azure OpenAI API version to use for authentication. If None, then this will be loaded from the environment variable <code>AZURE_OPENAI_API_VERSION</code>. Defaults to None.</p> </li> <li> <p>force :  bool \u2014</p> <p>Whether to force evaluations of models, even if they have been benchmarked already. Defaults to False.</p> </li> <li> <p>verbose :  bool \u2014</p> <p>Whether to output additional output. Defaults to False.</p> </li> <li> <p>trust_remote_code :  bool \u2014</p> <p>Whether to trust remote code when loading models. Defaults to False.</p> </li> <li> <p>load_in_4bit :  bool | None \u2014</p> <p>Whether to load models in 4-bit precision. If None then this will be done if CUDA is available and the model is a decoder model. Defaults to None.</p> </li> <li> <p>use_flash_attention :  bool | None \u2014</p> <p>Whether to use Flash Attention. If None then it will be used if it is installed and the model is a decoder model. Defaults to None.</p> </li> <li> <p>clear_model_cache :  bool \u2014</p> <p>Whether to clear the model cache after benchmarking each model. Defaults to False.</p> </li> <li> <p>only_validation_split :  bool \u2014</p> <p>Whether to only evaluate the validation split of the datasets. Defaults to False.</p> </li> <li> <p>few_shot :  bool \u2014</p> <p>Whether to only evaluate the model using few-shot evaluation. Only relevant if the model is generative. Defaults to True.</p> </li> <li> <p>num_iterations :  int \u2014</p> <p>The number of times each model should be evaluated. This is only meant to be used for power users, and scores will not be allowed on the leaderboards if this is changed. Defaults to 10.</p> </li> <li> <p>debug :  bool \u2014</p> <p>Whether to output debug information. Defaults to False.</p> </li> <li> <p>run_with_cli :  bool \u2014</p> <p>Whether the benchmarker is being run from the command-line interface. Defaults to False.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014</p> <p>If both <code>task</code> and <code>dataset</code> are specified.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>benchmark \u2014 Benchmarks models on datasets.</p> </li> </ul> <p> source property Benchmarker.benchmark_results: list[BenchmarkResult] </p> <p>The benchmark results.</p> <p> source method Benchmarker.benchmark(model: list[str] | str | None = None, task: str | list[str] | None = None, dataset: list[str] | str | None = None, progress_bar: bool | None = None, save_results: bool | None = None, language: str | list[str] | None = None, model_language: str | list[str] | None = None, dataset_language: str | list[str] | None = None, framework: Framework | str | None = None, device: Device | None = None, batch_size: int | None = None, evaluate_train: bool | None = None, raise_errors: bool | None = None, cache_dir: str | None = None, token: bool | str | None = None, openai_api_key: str | None = None, azure_openai_api_key: str | None = None, azure_openai_endpoint: str | None = None, azure_openai_api_version: str | None = None, force: bool | None = None, verbose: bool | None = None, trust_remote_code: bool | None = None, load_in_4bit: bool | None = None, use_flash_attention: bool | None = None, clear_model_cache: bool | None = None, only_validation_split: bool | None = None, few_shot: bool | None = None, num_iterations: int | None = None) \u2192 list[BenchmarkResult] </p> <p>Benchmarks models on datasets.</p> <p> Parameters </p> <ul> <li> <p>model :  list[str] | str | None \u2014</p> <p>The full Hugging Face Hub path(s) to the pretrained transformer model. The specific model version to use can be added after the suffix '@': \"model@v1.0.0\". It can be a branch name, a tag name, or a commit id, and defaults to the latest version if not specified. If None then all relevant model IDs will be benchmarked. Defaults to None.</p> </li> <li> <p>task :  str | list[str] | None \u2014</p> <p>The tasks benchmark the model(s) on. Mutually exclusive with <code>dataset</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked. Defaults to None.</p> </li> <li> <p>dataset :  list[str] | str | None \u2014</p> <p>The datasets to benchmark on. Mutually exclusive with <code>task</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked. Defaults to None.</p> </li> <li> <p>progress_bar :  bool | None \u2014</p> <p>Whether progress bars should be shown. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>save_results :  bool | None \u2014</p> <p>Whether to save the benchmark results to 'scandeval_benchmark_results.jsonl'. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include, both for models and datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this to 'all' if all languages (also non-Scandinavian) should be considered. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>model_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for models. If specified then this overrides the <code>language</code> parameter for model languages. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>dataset_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for datasets. If specified then this overrides the <code>language</code> parameter for dataset languages. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>framework :  Framework | str | None \u2014</p> <p>The model framework to use. Only relevant if <code>model-id</code> refers to a local path. Otherwise, the framework will be set automatically. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>device :  Device | None \u2014</p> <p>The device to use for benchmarking. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>batch_size :  int | None \u2014</p> <p>The batch size to use. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>evaluate_train :  bool | None \u2014</p> <p>Whether to evaluate the training set as well. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>raise_errors :  bool | None \u2014</p> <p>Whether to raise errors instead of skipping the model evaluation.</p> </li> <li> <p>cache_dir :  str | None \u2014</p> <p>Directory to store cached models. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>token :  bool | str | None \u2014</p> <p>The authentication token for the Hugging Face Hub. If a boolean value is specified then the token will be fetched from the Hugging Face CLI, where the user has logged in through <code>huggingface-cli login</code>. If a string is specified then it will be used as the token. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>openai_api_key :  str | None \u2014</p> <p>The OpenAI API key to use for authentication. If None, then this will be loaded from the environment variable <code>OPENAI_API_KEY</code>. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>azure_openai_api_key :  str | None \u2014</p> <p>The Azure OpenAI API key to use for authentication. If None, then this will be loaded from the environment variable <code>AZURE_OPENAI_API_KEY</code>. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>azure_openai_endpoint :  str | None \u2014</p> <p>The Azure OpenAI endpoint to use for authentication. If None, then this will be loaded from the environment variable <code>AZURE_OPENAI_ENDPOINT</code>. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>azure_openai_api_version :  str | None \u2014</p> <p>The api version for the Azure OpenAI API, e.g. \"2023-12-01-preview\". If None then the environment varaible <code>AZURE_OPENAI_API_VERSION</code> will be used. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>force :  bool | None \u2014</p> <p>Whether to force evaluations of models, even if they have been benchmarked already. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>verbose :  bool | None \u2014</p> <p>Whether to output additional output. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>trust_remote_code :  bool | None \u2014</p> <p>Whether to trust remote code when loading models. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>load_in_4bit :  bool | None \u2014</p> <p>Whether to load models in 4-bit precision. If None then this will be done if CUDA is available and the model is a decoder model. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>use_flash_attention :  bool | None \u2014</p> <p>Whether to use Flash Attention. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>clear_model_cache :  bool | None \u2014</p> <p>Whether to clear the model cache after benchmarking each model. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>only_validation_split :  bool | None \u2014</p> <p>Whether to only evaluate the validation split of the datasets. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>few_shot :  bool | None \u2014</p> <p>Whether to only evaluate the model using few-shot evaluation. Only relevant if the model is generative. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>num_iterations :  int | None \u2014</p> <p>The number of times each model should be evaluated. This is only meant to be used for power users, and scores will not be allowed on the leaderboards if this is changed. Defaults to the value specified when initialising the benchmarker.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[BenchmarkResult] \u2014 A list of benchmark results.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014</p> <p>If both <code>task</code> and <code>dataset</code> are specified.</p> </li> <li> <p>e</p> </li> </ul> <p> source model_has_been_benchmarked(model_id: str, dataset: str, few_shot: bool, validation_split: bool, benchmark_results: list[BenchmarkResult]) \u2192 bool </p> <p>Checks whether a model has already been benchmarked on a dataset.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>dataset :  str \u2014</p> <p>The dataset.</p> </li> <li> <p>few_shot :  bool \u2014</p> <p>Whether the model was evaluated using few-shot evaluation.</p> </li> <li> <p>validation_split :  bool \u2014</p> <p>Whether the model was evaluated on the validation split.</p> </li> <li> <p>benchmark_results :  list[BenchmarkResult] \u2014</p> <p>The benchmark results.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool \u2014 Whether the model has already been evaluated on the dataset.</p> </li> </ul> <p> source adjust_logging_level(verbose: bool) \u2192 None </p> <p>Adjust the logging level based on verbosity.</p> <p> Parameters </p> <ul> <li> <p>verbose :  bool \u2014</p> <p>Whether to output additional output.</p> </li> </ul> <p> source clear_model_cache_fn(cache_dir: str) \u2192 None </p> <p>Clear the model cache.</p> <p>Note that this will not remove the stored completions.</p> <p> Parameters </p> <ul> <li> <p>cache_dir :  str \u2014</p> <p>The path to the cache directory.</p> </li> </ul> <p> source prepare_dataset_configs(dataset_names: list[str]) \u2192 list[DatasetConfig] </p> <p>Prepare the dataset configuration(s) to be benchmarked.</p> <p> Parameters </p> <ul> <li> <p>dataset_names :  list[str] \u2014</p> <p>The dataset names to benchmark.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[DatasetConfig] \u2014 The prepared list of model IDs.</p> </li> </ul>"},{"location":"src/scandeval/benchmarker/","title":"scandeval.benchmarker","text":"scandeval.benchmarker<p> docs module scandeval.benchmarker </p> <pre><code>\"\"\"Class that benchmarks Scandinavian language models.\"\"\"\n\nimport importlib.metadata\nimport json\nimport logging\nimport re\nimport sys\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom shutil import rmtree\nfrom time import sleep\nfrom typing import TYPE_CHECKING\n\nfrom pydantic import BaseModel, ConfigDict\n\nfrom .benchmark_config_factory import build_benchmark_config\nfrom .config import BenchmarkConfig\nfrom .dataset_configs import get_all_dataset_configs\nfrom .dataset_factory import DatasetFactory\nfrom .enums import Device, Framework\nfrom .exceptions import InvalidBenchmark, InvalidModel\nfrom .types import ScoreDict\nfrom .utils import get_huggingface_model_lists\n\nif TYPE_CHECKING:\n    from .config import DatasetConfig, Language\n    from .protocols import GenerativeModel, Tokenizer\n\n\nlogger = logging.getLogger(__package__)\n\n\nclass BenchmarkConfigParams(BaseModel):docs\n    \"\"\"The parameters for the benchmark configuration.\"\"\"\n\n    model_config = ConfigDict(protected_namespaces=())\n\n    progress_bar: bool\n    save_results: bool\n    task: str | list[str] | None\n    dataset: str | list[str] | None\n    language: str | list[str]\n    model_language: str | list[str] | None\n    dataset_language: str | list[str] | None\n    framework: Framework | str | None\n    device: Device | None\n    batch_size: int\n    evaluate_train: bool\n    raise_errors: bool\n    cache_dir: str\n    token: bool | str\n    openai_api_key: str | None\n    prefer_azure: bool\n    azure_openai_api_key: str | None\n    azure_openai_endpoint: str | None\n    azure_openai_api_version: str | None\n    force: bool\n    verbose: bool\n    trust_remote_code: bool\n    load_in_4bit: bool | None\n    use_flash_attention: bool | None\n    clear_model_cache: bool\n    only_validation_split: bool\n    few_shot: bool\n    num_iterations: int\n    debug: bool\n    run_with_cli: bool\n\n\nclass BenchmarkResult(BaseModel):docs\n    \"\"\"A benchmark result.\"\"\"\n\n    dataset: str\n    task: str\n    dataset_languages: list[str]\n    model: str\n    results: ScoreDict\n    num_model_parameters: int\n    max_sequence_length: int\n    vocabulary_size: int\n    generative: bool\n    few_shot: bool\n    validation_split: bool\n    scandeval_version: str = importlib.metadata.version(__package__)\n\n    @classmethod\n    def from_dict(cls, config: dict) -&gt; \"BenchmarkResult\":docs\n        \"\"\"Create a benchmark result from a dictionary.\n\n        Args:\n            config:\n                The configuration dictionary.\n\n        Returns:\n            The benchmark result.\n        \"\"\"\n        # To be backwards compatible, we accept old results which changed the model\n        # name with parameters rather than adding them as explicit parameters\n        val_matches = re.search(r\"\\(.*val.*\\)$\", config[\"model\"])\n        few_shot_matches = re.search(r\"\\(.*few-shot.*\\)$\", config[\"model\"])\n        config[\"model\"] = re.sub(\n            r\"\\(.*(few-shot|val).*\\)$\", \"\", config[\"model\"]\n        ).strip()\n\n        # The default value for `few_shot` is True. It won't do anything if the model\n        # is not generative, so this is fine\n        if \"generative\" not in config:\n            config[\"generative\"] = few_shot_matches is not None\n        if \"few_shot\" not in config:\n            config[\"few_shot\"] = True\n\n        if \"validation_split\" not in config:\n            config[\"validation_split\"] = val_matches is not None\n\n        return cls(**config)\n\n    def append_to_results(self, results_path: Path) -&gt; None:docs\n        \"\"\"Append the benchmark result to the results file.\n\n        Args:\n            results_path:\n                The path to the results file.\n        \"\"\"\n        json_str = json.dumps(self.model_dump())\n        with results_path.open(\"a\") as f:\n            f.write(\"\\n\" + json_str)\n\n\nclass Benchmarker:docs\n    \"\"\"Benchmarking all the Scandinavian language models.\n\n    Attributes:\n        benchmark_config_default_params:\n            The default parameters for the benchmark configuration.\n        benchmark_config:\n            The benchmark configuration.\n        force:\n            Whether to force evaluations of models, even if they have been benchmarked\n            already.\n        dataset_factory:\n            The factory for creating datasets.\n        results_path:\n            The path to the results file.\n        benchmark_results:\n            The benchmark results.\n    \"\"\"\n\n    def __init__(\n        self,\n        progress_bar: bool = True,\n        save_results: bool = True,\n        task: str | list[str] | None = None,\n        dataset: list[str] | str | None = None,\n        language: str | list[str] = \"all\",\n        model_language: str | list[str] | None = None,\n        dataset_language: str | list[str] | None = None,\n        framework: Framework | str | None = None,\n        device: Device | None = None,\n        batch_size: int = 32,\n        evaluate_train: bool = False,\n        raise_errors: bool = False,\n        cache_dir: str = \".scandeval_cache\",\n        token: bool | str = True,\n        openai_api_key: str | None = None,\n        prefer_azure: bool = False,\n        azure_openai_api_key: str | None = None,\n        azure_openai_endpoint: str | None = None,\n        azure_openai_api_version: str | None = None,\n        force: bool = False,\n        verbose: bool = False,\n        trust_remote_code: bool = False,\n        load_in_4bit: bool | None = None,\n        use_flash_attention: bool | None = None,\n        clear_model_cache: bool = False,\n        only_validation_split: bool = False,\n        few_shot: bool = True,\n        num_iterations: int = 10,\n        debug: bool = False,\n        run_with_cli: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialise the benchmarker.\n\n        Args:\n            progress_bar:\n                Whether progress bars should be shown. Defaults to True.\n            save_results:\n                Whether to save the benchmark results to\n                'scandeval_benchmark_results.jsonl'. Defaults to True.\n            task:\n                The tasks benchmark the model(s) on. Mutually exclusive with `dataset`.\n                If both `task` and `dataset` are None then all datasets will be\n                benchmarked.\n            dataset:\n                The datasets to benchmark on. Mutually exclusive with `task`. If both\n                `task` and `dataset` are None then all datasets will be benchmarked.\n            language:\n                The language codes of the languages to include, both for models and\n                datasets. Set this to 'all' if all languages should be considered.\n                Defaults to \"all\".\n            model_language:\n                The language codes of the languages to include for models. If specified\n                then this overrides the `language` parameter for model languages.\n                Defaults to None.\n            dataset_language:\n                The language codes of the languages to include for datasets. If\n                specified then this overrides the `language` parameter for dataset\n                languages. Defaults to None.\n            framework:\n                The model framework to use. Only relevant if `model-id` refers to a\n                local path. Otherwise, the framework will be set automatically.\n                Defaults to None.\n            device:\n                The device to use for benchmarking. Defaults to None.\n            batch_size:\n                The batch size to use. Defaults to 32.\n            evaluate_train:\n                Whether to evaluate the training set as well. Defaults to False.\n            raise_errors:\n                Whether to raise errors instead of skipping the model evaluation.\n                Defaults to False.\n            cache_dir:\n                Directory to store cached models. Defaults to '.scandeval_cache'.\n            token:\n                The authentication token for the Hugging Face Hub. If a boolean value\n                is specified then the token will be fetched from the Hugging Face CLI,\n                where the user has logged in through `huggingface-cli login`. If a\n                string is specified then it will be used as the token. Defaults to\n                True.\n            openai_api_key:\n                The OpenAI API key to use for authentication. If None, then this will\n                be loaded from the environment variable `OPENAI_API_KEY`. Defaults to\n                None.\n            prefer_azure:\n                In the case where both OpenAI and Azure OpenAI models are available,\n                whether to use the Azure OpenAI models. Defaults to False.\n            azure_openai_api_key:\n                The Azure OpenAI API key to use for authentication. If None, then this\n                will be loaded from the environment variable `AZURE_OPENAI_API_KEY`.\n                Defaults to None.\n            azure_openai_endpoint:\n                The Azure OpenAI endpoint to use for authentication. If None, then this\n                will be loaded from the environment variable `AZURE_OPENAI_ENDPOINT`.\n                Defaults to None.\n            azure_openai_api_version:\n                The Azure OpenAI API version to use for authentication. If None, then this\n                will be loaded from the environment variable `AZURE_OPENAI_API_VERSION`.\n                Defaults to None.\n            force:\n                Whether to force evaluations of models, even if they have been\n                benchmarked already. Defaults to False.\n            verbose:\n                Whether to output additional output. Defaults to False.\n            trust_remote_code:\n                Whether to trust remote code when loading models. Defaults to False.\n            load_in_4bit:\n                Whether to load models in 4-bit precision. If None then this will be\n                done if CUDA is available and the model is a decoder model. Defaults to\n                None.\n            use_flash_attention:\n                Whether to use Flash Attention. If None then it will be used if it is\n                installed and the model is a decoder model. Defaults to None.\n            clear_model_cache:\n                Whether to clear the model cache after benchmarking each model.\n                Defaults to False.\n            only_validation_split:\n                Whether to only evaluate the validation split of the datasets. Defaults\n                to False.\n            few_shot:\n                Whether to only evaluate the model using few-shot evaluation. Only\n                relevant if the model is generative. Defaults to True.\n            num_iterations:\n                The number of times each model should be evaluated. This is only meant\n                to be used for power users, and scores will not be allowed on the\n                leaderboards if this is changed. Defaults to 10.\n            debug:\n                Whether to output debug information. Defaults to False.\n            run_with_cli:\n                Whether the benchmarker is being run from the command-line interface.\n                Defaults to False.\n\n        Raises:\n            ValueError:\n                If both `task` and `dataset` are specified.\n        \"\"\"\n        if task is not None and dataset is not None:\n            raise ValueError(\"Only one of `task` and `dataset` can be specified.\")\n\n        self.benchmark_config_default_params = BenchmarkConfigParams(\n            progress_bar=progress_bar,\n            save_results=save_results,\n            task=task,\n            dataset=dataset,\n            language=language,\n            model_language=model_language,\n            dataset_language=dataset_language,\n            framework=framework,\n            device=device,\n            batch_size=batch_size,\n            evaluate_train=evaluate_train,\n            raise_errors=raise_errors,\n            cache_dir=cache_dir,\n            token=token,\n            openai_api_key=openai_api_key,\n            prefer_azure=prefer_azure,\n            azure_openai_api_key=azure_openai_api_key,\n            azure_openai_endpoint=azure_openai_endpoint,\n            azure_openai_api_version=azure_openai_api_version,\n            force=force,\n            verbose=verbose,\n            trust_remote_code=trust_remote_code,\n            load_in_4bit=load_in_4bit,\n            use_flash_attention=use_flash_attention,\n            clear_model_cache=clear_model_cache,\n            only_validation_split=only_validation_split,\n            few_shot=few_shot,\n            num_iterations=num_iterations,\n            debug=debug,\n            run_with_cli=run_with_cli,\n        )\n\n        self.benchmark_config = build_benchmark_config(\n            first_time=True, **self.benchmark_config_default_params.model_dump()\n        )\n\n        # Initialise variable storing model lists, so we only have to fetch it once\n        self._model_lists: dict[str, list[str]] | None = None\n\n        # Set up the results path\n        self.results_path = Path.cwd() / \"scandeval_benchmark_results.jsonl\"\n\n        adjust_logging_level(verbose=self.benchmark_config.verbose)\n\n    @property\n    def benchmark_results(self) -&gt; list[BenchmarkResult]:docs\n        \"\"\"The benchmark results.\"\"\"\n        if self.results_path.exists():\n            with self.results_path.open() as f:\n                return [\n                    BenchmarkResult.from_dict(json.loads(line))\n                    for line in f\n                    if line.strip()\n                ]\n        else:\n            return list()\n\n    def benchmark(docs\n        self,\n        model: list[str] | str | None = None,\n        task: str | list[str] | None = None,\n        dataset: list[str] | str | None = None,\n        progress_bar: bool | None = None,\n        save_results: bool | None = None,\n        language: str | list[str] | None = None,\n        model_language: str | list[str] | None = None,\n        dataset_language: str | list[str] | None = None,\n        framework: Framework | str | None = None,\n        device: Device | None = None,\n        batch_size: int | None = None,\n        evaluate_train: bool | None = None,\n        raise_errors: bool | None = None,\n        cache_dir: str | None = None,\n        token: bool | str | None = None,\n        openai_api_key: str | None = None,\n        azure_openai_api_key: str | None = None,\n        azure_openai_endpoint: str | None = None,\n        azure_openai_api_version: str | None = None,\n        force: bool | None = None,\n        verbose: bool | None = None,\n        trust_remote_code: bool | None = None,\n        load_in_4bit: bool | None = None,\n        use_flash_attention: bool | None = None,\n        clear_model_cache: bool | None = None,\n        only_validation_split: bool | None = None,\n        few_shot: bool | None = None,\n        num_iterations: int | None = None,\n    ) -&gt; list[BenchmarkResult]:\n        \"\"\"Benchmarks models on datasets.\n\n        Args:\n            model:\n                The full Hugging Face Hub path(s) to the pretrained transformer model.\n                The specific model version to use can be added after the suffix '@':\n                \"model@v1.0.0\". It can be a branch name, a tag name, or a commit id,\n                and defaults to the latest version if not specified. If None then all\n                relevant model IDs will be benchmarked. Defaults to None.\n            task:\n                The tasks benchmark the model(s) on. Mutually exclusive with `dataset`.\n                If both `task` and `dataset` are None then all datasets will be\n                benchmarked. Defaults to None.\n            dataset:\n                The datasets to benchmark on. Mutually exclusive with `task`. If both\n                `task` and `dataset` are None then all datasets will be benchmarked.\n                Defaults to None.\n            progress_bar:\n                Whether progress bars should be shown. Defaults to the value specified\n                when initialising the benchmarker.\n            save_results:\n                Whether to save the benchmark results to\n                'scandeval_benchmark_results.jsonl'. Defaults to the value specified\n                when initialising the benchmarker.\n            language:\n                The language codes of the languages to include, both for models and\n                datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this\n                to 'all' if all languages (also non-Scandinavian) should be considered.\n                Defaults to the value specified when initialising the benchmarker.\n            model_language:\n                The language codes of the languages to include for models. If specified\n                then this overrides the `language` parameter for model languages.\n                Defaults to the value specified when initialising the benchmarker.\n            dataset_language:\n                The language codes of the languages to include for datasets. If\n                specified then this overrides the `language` parameter for dataset\n                languages. Defaults to the value specified when initialising the\n                benchmarker.\n            framework:\n                The model framework to use. Only relevant if `model-id` refers to a\n                local path. Otherwise, the framework will be set automatically.\n                Defaults to the value specified when initialising the benchmarker.\n            device:\n                The device to use for benchmarking. Defaults to the value specified when\n                initialising the benchmarker.\n            batch_size:\n                The batch size to use. Defaults to the value specified when initialising\n                the benchmarker.\n            evaluate_train:\n                Whether to evaluate the training set as well. Defaults to the value\n                specified when initialising the benchmarker.\n            raise_errors:\n                Whether to raise errors instead of skipping the model evaluation.\n            cache_dir:\n                Directory to store cached models. Defaults to the value specified when\n                initialising the benchmarker.\n            token:\n                The authentication token for the Hugging Face Hub. If a boolean value is\n                specified then the token will be fetched from the Hugging Face CLI, where\n                the user has logged in through `huggingface-cli login`. If a string is\n                specified then it will be used as the token. Defaults to the value\n                specified when initialising the benchmarker.\n            openai_api_key:\n                The OpenAI API key to use for authentication. If None, then this will be\n                loaded from the environment variable `OPENAI_API_KEY`. Defaults to the\n                value specified when initialising the benchmarker.\n            azure_openai_api_key:\n                The Azure OpenAI API key to use for authentication. If None, then this\n                will be loaded from the environment variable `AZURE_OPENAI_API_KEY`.\n                Defaults to the value specified when initialising the benchmarker.\n            azure_openai_endpoint:\n                The Azure OpenAI endpoint to use for authentication. If None, then this\n                will be loaded from the environment variable `AZURE_OPENAI_ENDPOINT`.\n                Defaults to the value specified when initialising the benchmarker.\n            azure_openai_api_version:\n                The api version for the Azure OpenAI API, e.g. \"2023-12-01-preview\". If\n                None then the environment varaible `AZURE_OPENAI_API_VERSION` will be used.\n                Defaults to the value specified when initialising the benchmarker.\n            force:\n                Whether to force evaluations of models, even if they have been\n                benchmarked already. Defaults to the value specified when initialising\n                the benchmarker.\n            verbose:\n                Whether to output additional output. Defaults to the value specified when\n                initialising the benchmarker.\n            trust_remote_code:\n                Whether to trust remote code when loading models. Defaults to the value\n                specified when initialising the benchmarker.\n            load_in_4bit:\n                Whether to load models in 4-bit precision. If None then this will be done\n                if CUDA is available and the model is a decoder model. Defaults to the\n                value specified when initialising the benchmarker.\n            use_flash_attention:\n                Whether to use Flash Attention. Defaults to the value specified when\n                initialising the benchmarker.\n            clear_model_cache:\n                Whether to clear the model cache after benchmarking each model. Defaults\n                to the value specified when initialising the benchmarker.\n            only_validation_split:\n                Whether to only evaluate the validation split of the datasets. Defaults\n                to the value specified when initialising the benchmarker.\n            few_shot:\n                Whether to only evaluate the model using few-shot evaluation. Only\n                relevant if the model is generative. Defaults to the value specified\n                when initialising the benchmarker.\n            num_iterations:\n                The number of times each model should be evaluated. This is only meant\n                to be used for power users, and scores will not be allowed on the\n                leaderboards if this is changed. Defaults to the value specified when\n                initialising the benchmarker.\n\n        Returns:\n            A list of benchmark results.\n\n        Raises:\n            ValueError:\n                If both `task` and `dataset` are specified.\n        \"\"\"\n        if task is not None and dataset is not None:\n            raise ValueError(\"Only one of `task` and `dataset` can be specified.\")\n\n        benchmark_config_params = deepcopy(self.benchmark_config_default_params)\n        if task is not None:\n            benchmark_config_params.task = task\n            benchmark_config_params.dataset = None\n        if dataset is not None:\n            benchmark_config_params.dataset = dataset\n            benchmark_config_params.task = None\n        if progress_bar is not None:\n            benchmark_config_params.progress_bar = progress_bar\n        if save_results is not None:\n            benchmark_config_params.save_results = save_results\n        if language is not None:\n            benchmark_config_params.language = language\n        if model_language is not None:\n            benchmark_config_params.model_language = model_language\n        if dataset_language is not None:\n            benchmark_config_params.dataset_language = dataset_language\n        if framework is not None:\n            benchmark_config_params.framework = framework\n        if device is not None:\n            benchmark_config_params.device = device\n        if batch_size is not None:\n            benchmark_config_params.batch_size = batch_size\n        if evaluate_train is not None:\n            benchmark_config_params.evaluate_train = evaluate_train\n        if raise_errors is not None:\n            benchmark_config_params.raise_errors = raise_errors\n        if cache_dir is not None:\n            benchmark_config_params.cache_dir = cache_dir\n        if token is not None:\n            benchmark_config_params.token = token\n        if openai_api_key is not None:\n            benchmark_config_params.openai_api_key = openai_api_key\n        if azure_openai_api_key is not None:\n            benchmark_config_params.azure_openai_api_key = azure_openai_api_key\n        if azure_openai_endpoint is not None:\n            benchmark_config_params.azure_openai_endpoint = azure_openai_endpoint\n        if azure_openai_api_version is not None:\n            benchmark_config_params.azure_openai_api_version = azure_openai_api_version\n        if force is not None:\n            benchmark_config_params.force = force\n        if verbose is not None:\n            benchmark_config_params.verbose = verbose\n        if trust_remote_code is not None:\n            benchmark_config_params.trust_remote_code = trust_remote_code\n        if load_in_4bit is not None:\n            benchmark_config_params.load_in_4bit = load_in_4bit\n        if use_flash_attention is not None:\n            benchmark_config_params.use_flash_attention = use_flash_attention\n        if clear_model_cache is not None:\n            benchmark_config_params.clear_model_cache = clear_model_cache\n        if only_validation_split is not None:\n            benchmark_config_params.only_validation_split = only_validation_split\n        if few_shot is not None:\n            benchmark_config_params.few_shot = few_shot\n        if num_iterations is not None:\n            benchmark_config_params.num_iterations = num_iterations\n\n        benchmark_config = build_benchmark_config(\n            **benchmark_config_params.model_dump()\n        )\n        adjust_logging_level(verbose=benchmark_config.verbose)\n\n        if benchmark_config.clear_model_cache:\n            clear_model_cache_fn(cache_dir=benchmark_config.cache_dir)\n\n        model_ids = self._prepare_model_ids(\n            model=model,\n            model_languages=benchmark_config.model_languages,\n            token=benchmark_config.token,\n        )\n        dataset_configs = prepare_dataset_configs(\n            dataset_names=benchmark_config.datasets\n        )\n\n        current_benchmark_results: list[BenchmarkResult] = list()\n        for m_id in model_ids:\n            m_id = m_id.rstrip(\" /\")\n            loaded_model = None\n            loaded_tokenizer = None\n\n            for dataset_config in dataset_configs:\n                # Skip if we have already benchmarked this model on this dataset and\n                # we are not forcing the benchmark\n                if not benchmark_config.force and model_has_been_benchmarked(\n                    model_id=m_id,\n                    dataset=dataset_config.name,\n                    few_shot=benchmark_config.few_shot,\n                    validation_split=benchmark_config.only_validation_split,\n                    benchmark_results=self.benchmark_results,\n                ):\n                    logger.debug(\n                        f\"Skipping benchmarking {m_id} on {dataset_config.pretty_name},\"\n                        \" as it has already been benchmarked.\"\n                    )\n                    continue\n\n                # Benchmark a single model on a single dataset\n                try:\n                    benchmark_output = self._benchmark_single(\n                        dataset_config=dataset_config,\n                        model_id=m_id,\n                        raise_errors=benchmark_config.raise_errors,\n                        model=loaded_model,\n                        tokenizer=loaded_tokenizer,\n                        benchmark_config=benchmark_config,\n                    )\n                except InvalidModel as e:\n                    if benchmark_config.raise_errors:\n                        raise e\n                    logger.info(e)\n                    break\n\n                # If the benchmark was unsuccessful then skip\n                if isinstance(benchmark_output, dict) and \"error\" in benchmark_output:\n                    error_msg = benchmark_output[\"error\"]\n                    logger.info(\n                        f\"{m_id} could not be benchmarked on \"\n                        f\"{dataset_config.pretty_name}. Skipping. The error message \"\n                        f\"raised was {error_msg!r}.\"\n                    )\n                    continue\n\n                assert isinstance(benchmark_output, tuple)\n                record, loaded_model, loaded_tokenizer = benchmark_output\n\n                # Save the benchmark results\n                assert isinstance(record, BenchmarkResult)\n                current_benchmark_results.append(record)\n                if benchmark_config.save_results:\n                    record.append_to_results(results_path=self.results_path)\n\n            if benchmark_config.clear_model_cache:\n                clear_model_cache_fn(cache_dir=benchmark_config.cache_dir)\n\n        return current_benchmark_results\n\n    def _prepare_model_ids(\n        self,\n        model: list[str] | str | None,\n        model_languages: list[\"Language\"],\n        token: bool | str | None,\n    ) -&gt; list[str]:\n        \"\"\"Prepare the model ID(s) to be benchmarked.\n\n        Args:\n            model:\n                The model ID(s) of the models to benchmark. If None then all model IDs\n                will be retrieved.\n            model_languages:\n                The languages of the models to fetch.\n            token:\n                The authentication token for the Hugging Face Hub.\n\n        Returns:\n            The prepared list of model IDs.\n        \"\"\"\n        model_ids: list[str]\n\n        # If `model_id` is not specified, then fetch all the relevant model IDs\n        if model is None:\n            model_ids = self._get_model_ids(languages=model_languages, token=token)\n\n        # Otherwise, if `model_id` is a string, ensure that it is a list\n        elif isinstance(model, str):\n            model_ids = [model]\n\n        # Otherwise `model_id` is already a list, so we do nothing\n        else:\n            model_ids = model\n\n        # Reorder the `model_ids` list to include the ones present in the benchmark\n        # results first\n        benchmarked_model_ids = [\n            re.sub(r\"\\(.+\\)\", \"\", record.model).strip()\n            for record in self.benchmark_results\n        ]\n        model_ids_sorted = [m_id for m_id in model_ids if m_id in benchmarked_model_ids]\n        model_ids_sorted += [\n            m_id for m_id in model_ids if m_id not in benchmarked_model_ids\n        ]\n\n        return model_ids_sorted\n\n    def _benchmark_single(\n        self,\n        dataset_config: \"DatasetConfig\",\n        model_id: str,\n        raise_errors: bool,\n        model: \"GenerativeModel | None\",\n        tokenizer: \"Tokenizer | None\",\n        benchmark_config: BenchmarkConfig,\n    ) -&gt; (\n        tuple[BenchmarkResult, \"GenerativeModel | None\", \"Tokenizer | None\"]\n        | dict[str, str]\n    ):\n        \"\"\"Benchmark a single model on a single dataset.\n\n        Args:\n            dataset_config:\n                The dataset configuration to use.\n            model_id:\n                The model ID to use.\n            raise_errors:\n                Whether to raise errors instead of skipping the model evaluation.\n            model:\n                The pre-loaded model, if available.\n            tokenizer:\n                The pre-loaded tokenizer, if available.\n            benchmark_config:\n                The benchmark configuration.\n\n        Returns:\n            The benchmark result, or a dictionary containing an error message.\n        \"\"\"\n        logger.info(f\"Benchmarking {model_id} on {dataset_config.pretty_name}\")\n        if dataset_config.unofficial:\n            logger.info(\n                f\"Note that the {dataset_config.name!r} dataset is unofficial, \"\n                \"meaning that the resulting evaluation will not be included in the \"\n                \"official leaderboard.\"\n            )\n        if benchmark_config.debug:\n            logger.info(\n                \"Running in debug mode. This will output additional information, as \"\n                \"well as store the model outputs in the current directory after each \"\n                \"batch. For this reason, evaluation will be slower.\"\n            )\n\n        while True:\n            try:\n                dataset_factory = DatasetFactory(benchmark_config=benchmark_config)\n                dataset = dataset_factory.build_dataset(dataset_config)\n                results, metadata_dict, model, tokenizer = dataset(\n                    model_id=model_id, model=model, tokenizer=tokenizer\n                )\n                record = BenchmarkResult(\n                    dataset=dataset_config.name,\n                    task=dataset_config.task.name,\n                    dataset_languages=[\n                        language.code for language in dataset_config.languages\n                    ],\n                    model=model_id,\n                    results=results,\n                    **metadata_dict,\n                )\n                logger.debug(f\"Results:\\n{results}\")\n                return record, model, tokenizer\n\n            except InvalidBenchmark as e:\n                # If the model ID is not valid then raise an error, if specified\n                model_err_msg = \"does not exist on the Hugging Face Hub\"\n                if raise_errors and model_err_msg in str(e):\n                    raise e\n\n                # Otherwise, if the error is due to Hugging Face Hub being down, then\n                # wait a bit and try again\n                if \"The Hugging Face Hub seems to be down.\" in str(e):\n                    wait_time = 30\n                    logger.debug(\n                        \"The Hugging Face Hub seems to be down. Retrying in \"\n                        f\"{wait_time} seconds.\"\n                    )\n                    sleep(wait_time)\n                    continue\n\n                # Otherwise, if the error is due to the MPS fallback not being enabled,\n                # then raise an error asking the user to enable it\n                elif \"PYTORCH_ENABLE_MPS_FALLBACK\" in str(e):\n                    raise RuntimeError(\n                        \"The benchmark failed because the environment variable \"\n                        \"`PYTORCH_ENABLE_MPS_FALLBACK` is not set. Please set this \"\n                        \"environment variable to `1` and try again.\"\n                    )\n\n                # Otherwise, raise the error or return the error message\n                else:\n                    if raise_errors:\n                        raise e\n                    return dict(error=str(e))\n\n    def __call__(self, *args, **kwargs) -&gt; list[BenchmarkResult]:\n        \"\"\"Call the benchmarker. See `Benchmarker.benchmark`.\"\"\"\n        return self.benchmark(*args, **kwargs)\n\n    def _get_model_ids(\n        self, languages: list[\"Language\"], token: bool | str | None\n    ) -&gt; list[str]:\n        \"\"\"Get list of model IDs from the Hugging Face Hub.\n\n        Args:\n            languages:\n                The languages of the models to fetch.\n            token:\n                The authentication token for the Hugging Face Hub.\n\n        Returns:\n            List of model IDs.\n        \"\"\"\n        # Specify boolean variables determining whether the input variables are new\n        new_languages = self._model_lists is not None and any(\n            lang.code not in self._model_lists for lang in languages\n        )\n\n        # If the model lists have not been fetched already, then do it\n        if self._model_lists is None or new_languages:\n            self._model_lists = get_huggingface_model_lists(\n                languages=languages, token=token\n            )\n\n        # Extract all the model IDs from the model lists, for the chosen languages\n        model_ids: list[str] = list()\n        for language in languages:\n            model_ids.extend(self._model_lists[language.code])\n\n        # Add the multilingual models\n        model_ids.extend(self._model_lists[\"multilingual\"])\n\n        # Add the fresh models\n        model_ids.extend(self._model_lists[\"fresh\"])\n\n        # Remove duplicate model IDs\n        model_ids = list(set(model_ids))\n\n        return model_ids\n\n\ndef model_has_been_benchmarked(docs\n    model_id: str,\n    dataset: str,\n    few_shot: bool,\n    validation_split: bool,\n    benchmark_results: list[BenchmarkResult],\n) -&gt; bool:\n    \"\"\"Checks whether a model has already been benchmarked on a dataset.\n\n    Args:\n        model_id:\n            The model ID.\n        dataset:\n            The dataset.\n        few_shot:\n            Whether the model was evaluated using few-shot evaluation.\n        validation_split:\n            Whether the model was evaluated on the validation split.\n        benchmark_results:\n            The benchmark results.\n\n    Returns:\n        Whether the model has already been evaluated on the dataset.\n    \"\"\"\n    for record in benchmark_results:\n        same_evaluation = record.model == model_id and record.dataset == dataset\n        same_validation_split_setting = record.validation_split == validation_split\n        same_few_shot_setting = record.few_shot == few_shot or not record.generative\n        if same_evaluation and same_validation_split_setting and same_few_shot_setting:\n            return True\n    return False\n\n\ndef adjust_logging_level(verbose: bool) -&gt; None:docs\n    \"\"\"Adjust the logging level based on verbosity.\n\n    Args:\n        verbose:\n            Whether to output additional output.\n    \"\"\"\n    if hasattr(sys, \"_called_from_test\"):\n        logging_level = logging.CRITICAL\n    elif verbose:\n        logging_level = logging.DEBUG\n    else:\n        logging_level = logging.INFO\n    logger.setLevel(logging_level)\n\n\ndef clear_model_cache_fn(cache_dir: str) -&gt; None:docs\n    \"\"\"Clear the model cache.\n\n    Note that this will not remove the stored completions.\n\n    Args:\n        cache_dir:\n            The path to the cache directory.\n    \"\"\"\n    model_cache_path = Path(cache_dir) / \"model_cache\"\n    model_cache_path.mkdir(parents=True, exist_ok=True)\n    for model_dir in model_cache_path.iterdir():\n        if model_dir.is_dir():\n            for sub_model_dir in model_dir.iterdir():\n                if sub_model_dir.is_dir():\n                    rmtree(sub_model_dir)\n\ndocs\ndef prepare_dataset_configs(dataset_names: list[str]) -&gt; list[\"DatasetConfig\"]:\n    \"\"\"Prepare the dataset configuration(s) to be benchmarked.\n\n    Args:\n        dataset_names:\n            The dataset names to benchmark.\n\n    Returns:\n        The prepared list of model IDs.\n    \"\"\"\n    return [\n        cfg for cfg in get_all_dataset_configs().values() if cfg.name in dataset_names\n    ]\n</code></pre>"},{"location":"api/scandeval/benchmark_config_factory/","title":"scandeval.benchmark_config_factory","text":"scandeval.benchmark_config_factory<p> source module scandeval.benchmark_config_factory </p> <p>Factory class for creating dataset configurations.</p> <p> Functions </p> <ul> <li> <p>build_benchmark_config \u2014 Create a benchmark configuration.</p> </li> <li> <p>get_correct_language_codes \u2014 Get correct language code(s).</p> </li> <li> <p>prepare_languages \u2014 Prepare language(s) for benchmarking.</p> </li> <li> <p>prepare_tasks_and_datasets \u2014 Prepare task(s) and dataset(s) for benchmarking.</p> </li> <li> <p>prepare_device \u2014 Prepare device for benchmarking.</p> </li> </ul> <p> source build_benchmark_config(progress_bar: bool, save_results: bool, task: str | list[str] | None, dataset: str | list[str] | None, language: str | list[str], model_language: str | list[str] | None, dataset_language: str | list[str] | None, framework: Framework | str | None, device: Device | None, batch_size: int, evaluate_train: bool, raise_errors: bool, cache_dir: str, token: bool | str | None, openai_api_key: str | None, prefer_azure: bool, azure_openai_api_key: str | None, azure_openai_endpoint: str | None, azure_openai_api_version: str | None, force: bool, verbose: bool, trust_remote_code: bool, load_in_4bit: bool | None, use_flash_attention: bool | None, clear_model_cache: bool, only_validation_split: bool, few_shot: bool, num_iterations: int, debug: bool, run_with_cli: bool, first_time: bool = False) \u2192 BenchmarkConfig </p> <p>Create a benchmark configuration.</p> <p> Parameters </p> <ul> <li> <p>progress_bar :  bool \u2014</p> <p>Whether to show a progress bar when running the benchmark.</p> </li> <li> <p>save_results :  bool \u2014</p> <p>Whether to save the benchmark results to a file.</p> </li> <li> <p>task :  str | list[str] | None \u2014</p> <p>The tasks to include for dataset. If None then datasets will not be filtered based on their task.</p> </li> <li> <p>dataset :  str | list[str] | None \u2014</p> <p>The datasets to include for task. If None then all datasets will be included, limited by the <code>task</code> parameter.</p> </li> <li> <p>language :  str | list[str] \u2014</p> <p>The language codes of the languages to include, both for models and datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this to 'all' if all languages (also non-Scandinavian) should be considered.</p> </li> <li> <p>model_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for models. If None then the <code>language</code> parameter will be used.</p> </li> <li> <p>dataset_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for datasets. If None then the <code>language</code> parameter will be used.</p> </li> <li> <p>framework :  Framework | str | None \u2014</p> <p>The framework to use for running the models. If None then the framework will be set automatically.</p> </li> <li> <p>device :  Device | None \u2014</p> <p>The device to use for running the models. If None then the device will be set automatically.</p> </li> <li> <p>batch_size :  int \u2014</p> <p>The batch size to use for running the models.</p> </li> <li> <p>evaluate_train :  bool \u2014</p> <p>Whether to evaluate the models on the training set.</p> </li> <li> <p>raise_errors :  bool \u2014</p> <p>Whether to raise errors when running the benchmark.</p> </li> <li> <p>cache_dir :  str \u2014</p> <p>The directory to use for caching the models.</p> </li> <li> <p>token :  bool | str | None \u2014</p> <p>The token to use for running the models.</p> </li> <li> <p>openai_api_key :  str | None \u2014</p> <p>The OpenAI API key to use for running the models.</p> </li> <li> <p>prefer_azure :  bool \u2014</p> <p>Whether to prefer the Azure OpenAI API for running the models, over the OpenAI API.</p> </li> <li> <p>azure_openai_api_key :  str | None \u2014</p> <p>The Azure OpenAI API key to use for running the models.</p> </li> <li> <p>azure_openai_endpoint :  str | None \u2014</p> <p>The Azure OpenAI endpoint to use for running the models.</p> </li> <li> <p>azure_openai_api_version :  str | None \u2014</p> <p>The Azure OpenAI api version to use for running the models.</p> </li> <li> <p>force :  bool \u2014</p> <p>Whether to force the benchmark to run even if the results are already cached.</p> </li> <li> <p>verbose :  bool \u2014</p> <p>Whether to print verbose output when running the benchmark.</p> </li> <li> <p>trust_remote_code :  bool \u2014</p> <p>Whether to trust remote code when running the benchmark.</p> </li> <li> <p>load_in_4bit :  bool | None \u2014</p> <p>Whether to load the models in 4-bit precision.</p> </li> <li> <p>use_flash_attention :  bool | None \u2014</p> <p>Whether to use Flash Attention for the models. If None then it will be used if it is available.</p> </li> <li> <p>clear_model_cache :  bool \u2014</p> <p>Whether to clear the model cache before running the benchmark.</p> </li> <li> <p>only_validation_split :  bool \u2014</p> <p>Whether to only use the validation split for the datasets.</p> </li> <li> <p>few_shot :  bool \u2014</p> <p>Whether to use few-shot learning for the models.</p> </li> <li> <p>num_iterations :  int \u2014</p> <p>The number of iterations each model should be evaluated for.</p> </li> <li> <p>debug :  bool \u2014</p> <p>Whether to run the benchmark in debug mode.</p> </li> <li> <p>run_with_cli :  bool \u2014</p> <p>Whether the benchmark is being run with the CLI.</p> </li> <li> <p>first_time :  bool \u2014</p> <p>Whether this is the first time the benchmark configuration is being created. Defaults to False.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> source get_correct_language_codes(language_codes: str | list[str]) \u2192 list[str] </p> <p>Get correct language code(s).</p> <p> Parameters </p> <ul> <li> <p>language_codes :  str | list[str] \u2014</p> <p>The language codes of the languages to include, both for models and datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this to 'all' if all languages (also non-Scandinavian) should be considered.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 The correct language codes.</p> </li> </ul> <p> source prepare_languages(language_codes: str | list[str] | None, default_language_codes: list[str]) \u2192 list[Language] </p> <p>Prepare language(s) for benchmarking.</p> <p> Parameters </p> <ul> <li> <p>language_codes :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for models or datasets. If specified then this overrides the <code>language</code> parameter for model or dataset languages.</p> </li> <li> <p>default_language_codes :  list[str] \u2014</p> <p>The default language codes of the languages to include.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[Language] \u2014 The prepared model or dataset languages.</p> </li> </ul> <p> source prepare_tasks_and_datasets(task: str | list[str] | None, dataset_languages: list[Language], dataset: str | list[str] | None) \u2192 tuple[list[Task], list[str]] </p> <p>Prepare task(s) and dataset(s) for benchmarking.</p> <p> Parameters </p> <ul> <li> <p>task :  str | list[str] | None \u2014</p> <p>The tasks to include for dataset. If None then datasets will not be filtered based on their task.</p> </li> <li> <p>dataset_languages :  list[Language] \u2014</p> <p>The languages of the datasets in the benchmark.</p> </li> <li> <p>dataset :  str | list[str] | None \u2014</p> <p>The datasets to include for task. If None then all datasets will be included, limited by the <code>task</code> and <code>dataset_languages</code> parameters.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[list[Task], list[str]] \u2014 The prepared tasks and datasets.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark \u2014</p> <p>If the task or dataset is not found in the benchmark tasks or datasets.</p> </li> </ul> <p> source prepare_device(device: Device | None) \u2192 torch.device </p> <p>Prepare device for benchmarking.</p> <p> Parameters </p> <ul> <li> <p>device :  Device | None \u2014</p> <p>The device to use for running the models. If None then the device will be set automatically.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>torch.device \u2014 The prepared device.</p> </li> </ul>"},{"location":"src/scandeval/benchmark_config_factory/","title":"scandeval.benchmark_config_factory","text":"scandeval.benchmark_config_factory<p> docs module scandeval.benchmark_config_factory </p> <pre><code>\"\"\"Factory class for creating dataset configurations.\"\"\"\n\nimport importlib.util\nimport logging\nimport os\nimport sys\nfrom typing import TYPE_CHECKING\n\nimport torch\n\nfrom .config import BenchmarkConfig\nfrom .dataset_configs import get_all_dataset_configs\nfrom .enums import Device, Framework\nfrom .exceptions import InvalidBenchmark\nfrom .languages import get_all_languages\nfrom .tasks import get_all_tasks\n\nif TYPE_CHECKING:\n    from .config import Language, Task\n\n\nlogger = logging.getLogger(__package__)\n\n\ndef build_benchmark_config(docs\n    progress_bar: bool,\n    save_results: bool,\n    task: str | list[str] | None,\n    dataset: str | list[str] | None,\n    language: str | list[str],\n    model_language: str | list[str] | None,\n    dataset_language: str | list[str] | None,\n    framework: Framework | str | None,\n    device: Device | None,\n    batch_size: int,\n    evaluate_train: bool,\n    raise_errors: bool,\n    cache_dir: str,\n    token: bool | str | None,\n    openai_api_key: str | None,\n    prefer_azure: bool,\n    azure_openai_api_key: str | None,\n    azure_openai_endpoint: str | None,\n    azure_openai_api_version: str | None,\n    force: bool,\n    verbose: bool,\n    trust_remote_code: bool,\n    load_in_4bit: bool | None,\n    use_flash_attention: bool | None,\n    clear_model_cache: bool,\n    only_validation_split: bool,\n    few_shot: bool,\n    num_iterations: int,\n    debug: bool,\n    run_with_cli: bool,\n    first_time: bool = False,\n) -&gt; BenchmarkConfig:\n    \"\"\"Create a benchmark configuration.\n\n    Args:\n        progress_bar:\n            Whether to show a progress bar when running the benchmark.\n        save_results:\n            Whether to save the benchmark results to a file.\n        task:\n            The tasks to include for dataset. If None then datasets will not be\n            filtered based on their task.\n        dataset:\n            The datasets to include for task. If None then all datasets will be\n            included, limited by the `task` parameter.\n        language:\n            The language codes of the languages to include, both for models and\n            datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this\n            to 'all' if all languages (also non-Scandinavian) should be considered.\n        model_language:\n            The language codes of the languages to include for models. If None then\n            the `language` parameter will be used.\n        dataset_language:\n            The language codes of the languages to include for datasets. If None then\n            the `language` parameter will be used.\n        framework:\n            The framework to use for running the models. If None then the framework\n            will be set automatically.\n        device:\n            The device to use for running the models. If None then the device will be\n            set automatically.\n        batch_size:\n            The batch size to use for running the models.\n        evaluate_train:\n            Whether to evaluate the models on the training set.\n        raise_errors:\n            Whether to raise errors when running the benchmark.\n        cache_dir:\n            The directory to use for caching the models.\n        token:\n            The token to use for running the models.\n        openai_api_key:\n            The OpenAI API key to use for running the models.\n        prefer_azure:\n            Whether to prefer the Azure OpenAI API for running the models, over the\n            OpenAI API.\n        azure_openai_api_key:\n            The Azure OpenAI API key to use for running the models.\n        azure_openai_endpoint:\n            The Azure OpenAI endpoint to use for running the models.\n        azure_openai_api_version:\n            The Azure OpenAI api version to use for running the models.\n        force:\n            Whether to force the benchmark to run even if the results are already\n            cached.\n        verbose:\n            Whether to print verbose output when running the benchmark.\n        trust_remote_code:\n            Whether to trust remote code when running the benchmark.\n        load_in_4bit:\n            Whether to load the models in 4-bit precision.\n        use_flash_attention:\n            Whether to use Flash Attention for the models. If None then it will be used\n            if it is available.\n        clear_model_cache:\n            Whether to clear the model cache before running the benchmark.\n        only_validation_split:\n            Whether to only use the validation split for the datasets.\n        few_shot:\n            Whether to use few-shot learning for the models.\n        num_iterations:\n            The number of iterations each model should be evaluated for.\n        debug:\n            Whether to run the benchmark in debug mode.\n        run_with_cli:\n            Whether the benchmark is being run with the CLI.\n        first_time:\n            Whether this is the first time the benchmark configuration is being created.\n            Defaults to False.\n\n    Returns:\n        The benchmark configuration.\n    \"\"\"\n    language_codes = get_correct_language_codes(language_codes=language)\n    model_languages = prepare_languages(\n        language_codes=model_language, default_language_codes=language_codes\n    )\n    dataset_languages = prepare_languages(\n        language_codes=dataset_language, default_language_codes=language_codes\n    )\n\n    tasks, datasets = prepare_tasks_and_datasets(\n        task=task, dataset=dataset, dataset_languages=dataset_languages\n    )\n\n    torch_device = prepare_device(device=device)\n\n    if openai_api_key is None:\n        openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n    if azure_openai_api_key is None:\n        azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n    if azure_openai_endpoint is None:\n        azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n    if azure_openai_api_version is None:\n        azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n\n    # Ensure that we are not using both OpenAI and Azure OpenAI API keys\n    if all(\n        value is not None\n        for value in (\n            openai_api_key,\n            azure_openai_api_key,\n            azure_openai_endpoint,\n            azure_openai_api_version,\n        )\n    ):\n        if prefer_azure:\n            logger.info(\n                \"Both OpenAI and Azure OpenAI API keys are set. Using Azure OpenAI.\"\n            )\n            openai_api_key = None\n        else:\n            if run_with_cli:\n                logger.info(\n                    \"Both OpenAI and Azure OpenAI API keys are set. Using OpenAI since \"\n                    \"the `--prefer-azure` flag is not set.\"\n                )\n            else:\n                logger.info(\n                    \"Both OpenAI and Azure OpenAI API keys are set. Using OpenAI since \"\n                    \"the `prefer_azure` argument is not set to True.\"\n                )\n            azure_openai_api_key = None\n            azure_openai_endpoint = None\n\n    # Sanity check\n    assert not (openai_api_key is not None and azure_openai_api_key is not None)\n\n    framework_obj = Framework(framework) if framework is not None else None\n\n    if token is True:\n        token = None\n\n    if use_flash_attention is None:\n        if torch_device.type != \"cuda\":\n            use_flash_attention = False\n        elif (\n            importlib.util.find_spec(\"flash_attn\") is None\n            and importlib.util.find_spec(\"vllm_flash_attn\") is None\n        ):\n            use_flash_attention = False\n            if first_time and torch_device.type == \"cuda\":\n                message = (\n                    \"Flash attention has not been installed, so this will not be used. \"\n                    \"To install it, run `pip install -U wheel &amp;&amp; \"\n                    \"FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE pip install flash-attn \"\n                    \"--no-build-isolation`. Alternatively, you can disable this \"\n                    \"message by setting \"\n                )\n                if run_with_cli:\n                    message += \"the flag `--no-use-flash-attention`.\"\n                else:\n                    message += (\n                        \"the argument `use_flash_attention=False` in the `Benchmarker`.\"\n                    )\n                logger.info(message)\n\n    # Set variable with number of iterations\n    if hasattr(sys, \"_called_from_test\"):\n        num_iterations = 1\n\n    return BenchmarkConfig(\n        model_languages=model_languages,\n        dataset_languages=dataset_languages,\n        tasks=tasks,\n        datasets=datasets,\n        batch_size=batch_size,\n        raise_errors=raise_errors,\n        cache_dir=cache_dir,\n        evaluate_train=evaluate_train,\n        token=token,\n        openai_api_key=openai_api_key,\n        azure_openai_api_key=azure_openai_api_key,\n        azure_openai_endpoint=azure_openai_endpoint,\n        azure_openai_api_version=azure_openai_api_version,\n        force=force,\n        progress_bar=progress_bar,\n        save_results=save_results,\n        verbose=verbose,\n        framework=framework_obj,\n        device=torch_device,\n        trust_remote_code=trust_remote_code,\n        load_in_4bit=load_in_4bit,\n        use_flash_attention=use_flash_attention,\n        clear_model_cache=clear_model_cache,\n        only_validation_split=only_validation_split,\n        few_shot=few_shot,\n        num_iterations=num_iterations,\n        debug=debug,\n        run_with_cli=run_with_cli,\n    )\n\ndocs\ndef get_correct_language_codes(language_codes: str | list[str]) -&gt; list[str]:\n    \"\"\"Get correct language code(s).\n\n    Args:\n        language_codes:\n            The language codes of the languages to include, both for models and\n            datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this\n            to 'all' if all languages (also non-Scandinavian) should be considered.\n\n    Returns:\n        The correct language codes.\n    \"\"\"\n    # Create a dictionary that maps languages to their associated language objects\n    language_mapping = get_all_languages()\n\n    # Create the list `languages`\n    if \"all\" in language_codes:\n        languages = list(language_mapping.keys())\n    elif isinstance(language_codes, str):\n        languages = [language_codes]\n    else:\n        languages = language_codes\n\n    # If `languages` contains 'no' then also include 'nb' and 'nn'. Conversely, if\n    # either 'nb' or 'nn' are specified then also include 'no'.\n    if \"no\" in languages:\n        languages = list(set(languages) | {\"nb\", \"nn\"})\n    elif \"nb\" in languages or \"nn\" in languages:\n        languages = list(set(languages) | {\"no\"})\n\n    return languages\n\n\ndef prepare_languages(docs\n    language_codes: str | list[str] | None, default_language_codes: list[str]\n) -&gt; list[\"Language\"]:\n    \"\"\"Prepare language(s) for benchmarking.\n\n    Args:\n        language_codes:\n            The language codes of the languages to include for models or datasets.\n            If specified then this overrides the `language` parameter for model or\n            dataset languages.\n        default_language_codes:\n            The default language codes of the languages to include.\n\n    Returns:\n        The prepared model or dataset languages.\n    \"\"\"\n    # Create a dictionary that maps languages to their associated language objects\n    language_mapping = get_all_languages()\n\n    # Create the list `languages_str` of language codes to use for models or datasets\n    languages_str: list[str]\n    if language_codes is None:\n        languages_str = default_language_codes\n    elif isinstance(language_codes, str):\n        languages_str = [language_codes]\n    else:\n        languages_str = language_codes\n\n    # Convert the model languages to language objects\n    if \"all\" in languages_str:\n        prepared_languages = list(language_mapping.values())\n    else:\n        prepared_languages = [language_mapping[language] for language in languages_str]\n\n    return prepared_languages\n\n\ndef prepare_tasks_and_datasets(docs\n    task: str | list[str] | None,\n    dataset_languages: list[\"Language\"],\n    dataset: str | list[str] | None,\n) -&gt; tuple[list[\"Task\"], list[str]]:\n    \"\"\"Prepare task(s) and dataset(s) for benchmarking.\n\n    Args:\n        task:\n            The tasks to include for dataset. If None then datasets will not be\n            filtered based on their task.\n        dataset_languages:\n            The languages of the datasets in the benchmark.\n        dataset:\n            The datasets to include for task. If None then all datasets will be\n            included, limited by the `task` and `dataset_languages` parameters.\n\n    Returns:\n        The prepared tasks and datasets.\n\n    Raises:\n        InvalidBenchmark:\n            If the task or dataset is not found in the benchmark tasks or datasets.\n    \"\"\"\n    # Create a dictionary that maps benchmark tasks to their associated benchmark\n    # task objects, and a dictionary that maps dataset names to their associated\n    # dataset configuration objects\n    task_mapping = get_all_tasks()\n    all_dataset_configs = get_all_dataset_configs()\n\n    # Create the list of dataset tasks\n    try:\n        if task is None:\n            tasks = list(task_mapping.values())\n        elif isinstance(task, str):\n            tasks = [task_mapping[task]]\n        else:\n            tasks = [task_mapping[t] for t in task]\n    except KeyError as e:\n        raise InvalidBenchmark(f\"Task {e} not found in the benchmark tasks.\") from e\n\n    all_official_datasets = [\n        dataset_name\n        for dataset_name, dataset_config in all_dataset_configs.items()\n        if not dataset_config.unofficial\n    ]\n    if dataset is None:\n        dataset = all_official_datasets\n    elif isinstance(dataset, str):\n        dataset = [dataset]\n\n    all_datasets = list(all_dataset_configs.keys())\n    invalid_datasets = set(dataset) - set(all_datasets)\n    if invalid_datasets:\n        raise InvalidBenchmark(\n            f\"Dataset(s) {', '.join(invalid_datasets)} not found in the benchmark \"\n            \"datasets.\"\n        )\n\n    datasets = [\n        dataset_name\n        for dataset_name, dataset_config in all_dataset_configs.items()\n        if dataset_name in dataset\n        and dataset_config.task in tasks\n        and set(dataset_config.languages).intersection(dataset_languages)\n    ]\n\n    return tasks, datasets\n\n\ndef prepare_device(device: Device | None) -&gt; torch.device:docs\n    \"\"\"Prepare device for benchmarking.\n\n    Args:\n        device:\n            The device to use for running the models. If None then the device will be\n            set automatically.\n\n    Returns:\n        The prepared device.\n    \"\"\"\n    device_mapping = {\n        Device.CPU: torch.device(\"cpu\"),\n        Device.CUDA: torch.device(\"cuda\"),\n        Device.MPS: torch.device(\"mps\"),\n    }\n    if isinstance(device, Device):\n        return device_mapping[device]\n\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    elif torch.backends.mps.is_available():\n        return torch.device(\"mps\")\n    else:\n        return torch.device(\"cpu\")\n</code></pre>"},{"location":"api/scandeval/benchmark_dataset/","title":"scandeval.benchmark_dataset","text":"scandeval.benchmark_dataset<p> source module scandeval.benchmark_dataset </p> <p>Abstract benchmarking dataset class.</p> <p> Classes </p> <ul> <li> <p>BenchmarkDataset \u2014 Abstract benchmarking dataset class.</p> </li> </ul> <p> source class BenchmarkDataset(benchmark_config: BenchmarkConfig) </p> <p><p>Bases : ABC</p></p> <p>Abstract benchmarking dataset class.</p> <p>Initialise the dataset.</p> <p> Parameters </p> <ul> <li> <p>dataset_config :  DatasetConfig \u2014 The configuration of the dataset.</p> <p>The configuration for the dataset.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014 The configuration of the benchmark.</p> <p>The configuration for the benchmark.</p> </li> </ul> <p> Attributes </p> <ul> <li> <p>dataset_config \u2014</p> <p>The configuration of the dataset.</p> </li> <li> <p>benchmark_config \u2014</p> <p>The configuration of the benchmark.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>benchmark \u2014 Benchmark a model.</p> </li> </ul> <p> source method BenchmarkDataset.benchmark(model_id: str, model: GenerativeModel | None = None, tokenizer: Tokenizer | None = None) \u2192 tuple[ScoreDict, dict[str, bool | int], GenerativeModel | None, Tokenizer | None] </p> <p>Benchmark a model.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The full Hugging Face Hub path to the pretrained transformer model. The specific model version to use can be added after the suffix '@': \"model_id@v1.0.0\". It can be a branch name, a tag name, or a commit id, and defaults to the latest version if not specified.</p> </li> <li> <p>model :  GenerativeModel | None \u2014</p> <p>The model to benchmark. If not provided, the model will be loaded.</p> </li> <li> <p>tokenizer :  Tokenizer | None \u2014</p> <p>The tokenizer to use with the model. If not provided, the tokenizer will be loaded.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[ScoreDict, dict[str, bool | int], GenerativeModel | None, Tokenizer | None] \u2014 A pair (scores, metadata_dict, model, tokenizer), with <code>scores</code> being a dictionary containing the scores, and <code>metadata_dict</code> being a dictionary containing various model metadata, such as the number of model parameters, the model's maximum sequence length and the size of the model's vocabulary. The keys in <code>score_dict</code> are 'raw' and 'total', with all the raw scores in the first dictionary and the aggregated scores in the second. The tokenizer and model are only not <code>None</code> if the model is generative.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>RuntimeError \u2014</p> <p>If the extracted framework is not recognized.</p> </li> </ul>"},{"location":"src/scandeval/benchmark_dataset/","title":"scandeval.benchmark_dataset","text":"scandeval.benchmark_dataset<p> docs module scandeval.benchmark_dataset </p> <pre><code>\"\"\"Abstract benchmarking dataset class.\"\"\"\n\nimport logging\nimport sys\nfrom abc import ABC, abstractmethod\nfrom functools import partial\nfrom typing import TYPE_CHECKING, Any, Type\n\nimport evaluate\nimport torch\nfrom datasets.dataset_dict import DatasetDict\nfrom datasets.load import load_dataset\nfrom huggingface_hub import HfApi\nfrom huggingface_hub.hf_api import ModelInfo\nfrom huggingface_hub.utils import HfHubHTTPError, HFValidationError\nfrom requests import RequestException\nfrom tqdm.auto import tqdm\nfrom transformers import Trainer\nfrom transformers.modeling_utils import PreTrainedModel\n\nfrom .exceptions import InvalidBenchmark\nfrom .finetuning import finetune\nfrom .generation import generate\nfrom .model_config import get_model_config\nfrom .model_loading import load_model\nfrom .scores import log_scores\nfrom .speed_benchmark import benchmark_speed\nfrom .tasks import SPEED\nfrom .utils import (\n    GENERATIVE_MODEL_TASKS,\n    enforce_reproducibility,\n    get_model_max_length,\n    model_is_generative,\n    should_prompts_be_stripped,\n    unscramble,\n)\n\nif TYPE_CHECKING:\n    from datasets.arrow_dataset import Dataset\n    from numpy.random import Generator\n    from transformers import PretrainedConfig\n    from transformers.modeling_utils import ModelOutput\n\n    from .config import BenchmarkConfig, DatasetConfig, ModelConfig\n    from .protocols import GenerativeModel, Tokenizer\n    from .types import Labels, Predictions, ScoreDict\n\n\nlogger = logging.getLogger(__package__)\n\n\nclass BenchmarkDataset(ABC):docs\n    \"\"\"Abstract benchmarking dataset class.\n\n    Args:\n        dataset_config:\n            The configuration of the dataset.\n        benchmark_config:\n            The configuration of the benchmark.\n\n    Attributes:\n        dataset_config:\n            The configuration of the dataset.\n        benchmark_config:\n            The configuration of the benchmark.\n    \"\"\"\n\n    def __init__(\n        self, dataset_config: \"DatasetConfig\", benchmark_config: \"BenchmarkConfig\"\n    ) -&gt; None:\n        \"\"\"Initialise the dataset.\n\n        Args:\n            dataset_config:\n                The configuration for the dataset.\n            benchmark_config:\n                The configuration for the benchmark.\n        \"\"\"\n        self.dataset_config = dataset_config\n        self.benchmark_config = benchmark_config\n        self._metrics = {\n            metric_cfg.name: (\n                evaluate.load(\n                    path=metric_cfg.huggingface_id,\n                    cache_dir=self.benchmark_config.cache_dir,\n                )\n                if metric_cfg.huggingface_id != \"\"\n                else None\n            )\n            for metric_cfg in dataset_config.task.metrics\n        }\n\n        # Set logging level based on verbosity\n        if hasattr(sys, \"_called_from_test\"):\n            logging_level = logging.CRITICAL\n        elif self.benchmark_config.verbose:\n            logging_level = logging.DEBUG\n        else:\n            logging_level = logging.INFO\n        logger.setLevel(logging_level)\n\n    def benchmark(docs\n        self,\n        model_id: str,\n        model: \"GenerativeModel | None\" = None,\n        tokenizer: \"Tokenizer | None\" = None,\n    ) -&gt; \"tuple[ScoreDict, dict[str, bool | int], GenerativeModel | None, Tokenizer | None]\":\n        \"\"\"Benchmark a model.\n\n        Args:\n            model_id:\n                The full Hugging Face Hub path to the pretrained transformer model. The\n                specific model version to use can be added after the suffix '@':\n                \"model_id@v1.0.0\". It can be a branch name, a tag name, or a commit id,\n                and defaults to the latest version if not specified.\n            model:\n                The model to benchmark. If not provided, the model will be loaded.\n            tokenizer:\n                The tokenizer to use with the model. If not provided, the tokenizer will\n                be loaded.\n\n        Returns:\n            A pair (scores, metadata_dict, model, tokenizer), with `scores` being a\n            dictionary containing the scores, and `metadata_dict` being a dictionary\n            containing various model metadata, such as the number of model parameters,\n            the model's maximum sequence length and the size of the model's vocabulary.\n            The keys in `score_dict` are 'raw' and 'total', with all the raw scores in\n            the first dictionary and the aggregated scores in the second. The tokenizer\n            and model are only not `None` if the model is generative.\n\n        Raises:\n            RuntimeError:\n                If the extracted framework is not recognized.\n        \"\"\"\n        model_config = get_model_config(\n            model_id=model_id, benchmark_config=self.benchmark_config\n        )\n\n        # Set random seeds to enforce reproducibility of the randomly initialised\n        # weights\n        rng = enforce_reproducibility(framework=model_config.framework)\n\n        if model is None or tokenizer is None:\n            logger.info(\"Loading model and tokenizer...\")\n            model, tokenizer = load_model(\n                model_config=model_config,\n                dataset_config=self.dataset_config,\n                benchmark_config=self.benchmark_config,\n            )\n\n        benchmarking_generative_model = model_is_generative(model=model)\n\n        # This happens when a local model is used, as we cannot fetch the model\n        # metadata. Note that this is only the case if the model type is not any of the\n        # ones hardcoded in `local.py`\n        if model_config.task == \"unknown\":\n            if benchmarking_generative_model:\n                model_config.task = GENERATIVE_MODEL_TASKS[0]\n            else:\n                model_config.task = \"fill-mask\"\n\n        metadata_dict = self._get_metadata(\n            model_id=model_id,\n            model=model,\n            tokenizer=tokenizer,\n            benchmarking_generative_model=benchmarking_generative_model,\n        )\n\n        if self.dataset_config.task != SPEED:\n            train, val, tests = self._load_data(rng=rng)\n            prepared_train, prepared_val, prepared_tests = self._load_prepared_data(\n                train=train,\n                val=val,\n                tests=tests,\n                model_config=model_config,\n                hf_model_config=model.config,\n                tokenizer=tokenizer,\n                benchmarking_generative_model=benchmarking_generative_model,\n            )\n\n        # Set up progress bar\n        itr = tqdm(\n            iterable=range(self.benchmark_config.num_iterations),\n            desc=\"Benchmarking\",\n            disable=not self.benchmark_config.progress_bar,\n        )\n\n        data_collator = self._load_data_collator(tokenizer=tokenizer, model=model)\n\n        if self.dataset_config.task == SPEED:\n            scores = benchmark_speed(\n                itr=itr,\n                tokenizer=tokenizer,\n                model=model,\n                model_config=model_config,\n                dataset_config=self.dataset_config,\n                benchmark_config=self.benchmark_config,\n            )\n        elif benchmarking_generative_model:\n            scores = generate(\n                itr=itr,\n                prepared_train=prepared_train,\n                prepared_tests=prepared_tests,\n                model=model,\n                model_config=model_config,\n                tokenizer=tokenizer,\n                data_collator=data_collator,\n                compute_metrics=self._compute_metrics,\n                extract_labels_fn=self._extract_labels_from_generation,\n                benchmark_config=self.benchmark_config,\n                dataset_config=self.dataset_config,\n            )\n        else:\n            scores = finetune(\n                itr=itr,\n                train=train,\n                val=val,\n                tests=tests,\n                prepared_train=prepared_train,\n                prepared_val=prepared_val,\n                prepared_tests=prepared_tests,\n                model=model,\n                tokenizer=tokenizer,\n                model_config=model_config,\n                dataset_config=self.dataset_config,\n                benchmark_config=self.benchmark_config,\n                compute_metrics=self._compute_metrics,\n                data_collator=data_collator,\n                trainer_class=self._get_trainer_class(),\n                evaluate_inputs_fn=self._get_evaluate_inputs,\n                preprocess_logits_for_metrics=self._preprocess_logits_for_metrics,\n            )\n\n        all_scores = log_scores(\n            dataset_name=self.dataset_config.pretty_name,\n            metric_configs=self.dataset_config.task.metrics,\n            scores=scores,\n            model_id=model_config.model_id,\n        )\n\n        if benchmarking_generative_model:\n            return all_scores, metadata_dict, model, tokenizer\n        else:\n            return all_scores, metadata_dict, None, None\n\n    def _get_metadata(\n        self,\n        model_id: str,\n        model: \"PreTrainedModel | GenerativeModel\",\n        tokenizer: \"Tokenizer\",\n        benchmarking_generative_model: bool,\n    ) -&gt; dict[str, int]:\n        \"\"\"Get metadata about the model.\n\n        Args:\n            model_id:\n                The Hugging Face model ID.\n            model:\n                The model to get metadata about.\n            tokenizer:\n                The tokenizer to get metadata about.\n            benchmarking_generative_model:\n                Whether the model is a generative model.\n\n        Returns:\n            A dictionary containing metadata about the model, with the keys being the\n            metadata names and the values being the metadata values.\n        \"\"\"\n        api = HfApi()\n        try:\n            repo_info = api.repo_info(repo_id=model_id, repo_type=\"model\")\n            assert isinstance(repo_info, ModelInfo)\n        except (RequestException, HFValidationError):\n            repo_info = None\n\n        if (\n            repo_info is not None\n            and hasattr(repo_info, \"safetensors\")\n            and repo_info.safetensors is not None\n            and \"total\" in repo_info.safetensors\n        ):\n            num_params = repo_info.safetensors[\"total\"]\n        elif (\n            repo_info is not None\n            and hasattr(repo_info, \"card_data\")\n            and repo_info.card_data is not None\n            and \"base_model\" in repo_info.card_data\n            and repo_info.card_data[\"base_model\"] is not None\n            and len(repo_info.card_data[\"base_model\"]) &gt; 0\n            and (\n                base_repo_info := api.repo_info(\n                    repo_info.card_data[\"base_model\"][-1], repo_type=\"model\"\n                )\n            )\n            is not None\n            and hasattr(base_repo_info, \"safetensors\")\n            and base_repo_info.safetensors is not None\n            and \"total\" in base_repo_info.safetensors\n        ):\n            num_params = base_repo_info.safetensors[\"total\"]\n        elif (\n            hasattr(model.config, \"num_params\") and model.config.num_params is not None\n        ):\n            num_params = model.config.num_params\n        elif isinstance(model, PreTrainedModel):\n            num_params = sum(p.numel() for p in model.parameters())\n        else:\n            num_params = -1\n\n        max_seq_length = get_model_max_length(model=model)\n\n        if hasattr(model.config, \"vocab_size\") and model.config.vocab_size is not None:\n            vocab_size = model.config.vocab_size\n        elif hasattr(tokenizer, \"vocab_size\") and tokenizer.vocab_size is not None:\n            vocab_size = tokenizer.vocab_size\n        else:\n            vocab_size = -1\n\n        # Store the metadata in a dictionary\n        metadata_dict = dict(\n            num_model_parameters=num_params,\n            max_sequence_length=max_seq_length,\n            vocabulary_size=vocab_size,\n            generative=benchmarking_generative_model,\n            few_shot=self.benchmark_config.few_shot,\n            validation_split=self.benchmark_config.only_validation_split,\n        )\n\n        # Log the metadata\n        logging_msg: str = \"\"\n        if num_params &lt; 0:\n            logging_msg += \"The model has an unknown number of parameters, \"\n        else:\n            logging_msg += f\"The model has {num_params:,} parameters, \"\n        if vocab_size &lt; 0:\n            logging_msg += \"an unknown vocabulary size, \"\n        else:\n            logging_msg += f\"a vocabulary size of {vocab_size:,}, \"\n        if max_seq_length &lt; 0:\n            logging_msg += \"and an unknown maximum sequence length.\"\n        else:\n            logging_msg += f\"and a maximum sequence length of {max_seq_length:,}.\"\n        logger.info(logging_msg)\n\n        return metadata_dict\n\n    def _load_data(\n        self, rng: \"Generator\"\n    ) -&gt; tuple[\"Dataset\", \"Dataset\", list[\"Dataset\"]]:\n        \"\"\"Load the raw bootstrapped datasets.\n\n        Args:\n            rng:\n                The random number generator to use.\n\n        Returns:\n            A tuple containing the training, validation and test datasets.\n        \"\"\"\n        # Download dataset from the HF Hub\n        try:\n            dataset_dict = load_dataset(\n                path=self.dataset_config.huggingface_id,\n                cache_dir=self.benchmark_config.cache_dir,\n                token=unscramble(\"HjccJFhIozVymqXDVqTUTXKvYhZMTbfIjMxG_\"),\n            )\n        except HfHubHTTPError:\n            raise InvalidBenchmark(\"The Hugging Face Hub seems to be down.\")\n\n        # If the dataset turns out not to be a DatasetDict, then we raise an error\n        if not isinstance(dataset_dict, DatasetDict):\n            raise InvalidBenchmark(\n                f\"Expected `dataset_dict` to be a `DatasetDict`, but got \"\n                f\"{type(dataset_dict)}.\"\n            )\n\n        # Remove all other keys than 'train', 'val' and 'test'\n        dataset_dict = DatasetDict(\n            {key: dataset_dict[key] for key in [\"train\", \"val\", \"test\"]}\n        )\n\n        # Process the datasets\n        dataset_dict = self._process_data(dataset_dict)\n\n        # Extract the dataset splits\n        train = dataset_dict[\"train\"]\n        val = dataset_dict[\"val\"]\n        test = dataset_dict[\"test\"]\n\n        if self.benchmark_config.only_validation_split:\n            test = val\n\n        # Remove empty examples from the datasets\n        for text_feature in [\"tokens\", \"text\"]:\n            if text_feature in train.features:\n                train = train.filter(lambda x: len(x[text_feature]) &gt; 0)\n                val = val.filter(lambda x: len(x[text_feature]) &gt; 0)\n                test = test.filter(lambda x: len(x[text_feature]) &gt; 0)\n\n        # If we are testing then truncate the test set\n        if hasattr(sys, \"_called_from_test\"):\n            test = test.select(range(1))\n\n        # Bootstrap the test set\n        test_bidxs = rng.integers(\n            0, len(test), size=(self.benchmark_config.num_iterations, len(test))\n        )\n        tests = [test.select(test_bidxs[idx]) for idx in range(test_bidxs.shape[0])]\n\n        return train, val, tests\n\n    def _load_prepared_data(\n        self,\n        train: \"Dataset\",\n        val: \"Dataset\",\n        tests: list[\"Dataset\"],\n        model_config: \"ModelConfig\",\n        hf_model_config: \"PretrainedConfig\",\n        tokenizer: \"Tokenizer\",\n        benchmarking_generative_model: bool,\n    ) -&gt; tuple[\"Dataset\", \"Dataset\", list[\"Dataset\"]]:\n        \"\"\"Load the data and prepare it for training.\n\n        Args:\n            train:\n                The raw training dataset.\n            val:\n                The raw validation dataset.\n            tests:\n                The raw bootstrapped test datasets.\n            model_config:\n                The model configuration.\n            hf_model_config:\n                The Hugging Face model configuration.\n            tokenizer:\n                The tokenizer.\n            benchmarking_generative_model:\n                Whether the model is a generative model.\n\n        Returns:\n            A tuple containing the prepared training, validation and test datasets.\n        \"\"\"\n        # Set up the preprocessing parameters\n        preprocess_params: dict[str, Any] = dict(\n            hf_model_config=hf_model_config,\n            model_config=model_config,\n            tokenizer=tokenizer,\n            generative_model=benchmarking_generative_model,\n        )\n\n        # Prepare the train and validation datasets\n        with tqdm(\n            total=2 + self.benchmark_config.num_iterations,\n            desc=\"Preprocessing data splits\",\n            disable=hasattr(sys, \"_called_from_test\"),\n        ) as pbar:\n            # When evaluating generative models we only need the test split, so\n            # there's no need to prepare the train split\n            try:\n                prepared_train = train\n                if not benchmarking_generative_model:\n                    prepared_train = self._preprocess_data(\n                        train, split=\"train\", **preprocess_params\n                    )\n                pbar.update(1)\n            except ValueError:\n                raise InvalidBenchmark(\n                    \"Preprocessing of the training dataset could not be done.\"\n                )\n\n            # When evaluating generative models we only need the test split, so\n            # there's no need to prepare the validation split\n            try:\n                prepared_val = val\n                if not benchmarking_generative_model:\n                    prepared_val = self._preprocess_data(\n                        val, split=\"val\", **preprocess_params\n                    )\n                pbar.update(1)\n            except ValueError:\n                raise InvalidBenchmark(\n                    \"Preprocessing of the validation dataset could not be done.\"\n                )\n\n            try:\n                prepared_tests: list[\"Dataset\"] = list()\n                for itr_idx, test in enumerate(tests):\n                    if benchmarking_generative_model:\n                        if self.benchmark_config.few_shot:\n                            itr_seed = 4242 + itr_idx\n                            few_shot_examples = self._extract_few_shot_examples(\n                                train_dataset=train, random_seed=itr_seed\n                            )\n                            few_shot_fn = partial(\n                                self._apply_few_shot_prompt,\n                                few_shot_examples=few_shot_examples,\n                                tokenizer=tokenizer,\n                            )\n                            test = test.map(\n                                few_shot_fn,\n                                batched=True,\n                                load_from_cache_file=False,\n                                keep_in_memory=True,\n                            )\n                        else:\n                            instruction_fn = partial(\n                                self._apply_instruction_prompt, tokenizer=tokenizer\n                            )\n                            test = test.map(\n                                function=instruction_fn,\n                                batched=True,\n                                load_from_cache_file=False,\n                                keep_in_memory=True,\n                            )\n\n                        # Determine if we should strip the prompts. This is the case if\n                        # the tokenizer needs to include the space as part of the label\n                        # token\n                        labels_to_be_generated = list(\n                            self.dataset_config.prompt_label_mapping.values()\n                        )\n                        strip_prompts = should_prompts_be_stripped(\n                            labels_to_be_generated=labels_to_be_generated,\n                            tokenizer=tokenizer,\n                        )\n                        if strip_prompts:\n                            test = test.map(\n                                lambda x: dict(text=x[\"text\"].strip()),\n                                load_from_cache_file=False,\n                                keep_in_memory=True,\n                            )\n\n                    prepared_test = self._preprocess_data(\n                        test, split=\"test\", **preprocess_params\n                    )\n\n                    if benchmarking_generative_model:\n                        prepared_test = prepared_test.map(\n                            lambda examples: dict(\n                                text=tokenizer.batch_decode(\n                                    sequences=examples[\"input_ids\"],\n                                    skip_special_tokens=True,\n                                )\n                            ),\n                            batched=True,\n                            load_from_cache_file=False,\n                            keep_in_memory=True,\n                        )\n\n                    prepared_tests.append(prepared_test)\n                    pbar.update(1)\n            except ValueError:\n                raise InvalidBenchmark(\n                    \"Preprocessing of the test dataset could not be done.\"\n                )\n\n        return prepared_train, prepared_val, prepared_tests\n\n    def _preprocess_logits_for_metrics(\n        self, model_outputs: torch.Tensor | tuple, labels: torch.Tensor\n    ) -&gt; torch.Tensor | tuple:\n        \"\"\"Ensure that only the logits are returned from the model.\n\n        This is to avoid memory issues when the model returns hidden states as well.\n\n        Args:\n            model_outputs:\n                The raw model outputs.\n            labels:\n                The ground truth labels.\n\n        Returns:\n            The preprocessed logits.\n        \"\"\"\n        if isinstance(model_outputs, tuple) and isinstance(\n            model_outputs[0], torch.Tensor\n        ):\n            model_output_tensors = [\n                model_output\n                for model_output in model_outputs\n                if isinstance(model_output, torch.Tensor)\n            ]\n            if len(model_output_tensors) == 1:\n                return model_output_tensors[0]\n            return tuple(model_output_tensors)\n        else:\n            return model_outputs\n\n    def __call__(self, *args, **kwargs):\n        \"\"\"Call the benchmark method. See `benchmark` for details.\"\"\"\n        return self.benchmark(*args, **kwargs)\n\n    def _process_data(self, dataset_dict: DatasetDict) -&gt; DatasetDict:\n        \"\"\"Process the data.\n\n        Args:\n            dataset_dict:\n                The dataset dictionary.\n\n        Returns:\n            The processed dataset dictionary.\n        \"\"\"\n        return dataset_dict\n\n    def _get_trainer_class(self) -&gt; Type[Trainer]:\n        \"\"\"Returns the trainer class to use.\"\"\"\n        return Trainer\n\n    def _get_evaluate_inputs(\n        self, dataset: \"Dataset\", prepared_dataset: \"Dataset\", metric_key_prefix: str\n    ) -&gt; dict[str, Any]:\n        \"\"\"Returns the inputs to the `Trainer.evaluate` method.\n\n        Args:\n            dataset:\n                The raw dataset.\n            prepared_dataset:\n                The prepared dataset.\n            metric_key_prefix:\n                The prefix to use for the metric keys.\n        \"\"\"\n        return dict(eval_dataset=prepared_dataset, metric_key_prefix=metric_key_prefix)\n\n    @abstractmethod\n    def _preprocess_data(self, dataset: \"Dataset\", **kwargs) -&gt; \"Dataset\":\n        \"\"\"Preprocess a dataset.\n\n        Args:\n            dataset:\n                The dataset to preprocess.\n            kwargs:\n                Extra keyword arguments containing objects used in preprocessing the\n                dataset.\n\n        Returns:\n            The preprocessed dataset.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _load_data_collator(\n        self,\n        tokenizer: \"Tokenizer | None\" = None,\n        model: \"PreTrainedModel | GenerativeModel | None\" = None,\n    ):\n        \"\"\"Load the data collator used to prepare samples during finetuning.\n\n        Args:\n            tokenizer:\n                A pretrained tokenizer. Can be None if the tokenizer is not used in the\n                initialisation of the data collator. Defaults to None.\n            model:\n                A pretrained model. Can be None if the model is not used in the\n                initialisation of the data collator. Defaults to None.\n\n        Returns:\n            The data collator.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _compute_metrics(\n        self,\n        model_outputs_and_labels: tuple[\"Predictions\", \"Labels\"],\n        id2label: dict[int, str],\n    ) -&gt; dict[str, float]:\n        \"\"\"Compute the metrics needed for evaluation.\n\n        Args:\n            model_outputs_and_labels:\n                The first sequence contains the model outputs and the second sequence\n                contains the true labels.\n            id2label:\n                Conversion of indices to labels.\n\n        Returns:\n            A dictionary with the names of the metrics as keys and the metric values as\n            values.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _extract_few_shot_examples(\n        self, train_dataset: \"Dataset\", random_seed: int\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"Extract few-shot examples from the training dataset.\n\n        Args:\n            train_dataset:\n                The training dataset.\n            random_seed:\n                The random seed to use when extracting the few-shot examples.\n\n        Returns:\n            The few-shot examples.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _apply_few_shot_prompt(\n        self, examples: dict, few_shot_examples: list[dict], tokenizer: \"Tokenizer\"\n    ) -&gt; dict:\n        \"\"\"Apply a few-shot prompt to the examples.\n\n        Args:\n            examples:\n                The examples to apply the prompt to.\n            few_shot_examples:\n                The examples to be included in the few-shot prompt.\n            tokenizer:\n                The tokenizer to use to encode the few-shot prompt.\n\n        Returns:\n            The examples with the few-shot prompt applied.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _apply_instruction_prompt(self, examples: dict, tokenizer: \"Tokenizer\") -&gt; dict:\n        \"\"\"Apply an instruction prompt to the examples.\n\n        Args:\n            examples:\n                The examples to apply the prompt to.\n            tokenizer:\n                The tokenizer to use to encode the instruction prompt.\n\n        Returns:\n            The examples with the instruction prompt applied.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _extract_labels_from_generation(\n        self,\n        input_batch: dict[str, list],\n        model_output: \"ModelOutput\",\n        tokenizer: \"Tokenizer\",\n    ) -&gt; list[Any]:\n        \"\"\"Extract the predicted labels from the generated output.\n\n        Args:\n            input_batch:\n                The input batch, where the keys are the feature names and the values\n                are lists with the feature values.\n            model_output:\n                The raw generated output of the model.\n            tokenizer:\n                The tokenizer used together with the model.\n\n        Returns:\n            The predicted labels.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/scandeval/callbacks/","title":"scandeval.callbacks","text":"scandeval.callbacks<p> source module scandeval.callbacks </p> <p>Callbacks for the Hugging Face Trainer.</p> <p> Classes </p> <ul> <li> <p>NeverLeaveProgressCallback \u2014 Progress callback which never leaves the progress bar.</p> </li> </ul> <p> source class NeverLeaveProgressCallback(**kwargs) </p> <p><p>Bases : ProgressCallback</p></p> <p>Progress callback which never leaves the progress bar.</p> <p>Initialise the callback.</p> <p> Methods </p> <ul> <li> <p>on_train_begin \u2014 Callback actions when training begins.</p> </li> <li> <p>on_step_end \u2014 Callback actions when a training step ends.</p> </li> <li> <p>on_prediction_step \u2014 Callback actions when a prediction step ends.</p> </li> </ul> <p> source method NeverLeaveProgressCallback.on_train_begin(args, state, control, **kwargs) </p> <p>Callback actions when training begins.</p> <p> source method NeverLeaveProgressCallback.on_step_end(args, state, control, **kwargs) </p> <p>Callback actions when a training step ends.</p> <p> source method NeverLeaveProgressCallback.on_prediction_step(args, state, control, eval_dataloader=None, **kwargs) </p> <p>Callback actions when a prediction step ends.</p>"},{"location":"src/scandeval/callbacks/","title":"scandeval.callbacks","text":"scandeval.callbacks<p> docs module scandeval.callbacks </p> <pre><code>\"\"\"Callbacks for the Hugging Face Trainer.\"\"\"\n\nimport sys\nfrom collections.abc import Sized\n\nfrom tqdm.auto import tqdm\nfrom transformers.trainer_callback import ProgressCallback\n\n\nclass NeverLeaveProgressCallback(ProgressCallback):docs\n    \"\"\"Progress callback which never leaves the progress bar.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialise the callback.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.training_bar: tqdm\n        self.prediction_bar: tqdm | None\n\n    def on_train_begin(self, args, state, control, **kwargs):docs\n        \"\"\"Callback actions when training begins.\"\"\"\n        if state.is_local_process_zero:\n            desc = \"Finetuning model\"\n            self.training_bar = tqdm(\n                total=None,\n                leave=False,\n                desc=desc,\n                disable=hasattr(sys, \"_called_from_test\"),\n            )\n        self.current_step = 0\n\n    def on_step_end(self, args, state, control, **kwargs):docs\n        \"\"\"Callback actions when a training step ends.\"\"\"\n        if state.is_local_process_zero:\n            self.training_bar.update(state.global_step - self.current_step)\n            self.current_step = state.global_step\ndocs\n    def on_prediction_step(self, args, state, control, eval_dataloader=None, **kwargs):\n        \"\"\"Callback actions when a prediction step ends.\"\"\"\n        if eval_dataloader is None:\n            return\n        correct_dtype = isinstance(eval_dataloader.dataset, Sized)\n        if state.is_local_process_zero and correct_dtype:\n            if self.prediction_bar is None:\n                desc = \"Evaluating model\"\n                self.prediction_bar = tqdm(\n                    total=len(eval_dataloader),\n                    leave=False,\n                    desc=desc,\n                    disable=hasattr(sys, \"_called_from_test\"),\n                )\n            self.prediction_bar.update(1)\n</code></pre>"},{"location":"api/scandeval/cli/","title":"scandeval.cli","text":"scandeval.cli<p> source module scandeval.cli </p> <p>Command-line interface for benchmarking.</p> <p> Functions </p> <ul> <li> <p>benchmark \u2014 Benchmark pretrained language models on language tasks.</p> </li> </ul> <p> source benchmark(model: tuple[str], dataset: tuple[str], language: tuple[str], model_language: tuple[str], dataset_language: tuple[str], raise_errors: bool, task: tuple[str], batch_size: str, evaluate_train: bool, progress_bar: bool, save_results: bool, cache_dir: str, use_token: bool, token: str, openai_api_key: str | None, prefer_azure: bool, azure_openai_api_key: str | None, azure_openai_endpoint: str | None, azure_openai_api_version: str | None, force: bool, verbose: bool, framework: str | None, device: str | None, trust_remote_code: bool, load_in_4bit: bool | None, use_flash_attention: bool | None, clear_model_cache: bool, only_validation_split: bool, few_shot: bool, num_iterations: int, debug: bool) \u2192 None </p> <p>Benchmark pretrained language models on language tasks.</p>"},{"location":"src/scandeval/cli/","title":"scandeval.cli","text":"scandeval.cli<p> docs module scandeval.cli </p> <pre><code>\"\"\"Command-line interface for benchmarking.\"\"\"\n\nimport click\n\nfrom .benchmarker import Benchmarker\nfrom .dataset_configs import get_all_dataset_configs\nfrom .enums import Device, Framework\nfrom .languages import get_all_languages\nfrom .tasks import get_all_tasks\n\n\n@click.command()\n@click.option(\n    \"--model\",\n    \"-m\",\n    default=None,\n    show_default=True,\n    multiple=True,\n    help=\"\"\"The Hugging Face model ID of the model(s) to be benchmarked. If not\n    specified then all models will be benchmarked, filtered by `model_language` or\n    `language`. The specific model revision to use can be added after the suffix \"@\":\n    \"&lt;model&gt;@v1.0.0\". It can be a branch name, a tag name, or a commit id, and\n    defaults to the latest version if not specified.\"\"\",\n)\n@click.option(\n    \"--task\",\n    \"-t\",\n    default=None,\n    show_default=True,\n    multiple=True,\n    type=click.Choice(list(get_all_tasks().keys())),\n    help=\"The dataset tasks to benchmark the model(s) on.\",\n)\n@click.option(\n    \"--language\",\n    \"-l\",\n    default=[\"all\"],\n    show_default=True,\n    multiple=True,\n    metavar=\"ISO 639-1 LANGUAGE CODE\",\n    type=click.Choice([\"all\"] + list(get_all_languages().keys())),\n    help=\"\"\"The languages to benchmark, both for models and datasets. If \"all\" then all\n    models will be benchmarked on all datasets.\"\"\",\n)\n@click.option(\n    \"--model-language\",\n    \"-ml\",\n    default=None,\n    show_default=True,\n    multiple=True,\n    metavar=\"ISO 639-1 LANGUAGE CODE\",\n    type=click.Choice([\"all\"] + list(get_all_languages().keys())),\n    help=\"\"\"The model languages to benchmark. If not specified then this will use the\n    `language` value.\"\"\",\n)\n@click.option(\n    \"--dataset-language\",\n    \"-dl\",\n    default=None,\n    show_default=True,\n    multiple=True,\n    metavar=\"ISO 639-1 LANGUAGE CODE\",\n    type=click.Choice([\"all\"] + list(get_all_languages().keys())),\n    help=\"\"\"The dataset languages to benchmark. If \"all\" then the models will be\n    benchmarked on all datasets. If not specified then this will use the `language`\n    value.\"\"\",\n)\n@click.option(\n    \"--dataset\",\n    default=None,\n    show_default=True,\n    multiple=True,\n    type=click.Choice(list(get_all_dataset_configs().keys())),\n    help=\"\"\"The name of the benchmark dataset. We recommend to use the `task` and\n    `language` options instead of this option.\"\"\",\n)\n@click.option(\n    \"--batch-size\",\n    default=\"32\",\n    type=click.Choice([\"1\", \"2\", \"4\", \"8\", \"16\", \"32\"]),\n    help=\"The batch size to use.\",\n)\n@click.option(\n    \"--evaluate-train/--no-evaluate-train\",\n    default=False,\n    show_default=True,\n    help=\"Whether to evaluate the model on the training set.\",\n)\n@click.option(\n    \"--progress-bar/--no-progress-bar\",\n    default=True,\n    show_default=True,\n    help=\"Whether to show a progress bar.\",\n)\n@click.option(\n    \"--raise-errors/--no-raise-errors\",\n    default=False,\n    show_default=True,\n    help=\"Whether to raise errors instead of skipping the evaluation.\",\n)\n@click.option(\n    \"--verbose/--no-verbose\",\n    \"-v/-nv\",\n    default=False,\n    show_default=True,\n    help=\"Whether extra input should be outputted during benchmarking\",\n)\n@click.option(\n    \"--save-results/--no-save-results\",\n    \"-s/-ns\",\n    default=True,\n    show_default=True,\n    help=\"Whether results should not be stored to disk.\",\n)\n@click.option(\n    \"--cache-dir\",\n    \"-c\",\n    default=\".scandeval_cache\",\n    show_default=True,\n    help=\"The directory where models are datasets are cached.\",\n)\n@click.option(\n    \"--token\",\n    type=str,\n    default=\"\",\n    show_default=True,\n    help=\"\"\"The authentication token for the Hugging Face Hub. If specified then the\n    `--token` flag will be set to True.\"\"\",\n)\n@click.option(\n    \"--use-token/--no-use-token\",\n    default=True,\n    show_default=True,\n    help=\"\"\"Whether an authentication token should be used, enabling evaluation of\n    private models. Requires that you are logged in via the `huggingface-cli login`\n    command.\"\"\",\n)\n@click.option(\n    \"--openai-api-key\",\n    type=str,\n    default=None,\n    show_default=True,\n    help=\"\"\"The API key for the OpenAI API. If not specified then the environment\n    variable `OPENAI_API_KEY` will be used.\"\"\",\n)\n@click.option(\n    \"--prefer-azure/--no-prefer-azure\",\n    default=False,\n    show_default=True,\n    help=\"\"\"Whether to prefer the Azure OpenAI API over the OpenAI API, if both are\n    available.\"\"\",\n)\n@click.option(\n    \"--azure-openai-api-key\",\n    type=str,\n    default=None,\n    show_default=True,\n    help=\"\"\"The API key for the Azure OpenAI API. If not specified then the environment\n    variable `AZURE_OPENAI_API_KEY` will be used.\"\"\",\n)\n@click.option(\n    \"--azure-openai-endpoint\",\n    type=str,\n    default=None,\n    show_default=True,\n    help=\"\"\"The endpoint for the Azure OpenAI API. If not specified then the environment\n    variable `AZURE_OPENAI_ENDPOINT` will be used.\"\"\",\n)\n@click.option(\n    \"--azure-openai-api-version\",\n    type=str,\n    default=None,\n    show_default=True,\n    help=\"\"\"The api version for the Azure OpenAI API, e.g. \"2023-12-01-preview\". If\n            None then the environment varaible `AZURE_OPENAI_API_VERSION` will be used.\"\"\",\n)\n@click.option(\n    \"--force/--no-force\",\n    \"-f\",\n    default=False,\n    show_default=True,\n    help=\"\"\"Whether to force evaluation of models which have already been evaluated,\n    with scores lying in the 'scandeval_benchmark_results.jsonl' file.\"\"\",\n)\n@click.option(\n    \"--framework\",\n    default=None,\n    show_default=True,\n    type=click.Choice([framework.lower() for framework in Framework.__members__]),\n    help=\"\"\"The model framework to use. Only relevant if `model` refers to a local\n    path. Otherwise, the framework will be set automatically.\"\"\",\n)\n@click.option(\n    \"--device\",\n    default=None,\n    show_default=True,\n    type=click.Choice([device.lower() for device in Device.__members__]),\n    help=\"\"\"The device to use for evaluation. If not specified then the device will be\n    set automatically.\"\"\",\n)\n@click.option(\n    \"--trust-remote-code/--no-trust-remote-code\",\n    default=False,\n    show_default=True,\n    help=\"\"\"Whether to trust remote code. Only set this flag if you trust the supplier\n    of the model.\"\"\",\n)\n@click.option(\n    \"--load-in-4bit/--no-load-in-4bit\",\n    default=None,\n    show_default=True,\n    help=\"\"\"Whether to load the model in 4-bit precision. If not specified then the\n    model will be loaded in 4-bit precision if possible.\"\"\",\n)\n@click.option(\n    \"--use-flash-attention/--no-use-flash-attention\",\n    default=None,\n    show_default=True,\n    help=\"\"\"Whether to use Flash Attention. If not specified then the model will use\n    Flash Attention for generative models if a CUDA GPU is available and `flash-attn`\n    or `vllm-flash-attn` are installed.\"\"\",\n)\n@click.option(\n    \"--clear-model-cache/--no-clear-model-cache\",\n    default=False,\n    show_default=True,\n    help=\"\"\"Whether to clear the model cache after benchmarking each model. Note that\n    this will only remove the model files, and not the cached model outputs (which\n    don't take up a lot of disk space). This is useful when benchmarking many models,\n    to avoid running out of disk space.\"\"\",\n)\n@click.option(\n    \"--only-validation-split/--no-only-validation-split\",\n    default=False,\n    show_default=True,\n    help=\"\"\"Whether to only evaluate on the validation split. This is useful if you're\n    optimising hyperparameters, to avoid overfitting to the test sets.\"\"\",\n)\n@click.option(\n    \"--few-shot/--zero-shot\",\n    default=True,\n    show_default=True,\n    help=\"Whether to only evaluate the model using few-shot evaluation. Only relevant \"\n    \"if the model is generative.\",\n)\n@click.option(\n    \"--num-iterations\",\n    default=10,\n    show_default=True,\n    help=\"\"\"The number of times each model should be evaluated. This is only meant to\n    be used for power users, and scores will not be allowed on the leaderboards if this\n    is changed.\"\"\",\n)\n@click.option(\n    \"--debug/--no-debug\",\n    default=False,\n    show_default=True,\n    help=\"Whether to run the benchmark in debug mode. This prints out extra information \"\n    \"and stores all outputs to the current working directory. Only relevant if the \"\n    \"model is generative.\",\n)\ndef benchmark(docs\n    model: tuple[str],\n    dataset: tuple[str],\n    language: tuple[str],\n    model_language: tuple[str],\n    dataset_language: tuple[str],\n    raise_errors: bool,\n    task: tuple[str],\n    batch_size: str,\n    evaluate_train: bool,\n    progress_bar: bool,\n    save_results: bool,\n    cache_dir: str,\n    use_token: bool,\n    token: str,\n    openai_api_key: str | None,\n    prefer_azure: bool,\n    azure_openai_api_key: str | None,\n    azure_openai_endpoint: str | None,\n    azure_openai_api_version: str | None,\n    force: bool,\n    verbose: bool,\n    framework: str | None,\n    device: str | None,\n    trust_remote_code: bool,\n    load_in_4bit: bool | None,\n    use_flash_attention: bool | None,\n    clear_model_cache: bool,\n    only_validation_split: bool,\n    few_shot: bool,\n    num_iterations: int,\n    debug: bool,\n) -&gt; None:\n    \"\"\"Benchmark pretrained language models on language tasks.\"\"\"\n    # Set up language variables\n    models = None if len(model) == 0 else list(model)\n    datasets = None if len(dataset) == 0 else list(dataset)\n    languages: list[str] = list(language)\n    model_languages = None if len(model_language) == 0 else list(model_language)\n    dataset_languages = None if len(dataset_language) == 0 else list(dataset_language)\n    tasks = None if len(task) == 0 else list(task)\n    batch_size_int = int(batch_size)\n    device = Device[device.upper()] if device is not None else None\n    token_str_bool: str | bool = token if token != \"\" else use_token\n\n    # Initialise the benchmarker class\n    benchmarker = Benchmarker(\n        language=languages,\n        model_language=model_languages,\n        dataset_language=dataset_languages,\n        task=tasks,\n        dataset=datasets,\n        batch_size=batch_size_int,\n        progress_bar=progress_bar,\n        save_results=save_results,\n        evaluate_train=evaluate_train,\n        raise_errors=raise_errors,\n        verbose=verbose,\n        token=token_str_bool,\n        openai_api_key=openai_api_key,\n        prefer_azure=prefer_azure,\n        azure_openai_api_key=azure_openai_api_key,\n        azure_openai_endpoint=azure_openai_endpoint,\n        azure_openai_api_version=azure_openai_api_version,\n        force=force,\n        cache_dir=cache_dir,\n        framework=framework,\n        device=device,\n        trust_remote_code=trust_remote_code,\n        load_in_4bit=load_in_4bit,\n        use_flash_attention=use_flash_attention,\n        clear_model_cache=clear_model_cache,\n        only_validation_split=only_validation_split,\n        few_shot=few_shot,\n        num_iterations=num_iterations,\n        debug=debug,\n        run_with_cli=True,\n    )\n\n    # Perform the benchmark evaluation\n    benchmarker(model=models)\n\n\nif __name__ == \"__main__\":\n    benchmark()\n</code></pre>"},{"location":"api/scandeval/config/","title":"scandeval.config","text":"scandeval.config<p> source module scandeval.config </p> <p>Configuration classes used throughout the project.</p> <p> Classes </p> <ul> <li> <p>MetricConfig \u2014 Configuration for a metric.</p> </li> <li> <p>Task \u2014 A dataset task.</p> </li> <li> <p>Language \u2014 A benchmarkable language.</p> </li> <li> <p>BenchmarkConfig \u2014 General benchmarking configuration, across datasets and models.</p> </li> <li> <p>DatasetConfig \u2014 Configuration for a dataset.</p> </li> <li> <p>ModelConfig \u2014 Configuration for a model.</p> </li> </ul> <p> source dataclass MetricConfig(name: str, pretty_name: str, huggingface_id: str, results_key: str, compute_kwargs: dict[str, Any] = field(default_factory=dict), postprocessing_fn: Callable[[float], tuple[float, str]] = field(default_factory=lambda: lambda raw_score: (100 * raw_score, f'{raw_score:.2%}'))) </p> <p>Configuration for a metric.</p> <p> Attributes </p> <ul> <li> <p>name :  str \u2014</p> <p>The name of the metric.</p> </li> <li> <p>pretty_name :  str \u2014</p> <p>A longer prettier name for the metric, which allows cases and spaces. Used for logging.</p> </li> <li> <p>huggingface_id :  str \u2014</p> <p>The Hugging Face ID of the metric.</p> </li> <li> <p>results_key :  str \u2014</p> <p>The name of the key used to extract the metric scores from the results dictionary.</p> </li> <li> <p>compute_kwargs :  dict[str, Any] \u2014</p> <p>Keyword arguments to pass to the metric's compute function. Defaults to an empty dictionary.</p> </li> <li> <p>postprocessing_fn :  Callable[[float], tuple[float, str]] \u2014</p> <p>A function to apply to the metric scores after they are computed, taking the score to the postprocessed score along with its string representation. Defaults to x -&gt; (100 * x, f\"{x:.2%}\").</p> </li> </ul> <p> source dataclass Task(name: str, supertask: str, metrics: list[MetricConfig], labels: list[str]) </p> <p>A dataset task.</p> <p> Attributes </p> <ul> <li> <p>name :  str \u2014</p> <p>The name of the task.</p> </li> <li> <p>supertask :  str \u2014</p> <p>The supertask of the task, describing the overall type of task.</p> </li> <li> <p>metrics :  list[MetricConfig] \u2014</p> <p>The metrics used to evaluate the task.</p> </li> <li> <p>labels :  list[str] \u2014</p> <p>The labels used in the task.</p> </li> </ul> <p> source dataclass Language(code: str, name: str) </p> <p>A benchmarkable language.</p> <p> Attributes </p> <ul> <li> <p>code :  str \u2014</p> <p>The ISO 639-1 language code of the language.</p> </li> <li> <p>name :  str \u2014</p> <p>The name of the language.</p> </li> </ul> <p> source dataclass BenchmarkConfig(model_languages: list[Language], dataset_languages: list[Language], tasks: list[Task], datasets: list[str], framework: Framework | None, batch_size: int, raise_errors: bool, cache_dir: str, evaluate_train: bool, token: bool | str | None, openai_api_key: str | None, azure_openai_api_key: str | None, azure_openai_endpoint: str | None, azure_openai_api_version: str | None, force: bool, progress_bar: bool, save_results: bool, device: torch.device, verbose: bool, trust_remote_code: bool, load_in_4bit: bool | None, use_flash_attention: bool | None, clear_model_cache: bool, only_validation_split: bool, few_shot: bool, num_iterations: int, debug: bool, run_with_cli: bool) </p> <p>General benchmarking configuration, across datasets and models.</p> <p> Attributes </p> <ul> <li> <p>model_languages :  list[Language] \u2014</p> <p>The languages of the models to benchmark.</p> </li> <li> <p>dataset_languages :  list[Language] \u2014</p> <p>The languages of the datasets in the benchmark.</p> </li> <li> <p>tasks :  list[Task] \u2014</p> <p>The tasks benchmark the model(s) on.</p> </li> <li> <p>datasets :  list[str] \u2014</p> <p>The datasets to benchmark on.</p> </li> <li> <p>framework :  Framework | None \u2014</p> <p>The framework of the models to benchmark. If None then the framework will be inferred.</p> </li> <li> <p>batch_size :  int \u2014</p> <p>The batch size to use.</p> </li> <li> <p>raise_errors :  bool \u2014</p> <p>Whether to raise errors instead of skipping them.</p> </li> <li> <p>cache_dir :  str \u2014</p> <p>Directory to store cached models and datasets.</p> </li> <li> <p>evaluate_train :  bool \u2014</p> <p>Whether to evaluate on the training set.</p> </li> <li> <p>token :  bool | str | None \u2014</p> <p>The authentication token for the Hugging Face Hub. If a boolean value is specified then the token will be fetched from the Hugging Face CLI, where the user has logged in through <code>huggingface-cli login</code>. If a string is specified then it will be used as the token.</p> </li> <li> <p>openai_api_key :  str | None \u2014</p> <p>The API key for the OpenAI API. If None then OpenAI models will not be benchmarked.</p> </li> <li> <p>azure_openai_api_key :  str | None \u2014</p> <p>The API key for the Azure OpenAI API. If None then Azure OpenAI models will not be benchmarked.</p> </li> <li> <p>azure_openai_endpoint :  str | None \u2014</p> <p>The endpoint for the Azure OpenAI API. If None then Azure OpenAI models will not be benchmarked.</p> </li> <li> <p>azure_openai_api_version :  str | None \u2014</p> <p>The api version for the Azure OpenAI API, e.g. \"2023-12-01-preview\". If None then Azure OpenAI models will not be benchmarked.</p> </li> <li> <p>force :  bool \u2014</p> <p>Whether to force the benchmark to run even if the results are already cached.</p> </li> <li> <p>progress_bar :  bool \u2014</p> <p>Whether to show a progress bar.</p> </li> <li> <p>save_results :  bool \u2014</p> <p>Whether to save the benchmark results to 'scandeval_benchmark_results.json'.</p> </li> <li> <p>device :  torch.device \u2014</p> <p>The device to use for benchmarking.</p> </li> <li> <p>verbose :  bool \u2014</p> <p>Whether to print verbose output.</p> </li> <li> <p>trust_remote_code :  bool \u2014</p> <p>Whether to trust remote code when loading models from the Hugging Face Hub.</p> </li> <li> <p>load_in_4bit :  bool | None \u2014</p> <p>Whether to load models in 4-bit precision. If None then this will be done if CUDA is available and the model is a decoder model.</p> </li> <li> <p>use_flash_attention :  bool | None \u2014</p> <p>Whether to use Flash Attention. If None then this will be used for generative models.</p> </li> <li> <p>clear_model_cache :  bool \u2014</p> <p>Whether to clear the model cache after benchmarking each model.</p> </li> <li> <p>only_validation_split :  bool \u2014</p> <p>Whether to only evaluate on the validation split.</p> </li> <li> <p>few_shot :  bool \u2014</p> <p>Whether to only evaluate the model using few-shot evaluation. Only relevant if the model is generative.</p> </li> <li> <p>num_iterations :  int \u2014</p> <p>The number of iterations each model should be evaluated for.</p> </li> <li> <p>debug :  bool \u2014</p> <p>Whether to run the benchmark in debug mode.</p> </li> <li> <p>run_with_cli :  bool \u2014</p> <p>Whether the benchmark is being run with the CLI.</p> </li> </ul> <p> source dataclass DatasetConfig(name: str, pretty_name: str, huggingface_id: str, task: Task, languages: list[Language], prompt_template: str, max_generated_tokens: int, prompt_prefix: str, num_few_shot_examples: int, instruction_prompt: str, prompt_label_mapping: dict[str, str] = field(default_factory=dict), unofficial: bool = False) </p> <p>Configuration for a dataset.</p> <p> Attributes </p> <ul> <li> <p>name :  str \u2014</p> <p>The name of the dataset. Must be lower case with no spaces.</p> </li> <li> <p>pretty_name :  str \u2014</p> <p>A longer prettier name for the dataset, which allows cases and spaces. Used for logging.</p> </li> <li> <p>huggingface_id :  str \u2014</p> <p>The Hugging Face ID of the dataset.</p> </li> <li> <p>task :  Task \u2014</p> <p>The task of the dataset.</p> </li> <li> <p>languages :  list[Language] \u2014</p> <p>The ISO 639-1 language codes of the entries in the dataset.</p> </li> <li> <p>id2label :  dict[int, str] \u2014</p> <p>The mapping from ID to label.</p> </li> <li> <p>label2id :  dict[str, int] \u2014</p> <p>The mapping from label to ID.</p> </li> <li> <p>num_labels :  int \u2014</p> <p>The number of labels in the dataset.</p> </li> <li> <p>prompt_template :  str \u2014</p> <p>The template for the prompt to use when benchmarking the dataset using few-shot evaluation.</p> </li> <li> <p>max_generated_tokens :  int \u2014</p> <p>The maximum number of tokens to generate when benchmarking the dataset using few-shot evaluation.</p> </li> <li> <p>prompt_prefix :  str \u2014</p> <p>The prefix to use in the few-shot prompt.</p> </li> <li> <p>num_few_shot_examples :  int \u2014</p> <p>The number of examples to use when benchmarking the dataset using few-shot evaluation. For a classification task, these will be drawn evenly from each label.</p> </li> <li> <p>instruction_prompt :  str \u2014</p> <p>The prompt to use when benchmarking the dataset using instruction-based evaluation.</p> </li> <li> <p>prompt_label_mapping :  optional \u2014</p> <p>A mapping from the labels to another phrase which is used as a substitute for the label in few-shot evaluation. Defaults to an empty dictionary.</p> </li> <li> <p>unofficial :  optional \u2014</p> <p>Whether the dataset is unofficial. Defaults to False.</p> </li> </ul> <p> source property DatasetConfig.id2label: dict[int, str] </p> <p>The mapping from ID to label.</p> <p> source property DatasetConfig.label2id: dict[str, int] </p> <p>The mapping from label to ID.</p> <p> source property DatasetConfig.num_labels: int </p> <p>The number of labels in the dataset.</p> <p> source dataclass ModelConfig(model_id: str, revision: str, framework: Framework, task: str, languages: list[Language], model_type: ModelType | str, model_cache_dir: str, adapter_base_model_id: str | None) </p> <p>Configuration for a model.</p> <p> Attributes </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The ID of the model.</p> </li> <li> <p>revision :  str \u2014</p> <p>The revision of the model.</p> </li> <li> <p>framework :  Framework \u2014</p> <p>The framework of the model.</p> </li> <li> <p>task :  str \u2014</p> <p>The task that the model was trained on.</p> </li> <li> <p>languages :  list[Language] \u2014</p> <p>The languages of the model.</p> </li> <li> <p>model_type :  ModelType | str \u2014</p> <p>The type of the model.</p> </li> <li> <p>model_cache_dir :  str \u2014</p> <p>The directory to cache the model in.</p> </li> <li> <p>adapter_base_model_id :  str | None \u2014</p> <p>The model ID of the base model if the model is an adapter model. Can be None if the model is not an adapter model.</p> </li> </ul>"},{"location":"src/scandeval/config/","title":"scandeval.config","text":"scandeval.config<p> docs module scandeval.config </p> <pre><code>\"\"\"Configuration classes used throughout the project.\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import TYPE_CHECKING, Any, Callable\n\nimport torch\n\nif TYPE_CHECKING:\n    from .enums import Framework, ModelType\n\n\n@dataclass\nclass MetricConfig:docs\n    \"\"\"Configuration for a metric.\n\n    Attributes:\n        name:\n            The name of the metric.\n        pretty_name:\n            A longer prettier name for the metric, which allows cases and spaces. Used\n            for logging.\n        huggingface_id:\n            The Hugging Face ID of the metric.\n        results_key:\n            The name of the key used to extract the metric scores from the results\n            dictionary.\n        compute_kwargs:\n            Keyword arguments to pass to the metric's compute function. Defaults to\n            an empty dictionary.\n        postprocessing_fn:\n            A function to apply to the metric scores after they are computed, taking\n            the score to the postprocessed score along with its string representation.\n            Defaults to x -&gt; (100 * x, f\"{x:.2%}\").\n    \"\"\"\n\n    name: str\n    pretty_name: str\n    huggingface_id: str\n    results_key: str\n    compute_kwargs: dict[str, Any] = field(default_factory=dict)\n    postprocessing_fn: Callable[[float], tuple[float, str]] = field(\n        default_factory=lambda: lambda raw_score: (100 * raw_score, f\"{raw_score:.2%}\")\n    )\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hash of the metric configuration.\"\"\"\n        return hash(self.name)\n\n\n@dataclass\nclass Task:docs\n    \"\"\"A dataset task.\n\n    Attributes:\n        name:\n            The name of the task.\n        supertask:\n            The supertask of the task, describing the overall type of task.\n        metrics:\n            The metrics used to evaluate the task.\n        labels:\n            The labels used in the task.\n    \"\"\"\n\n    name: str\n    supertask: str\n    metrics: list[MetricConfig]\n    labels: list[str]\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hash of the task.\"\"\"\n        return hash(self.name)\n\n\n@dataclass\nclass Language:docs\n    \"\"\"A benchmarkable language.\n\n    Attributes:\n        code:\n            The ISO 639-1 language code of the language.\n        name:\n            The name of the language.\n    \"\"\"\n\n    code: str\n    name: str\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hash of the language.\"\"\"\n        return hash(self.code)\n\n\n@dataclass\nclass BenchmarkConfig:docs\n    \"\"\"General benchmarking configuration, across datasets and models.\n\n    Attributes:\n        model_languages:\n            The languages of the models to benchmark.\n        dataset_languages:\n            The languages of the datasets in the benchmark.\n        tasks:\n            The tasks benchmark the model(s) on.\n        datasets:\n            The datasets to benchmark on.\n        framework:\n            The framework of the models to benchmark. If None then the framework will be\n            inferred.\n        batch_size:\n            The batch size to use.\n        raise_errors:\n            Whether to raise errors instead of skipping them.\n        cache_dir:\n            Directory to store cached models and datasets.\n        evaluate_train:\n            Whether to evaluate on the training set.\n        token:\n            The authentication token for the Hugging Face Hub. If a boolean value is\n            specified then the token will be fetched from the Hugging Face CLI, where\n            the user has logged in through `huggingface-cli login`. If a string is\n            specified then it will be used as the token.\n        openai_api_key:\n            The API key for the OpenAI API. If None then OpenAI models will not be\n            benchmarked.\n        azure_openai_api_key:\n            The API key for the Azure OpenAI API. If None then Azure OpenAI models will\n            not be benchmarked.\n        azure_openai_endpoint:\n            The endpoint for the Azure OpenAI API. If None then Azure OpenAI models will\n            not be benchmarked.\n        azure_openai_api_version:\n            The api version for the Azure OpenAI API, e.g. \"2023-12-01-preview\". If\n            None then Azure OpenAI models will not be benchmarked.\n        force:\n            Whether to force the benchmark to run even if the results are already\n            cached.\n        progress_bar:\n            Whether to show a progress bar.\n        save_results:\n            Whether to save the benchmark results to 'scandeval_benchmark_results.json'.\n        device:\n            The device to use for benchmarking.\n        verbose:\n            Whether to print verbose output.\n        trust_remote_code:\n            Whether to trust remote code when loading models from the Hugging Face Hub.\n        load_in_4bit:\n            Whether to load models in 4-bit precision. If None then this will be done\n            if CUDA is available and the model is a decoder model.\n        use_flash_attention:\n            Whether to use Flash Attention. If None then this will be used for\n            generative models.\n        clear_model_cache:\n            Whether to clear the model cache after benchmarking each model.\n        only_validation_split:\n            Whether to only evaluate on the validation split.\n        few_shot:\n            Whether to only evaluate the model using few-shot evaluation. Only relevant\n            if the model is generative.\n        num_iterations:\n            The number of iterations each model should be evaluated for.\n        debug:\n            Whether to run the benchmark in debug mode.\n        run_with_cli:\n            Whether the benchmark is being run with the CLI.\n    \"\"\"\n\n    model_languages: list[Language]\n    dataset_languages: list[Language]\n    tasks: list[Task]\n    datasets: list[str]\n    framework: \"Framework | None\"\n    batch_size: int\n    raise_errors: bool\n    cache_dir: str\n    evaluate_train: bool\n    token: bool | str | None\n    openai_api_key: str | None\n    azure_openai_api_key: str | None\n    azure_openai_endpoint: str | None\n    azure_openai_api_version: str | None\n    force: bool\n    progress_bar: bool\n    save_results: bool\n    device: torch.device\n    verbose: bool\n    trust_remote_code: bool\n    load_in_4bit: bool | None\n    use_flash_attention: bool | None\n    clear_model_cache: bool\n    only_validation_split: bool\n    few_shot: bool\n    num_iterations: int\n    debug: bool\n    run_with_cli: bool\n\n\n@dataclass\nclass DatasetConfig:docs\n    \"\"\"Configuration for a dataset.\n\n    Attributes:\n        name:\n            The name of the dataset. Must be lower case with no spaces.\n        pretty_name:\n            A longer prettier name for the dataset, which allows cases and spaces. Used\n            for logging.\n        huggingface_id:\n            The Hugging Face ID of the dataset.\n        task:\n            The task of the dataset.\n        languages:\n            The ISO 639-1 language codes of the entries in the dataset.\n        id2label:\n            The mapping from ID to label.\n        label2id:\n            The mapping from label to ID.\n        num_labels:\n            The number of labels in the dataset.\n        prompt_template:\n            The template for the prompt to use when benchmarking the dataset using\n            few-shot evaluation.\n        max_generated_tokens:\n            The maximum number of tokens to generate when benchmarking the dataset\n            using few-shot evaluation.\n        prompt_prefix:\n            The prefix to use in the few-shot prompt.\n        num_few_shot_examples:\n            The number of examples to use when benchmarking the dataset using few-shot\n            evaluation. For a classification task, these will be drawn evenly from\n            each label.\n        instruction_prompt:\n            The prompt to use when benchmarking the dataset using instruction-based\n            evaluation.\n        prompt_label_mapping (optional):\n            A mapping from the labels to another phrase which is used as a substitute\n            for the label in few-shot evaluation. Defaults to an empty dictionary.\n        unofficial (optional):\n            Whether the dataset is unofficial. Defaults to False.\n    \"\"\"\n\n    name: str\n    pretty_name: str\n    huggingface_id: str\n    task: Task\n    languages: list[Language]\n    prompt_template: str\n    max_generated_tokens: int\n    prompt_prefix: str\n    num_few_shot_examples: int\n    instruction_prompt: str\n    prompt_label_mapping: dict[str, str] = field(default_factory=dict)\n    unofficial: bool = False\n\n    @property\n    def id2label(self) -&gt; dict[int, str]:docs\n        \"\"\"The mapping from ID to label.\"\"\"\n        return {idx: label for idx, label in enumerate(self.task.labels)}\n\n    @property\n    def label2id(self) -&gt; dict[str, int]:docs\n        \"\"\"The mapping from label to ID.\"\"\"\n        return {label: i for i, label in enumerate(self.task.labels)}\n\n    @property\n    def num_labels(self) -&gt; int:docs\n        \"\"\"The number of labels in the dataset.\"\"\"\n        return len(self.task.labels)\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hash of the dataset configuration.\"\"\"\n        return hash(self.name)\n\n\n@dataclass\nclass ModelConfig:docs\n    \"\"\"Configuration for a model.\n\n    Attributes:\n        model_id:\n            The ID of the model.\n        revision:\n            The revision of the model.\n        framework:\n            The framework of the model.\n        task:\n            The task that the model was trained on.\n        languages:\n            The languages of the model.\n        model_type:\n            The type of the model.\n        model_cache_dir:\n            The directory to cache the model in.\n        adapter_base_model_id:\n            The model ID of the base model if the model is an adapter model. Can be None\n            if the model is not an adapter model.\n    \"\"\"\n\n    model_id: str\n    revision: str\n    framework: \"Framework\"\n    task: str\n    languages: list[Language]\n    model_type: \"ModelType | str\"\n    model_cache_dir: str\n    adapter_base_model_id: str | None\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hash of the model configuration.\"\"\"\n        return hash(self.model_id)\n</code></pre>"},{"location":"api/scandeval/dataset_configs/","title":"scandeval.dataset_configs","text":"scandeval.dataset_configs<p> source module scandeval.dataset_configs </p> <p>All dataset configurations used in ScandEval.</p> <p> Functions </p> <ul> <li> <p>get_all_dataset_configs \u2014 Get a mapping of all the dataset configurations.</p> </li> <li> <p>get_dataset_config \u2014 Get the dataset configuration for a dataset.</p> </li> </ul> <p> source get_all_dataset_configs() \u2192 dict[str, DatasetConfig] </p> <p>Get a mapping of all the dataset configurations.</p> <p> Returns </p> <ul> <li> <p>dict[str, DatasetConfig] \u2014 A mapping between names of datasets and their configurations.</p> </li> </ul> <p> source get_dataset_config(dataset_name: str) \u2192 DatasetConfig </p> <p>Get the dataset configuration for a dataset.</p> <p> Parameters </p> <ul> <li> <p>dataset_name :  str \u2014</p> <p>The name of the dataset.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetConfig \u2014 The dataset configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014</p> <p>If the dataset is not found.</p> </li> </ul>"},{"location":"src/scandeval/dataset_configs/","title":"scandeval.dataset_configs","text":"scandeval.dataset_configs<p> docs module scandeval.dataset_configs </p> <pre><code>\"\"\"All dataset configurations used in ScandEval.\"\"\"\n\nfrom .config import DatasetConfig\nfrom .languages import DA, DE, EN, FO, IS, NB, NL, NN, NO, SV, get_all_languages\nfrom .tasks import COMMON_SENSE, KNOW, LA, MCRC, NER, RC, SENT, SPEED, SUMM\n\n\ndef get_all_dataset_configs() -&gt; dict[str, DatasetConfig]:docs\n    \"\"\"Get a mapping of all the dataset configurations.\n\n    Returns:\n        A mapping between names of datasets and their configurations.\n    \"\"\"\n    dataset_configs = [\n        cfg for cfg in globals().values() if isinstance(cfg, DatasetConfig)\n    ]\n    assert len(dataset_configs) == len({cfg.name for cfg in dataset_configs}), (\n        \"There are duplicate dataset configurations. Please ensure that each dataset \"\n        \"has a unique name.\"\n    )\n    return {cfg.name: cfg for cfg in dataset_configs}\n\n\ndef get_dataset_config(dataset_name: str) -&gt; DatasetConfig:docs\n    \"\"\"Get the dataset configuration for a dataset.\n\n    Args:\n        dataset_name:\n            The name of the dataset.\n\n    Returns:\n        The dataset configuration.\n\n    Raises:\n        ValueError:\n            If the dataset is not found.\n    \"\"\"\n    # Get mapping of all dataset configs\n    dataset_configs = get_all_dataset_configs()\n\n    # If there are no matches for the dataset name, raise an error\n    if dataset_name not in dataset_configs:\n        raise ValueError(f\"No dataset config found for dataset {dataset_name}.\")\n\n    # Otherwise, return the dataset configuration\n    return dataset_configs[dataset_name]\n\n\n### SENTIMENT DATASETS ###\n\nSWEREC_CONFIG = DatasetConfig(\n    name=\"swerec\",\n    pretty_name=\"the truncated version of the Swedish sentiment classification \"\n    \"dataset SweReC\",\n    huggingface_id=\"ScandEval/swerec-mini\",\n    task=SENT,\n    languages=[SV],\n    prompt_prefix=\"F\u00f6ljande \u00e4r recensioner och deras sentiment, som kan vara \"\n    \"'positiv', 'neutral' eller 'negativ'.\",\n    prompt_template=\"Recension: {text}\\nSentiment: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positiv\", neutral=\"neutral\", negative=\"negativ\"\n    ),\n    instruction_prompt=\"Recension: {text}\\n\\nKlassificera sentimentet i recensionen. \"\n    \"Svara med 'positiv', 'neutral' eller 'negativ'.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nANGRY_TWEETS_CONFIG = DatasetConfig(\n    name=\"angry-tweets\",\n    pretty_name=\"the truncated version of the Danish sentiment classification \"\n    \"dataset AngryTweets\",\n    huggingface_id=\"ScandEval/angry-tweets-mini\",\n    task=SENT,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er tweets og deres sentiment, som kan v\u00e6re 'positiv', \"\n    \"'neutral' eller 'negativ'.\",\n    prompt_template=\"Tweet: {text}\\nSentiment: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positiv\", neutral=\"neutral\", negative=\"negativ\"\n    ),\n    instruction_prompt=\"Tweet: {text}\\n\\nKlassificer sentimentet i tweetet. Svar kun \"\n    \"med 'positiv', 'neutral' eller 'negativ'.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nNOREC_CONFIG = DatasetConfig(\n    name=\"norec\",\n    pretty_name=\"the truncated version of the Norwegian sentiment classification \"\n    \"dataset NoReC\",\n    huggingface_id=\"ScandEval/norec-mini\",\n    task=SENT,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"F\u00f8lgende er anmeldelser og deres sentiment, som kan v\u00e6re 'positiv', \"\n    \"'n\u00f8ytral' eller 'negativ'.\",\n    prompt_template=\"Anmeldelse: {text}\\nSentiment: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positiv\", neutral=\"n\u00f8ytral\", negative=\"negativ\"\n    ),\n    instruction_prompt=\"Anmeldelse: {text}\\n\\nKlassifiser sentimentet i anmeldelsen. \"\n    \"Svar med 'positiv', 'n\u00f8ytral' eller 'negativ'.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nHOTTER_AND_COLDER_SENTIMENT_CONFIG = DatasetConfig(\n    name=\"hotter-and-colder-sentiment\",\n    pretty_name=\"the sentiment classification part of the Icelandic dataset Hotter \"\n    \"and Colder\",\n    huggingface_id=\"ScandEval/hotter-and-colder-sentiment\",\n    task=SENT,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru yfirfer\u00f0ir \u00e1samt lyndisgildi \u00feeirra, sem getur \"\n    \"veri\u00f0 'j\u00e1kv\u00e6tt', 'hlutlaust' e\u00f0a 'neikv\u00e6tt'.\",\n    prompt_template=\"Yfirfer\u00f0: {text}\\nLyndi: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"j\u00e1kv\u00e6tt\", neutral=\"hlutlaust\", negative=\"neikv\u00e6tt\"\n    ),\n    instruction_prompt=\"Texti: {text}\\n\\nFlokka\u00f0u tilfinninguna \u00ed textanum. \"\n    \"Svara\u00f0u me\u00f0 'j\u00e1kv\u00e6tt', 'hlutlaust' e\u00f0a 'neikv\u00e6tt'.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nSB10K_CONFIG = DatasetConfig(\n    name=\"sb10k\",\n    pretty_name=\"the truncated version of the German sentiment classification \"\n    \"dataset SB10k\",\n    huggingface_id=\"ScandEval/sb10k-mini\",\n    task=SENT,\n    languages=[DE],\n    prompt_prefix=\"Im Folgenden sind Tweets und ihre Stimmung aufgef\u00fchrt, die \"\n    \"'positiv', 'neutral' oder 'negativ' sein kann.\",\n    prompt_template=\"Tweet: {text}\\nStimmungslage: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positiv\", neutral=\"neutral\", negative=\"negativ\"\n    ),\n    instruction_prompt=\"Tweet: {text}\\n\\nKlassifizieren Sie die Stimmung im Tweet. \"\n    \"Antworten Sie mit 'positiv', 'neutral' oder 'negativ'.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nDUTCH_SOCIAL_CONFIG = DatasetConfig(\n    name=\"dutch-social\",\n    pretty_name=\"the truncated version of the Dutch sentiment classification \"\n    \"dataset Dutch Social\",\n    huggingface_id=\"ScandEval/dutch-social-mini\",\n    task=SENT,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan tweets en hun sentiment, dat 'positief', \"\n    \"'neutraal' of 'negatief' kan zijn.\",\n    prompt_template=\"Tweet: {text}\\nSentiment: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positief\", neutral=\"neutraal\", negative=\"negatief\"\n    ),\n    instruction_prompt=\"Tweet: {text}\\n\\nClassificeer het sentiment in de tweet. \"\n    \"Antwoord met 'positief', 'neutraal' of 'negatief'.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nSST5_CONFIG = DatasetConfig(\n    name=\"sst5\",\n    pretty_name=\"the truncated version of the English sentiment classification \"\n    \"dataset SST5\",\n    huggingface_id=\"ScandEval/sst5-mini\",\n    task=SENT,\n    languages=[EN],\n    prompt_prefix=\"The following are texts and their sentiment, which can be \"\n    \"'positive', 'neutral' or 'negative'.\",\n    prompt_template=\"Text: {text}\\nSentiment: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positive\", neutral=\"neutral\", negative=\"negative\"\n    ),\n    instruction_prompt=\"Text: {text}\\n\\nClassify the sentiment in the text. Answer \"\n    \"with 'positive', 'neutral' or 'negative'.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nFOSENT_CONFIG = DatasetConfig(\n    name=\"fosent\",\n    pretty_name=\"the Faroese sentiment classification dataset FoSent\",\n    huggingface_id=\"ScandEval/fosent\",\n    task=SENT,\n    languages=[FO],\n    prompt_prefix=\"Her eru nakrir tekstir flokka\u00f0ir eftir lyndi, sum kann vera \"\n    \"'positivt', 'neutralt' ella 'negativt'.\",\n    prompt_template=\"Text: {text}\\nLyndi: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positivt\", neutral=\"neutralt\", negative=\"negativt\"\n    ),\n    instruction_prompt=\"Tekstur: {text}\\n\\nFlokka lyndi\u00f0 \u00ed tekstinum. Svara vi\u00f0 \"\n    \"'positivt', 'neutralt' ella 'negativt'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\n\n### NAMED ENTITY RECOGNITION DATASETS ###\n\nSUC3_CONFIG = DatasetConfig(\n    name=\"suc3\",\n    pretty_name=\"the truncated version of the Swedish named entity recognition \"\n    \"dataset SUC 3.0\",\n    huggingface_id=\"ScandEval/suc3-mini\",\n    task=NER,\n    languages=[SV],\n    prompt_prefix=\"F\u00f6ljande \u00e4r meningar och JSON-ordb\u00f6cker med de namngivna enheter \"\n    \"som f\u00f6rekommer i den givna meningen.\",\n    prompt_template=\"Mening: {text}\\nNamngivna entiteter: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"plats\",\n        \"i-loc\": \"plats\",\n        \"b-org\": \"organisation\",\n        \"i-org\": \"organisation\",\n        \"b-misc\": \"diverse\",\n        \"i-misc\": \"diverse\",\n    },\n    instruction_prompt=\"Mening: {text}\\n\\nIdentifiera de namngivna enheterna i \"\n    \"meningen. Du ska outputta detta som en JSON-ordbok med nycklarna 'person', \"\n    \"'plats', 'organisation' och 'diverse'. V\u00e4rdena ska vara listor \u00f6ver de namngivna \"\n    \"enheter av den typen, precis som de f\u00f6rekommer i meningen.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nDANSK_CONFIG = DatasetConfig(\n    name=\"dansk\",\n    pretty_name=\"the truncated version of the Danish named entity recognition \"\n    \"dataset DANSK\",\n    huggingface_id=\"ScandEval/dansk-mini\",\n    task=NER,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er s\u00e6tninger og JSON-ordb\u00f8ger med de navngivne enheder, \"\n    \"som forekommer i den givne s\u00e6tning.\",\n    prompt_template=\"S\u00e6tning: {text}\\nNavngivne enheder: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"sted\",\n        \"i-loc\": \"sted\",\n        \"b-org\": \"organisation\",\n        \"i-org\": \"organisation\",\n        \"b-misc\": \"diverse\",\n        \"i-misc\": \"diverse\",\n    },\n    instruction_prompt=\"S\u00e6tning: {text}\\n\\nIdentific\u00e9r de navngivne enheder i \"\n    \"s\u00e6tningen. Du skal outputte dette som en JSON-ordbog med n\u00f8glerne 'person', \"\n    \"'sted', 'organisation' og 'diverse'. V\u00e6rdierne skal v\u00e6re lister over de navngivne \"\n    \"enheder af den type, pr\u00e6cis som de forekommer i s\u00e6tningen.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nNORNE_NB_CONFIG = DatasetConfig(\n    name=\"norne-nb\",\n    pretty_name=\"the truncated version of the Bokm\u00e5l part of the Norwegian named \"\n    \"entity recognition dataset NorNE\",\n    huggingface_id=\"ScandEval/norne-nb-mini\",\n    task=NER,\n    languages=[NB, NO],\n    prompt_prefix=\"F\u00f8lgende er fraser og JSON-ordb\u00f8ker med de navngitte enhetene \"\n    \"som forekommer i den gitte frasen.\",\n    prompt_template=\"Frase: {text}\\nNavngitte enheter: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"sted\",\n        \"i-loc\": \"sted\",\n        \"b-org\": \"organisasjon\",\n        \"i-org\": \"organisasjon\",\n        \"b-misc\": \"diverse\",\n        \"i-misc\": \"diverse\",\n    },\n    instruction_prompt=\"Frase: {text}\\n\\nIdentifiser de navngitte enhetene i frasen. \"\n    \"Du b\u00f8r outputte dette som en JSON-ordbok med n\u00f8klene 'person', 'sted', \"\n    \"'organisasjon' og 'diverse'. Verdiene skal v\u00e6re lister over de navngitte enhetene \"\n    \"av den typen, akkurat som de vises i frasen.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nNORNE_NN_CONFIG = DatasetConfig(\n    name=\"norne-nn\",\n    pretty_name=\"the truncated version of the Nynorsk part of the Norwegian named \"\n    \"entity recognition dataset NorNE\",\n    huggingface_id=\"ScandEval/norne-nn-mini\",\n    task=NER,\n    languages=[NN],\n    prompt_prefix=\"F\u00f8lgende er fraser og JSON-ordb\u00f8ker med de navngitte enhetene \"\n    \"som forekommer i den gitte frasen.\",\n    prompt_template=\"Frase: {text}\\nNavngitte enheter: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"sted\",\n        \"i-loc\": \"sted\",\n        \"b-org\": \"organisasjon\",\n        \"i-org\": \"organisasjon\",\n        \"b-misc\": \"diverse\",\n        \"i-misc\": \"diverse\",\n    },\n    instruction_prompt=\"Frase: {text}\\n\\nIdentifiser de navngitte enhetene i frasen. \"\n    \"Du b\u00f8r outputte dette som en JSON-ordbok med n\u00f8klene 'person', 'sted', \"\n    \"'organisasjon' og 'diverse'. Verdiene skal v\u00e6re lister over de navngitte enhetene \"\n    \"av den typen, akkurat som de vises i frasen.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nMIM_GOLD_NER_CONFIG = DatasetConfig(\n    name=\"mim-gold-ner\",\n    pretty_name=\"the truncated version of the Icelandic named entity recognition \"\n    \"dataset MIM-GOLD-NER\",\n    huggingface_id=\"ScandEval/mim-gold-ner-mini\",\n    task=NER,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru setningar \u00e1samt JSON lyklum me\u00f0 nefndum einingum \"\n    \"sem koma fyrir \u00ed setningunum.\",\n    prompt_template=\"Setning: {text}\\nNefndar einingar: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"einstaklingur\",\n        \"i-per\": \"einstaklingur\",\n        \"b-loc\": \"sta\u00f0setning\",\n        \"i-loc\": \"sta\u00f0setning\",\n        \"b-org\": \"stofnun\",\n        \"i-org\": \"stofnun\",\n        \"b-misc\": \"\u00fdmislegt\",\n        \"i-misc\": \"\u00fdmislegt\",\n    },\n    instruction_prompt=\"Setning: {text}\\n\\nGreini\u00f0 nefndu einingarnar \u00ed setningunni. \"\n    \"\u00de\u00fa \u00e6ttir a\u00f0 skila \u00feessu sem JSON or\u00f0ab\u00f3k me\u00f0 lyklunum 'einstaklingur', \"\n    \"'sta\u00f0setning', 'stofnun' og '\u00fdmislegt'. Gildin \u00e6ttu a\u00f0 vera listi yfir nefndu \"\n    \"einingarnar af \u00feeirri ger\u00f0, n\u00e1kv\u00e6mlega eins og \u00fe\u00e6r koma fram \u00ed setningunni.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nFONE_CONFIG = DatasetConfig(\n    name=\"fone\",\n    pretty_name=\"the truncated version of the Faroese named entity recognition \"\n    \"dataset FoNE\",\n    huggingface_id=\"ScandEval/fone-mini\",\n    task=NER,\n    languages=[FO],\n    prompt_prefix=\"Her eru nakrir setningar og nakrar JSON or\u00f0ab\u00f8kur vi\u00f0 nevndar \"\n    \"eindir, sum eru \u00ed setningunum.\",\n    prompt_template=\"Setningur: {text}\\nNevndar eindir: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"pers\u00f3nur\",\n        \"i-per\": \"pers\u00f3nur\",\n        \"b-loc\": \"sta\u00f0ur\",\n        \"i-loc\": \"sta\u00f0ur\",\n        \"b-org\": \"felagsskapur\",\n        \"i-org\": \"felagsskapur\",\n        \"b-misc\": \"ymiskt\",\n        \"i-misc\": \"ymiskt\",\n    },\n    instruction_prompt=\"Setningur: {text}\\n\\nGreini\u00f0 nevndu einingarnar \u00ed setningunni. \"\n    \"\u00de\u00fa \u00e6ttir a\u00f0 skila \u00feessu sem JSON or\u00f0ab\u00f3k me\u00f0 lyklunum 'pers\u00f3nur', 'sta\u00f0ur', \"\n    \"'felagsskapur' og 'ymiskt'. Gildin \u00e6ttu a\u00f0 vera listi yfir nevndu einingarnar af \"\n    \"\u00feeirri ger\u00f0, n\u00e1kv\u00e6mlega eins og \u00fe\u00e6r koma fram \u00ed setningunni.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nGERMEVAL_CONFIG = DatasetConfig(\n    name=\"germeval\",\n    pretty_name=\"the truncated version of the German named entity recognition \"\n    \"dataset GermEval\",\n    huggingface_id=\"ScandEval/germeval-mini\",\n    task=NER,\n    languages=[DE],\n    prompt_prefix=\"Es folgen S\u00e4tze und JSON-W\u00f6rterb\u00fccher mit den benannten \"\n    \"Entit\u00e4ten, die in der angegebenen Phrase vorkommen.\",\n    prompt_template=\"Satz: {text}\\nBenannte Entit\u00e4ten: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"ort\",\n        \"i-loc\": \"ort\",\n        \"b-org\": \"organisation\",\n        \"i-org\": \"organisation\",\n        \"b-misc\": \"verschiedenes\",\n        \"i-misc\": \"verschiedenes\",\n    },\n    instruction_prompt=\"Satz: {text}\\n\\nIdentifizieren Sie die benannten Entit\u00e4ten im \"\n    \"Satz. Sie sollten dies als JSON-W\u00f6rterbuch mit den Schl\u00fcsseln 'person', 'ort', \"\n    \"'organisation' und 'verschiedenes' ausgeben. Die Werte sollten Listen der \"\n    \"benannten Entit\u00e4ten dieses Typs sein, genau wie sie im Satz erscheinen.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nCONLL_NL_CONFIG = DatasetConfig(\n    name=\"conll-nl\",\n    pretty_name=\"the Dutch part of the truncated version of the named entity \"\n    \"recognition dataset CoNLL 2002\",\n    huggingface_id=\"ScandEval/conll-nl-mini\",\n    task=NER,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan zinnen en JSON woordenboeken met de genoemde \"\n    \"entiteiten die voorkomen in de gegeven zin.\",\n    prompt_template=\"Zin: {text}\\nGenoemde entiteiten: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"persoon\",\n        \"i-per\": \"persoon\",\n        \"b-loc\": \"locatie\",\n        \"i-loc\": \"locatie\",\n        \"b-org\": \"organisatie\",\n        \"i-org\": \"organisatie\",\n        \"b-misc\": \"diversen\",\n        \"i-misc\": \"diversen\",\n    },\n    instruction_prompt=\"Zin: {text}\\n\\nIdentificeer de genoemde entiteiten in de zin. \"\n    \"Je moet dit uitvoeren als een JSON-woordenboek met de sleutels 'persoon', \"\n    \"'locatie', 'organisatie' en 'diversen'. De waarden moeten lijsten zijn van de \"\n    \"genoemde entiteiten van dat type, precies zoals ze voorkomen in de zin.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nCONLL_EN_CONFIG = DatasetConfig(\n    name=\"conll-en\",\n    pretty_name=\"the truncated version of the English named entity recognition \"\n    \"dataset CoNLL 2003\",\n    huggingface_id=\"ScandEval/conll-en-mini\",\n    task=NER,\n    languages=[EN],\n    prompt_prefix=\"Below are sentences and JSON dictionaries with the named \"\n    \"entities that occur in the given sentence.\",\n    prompt_template=\"Sentence: {text}\\nNamed entities: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"location\",\n        \"i-loc\": \"location\",\n        \"b-org\": \"organization\",\n        \"i-org\": \"organization\",\n        \"b-misc\": \"miscellaneous\",\n        \"i-misc\": \"miscellaneous\",\n    },\n    instruction_prompt=\"Sentence: {text}\\n\\nIdentify the named entities in the \"\n    \"sentence. You should output this as a JSON dictionary with the keys being \"\n    \"'person', 'location', 'organization' and 'miscellaneous'. The values should be \"\n    \"lists of the named entities of that type, exactly as they appear in the sentence.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nDANE_CONFIG = DatasetConfig(\n    name=\"dane\",\n    pretty_name=\"the truncated version of the Danish named entity recognition \"\n    \"dataset DaNE\",\n    huggingface_id=\"ScandEval/dane-mini\",\n    task=NER,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er s\u00e6tninger og JSON-ordb\u00f8ger med de navngivne enheder, \"\n    \"som forekommer i den givne s\u00e6tning.\",\n    prompt_template=\"S\u00e6tning: {text}\\nNavngivne enheder: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"sted\",\n        \"i-loc\": \"sted\",\n        \"b-org\": \"organisation\",\n        \"i-org\": \"organisation\",\n        \"b-misc\": \"diverse\",\n        \"i-misc\": \"diverse\",\n    },\n    instruction_prompt=\"S\u00e6tning: {text}\\n\\nIdentific\u00e9r de navngivne enheder i \"\n    \"s\u00e6tningen. Du skal outputte dette som en JSON-ordbog med n\u00f8glerne 'person', \"\n    \"'sted', 'organisation' og 'diverse'. V\u00e6rdierne skal v\u00e6re lister over de navngivne \"\n    \"enheder af den type, pr\u00e6cis som de forekommer i s\u00e6tningen.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n    unofficial=True,\n)\n\nWIKIANN_FO_CONFIG = DatasetConfig(\n    name=\"wikiann-fo\",\n    pretty_name=\"the truncated version of the Faroese part of the named entity \"\n    \"recognition dataset WikiANN\",\n    huggingface_id=\"ScandEval/wikiann-fo-mini\",\n    task=NER,\n    languages=[FO],\n    prompt_prefix=\"Her eru nakrir setningar og nakrar JSON or\u00f0ab\u00f8kur vi\u00f0 nevndar \"\n    \"eindir, sum eru \u00ed setningunum.\",\n    prompt_template=\"Setningur: {text}\\nNevndar eindir: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"pers\u00f3nur\",\n        \"i-per\": \"pers\u00f3nur\",\n        \"b-loc\": \"sta\u00f0ur\",\n        \"i-loc\": \"sta\u00f0ur\",\n        \"b-org\": \"felagsskapur\",\n        \"i-org\": \"felagsskapur\",\n        \"b-misc\": \"ymiskt\",\n        \"i-misc\": \"ymiskt\",\n    },\n    instruction_prompt=\"Setningur: {text}\\n\\nGreini\u00f0 nevndu einingarnar \u00ed setningunni. \"\n    \"\u00de\u00fa \u00e6ttir a\u00f0 skila \u00feessu sem JSON or\u00f0ab\u00f3k me\u00f0 lyklunum 'pers\u00f3nur', 'sta\u00f0ur', \"\n    \"'felagsskapur' og 'ymiskt'. Gildin \u00e6ttu a\u00f0 vera listi yfir nevndu einingarnar af \"\n    \"\u00feeirri ger\u00f0, n\u00e1kv\u00e6mlega eins og \u00fe\u00e6r koma fram \u00ed setningunni.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n    unofficial=True,\n)\n\n\n### LINGUISTIC ACCEPTABILITY DATASETS ###\n\nSCALA_SV_CONFIG = DatasetConfig(\n    name=\"scala-sv\",\n    pretty_name=\"The Swedish part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-sv\",\n    task=LA,\n    languages=[SV],\n    prompt_prefix=\"F\u00f6ljande \u00e4r meningar och huruvida de \u00e4r grammatiskt korrekta.\",\n    prompt_template=\"Mening: {text}\\nGrammatisk korrekt: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nej\"),\n    instruction_prompt=\"Mening: {text}\\n\\nBest\u00e4m om meningen \u00e4r grammatiskt korrekt \"\n    \"eller inte. Svara med 'ja' om meningen \u00e4r korrekt och 'nej' om den inte \u00e4r.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nSCALA_DA_CONFIG = DatasetConfig(\n    name=\"scala-da\",\n    pretty_name=\"the Danish part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-da\",\n    task=LA,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er s\u00e6tninger og om de er grammatisk korrekte.\",\n    prompt_template=\"S\u00e6tning: {text}\\nGrammatisk korrekt: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nej\"),\n    instruction_prompt=\"S\u00e6tning: {text}\\n\\nBestem om s\u00e6tningen er grammatisk korrekt \"\n    \"eller ej. Svar med 'ja', hvis s\u00e6tningen er korrekt, og 'nej', hvis den ikke er.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nSCALA_NB_CONFIG = DatasetConfig(\n    name=\"scala-nb\",\n    pretty_name=\"the Bokm\u00e5l part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-nb\",\n    task=LA,\n    languages=[NB, NO],\n    prompt_prefix=\"F\u00f8lgende er setninger og hvorvidt de er grammatisk korrekte.\",\n    prompt_template=\"Setning: {text}\\nGrammatisk korrekt: {label}\",\n    instruction_prompt=\"Setning: {text}\\n\\nBestem om setningen er grammatisk korrekt \"\n    \"eller ikke. Svar med 'ja' hvis setningen er korrekt og 'nei' hvis den ikke er.\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nei\"),\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nSCALA_NN_CONFIG = DatasetConfig(\n    name=\"scala-nn\",\n    pretty_name=\"the Nynorsk part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-nn\",\n    task=LA,\n    languages=[NN],\n    prompt_prefix=\"F\u00f8lgende er setninger og hvorvidt de er grammatisk korrekte.\",\n    prompt_template=\"Setning: {text}\\nGrammatisk korrekt: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nei\"),\n    instruction_prompt=\"Setning: {text}\\n\\nBestem om setningen er grammatisk korrekt \"\n    \"eller ikke. Svar med 'ja' hvis setningen er korrekt og 'nei' hvis den ikke er.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nSCALA_IS_CONFIG = DatasetConfig(\n    name=\"scala-is\",\n    pretty_name=\"the Icelandic part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-is\",\n    task=LA,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru setningar og hvort \u00fe\u00e6r eru m\u00e1lfr\u00e6\u00f0ilega r\u00e9ttar.\",\n    prompt_template=\"Setning: {text}\\nM\u00e1lfr\u00e6\u00f0ilega r\u00e9tt: {label}\",\n    prompt_label_mapping=dict(correct=\"j\u00e1\", incorrect=\"nei\"),\n    instruction_prompt=\"Setning: {text}\\n\\nGreini\u00f0 hvort setningin er m\u00e1lfr\u00e6\u00f0ilega \"\n    \"r\u00e9tt e\u00f0a ekki. Svari\u00f0 skal vera 'j\u00e1' ef setningin er r\u00e9tt og 'nei' ef h\u00fan er \"\n    \"ekki.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nSCALA_FO_CONFIG = DatasetConfig(\n    name=\"scala-fo\",\n    pretty_name=\"the Faroese part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-fo\",\n    task=LA,\n    languages=[FO],\n    prompt_prefix=\"Hetta eru nakrir setningar og um teir eru m\u00e1ll\u00e6ruliga r\u00e6ttir.\",\n    prompt_template=\"Setningur: {text}\\nM\u00e1ll\u00e6ruliga r\u00e6ttur: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nei\"),\n    instruction_prompt=\"Setningur: {text}\\n\\nGreini\u00f0 hvort setningurin er m\u00e1ll\u00e6ruliga \"\n    \"r\u00e6ttur ella ikki. Svari\u00f0 skal vera 'ja' um setningurin er r\u00e6ttur og 'nei' um \"\n    \"hann ikki er.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nSCALA_DE_CONFIG = DatasetConfig(\n    name=\"scala-de\",\n    pretty_name=\"the German part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-de\",\n    task=LA,\n    languages=[DE],\n    prompt_prefix=\"Die folgenden S\u00e4tze und ob sie grammatikalisch korrekt sind.\",\n    prompt_template=\"Satz: {text}\\nGrammatikalisch richtig: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nein\"),\n    instruction_prompt=\"Satz: {text}\\n\\nBestimmen Sie, ob der Satz grammatikalisch \"\n    \"korrekt ist oder nicht. Antworten Sie mit 'ja', wenn der Satz korrekt ist und \"\n    \"'nein', wenn er es nicht ist.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nSCALA_NL_CONFIG = DatasetConfig(\n    name=\"scala-nl\",\n    pretty_name=\"the Dutch part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-nl\",\n    task=LA,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan zinnen en of ze grammaticaal correct zijn.\",\n    prompt_template=\"Zin: {text}\\nGrammaticaal correct: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nee\"),\n    instruction_prompt=\"Zin: {text}\\n\\nBepaal of de zin grammaticaal correct is of \"\n    \"niet. Antwoord met 'ja' als de zin correct is en 'nee' als dat niet het geval is.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nSCALA_EN_CONFIG = DatasetConfig(\n    name=\"scala-en\",\n    pretty_name=\"the English part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"ScandEval/scala-en\",\n    task=LA,\n    languages=[EN],\n    prompt_prefix=\"The following are sentences and whether they are grammatically \"\n    \"correct.\",\n    prompt_template=\"Sentence: {text}\\nGrammatically correct: {label}\",\n    prompt_label_mapping=dict(correct=\"yes\", incorrect=\"no\"),\n    instruction_prompt=\"Sentence: {text}\\n\\nDetermine whether the sentence is \"\n    \"grammatically correct or not. Reply with 'yes' if the sentence is correct and \"\n    \"'no' if it is not.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n)\n\nDUTCH_COLA_CONFIG = DatasetConfig(\n    name=\"dutch-cola\",\n    pretty_name=\"the truncated version of the Dutch linguistic acceptability dataset \"\n    \"Dutch CoLA\",\n    huggingface_id=\"ScandEval/dutch-cola\",\n    task=LA,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan zinnen en of ze grammaticaal correct ('ja') of \"\n    \"incorrect ('nee') zijn.\",\n    prompt_template=\"Zin: {text}\\nGrammaticaal correct: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nee\"),\n    instruction_prompt=\"Zin: {text}\\n\\nBepaal of de zin grammaticaal correct is of \"\n    \"niet. Antwoord met 'ja' als de zin correct is en 'nee' als dat niet het geval is.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=3,\n    unofficial=True,\n)\n\nDUTCH_COLA_FULL_CONFIG = DatasetConfig(\n    name=\"dutch-cola-full\",\n    pretty_name=\"the Dutch linguistic acceptability dataset Dutch CoLA\",\n    huggingface_id=\"ScandEval/dutch-cola-full\",\n    task=LA,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan zinnen en of ze grammaticaal correct ('ja') of \"\n    \"incorrect ('nee') zijn.\",\n    prompt_template=\"Zin: {text}\\nGrammaticaal correct: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nee\"),\n    instruction_prompt=\"Zin: {text}\\n\\nBepaal of de zin grammaticaal correct is of \"\n    \"niet. Antwoord met 'ja' als de zin correct is en 'nee' als dat niet het geval is.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=3,\n    unofficial=True,\n)\n\nICE_EC_CONFIG = DatasetConfig(\n    name=\"ice-ec\",\n    pretty_name=\"the truncated version of the Icelandic Error Corpus\",\n    huggingface_id=\"ScandEval/ice-ec\",\n    task=LA,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru setningar og hvort \u00fe\u00e6r eru m\u00e1lfr\u00e6\u00f0ilega r\u00e9ttar.\",\n    prompt_template=\"Setning: {text}\\nM\u00e1lfr\u00e6\u00f0ilega r\u00e9tt: {label}\",\n    prompt_label_mapping=dict(correct=\"j\u00e1\", incorrect=\"nei\"),\n    instruction_prompt=\"Setning: {text}\\n\\nGreini\u00f0 hvort setningin er m\u00e1lfr\u00e6\u00f0ilega \"\n    \"r\u00e9tt e\u00f0a ekki. Svari\u00f0 skal vera 'j\u00e1' ef setningin er r\u00e9tt og 'nei' ef h\u00fan er \"\n    \"ekki.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nICE_EC_FULL_CONFIG = DatasetConfig(\n    name=\"ice-ec-full\",\n    pretty_name=\"the Icelandic Error Corpus\",\n    huggingface_id=\"ScandEval/ice-ec-full\",\n    task=LA,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru setningar og hvort \u00fe\u00e6r eru m\u00e1lfr\u00e6\u00f0ilega r\u00e9ttar.\",\n    prompt_template=\"Setning: {text}\\nM\u00e1lfr\u00e6\u00f0ilega r\u00e9tt: {label}\",\n    prompt_label_mapping=dict(correct=\"j\u00e1\", incorrect=\"nei\"),\n    instruction_prompt=\"Setning: {text}\\n\\nGreini\u00f0 hvort setningin er m\u00e1lfr\u00e6\u00f0ilega \"\n    \"r\u00e9tt e\u00f0a ekki. Svari\u00f0 skal vera 'j\u00e1' ef setningin er r\u00e9tt og 'nei' ef h\u00fan er ekki.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nICE_LINGUISTIC_CONFIG = DatasetConfig(\n    name=\"ice-linguistic\",\n    pretty_name=\"the Icelandic linguistic acceptability dataset IceLinguistic\",\n    huggingface_id=\"ScandEval/ice-linguistic\",\n    task=LA,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru setningar og hvort \u00fe\u00e6r eru m\u00e1lfr\u00e6\u00f0ilega r\u00e9ttar.\",\n    prompt_template=\"Setning: {text}\\nM\u00e1lfr\u00e6\u00f0ilega r\u00e9tt: {label}\",\n    prompt_label_mapping=dict(correct=\"j\u00e1\", incorrect=\"nei\"),\n    instruction_prompt=\"Setning: {text}\\n\\nGreini\u00f0 hvort setningin er m\u00e1lfr\u00e6\u00f0ilega \"\n    \"r\u00e9tt e\u00f0a ekki. Svari\u00f0 skal vera 'j\u00e1' ef setningin er r\u00e9tt og 'nei' ef h\u00fan er ekki.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\n\n### READING COMPREHENSION DATASETS ###\n\nSCANDIQA_DA_CONFIG = DatasetConfig(\n    name=\"scandiqa-da\",\n    pretty_name=\"the Danish part of the truncated version of the question answering \"\n    \"dataset ScandiQA\",\n    huggingface_id=\"ScandEval/scandiqa-da-mini\",\n    task=RC,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er tekster med tilh\u00f8rende sp\u00f8rgsm\u00e5l og svar.\",\n    prompt_template=\"Tekst: {text}\\nSp\u00f8rgsm\u00e5l: {question}\\nSvar med maks. 3 ord: \"\n    \"{label}\",\n    instruction_prompt=\"Tekst: {text}\\n\\nBesvar f\u00f8lgende sp\u00f8rgsm\u00e5l om teksten ovenfor \"\n    \"med maks. 3 ord.\\n\\nSp\u00f8rgsm\u00e5l: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nNORQUAD_CONFIG = DatasetConfig(\n    name=\"norquad\",\n    pretty_name=\"the truncated version of the Norwegian question answering \"\n    \"dataset NorQuAD\",\n    huggingface_id=\"ScandEval/norquad-mini\",\n    task=RC,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"Her f\u00f8lger tekster med tilh\u00f8rende sp\u00f8rsm\u00e5l og svar.\",\n    prompt_template=\"Tekst: {text}\\nSp\u00f8rsm\u00e5l: {question}\\nSvar p\u00e5 maks 3 ord: {label}\",\n    instruction_prompt=\"Tekst: {text}\\n\\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l om teksten ovenfor \"\n    \"med maks 3 ord.\\n\\nSp\u00f8rsm\u00e5l: {question}\",\n    num_few_shot_examples=2,\n    max_generated_tokens=32,\n)\n\nSCANDIQA_SV_CONFIG = DatasetConfig(\n    name=\"scandiqa-sv\",\n    pretty_name=\"the Swedish part of the truncated version of the question answering \"\n    \"dataset ScandiQA\",\n    huggingface_id=\"ScandEval/scandiqa-sv-mini\",\n    task=RC,\n    languages=[SV],\n    prompt_prefix=\"Nedan f\u00f6ljer texter med tillh\u00f6rande fr\u00e5gor och svar.\",\n    prompt_template=\"Text: {text}\\nFr\u00e5ga: {question}\\nSvar p\u00e5 max 3 ord: {label}\",\n    instruction_prompt=\"Text: {text}\\n\\nBesvara f\u00f6ljande fr\u00e5ga om texten ovan med \"\n    \"h\u00f6gst 3 ord.\\n\\nFr\u00e5ga: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nNQII_CONFIG = DatasetConfig(\n    name=\"nqii\",\n    pretty_name=\"the truncated version of the Icelandic question answering dataset \"\n    \"Natural Questions in Icelandic\",\n    huggingface_id=\"ScandEval/nqii-mini\",\n    task=RC,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru textar me\u00f0 tilheyrandi spurningum og sv\u00f6rum.\",\n    prompt_template=\"Texti: {text}\\nSpurning: {question}\\nSvara\u00f0u me\u00f0 a\u00f0 h\u00e1marki 3 \"\n    \"or\u00f0um: {label}\",\n    instruction_prompt=\"Texti: {text}\\n\\nSvara\u00f0u eftirfarandi spurningu um textann a\u00f0 \"\n    \"h\u00e1marki \u00ed 3 or\u00f0um.\\n\\nSpurning: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nFOQA_CONFIG = DatasetConfig(\n    name=\"foqa\",\n    pretty_name=\"the Faroese question answering dataset FoQA\",\n    huggingface_id=\"ScandEval/foqa\",\n    task=RC,\n    languages=[FO],\n    prompt_prefix=\"Hetta eru tekstir saman vi\u00f0 spurningum og svar.\",\n    prompt_template=\"{text}\\nSpurningur: {question}\\nSvara vi\u00f0 \u00ed mesta lagi trimum \"\n    \"or\u00f0um: {label}\",\n    instruction_prompt=\"Tekstur: {text}\\n\\nSvara hesum spurninginum um tekstin \"\n    \"uppiyvir vi\u00f0 \u00ed mesta lagi trimum or\u00f0um.\\n\\nSpurningur: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nGERMANQUAD_CONFIG = DatasetConfig(\n    name=\"germanquad\",\n    pretty_name=\"the truncated version of the German question answering dataset \"\n    \"GermanQuAD\",\n    huggingface_id=\"ScandEval/germanquad-mini\",\n    task=RC,\n    languages=[DE],\n    prompt_prefix=\"Im Folgenden finden Sie Texte mit den dazugeh\u00f6rigen Fragen und \"\n    \"Antworten.\",\n    prompt_template=\"Text: {text}\\nFragen: {question}\\nFragen Antwort in maximal 3 \"\n    \"W\u00f6rtern: {label}\",\n    instruction_prompt=\"Text: {text}\\n\\nBeantworten Sie die folgende Frage zum obigen \"\n    \"Text in h\u00f6chstens 3 W\u00f6rtern.\\n\\nFrage: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nSQUAD_CONFIG = DatasetConfig(\n    name=\"squad\",\n    pretty_name=\"the truncated version of the English question answering \"\n    \"dataset SQuAD\",\n    huggingface_id=\"ScandEval/squad-mini\",\n    task=RC,\n    languages=[EN],\n    prompt_prefix=\"The following are texts with accompanying questions and answers.\",\n    prompt_template=\"Text: {text}\\nQuestion: {question}\\nAnswer in max 3 words: \"\n    \"{label}\",\n    instruction_prompt=\"Text: {text}\\n\\nAnswer the following question about the \"\n    \"above text in at most 3 words.\\n\\nQuestion: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nSQUAD_NL_CONFIG = DatasetConfig(\n    name=\"squad-nl\",\n    pretty_name=\"the truncated version of the Dutch question answering dataset \"\n    \"SQuAD-nl, translated from the English SQuAD dataset\",\n    huggingface_id=\"ScandEval/squad-nl-mini\",\n    task=RC,\n    languages=[NL],\n    prompt_prefix=\"Hieronder volgen teksten met bijbehorende vragen en antwoorden.\",\n    prompt_template=\"Tekst: {text}\\nVraag: {question}\\nAntwoord in max 3 woorden: \"\n    \"{label}\",\n    instruction_prompt=\"Tekst: {text}\\n\\nBeantwoord de volgende vraag over de \"\n    \"bovenstaande tekst in maximaal 3 woorden.\\n\\nVraag: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nICELANDIC_QA_CONFIG = DatasetConfig(\n    name=\"icelandic-qa\",\n    pretty_name=\"the Icelandic question answering dataset about Icelandic culture and \"\n    \"history\",\n    huggingface_id=\"ScandEval/icelandic-qa\",\n    task=RC,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru textar me\u00f0 tilheyrandi spurningum og sv\u00f6rum.\",\n    prompt_template=\"Texti: {text}\\nSpurning: {question}\\nSvara\u00f0u me\u00f0 a\u00f0 h\u00e1marki 3 \"\n    \"or\u00f0um: {label}\",\n    instruction_prompt=\"Texti: {text}\\n\\nSvara\u00f0u eftirfarandi spurningu um textann a\u00f0 \"\n    \"h\u00e1marki \u00ed 3 or\u00f0um.\\n\\nSpurning: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n    unofficial=True,\n)\n\n### SUMMARIZATION DATASETS ###\n\nNORDJYLLAND_NEWS_CONFIG = DatasetConfig(\n    name=\"nordjylland-news\",\n    pretty_name=\"the truncated version of the Danish summarisation dataset \"\n    \"Nordjylland News\",\n    huggingface_id=\"ScandEval/nordjylland-news-mini\",\n    task=SUMM,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er nyhedsartikler med tilh\u00f8rende resum\u00e9er.\",\n    prompt_template=\"Nyhedsartikel: {text}\\nResum\u00e9: {target_text}\",\n    instruction_prompt=\"Nyhedsartikel: {text}\\n\\nSkriv et resum\u00e9 af ovenst\u00e5ende \"\n    \"artikel.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nMLSUM_CONFIG = DatasetConfig(\n    name=\"mlsum\",\n    pretty_name=\"the truncated version of the German summarisation dataset MLSum\",\n    huggingface_id=\"ScandEval/mlsum-mini\",\n    task=SUMM,\n    languages=[DE],\n    prompt_prefix=\"Im Folgenden finden Sie Nachrichtenartikel mit den dazugeh\u00f6rigen \"\n    \"Zusammenfassungen.\",\n    prompt_template=\"Nachrichtenartikel: {text}\\nZusammenfassung: {target_text}\",\n    instruction_prompt=\"Nachrichtenartikel: {text}\\n\\nSchreiben Sie eine \"\n    \"Zusammenfassung des obigen Artikels.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nRRN_CONFIG = DatasetConfig(\n    name=\"rrn\",\n    pretty_name=\"the truncated version of the Icelandic summarisation dataset \"\n    \"R\u00daV Radio News\",\n    huggingface_id=\"ScandEval/rrn-mini\",\n    task=SUMM,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru fr\u00e9ttagreinar me\u00f0 tilheyrandi samantektum.\",\n    prompt_template=\"Fr\u00e9ttagrein: {text}\\nSamantekt: {target_text}\",\n    instruction_prompt=\"Fr\u00e9ttagrein: {text}\\n\\nSkrifa\u00f0u samantekt um ofangreindu \"\n    \"grein.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nICESUM_CONFIG = DatasetConfig(\n    name=\"icesum\",\n    pretty_name=\"the Icelandic summarisation dataset IceSum\",\n    huggingface_id=\"ScandEval/icesum\",\n    task=SUMM,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru fr\u00e9ttagreinar me\u00f0 tilheyrandi samantektum.\",\n    prompt_template=\"Fr\u00e9ttagrein: {text}\\nSamantekt: {target_text}\",\n    instruction_prompt=\"Fr\u00e9ttagrein: {text}\\n\\nSkrifa\u00f0u samantekt um ofangreindu \"\n    \"grein.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n    unofficial=True,\n)\n\nNO_SAMMENDRAG_CONFIG = DatasetConfig(\n    name=\"no-sammendrag\",\n    pretty_name=\"the truncated version of the Norwegian summarisation dataset \"\n    \"Norske Sammendrag\",\n    huggingface_id=\"ScandEval/no-sammendrag-mini\",\n    task=SUMM,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"Her f\u00f8lger nyhetsartikler med tilh\u00f8rende sammendrag.\",\n    prompt_template=\"Nyhetsartikkel: {text}\\nSammendrag: {target_text}\",\n    instruction_prompt=\"Nyhetsartikkel: {text}\\n\\nSkriv et sammendrag av den \"\n    \"ovennevnte artikkelen.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nNORGLM_MULTI_SUM = DatasetConfig(\n    name=\"norglm-multi-sum\",\n    pretty_name=\"the summarisation part of the Norwegian NorGLM multi-task human \"\n    \"annotated dataset NO-Multi-QA-Sum\",\n    huggingface_id=\"ScandEval/norglm-multi-sum\",\n    task=SUMM,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"Her f\u00f8lger nyhetsartikler med tilh\u00f8rende sammendrag.\",\n    prompt_template=\"Nyhetsartikkel: {text}\\nSammendrag: {target_text}\",\n    instruction_prompt=\"Nyhetsartikkel: {text}\\n\\nSkriv et sammendrag av den \"\n    \"ovennevnte artikkelen.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n    unofficial=True,\n)\n\nWIKI_LINGUA_NL_CONFIG = DatasetConfig(\n    name=\"wiki-lingua-nl\",\n    pretty_name=\"the Dutch part of the truncated version of the summarisation dataset \"\n    \"WikiLingua\",\n    huggingface_id=\"ScandEval/wiki-lingua-nl-mini\",\n    task=SUMM,\n    languages=[NL],\n    prompt_prefix=\"Hieronder volgen artikelen met bijbehorende samenvattingen.\",\n    prompt_template=\"Artikel: {text}\\nSamenvatting: {target_text}\",\n    instruction_prompt=\"Artikel: {text}\\n\\nSchrijf een samenvatting van het \"\n    \"bovenstaande artikel.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nSWEDN_CONFIG = DatasetConfig(\n    name=\"swedn\",\n    pretty_name=\"the truncated version of the Swedish summarisation dataset SweDN\",\n    huggingface_id=\"ScandEval/swedn-mini\",\n    task=SUMM,\n    languages=[SV],\n    prompt_prefix=\"Nedan f\u00f6ljer artiklar med tillh\u00f6rande sammanfattningar.\",\n    prompt_template=\"Artikel: {text}\\nSammanfattning: {target_text}\",\n    instruction_prompt=\"Artikel: {text}\\n\\nSkriv en sammanfattning av artikeln ovan.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nCNN_DAILYMAIL_CONFIG = DatasetConfig(\n    name=\"cnn-dailymail\",\n    pretty_name=\"the truncated version of the English summarisation dataset \"\n    \"CNN-DailyMail\",\n    huggingface_id=\"ScandEval/cnn-dailymail-mini\",\n    task=SUMM,\n    languages=[EN],\n    prompt_prefix=\"The following are articles with accompanying summaries.\",\n    prompt_template=\"News article: {text}\\nSummary: {target_text}\",\n    instruction_prompt=\"News article: {text}\\n\\nWrite a summary of the above \"\n    \"article.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nSCHIBSTED_SV = DatasetConfig(\n    name=\"schibsted-sv\",\n    pretty_name=\"article summaries from Schibsted Media Swedish newsrooms.\",\n    huggingface_id=\"ScandEval/schibsted-article-summaries-sv\",\n    task=SUMM,\n    languages=[SV],\n    prompt_prefix=\"Nedan f\u00f6ljer artiklar med tillh\u00f6rande sammanfattningar.\",\n    prompt_template=\"Artikel: {text}\\nSammanfattning: {target_text}\",\n    instruction_prompt=\"Artikel: {text}\\n\\nSkriv en sammanfattning av artikeln ovan.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n    unofficial=True,\n)\n\nSCHIBSTED_NO = DatasetConfig(\n    name=\"schibsted-no\",\n    pretty_name=\"article summaries from Schibsted Medias Norwegian newsrooms.\",\n    huggingface_id=\"ScandEval/schibsted-article-summaries-no\",\n    task=SUMM,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"Her f\u00f8lger nyhetsartikler med tilh\u00f8rende sammendrag.\",\n    prompt_template=\"Nyhetsartikkel: {text}\\nSammendrag: {target_text}\",\n    instruction_prompt=\"Nyhetsartikkel: {text}\\n\\nSkriv et sammendrag av den \"\n    \"ovennevnte artikkelen.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n    unofficial=True,\n)\n\n# TODO: Faroese summarization\n\n\n### KNOWLEDGE DATASETS ###\n\nDANSKE_TALEMAADER_CONFIG = DatasetConfig(\n    name=\"danske-talemaader\",\n    pretty_name=\"the truncated version of the Danish knowledge dataset Danske \"\n    \"Talem\u00e5der\",\n    huggingface_id=\"ScandEval/danske-talemaader-mini\",\n    task=KNOW,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\",\n    prompt_template=\"Hvad er betydningen af f\u00f8lgende talem\u00e5de: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rgsm\u00e5l: {text}\\n\\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at \"\n    \"svare med 'a', 'b', 'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nDANISH_CITIZEN_TESTS_CONFIG = DatasetConfig(\n    name=\"danish-citizen-tests\",\n    pretty_name=\"the Danish knowledge dataset Danish Citizen Tests\",\n    huggingface_id=\"ScandEval/danish-citizen-tests\",\n    task=KNOW,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rgsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rgsm\u00e5l: {text}\\n\\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at \"\n    \"svare med 'a', 'b', 'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nMMLU_NO_CONFIG = DatasetConfig(\n    name=\"mmlu-no\",\n    pretty_name=\"the truncated version of the Norwegian knowledge dataset MMLU-no, \"\n    \"translated from the English MMLU dataset\",\n    huggingface_id=\"ScandEval/mmlu-no-mini\",\n    task=KNOW,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"F\u00f8lgende er flervalgssp\u00f8rsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rsm\u00e5l: {text}\\n\\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', \"\n    \"'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nMMLU_SV_CONFIG = DatasetConfig(\n    name=\"mmlu-sv\",\n    pretty_name=\"the truncated version of the Swedish knowledge dataset MMLU-sv, \"\n    \"translated from the English MMLU dataset\",\n    huggingface_id=\"ScandEval/mmlu-sv-mini\",\n    task=KNOW,\n    languages=[SV],\n    prompt_prefix=\"F\u00f6ljande \u00e4r flervalsfr\u00e5gor (med svar).\",\n    prompt_template=\"Fr\u00e5ga: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Fr\u00e5ga: {text}\\n\\nBesvara f\u00f6ljande fr\u00e5ga med 'a', 'b', 'c' \"\n    \"eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nMMLU_IS_CONFIG = DatasetConfig(\n    name=\"mmlu-is\",\n    pretty_name=\"the truncated version of the Icelandic knowledge dataset MMLU-is, \"\n    \"translated from the English MMLU dataset\",\n    huggingface_id=\"ScandEval/mmlu-is-mini\",\n    task=KNOW,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\",\n    prompt_template=\"Spurningar: {text}\\nSvara: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Spurningar: {text}\\n\\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', \"\n    \"'b', 'c' e\u00f0a 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nMMLU_DE_CONFIG = DatasetConfig(\n    name=\"mmlu-de\",\n    pretty_name=\"the truncated version of the German knowledge dataset MMLU-de, \"\n    \"translated from the English MMLU dataset\",\n    huggingface_id=\"ScandEval/mmlu-de-mini\",\n    task=KNOW,\n    languages=[DE],\n    prompt_prefix=\"Die folgenden Fragen sind Multiple-Choice-Fragen (mit Antworten).\",\n    prompt_template=\"Frage: {text}\\nAntwort: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Frage: {text}\\n\\nBeantworten Sie die obige Frage mit 'a', 'b', \"\n    \"'c' oder 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nMMLU_NL_CONFIG = DatasetConfig(\n    name=\"mmlu-nl\",\n    pretty_name=\"the truncated version of the Dutch knowledge dataset MMLU-nl, \"\n    \"translated from the English MMLU dataset\",\n    huggingface_id=\"ScandEval/mmlu-nl-mini\",\n    task=KNOW,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan meerkeuzevragen (met antwoorden).\",\n    prompt_template=\"Vraag: {text}\\nAntwoord: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Vraag: {text}\\n\\nBeantwoord de bovenstaande vraag met 'a', 'b', \"\n    \"'c' of 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nMMLU_CONFIG = DatasetConfig(\n    name=\"mmlu\",\n    pretty_name=\"the truncated version of the English knowledge dataset MMLU\",\n    huggingface_id=\"ScandEval/mmlu-mini\",\n    task=KNOW,\n    languages=[EN],\n    prompt_prefix=\"The following are multiple choice questions (with answers).\",\n    prompt_template=\"Question: {text}\\nAnswer: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Question: {text}\\n\\nAnswer the above question by \"\n    \"replying with 'a', 'b', 'c' or 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nMMLU_DA_CONFIG = DatasetConfig(\n    name=\"mmlu-da\",\n    pretty_name=\"the truncated version of the Danish knowledge dataset MMLU-da, \"\n    \"translated from the English MMLU dataset\",\n    huggingface_id=\"ScandEval/mmlu-da-mini\",\n    task=KNOW,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rgsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rgsm\u00e5l: {text}\\n\\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at \"\n    \"svare med 'a', 'b', 'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nARC_DA_CONFIG = DatasetConfig(\n    name=\"arc-da\",\n    pretty_name=\"the truncated version of the Danish knowledge dataset ARC-da, \"\n    \"translated from the English ARC dataset\",\n    huggingface_id=\"ScandEval/arc-da-mini\",\n    task=KNOW,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rgsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rgsm\u00e5l: {text}\\n\\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at \"\n    \"svare med 'a', 'b', 'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nARC_NO_CONFIG = DatasetConfig(\n    name=\"arc-no\",\n    pretty_name=\"the truncated version of the Norwegian knowledge dataset ARC-no, \"\n    \"translated from the English ARC dataset\",\n    huggingface_id=\"ScandEval/arc-no-mini\",\n    task=KNOW,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"F\u00f8lgende er flervalgssp\u00f8rsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rsm\u00e5l: {text}\\n\\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', \"\n    \"'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nARC_SV_CONFIG = DatasetConfig(\n    name=\"arc-sv\",\n    pretty_name=\"the truncated version of the Swedish knowledge dataset ARC-sv, \"\n    \"translated from the English ARC dataset\",\n    huggingface_id=\"ScandEval/arc-sv-mini\",\n    task=KNOW,\n    languages=[SV],\n    prompt_prefix=\"F\u00f6ljande \u00e4r flervalsfr\u00e5gor (med svar).\",\n    prompt_template=\"Fr\u00e5ga: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Fr\u00e5ga: {text}\\n\\nBesvara f\u00f6ljande fr\u00e5ga med 'a', 'b', 'c' \"\n    \"eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nARC_IS_CONFIG = DatasetConfig(\n    name=\"arc-is\",\n    pretty_name=\"the truncated version of the Icelandic knowledge dataset ARC-is, \"\n    \"translated from the English ARC dataset\",\n    huggingface_id=\"ScandEval/arc-is-mini\",\n    task=KNOW,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\",\n    prompt_template=\"Spurningar: {text}\\nSvara: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Spurningar: {text}\\n\\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', \"\n    \"'b', 'c' e\u00f0a 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nARC_DE_CONFIG = DatasetConfig(\n    name=\"arc-de\",\n    pretty_name=\"the truncated version of the German knowledge dataset ARC-de, \"\n    \"translated from the English ARC dataset\",\n    huggingface_id=\"ScandEval/arc-de-mini\",\n    task=KNOW,\n    languages=[DE],\n    prompt_prefix=\"Die folgenden Fragen sind Multiple-Choice-Fragen (mit Antworten).\",\n    prompt_template=\"Frage: {text}\\nAntwort: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Frage: {text}\\n\\nBeantworten Sie die obige Frage mit 'a', 'b', \"\n    \"'c' oder 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nARC_NL_CONFIG = DatasetConfig(\n    name=\"arc-nl\",\n    pretty_name=\"the truncated version of the Dutch knowledge dataset ARC-nl, \"\n    \"translated from the English ARC dataset\",\n    huggingface_id=\"ScandEval/arc-nl-mini\",\n    task=KNOW,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan meerkeuzevragen (met antwoorden).\",\n    prompt_template=\"Vraag: {text}\\nAntwoord: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Vraag: {text}\\n\\nBeantwoord de bovenstaande vraag met 'a', \"\n    \"'b', 'c' of 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nARC_CONFIG = DatasetConfig(\n    name=\"arc\",\n    pretty_name=\"the truncated version of the English knowledge dataset ARC\",\n    huggingface_id=\"ScandEval/arc-mini\",\n    task=KNOW,\n    languages=[EN],\n    prompt_prefix=\"The following are multiple choice questions (with answers).\",\n    prompt_template=\"Question: {text}\\nAnswer: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Question: {text}\\n\\nAnswer the above question by \"\n    \"replying with 'a', 'b', 'c' or 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\n# TODO: Faroese knowledge\n\n\n### COMMON SENSE REASONING DATASETS ###\n\nHELLASWAG_DA_CONFIG = DatasetConfig(\n    name=\"hellaswag-da\",\n    pretty_name=\"the truncated version of the Danish common-sense reasoning dataset \"\n    \"HellaSwag-da, translated from the English HellaSwag dataset\",\n    huggingface_id=\"ScandEval/hellaswag-da-mini\",\n    task=COMMON_SENSE,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rgsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rgsm\u00e5l: {text}\\n\\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at \"\n    \"svare med 'a', 'b', 'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nHELLASWAG_NO_CONFIG = DatasetConfig(\n    name=\"hellaswag-no\",\n    pretty_name=\"the truncated version of the Norwegian common-sense reasoning dataset \"\n    \"HellaSwag-no, translated from the English HellaSwag dataset\",\n    huggingface_id=\"ScandEval/hellaswag-no-mini\",\n    task=COMMON_SENSE,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"F\u00f8lgende er flervalgssp\u00f8rsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rsm\u00e5l: {text}\\n\\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', \"\n    \"'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nHELLASWAG_SV_CONFIG = DatasetConfig(\n    name=\"hellaswag-sv\",\n    pretty_name=\"the truncated version of the Swedish common-sense reasoning dataset \"\n    \"HellaSwag-sv, translated from the English HellaSwag dataset\",\n    huggingface_id=\"ScandEval/hellaswag-sv-mini\",\n    task=COMMON_SENSE,\n    languages=[SV],\n    prompt_prefix=\"F\u00f6ljande \u00e4r flervalsfr\u00e5gor (med svar).\",\n    prompt_template=\"Fr\u00e5ga: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Fr\u00e5ga: {text}\\n\\nBesvara f\u00f6ljande fr\u00e5ga med 'a', 'b', 'c' \"\n    \"eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nHELLASWAG_IS_CONFIG = DatasetConfig(\n    name=\"hellaswag-is\",\n    pretty_name=\"the truncated version of the Icelandic common-sense reasoning dataset \"\n    \"HellaSwag-is, translated from the English HellaSwag dataset\",\n    huggingface_id=\"ScandEval/hellaswag-is-mini\",\n    task=COMMON_SENSE,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\",\n    prompt_template=\"Spurningar: {text}\\nSvara: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Spurningar: {text}\\n\\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', \"\n    \"'b', 'c' e\u00f0a 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nWINOGRANDE_IS = DatasetConfig(\n    name=\"winogrande-is\",\n    pretty_name=\"the Icelandic common-sense reasoning dataset \"\n    \"Winogrande-is, manually translated from the English Winogrande dataset\",\n    huggingface_id=\"ScandEval/winogrande-is\",\n    task=COMMON_SENSE,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\",\n    prompt_template=\"Spurningar: {text}\\nSvara: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Spurningar: {text}\\n\\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', \"\n    \"'b', 'c' e\u00f0a 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nHELLASWAG_DE_CONFIG = DatasetConfig(\n    name=\"hellaswag-de\",\n    pretty_name=\"the truncated version of the German common-sense reasoning dataset \"\n    \"HellaSwag-de, translated from the English HellaSwag dataset\",\n    huggingface_id=\"ScandEval/hellaswag-de-mini\",\n    task=COMMON_SENSE,\n    languages=[DE],\n    prompt_prefix=\"Die folgenden Fragen sind Multiple-Choice-Fragen (mit Antworten).\",\n    prompt_template=\"Frage: {text}\\nAntwort: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Frage: {text}\\n\\nBeantworten Sie die obige Frage mit 'a', 'b', \"\n    \"'c' oder 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nHELLASWAG_NL_CONFIG = DatasetConfig(\n    name=\"hellaswag-nl\",\n    pretty_name=\"the truncated version of the Dutch common-sense reasoning dataset \"\n    \"HellaSwag-nl, translated from the English HellaSwag dataset\",\n    huggingface_id=\"ScandEval/hellaswag-nl-mini\",\n    task=COMMON_SENSE,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan meerkeuzevragen (met antwoorden).\",\n    prompt_template=\"Vraag: {text}\\nAntwoord: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Vraag: {text}\\n\\nBeantwoord de bovenstaande vraag met 'a', 'b', \"\n    \"'c' of 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\nHELLASWAG_CONFIG = DatasetConfig(\n    name=\"hellaswag\",\n    pretty_name=\"the truncated version of the English common-sense reasoning \"\n    \"dataset HellaSwag\",\n    huggingface_id=\"ScandEval/hellaswag-mini\",\n    task=COMMON_SENSE,\n    languages=[EN],\n    prompt_prefix=\"The following are multiple choice questions (with answers).\",\n    prompt_template=\"Question: {text}\\nAnswer: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Question: {text}\\n\\nAnswer the above question by \"\n    \"replying with 'a', 'b', 'c' or 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n)\n\n# TODO: Faroese common sense reasoning\n\n\n### MULTIPLE CHOICE READING COMPREHENSION DATASETS ###\n\nBELEBELE_DA_CONFIG = DatasetConfig(\n    name=\"belebele-da\",\n    pretty_name=\"the Danish multiple choice reading comprehension dataset BeleBele-da, \"\n    \"translated from the English BeleBele dataset\",\n    huggingface_id=\"ScandEval/belebele-da-mini\",\n    task=MCRC,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er tekster med tilh\u00f8rende multiple choice sp\u00f8rgsm\u00e5l og \"\n    \"svar.\",\n    prompt_template=\"{text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rgsm\u00e5l: {text}\\n\\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at \"\n    \"svare med 'a', 'b', 'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nBELEBELE_SV_CONFIG = DatasetConfig(\n    name=\"belebele-sv\",\n    pretty_name=\"the Swedish multiple choice reading comprehension dataset \"\n    \"BeleBele-sv, translated from the English BeleBele dataset\",\n    huggingface_id=\"ScandEval/belebele-sv-mini\",\n    task=MCRC,\n    languages=[SV],\n    prompt_prefix=\"Nedan f\u00f6ljer texter med tillh\u00f6rande multiple choice fr\u00e5gor och \"\n    \"svar.\",\n    prompt_template=\"{text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Fr\u00e5ga: {text}\\n\\nBesvara f\u00f6ljande fr\u00e5ga med 'a', 'b', 'c' \"\n    \"eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nBELEBELE_NO_CONFIG = DatasetConfig(\n    name=\"belebele-no\",\n    pretty_name=\"the Norwegian multiple choice reading comprehension dataset \"\n    \"BeleBele-no, translated from the English BeleBele dataset\",\n    huggingface_id=\"ScandEval/belebele-no-mini\",\n    task=MCRC,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"Her f\u00f8lger tekster med tilh\u00f8rende multiple choice sp\u00f8rsm\u00e5l og svar.\",\n    prompt_template=\"{text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rsm\u00e5l: {text}\\n\\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', \"\n    \"'c' eller 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nBELEBELE_IS_CONFIG = DatasetConfig(\n    name=\"belebele-is\",\n    pretty_name=\"the Icelandic multiple choice reading comprehension dataset \"\n    \"BeleBele-is, translated from the English BeleBele dataset\",\n    huggingface_id=\"ScandEval/belebele-is-mini\",\n    task=MCRC,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru textar me\u00f0 tilheyrandi fj\u00f6lvalsspurningum og \"\n    \"sv\u00f6rum.\",\n    prompt_template=\"{text}\\nSvara: {label}\",\n    instruction_prompt=\"Spurningar: {text}\\n\\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', \"\n    \"'b', 'c' e\u00f0a 'd'.\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nBELEBELE_DE_CONFIG = DatasetConfig(\n    name=\"belebele-de\",\n    pretty_name=\"the German multiple choice reading comprehension dataset BeleBele-de, \"\n    \"translated from the English BeleBele dataset\",\n    huggingface_id=\"ScandEval/belebele-de-mini\",\n    task=MCRC,\n    languages=[DE],\n    prompt_prefix=\"Die folgenden Texte sind mit dazugeh\u00f6rigen Multiple-Choice-Fragen \"\n    \"und Antworten.\",\n    prompt_template=\"{text}\\nAntwort: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Frage: {text}\\n\\nBeantworten Sie die obige Frage mit 'a', 'b', \"\n    \"'c' oder 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nBELEBELE_NL_CONFIG = DatasetConfig(\n    name=\"belebele-nl\",\n    pretty_name=\"the Dutch multiple choice reading comprehension dataset BeleBele-nl, \"\n    \"translated from the English BeleBele dataset\",\n    huggingface_id=\"ScandEval/belebele-nl-mini\",\n    task=MCRC,\n    languages=[NL],\n    prompt_prefix=\"Hieronder staan teksten met bijbehorende multiple choice vragen en \"\n    \"antwoorden.\",\n    prompt_template=\"{text}\\nAntwoord: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Vraag: {text}\\n\\nBeantwoord de bovenstaande vraag met 'a', \"\n    \"'b', 'c' of 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\nBELEBELE_CONFIG = DatasetConfig(\n    name=\"belebele\",\n    pretty_name=\"the English multiple choice reading comprehension dataset BeleBele\",\n    huggingface_id=\"ScandEval/belebele-mini\",\n    task=MCRC,\n    languages=[EN],\n    prompt_prefix=\"The following are texts with accompanying multiple choice questions \"\n    \"and answers.\",\n    prompt_template=\"{text}\\nAnswer: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Question: {text}\\n\\nAnswer the above question by \"\n    \"replying with 'a', 'b', 'c' or 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=1,\n    unofficial=True,\n)\n\n\n### SPEED ESTIMATION DATASETS ###\n\nSPEED_CONFIG = DatasetConfig(\n    name=\"speed\",\n    pretty_name=\"the speed estimation benchmark\",\n    huggingface_id=\"\",\n    task=SPEED,\n    languages=list(get_all_languages().values()),\n    prompt_prefix=\"\",\n    prompt_template=\"\",\n    instruction_prompt=\"\",\n    num_few_shot_examples=0,\n    max_generated_tokens=1,\n)\n</code></pre>"},{"location":"api/scandeval/dataset_factory/","title":"scandeval.dataset_factory","text":"scandeval.dataset_factory<p> source module scandeval.dataset_factory </p> <p>Factory which produces datasets from a configuration.</p> <p> Classes </p> <ul> <li> <p>DatasetFactory \u2014 Factory which produces datasets from a configuration.</p> </li> </ul> <p> source class DatasetFactory() </p> <p>Factory which produces datasets from a configuration.</p> <p>Initialize the dataset factory.</p> <p> Attributes </p> <ul> <li> <p>benchmark_config \u2014</p> <p>The benchmark configuration to be used in all datasets constructed.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration to be used in all datasets constructed.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>build_dataset \u2014 Build a dataset from a configuration or a name.</p> </li> </ul> <p> source method DatasetFactory.build_dataset(dataset: str | DatasetConfig) \u2192 BenchmarkDataset </p> <p>Build a dataset from a configuration or a name.</p> <p> Parameters </p> <ul> <li> <p>dataset :  str | DatasetConfig \u2014</p> <p>The name of the dataset, or the dataset configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BenchmarkDataset \u2014</p> <pre><code>The benchmark dataset.\n</code></pre> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError</p> </li> </ul>"},{"location":"src/scandeval/dataset_factory/","title":"scandeval.dataset_factory","text":"scandeval.dataset_factory<p> docs module scandeval.dataset_factory </p> <pre><code>\"\"\"Factory which produces datasets from a configuration.\"\"\"\n\nfrom typing import TYPE_CHECKING, Type\n\nfrom .dataset_configs import get_dataset_config\nfrom .utils import get_class_by_name\n\nif TYPE_CHECKING:\n    from .benchmark_dataset import BenchmarkDataset\n    from .config import BenchmarkConfig, DatasetConfig\n\n\nclass DatasetFactory:docs\n    \"\"\"Factory which produces datasets from a configuration.\n\n    Attributes:\n        benchmark_config:\n            The benchmark configuration to be used in all datasets constructed.\n    \"\"\"\n\n    def __init__(self, benchmark_config: \"BenchmarkConfig\") -&gt; None:\n        \"\"\"Initialize the dataset factory.\n\n        Args:\n            benchmark_config:\n                The benchmark configuration to be used in all datasets constructed.\n        \"\"\"\n        self.benchmark_config = benchmark_config\ndocs\n    def build_dataset(self, dataset: \"str | DatasetConfig\") -&gt; \"BenchmarkDataset\":\n        \"\"\"Build a dataset from a configuration or a name.\n\n        Args:\n            dataset:\n                The name of the dataset, or the dataset configuration.\n\n        Returns:\n            BenchmarkDataset:\n                The benchmark dataset.\n        \"\"\"\n        # Get the dataset configuration\n        if isinstance(dataset, str):\n            dataset_config = get_dataset_config(dataset)\n        else:\n            dataset_config = dataset\n\n        # Get the benchmark class based on the task\n        potential_class_names = [\n            dataset_config.name,\n            dataset_config.task.name,\n            dataset_config.task.supertask,\n        ]\n        benchmark_cls: Type[\"BenchmarkDataset\"] | None = get_class_by_name(\n            class_name=potential_class_names\n        )\n        if not benchmark_cls:\n            raise ValueError(\n                \"Could not find a benchmark class for any of the following potential \"\n                f\"names: {', '.join(potential_class_names)}.\"\n            )\n\n        # Create the dataset\n        dataset_obj = benchmark_cls(\n            dataset_config=dataset_config, benchmark_config=self.benchmark_config\n        )\n\n        return dataset_obj\n</code></pre>"},{"location":"api/scandeval/enums/","title":"scandeval.enums","text":"scandeval.enums<p> source module scandeval.enums </p> <p>Enums used in the project.</p> <p> Classes </p> <ul> <li> <p>AutoStrEnum \u2014 StrEnum where auto() returns the field name in lower case.</p> </li> <li> <p>Device \u2014 The compute device to use for the evaluation.</p> </li> <li> <p>Framework \u2014 The framework of a model.</p> </li> <li> <p>ModelType \u2014 The type of a model.</p> </li> <li> <p>DataType \u2014 The data type of the model weights.</p> </li> </ul> <p> source enum AutoStrEnum() </p> <p><p>Bases : str, Enum</p></p> <p>StrEnum where auto() returns the field name in lower case.</p> <p> source enum Device() </p> <p><p>Bases : AutoStrEnum</p></p> <p>The compute device to use for the evaluation.</p> <p> Attributes </p> <ul> <li> <p>CPU \u2014</p> <p>CPU device.</p> </li> <li> <p>MPS \u2014</p> <p>MPS GPU, used in M-series MacBooks.</p> </li> <li> <p>CUDA \u2014</p> <p>CUDA GPU, used with NVIDIA GPUs.</p> </li> </ul> <p> source enum Framework() </p> <p><p>Bases : AutoStrEnum</p></p> <p>The framework of a model.</p> <p> Attributes </p> <ul> <li> <p>PYTORCH \u2014</p> <p>PyTorch framework.</p> </li> <li> <p>JAX \u2014</p> <p>JAX framework.</p> </li> <li> <p>API \u2014</p> <p>Accessible via an API.</p> </li> <li> <p>HUMAN \u2014</p> <p>Human evaluator.</p> </li> </ul> <p> source enum ModelType() </p> <p><p>Bases : AutoStrEnum</p></p> <p>The type of a model.</p> <p> Attributes </p> <ul> <li> <p>FRESH \u2014</p> <p>Randomly initialised Hugging Face model.</p> </li> <li> <p>HF \u2014</p> <p>Model from the Hugging Face Hub.</p> </li> <li> <p>LOCAL \u2014</p> <p>Locally stored Hugging Face model.</p> </li> <li> <p>OPENAI \u2014</p> <p>Model from OpenAI.</p> </li> <li> <p>HUMAN \u2014</p> <p>Human evaluator.</p> </li> </ul> <p> source enum DataType() </p> <p><p>Bases : AutoStrEnum</p></p> <p>The data type of the model weights.</p> <p> Attributes </p> <ul> <li> <p>FP32 \u2014</p> <p>32-bit floating point.</p> </li> <li> <p>FP16 \u2014</p> <p>16-bit floating point.</p> </li> <li> <p>BF16 \u2014</p> <p>16-bit bfloat.</p> </li> </ul>"},{"location":"src/scandeval/enums/","title":"scandeval.enums","text":"scandeval.enums<p> docs module scandeval.enums </p> <pre><code>\"\"\"Enums used in the project.\"\"\"\n\nfrom enum import Enum, auto\n\n\nclass AutoStrEnum(str, Enum):docs\n    \"\"\"StrEnum where auto() returns the field name in lower case.\"\"\"\n\n    @staticmethod\n    def _generate_next_value_(\n        name: str, start: int, count: int, last_values: list\n    ) -&gt; str:\n        return name.lower()\n\n\nclass Device(AutoStrEnum):docs\n    \"\"\"The compute device to use for the evaluation.\n\n    Attributes:\n        CPU:\n            CPU device.\n        MPS:\n            MPS GPU, used in M-series MacBooks.\n        CUDA:\n            CUDA GPU, used with NVIDIA GPUs.\n    \"\"\"\n\n    CPU = auto()\n    MPS = auto()\n    CUDA = auto()\n\n\nclass Framework(AutoStrEnum):docs\n    \"\"\"The framework of a model.\n\n    Attributes:\n        PYTORCH:\n            PyTorch framework.\n        JAX:\n            JAX framework.\n        API:\n            Accessible via an API.\n        HUMAN:\n            Human evaluator.\n    \"\"\"\n\n    PYTORCH = auto()\n    JAX = auto()\n    API = auto()\n    HUMAN = auto()\n\n\nclass ModelType(AutoStrEnum):docs\n    \"\"\"The type of a model.\n\n    Attributes:\n        FRESH:\n            Randomly initialised Hugging Face model.\n        HF:\n            Model from the Hugging Face Hub.\n        LOCAL:\n            Locally stored Hugging Face model.\n        OPENAI:\n            Model from OpenAI.\n        HUMAN:\n            Human evaluator.\n    \"\"\"\n\n    FRESH = auto()\n    HF = auto()\n    LOCAL = auto()\n    OPENAI = auto()\n    HUMAN = auto()\n\n\nclass DataType(AutoStrEnum):docs\n    \"\"\"The data type of the model weights.\n\n    Attributes:\n        FP32:\n            32-bit floating point.\n        FP16:\n            16-bit floating point.\n        BF16:\n            16-bit bfloat.\n    \"\"\"\n\n    FP32 = auto()\n    FP16 = auto()\n    BF16 = auto()\n</code></pre>"},{"location":"api/scandeval/exceptions/","title":"scandeval.exceptions","text":"scandeval.exceptions<p> source module scandeval.exceptions </p> <p>Exceptions to used by other functions.</p> <p> Classes </p> <ul> <li> <p>InvalidBenchmark \u2014 The (model, dataset) combination cannot be benchmarked.</p> </li> <li> <p>InvalidModel \u2014 The model cannot be benchmarked on any datasets.</p> </li> <li> <p>HuggingFaceHubDown \u2014 The Hugging Face Hub seems to be down.</p> </li> <li> <p>NoInternetConnection \u2014 There seems to be no internet connection.</p> </li> <li> <p>NaNValueInModelOutput \u2014 There is a NaN value in the model output.</p> </li> <li> <p>FlashAttentionNotInstalled \u2014 The <code>flash-attn</code> package has not been installed.</p> </li> <li> <p>NeedsExtraInstalled \u2014 The evaluation requires extra to be installed.</p> </li> <li> <p>NeedsManualDependency \u2014 The evaluation requires a dependency to be manually installed.</p> </li> <li> <p>NeedsAdditionalArgument \u2014 The evaluation requires additional arguments to the <code>scandeval</code> command.</p> </li> <li> <p>MissingHuggingFaceToken \u2014 The evaluation requires a Hugging Face token.</p> </li> </ul> <p> source class InvalidBenchmark() </p> <p><p>Bases : Exception</p></p> <p>The (model, dataset) combination cannot be benchmarked.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to display.</p> </li> </ul> <p> source class InvalidModel() </p> <p><p>Bases : Exception</p></p> <p>The model cannot be benchmarked on any datasets.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to display.</p> </li> </ul> <p> source class HuggingFaceHubDown() </p> <p><p>Bases : Exception</p></p> <p>The Hugging Face Hub seems to be down.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to display.</p> </li> </ul> <p> source class NoInternetConnection() </p> <p><p>Bases : Exception</p></p> <p>There seems to be no internet connection.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to display.</p> </li> </ul> <p> source class NaNValueInModelOutput() </p> <p><p>Bases : Exception</p></p> <p>There is a NaN value in the model output.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to display.</p> </li> </ul> <p> source class FlashAttentionNotInstalled() </p> <p><p>Bases : Exception</p></p> <p>The <code>flash-attn</code> package has not been installed.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to display.</p> </li> </ul> <p> source class NeedsExtraInstalled() </p> <p><p>Bases : InvalidModel</p></p> <p>The evaluation requires extra to be installed.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>extra :  str \u2014</p> <p>The extra that needs to be installed.</p> </li> </ul> <p> source class NeedsManualDependency() </p> <p><p>Bases : InvalidModel</p></p> <p>The evaluation requires a dependency to be manually installed.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>package :  str \u2014</p> <p>The package that needs to be manually installed.</p> </li> </ul> <p> source class NeedsAdditionalArgument(script_argument: str, run_with_cli: bool) </p> <p><p>Bases : InvalidModel</p></p> <p>The evaluation requires additional arguments to the <code>scandeval</code> command.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>cli_argument :  str \u2014</p> <p>The argument that needs to be passed to the <code>scandeval</code> command.</p> </li> <li> <p>script_argument :  str \u2014</p> <p>The argument that needs to be passed to the <code>Benchmarker</code> class.</p> </li> <li> <p>run_with_cli :  bool \u2014</p> <p>Whether the benchmark is being run with the CLI.</p> </li> </ul> <p> source class MissingHuggingFaceToken() </p> <p><p>Bases : InvalidModel</p></p> <p>The evaluation requires a Hugging Face token.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>run_with_cli :  bool \u2014</p> <p>Whether the benchmark is being run with the CLI.</p> </li> </ul>"},{"location":"src/scandeval/exceptions/","title":"scandeval.exceptions","text":"scandeval.exceptions<p> docs module scandeval.exceptions </p> <pre><code>\"\"\"Exceptions to used by other functions.\"\"\"\n\n\nclass InvalidBenchmark(Exception):docs\n    \"\"\"The (model, dataset) combination cannot be benchmarked.\"\"\"\n\n    def __init__(\n        self, message: str = \"This model cannot be benchmarked on the given dataset.\"\n    ):\n        \"\"\"Initialize the exception.\n\n        Args:\n            message:\n                The message to display.\n        \"\"\"\n        self.message = message\n        super().__init__(self.message)\n\n\nclass InvalidModel(Exception):docs\n    \"\"\"The model cannot be benchmarked on any datasets.\"\"\"\n\n    def __init__(\n        self, message: str = \"The model cannot be benchmarked on any datasets.\"\n    ):\n        \"\"\"Initialize the exception.\n\n        Args:\n            message:\n                The message to display.\n        \"\"\"\n        self.message = message\n        super().__init__(self.message)\n\n\nclass HuggingFaceHubDown(Exception):docs\n    \"\"\"The Hugging Face Hub seems to be down.\"\"\"\n\n    def __init__(self, message: str = \"The Hugging Face Hub is currently down.\"):\n        \"\"\"Initialize the exception.\n\n        Args:\n            message:\n                The message to display.\n        \"\"\"\n        self.message = message\n        super().__init__(self.message)\n\n\nclass NoInternetConnection(Exception):docs\n    \"\"\"There seems to be no internet connection.\"\"\"\n\n    def __init__(self, message: str = \"There is currently no internet connection.\"):\n        \"\"\"Initialize the exception.\n\n        Args:\n            message:\n                The message to display.\n        \"\"\"\n        self.message = message\n        super().__init__(self.message)\n\n\nclass NaNValueInModelOutput(Exception):docs\n    \"\"\"There is a NaN value in the model output.\"\"\"\n\n    def __init__(self, message: str = \"There is a NaN value in the model output.\"):\n        \"\"\"Initialize the exception.\n\n        Args:\n            message:\n                The message to display.\n        \"\"\"\n        self.message = message\n        super().__init__(self.message)\n\n\nclass FlashAttentionNotInstalled(Exception):docs\n    \"\"\"The `flash-attn` package has not been installed.\"\"\"\n\n    def __init__(\n        self,\n        message: str = (\n            \"The model you are trying to load requires Flash Attention. To use Flash \"\n            \"Attention, please install the `flash-attn` package, which can be done by \"\n            \"running `pip install -U wheel &amp;&amp; FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE \"\n            \"pip install flash-attn --no-build-isolation`.\"\n        ),\n    ):\n        \"\"\"Initialize the exception.\n\n        Args:\n            message:\n                The message to display.\n        \"\"\"\n        self.message = message\n        super().__init__(self.message)\n\n\nclass NeedsExtraInstalled(InvalidModel):docs\n    \"\"\"The evaluation requires extra to be installed.\"\"\"\n\n    def __init__(self, extra: str):\n        \"\"\"Initialize the exception.\n\n        Args:\n            extra:\n                The extra that needs to be installed.\n        \"\"\"\n        self.message = (\n            f\"The model you are trying to load requires the `{extra}` extra to be \"\n            f\"installed. To install the `{extra}` extra, please run `pip install \"\n            f\"scandeval[{extra}]` or `pip install scandeval[all]`.\"\n        )\n        super().__init__(self.message)\n\n\nclass NeedsManualDependency(InvalidModel):docs\n    \"\"\"The evaluation requires a dependency to be manually installed.\"\"\"\n\n    def __init__(self, package: str):\n        \"\"\"Initialize the exception.\n\n        Args:\n            package:\n                The package that needs to be manually installed.\n        \"\"\"\n        self.message = (\n            f\"The model you are trying to load requires the `{package}` package to be \"\n            f\"installed - please run `pip install {package}` and try again.\"\n        )\n        super().__init__(self.message)\n\n\nclass NeedsAdditionalArgument(InvalidModel):docs\n    \"\"\"The evaluation requires additional arguments to the `scandeval` command.\"\"\"\n\n    def __init__(self, cli_argument: str, script_argument: str, run_with_cli: bool):\n        \"\"\"Initialize the exception.\n\n        Args:\n            cli_argument:\n                The argument that needs to be passed to the `scandeval` command.\n            script_argument:\n                The argument that needs to be passed to the `Benchmarker` class.\n            run_with_cli:\n                Whether the benchmark is being run with the CLI.\n        \"\"\"\n        if run_with_cli:\n            self.message = (\n                f\"The model you are trying to load requires the `{cli_argument}` \"\n                \"argument to be passed to the `scandeval` command. Please pass the \"\n                \"argument and try again.\"\n            )\n        else:\n            self.message = (\n                f\"The model you are trying to load requires the `{script_argument}` \"\n                \"argument  to be passed to the `Benchmarker` class. Please pass the \"\n                \"argument and try again.\"\n            )\n        super().__init__(self.message)\n\n\nclass MissingHuggingFaceToken(InvalidModel):docs\n    \"\"\"The evaluation requires a Hugging Face token.\"\"\"\n\n    def __init__(self, run_with_cli: bool):\n        \"\"\"Initialize the exception.\n\n        Args:\n            run_with_cli:\n                Whether the benchmark is being run with the CLI.\n        \"\"\"\n        self.message = (\n            \"The model you are trying to load requires a Hugging Face token. \"\n        )\n        if run_with_cli:\n            self.message += (\n                \"Please run `huggingface-cli login` to login to the Hugging Face \"\n                \"Hub and try again. Alternatively, you can pass your Hugging Face Hub \"\n                \"token directly to the `scandeval` command with the `--token &lt;token&gt;` \"\n                \"argument.\"\n            )\n        else:\n            self.message += (\n                \"Please pass your Hugging Face Hub token to the `Benchmarker` class \"\n                \"with the `token=&lt;token&gt;` argument  to the `Benchmarker` class and try \"\n                \"again. Alternatively, you can also simply pass `token=True` (which is \"\n                \"the default) to the `Benchmarker` class, which assumes that you have \"\n                \"logged into the Hugging Face Hub. This can be done by running \"\n                \"`huggingface-cli login` in the terminal or `from huggingface_hub \"\n                \"import login; login()` in a Python script.\"\n            )\n        super().__init__(self.message)\n</code></pre>"},{"location":"api/scandeval/finetuning/","title":"scandeval.finetuning","text":"scandeval.finetuning<p> source module scandeval.finetuning </p> <p>Functions related to the finetuning of models.</p> <p> Functions </p> <ul> <li> <p>finetune \u2014 Evaluate a model on a dataset through finetuning.</p> </li> <li> <p>finetune_single_iteration \u2014 Run a single iteration of a benchmark.</p> </li> <li> <p>get_training_args \u2014 Get the training arguments for the current iteration.</p> </li> </ul> <p> source finetune(itr: tqdm, train: Dataset, val: Dataset, tests: list[Dataset], prepared_train: Dataset, prepared_val: Dataset, prepared_tests: list[Dataset], model: PreTrainedModel, tokenizer: Tokenizer, model_config: ModelConfig, benchmark_config: BenchmarkConfig, dataset_config: DatasetConfig, compute_metrics: Callable, data_collator: DataCollator, trainer_class: Type[Trainer], evaluate_inputs_fn: Callable[..., dict[str, Any]], preprocess_logits_for_metrics: Callable[[torch.Tensor | tuple, torch.Tensor], torch.Tensor | tuple]) \u2192 dict[str, list[dict[str, float]]] </p> <p>Evaluate a model on a dataset through finetuning.</p> <p> Parameters </p> <ul> <li> <p>itr :  tqdm \u2014</p> <p>The progress bar iterator.</p> </li> <li> <p>train :  Dataset \u2014</p> <p>The training dataset.</p> </li> <li> <p>val :  Dataset \u2014</p> <p>The validation dataset.</p> </li> <li> <p>tests :  list[Dataset] \u2014</p> <p>The bootstrapped test datasets.</p> </li> <li> <p>prepared_train :  Dataset \u2014</p> <p>The prepared training dataset.</p> </li> <li> <p>prepared_val :  Dataset \u2014</p> <p>The prepared validation dataset.</p> </li> <li> <p>prepared_tests :  list[Dataset] \u2014</p> <p>The prepared bootstrapped test datasets.</p> </li> <li> <p>model :  PreTrainedModel \u2014</p> <p>The model to evaluate.</p> </li> <li> <p>tokenizer :  Tokenizer \u2014</p> <p>The tokenizer to use.</p> </li> <li> <p>model_config :  ModelConfig \u2014</p> <p>The configuration of the model.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>compute_metrics :  Callable \u2014</p> <p>The function used to compute the metrics.</p> </li> <li> <p>data_collator :  DataCollator \u2014</p> <p>The data collator to use.</p> </li> <li> <p>trainer_class :  Type[Trainer] \u2014</p> <p>The Trainer class to use.</p> </li> <li> <p>evaluate_inputs_fn :  Callable[..., dict[str, Any]] \u2014</p> <p>A function that generates the appropriate inputs for the <code>Trainer.evaluate</code> method.</p> </li> <li> <p>preprocess_logits_for_metrics :  Callable[[torch.Tensor | tuple, torch.Tensor], torch.Tensor | tuple] \u2014</p> <p>A function that preprocesses the logits before they are passed to the <code>compute_metrics</code> function. This helps prevent memory issues during evaluation.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, list[dict[str, float]]] \u2014 A dictionary containing the scores, with keys \"test\" and maybe \"train\", with values being lists of dicts containing the scores for each metric for each iteration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source finetune_single_iteration(iteration_idx: int, model_config: ModelConfig, train: Dataset, test: Dataset, prepared_train: Dataset, prepared_val: Dataset, prepared_test: Dataset, training_args: TrainingArguments, benchmark_config: BenchmarkConfig, dataset_config: DatasetConfig, data_collator: DataCollator, compute_metrics: Callable, tokenizer: Tokenizer | None, model: PreTrainedModel | None, trainer_class: Type[Trainer], evaluate_inputs_fn: Callable[..., dict[str, Any]], preprocess_logits_for_metrics: Callable[[torch.Tensor | tuple, torch.Tensor], torch.Tensor | tuple]) \u2192 dict[str, dict[str, float]] </p> <p>Run a single iteration of a benchmark.</p> <p> Parameters </p> <ul> <li> <p>iteration_idx :  int \u2014</p> <p>The index of the iteration.</p> </li> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>train :  Dataset \u2014</p> <p>The original training dataset.</p> </li> <li> <p>test :  Dataset \u2014</p> <p>The original test dataset.</p> </li> <li> <p>prepared_train :  Dataset \u2014</p> <p>The prepared training dataset.</p> </li> <li> <p>prepared_val :  Dataset \u2014</p> <p>The prepared validation dataset.</p> </li> <li> <p>prepared_test :  Dataset \u2014</p> <p>The prepared test dataset.</p> </li> <li> <p>training_args :  TrainingArguments \u2014</p> <p>The training arguments.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>data_collator :  DataCollator \u2014</p> <p>The data collator.</p> </li> <li> <p>compute_metrics :  Callable \u2014</p> <p>The function to compute the metrics.</p> </li> <li> <p>tokenizer :  Tokenizer | None \u2014</p> <p>The tokenizer to use in the benchmark. If None then a new tokenizer will be loaded.</p> </li> <li> <p>model :  PreTrainedModel | None \u2014</p> <p>The model to use in the benchmark. If None then a new model will be loaded.</p> </li> <li> <p>trainer_class :  Type[Trainer] \u2014</p> <p>The trainer class to use.</p> </li> <li> <p>evaluate_inputs_fn :  Callable[..., dict[str, Any]] \u2014</p> <p>A function that generates the appropriate inputs for the <code>Trainer.evaluate</code> method.</p> </li> <li> <p>preprocess_logits_for_metrics :  Callable[[torch.Tensor | tuple, torch.Tensor], torch.Tensor | tuple] \u2014</p> <p>A function that preprocesses the logits before they are passed to the <code>compute_metrics</code> function. This helps prevent memory issues during evaluation.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, dict[str, float]] \u2014 A dictionary containing the scores for the current iteration, with keys <code>train</code> and <code>test</code>.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>e</p> </li> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source get_training_args(benchmark_config: BenchmarkConfig, model_config: ModelConfig, iteration_idx: int, dtype: DataType, batch_size: int | None = None) \u2192 TrainingArguments </p> <p>Get the training arguments for the current iteration.</p> <p> Parameters </p> <ul> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>iteration_idx :  int \u2014</p> <p>The index of the current iteration. This is only used to generate a unique random seed for the current iteration.</p> </li> <li> <p>dtype :  DataType \u2014</p> <p>The data type to use for the model weights.</p> </li> <li> <p>batch_size :  int | None \u2014</p> <p>The batch size to use for the current iteration, or None if the batch size in the benchmark config should be used.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>TrainingArguments \u2014 The training arguments for the current iteration.</p> </li> </ul>"},{"location":"src/scandeval/finetuning/","title":"scandeval.finetuning","text":"scandeval.finetuning<p> docs module scandeval.finetuning </p> <pre><code>\"\"\"Functions related to the finetuning of models.\"\"\"\n\nimport importlib.util\nimport logging\nimport sys\nfrom collections import defaultdict\nfrom functools import partial\nfrom typing import TYPE_CHECKING, Any, Callable, Type\n\nimport torch\nfrom datasets import Dataset\nfrom tqdm.auto import tqdm\nfrom transformers import (\n    EarlyStoppingCallback,\n    IntervalStrategy,\n    PreTrainedModel,\n    PrinterCallback,\n    ProgressCallback,\n    TrainingArguments,\n)\nfrom transformers.trainer import OptimizerNames\n\nfrom .callbacks import NeverLeaveProgressCallback\nfrom .enums import DataType\nfrom .exceptions import InvalidBenchmark, NaNValueInModelOutput\nfrom .model_loading import load_model\nfrom .utils import block_terminal_output, clear_memory, enforce_reproducibility\n\nif TYPE_CHECKING:\n    from transformers import DataCollator, Trainer\n\n    from .config import BenchmarkConfig, DatasetConfig, ModelConfig\n    from .protocols import Tokenizer\n\nlogger = logging.getLogger(__package__)\n\n\ndef finetune(docs\n    itr: tqdm,\n    train: Dataset,\n    val: Dataset,\n    tests: list[Dataset],\n    prepared_train: Dataset,\n    prepared_val: Dataset,\n    prepared_tests: list[Dataset],\n    model: PreTrainedModel,\n    tokenizer: \"Tokenizer\",\n    model_config: \"ModelConfig\",\n    benchmark_config: \"BenchmarkConfig\",\n    dataset_config: \"DatasetConfig\",\n    compute_metrics: Callable,\n    data_collator: \"DataCollator\",\n    trainer_class: Type[\"Trainer\"],\n    evaluate_inputs_fn: Callable[..., dict[str, Any]],\n    preprocess_logits_for_metrics: Callable[\n        [torch.Tensor | tuple, torch.Tensor], torch.Tensor | tuple\n    ],\n) -&gt; dict[str, list[dict[str, float]]]:\n    \"\"\"Evaluate a model on a dataset through finetuning.\n\n    Args:\n        itr:\n            The progress bar iterator.\n        train:\n            The training dataset.\n        val:\n            The validation dataset.\n        tests:\n            The bootstrapped test datasets.\n        prepared_train:\n            The prepared training dataset.\n        prepared_val:\n            The prepared validation dataset.\n        prepared_tests:\n            The prepared bootstrapped test datasets.\n        model:\n            The model to evaluate.\n        tokenizer:\n            The tokenizer to use.\n        model_config:\n            The configuration of the model.\n        benchmark_config:\n            The benchmark configuration.\n        dataset_config:\n            The dataset configuration.\n        compute_metrics:\n            The function used to compute the metrics.\n        data_collator:\n            The data collator to use.\n        trainer_class:\n            The Trainer class to use.\n        evaluate_inputs_fn:\n            A function that generates the appropriate inputs for the `Trainer.evaluate`\n            method.\n        preprocess_logits_for_metrics:\n            A function that preprocesses the logits before they are passed to the\n            `compute_metrics` function. This helps prevent memory issues during\n            evaluation.\n\n    Returns:\n        A dictionary containing the scores, with keys \"test\" and maybe \"train\", with\n        values being lists of dicts containing the scores for each metric for each\n        iteration.\n    \"\"\"\n    scores: dict[str, list[dict[str, float]]] = defaultdict(list)\n\n    using_cuda = benchmark_config.device == torch.device(\"cuda\")\n    if using_cuda and torch.cuda.is_bf16_supported():\n        dtype = DataType.BF16\n    elif using_cuda:\n        dtype = DataType.FP16\n    else:\n        dtype = DataType.FP32\n\n    bs: int = benchmark_config.batch_size\n    for idx in itr:\n        # Set variable that tracks whether we need to initialize new models in\n        # the single iteration call\n        model_already_initialized = idx == 0\n\n        # Run a loop here to deal with automatic reduction of batch size\n        while True:\n            # Clear GPU memory\n            if not model_already_initialized:\n                try:\n                    del model\n                except UnboundLocalError:\n                    pass\n                try:\n                    del tokenizer\n                except UnboundLocalError:\n                    pass\n                clear_memory()\n\n            try:\n                test = tests[idx]\n                prepared_test = prepared_tests[idx]\n                assert isinstance(test, Dataset)\n                assert isinstance(prepared_test, Dataset)\n\n                # Re-block terminal output, as it gets unblocked by the `transformers`\n                # package before training\n                block_terminal_output()\n\n                training_args = get_training_args(\n                    benchmark_config=benchmark_config,\n                    model_config=model_config,\n                    iteration_idx=idx,\n                    dtype=dtype,\n                    batch_size=bs,\n                )\n\n                itr_scores = finetune_single_iteration(\n                    iteration_idx=idx,\n                    model_config=model_config,\n                    train=train,\n                    prepared_train=prepared_train,\n                    prepared_val=prepared_val,\n                    test=test,\n                    prepared_test=prepared_test,\n                    training_args=training_args,\n                    benchmark_config=benchmark_config,\n                    dataset_config=dataset_config,\n                    data_collator=data_collator,\n                    compute_metrics=compute_metrics,\n                    tokenizer=tokenizer if model_already_initialized else None,\n                    model=model if model_already_initialized else None,\n                    trainer_class=trainer_class,\n                    evaluate_inputs_fn=evaluate_inputs_fn,\n                    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n                )\n\n                if \"train\" in itr_scores:\n                    logger.debug(\n                        f\"Train scores for iteration {idx}: {itr_scores['train']}\"\n                    )\n                    scores[\"train\"].append(itr_scores[\"train\"])\n                scores[\"test\"].append(itr_scores[\"test\"])\n                logger.debug(f\"Test scores for iteration {idx}: {itr_scores['test']}\")\n\n                break\n\n            # NaN values can appear in the model output when using mixed precision, as\n            # the hidden states get overflowed. In this case we try to disable mixed\n            # precision and try again.\n            except NaNValueInModelOutput:\n                if dtype != DataType.FP32:\n                    dtype = DataType.FP32\n                    model_already_initialized = False\n                    logger.debug(\n                        \"NaN value detected in model outputs while using mixed \"\n                        \"precision. Retrying with full fp32 precision.\"\n                    )\n                else:\n                    raise InvalidBenchmark(\n                        \"NaN value detected in model outputs, even with mixed \"\n                        \"precision disabled.\"\n                    )\n\n            except Exception as e:\n                if \"CUDA\" not in str(e) and \"out of memory\" not in str(e):\n                    raise InvalidBenchmark(str(e))\n\n                if bs &lt;= 1:\n                    msg = \"Could not benchmark the model, even with a batch size of 1!\"\n                    if \"MPS\" in str(e):\n                        msg += (\n                            \" As you are using MPS, you can try running the evaluation \"\n                            \"with the `PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0` \"\n                            \"environment variable set, as this removes the upper bound \"\n                            \"on the memory usage.\"\n                        )\n                    raise InvalidBenchmark(msg)\n\n                model_already_initialized = False\n\n                bs //= 2\n                logger.debug(f\"Reduced batch size to {bs}\")\n\n    return scores\n\n\ndef finetune_single_iteration(docs\n    iteration_idx: int,\n    model_config: \"ModelConfig\",\n    train: Dataset,\n    test: Dataset,\n    prepared_train: Dataset,\n    prepared_val: Dataset,\n    prepared_test: Dataset,\n    training_args: TrainingArguments,\n    benchmark_config: \"BenchmarkConfig\",\n    dataset_config: \"DatasetConfig\",\n    data_collator: \"DataCollator\",\n    compute_metrics: Callable,\n    tokenizer: \"Tokenizer | None\",\n    model: PreTrainedModel | None,\n    trainer_class: Type[\"Trainer\"],\n    evaluate_inputs_fn: Callable[..., dict[str, Any]],\n    preprocess_logits_for_metrics: Callable[\n        [torch.Tensor | tuple, torch.Tensor], torch.Tensor | tuple\n    ],\n) -&gt; dict[str, dict[str, float]]:\n    \"\"\"Run a single iteration of a benchmark.\n\n    Args:\n        iteration_idx:\n            The index of the iteration.\n        model_config:\n            The model configuration.\n        train:\n            The original training dataset.\n        test:\n            The original test dataset.\n        prepared_train:\n            The prepared training dataset.\n        prepared_val:\n            The prepared validation dataset.\n        prepared_test:\n            The prepared test dataset.\n        training_args:\n            The training arguments.\n        benchmark_config:\n            The benchmark configuration.\n        dataset_config:\n            The dataset configuration.\n        data_collator:\n            The data collator.\n        compute_metrics:\n            The function to compute the metrics.\n        tokenizer:\n            The tokenizer to use in the benchmark. If None then a new tokenizer\n            will be loaded.\n        model:\n            The model to use in the benchmark. If None then a new model will be\n            loaded.\n        trainer_class:\n            The trainer class to use.\n        evaluate_inputs_fn:\n            A function that generates the appropriate inputs for the `Trainer.evaluate`\n            method.\n        preprocess_logits_for_metrics:\n            A function that preprocesses the logits before they are passed to the\n            `compute_metrics` function. This helps prevent memory issues during\n            evaluation.\n\n    Returns:\n        A dictionary containing the scores for the current iteration, with keys `train`\n        and `test`.\n    \"\"\"\n    scores: dict[str, dict[str, float]] = dict()\n\n    # Set random seeds to enforce reproducibility of the randomly initialised\n    # weights\n    seed = 4242 + iteration_idx\n    enforce_reproducibility(framework=model_config.framework, seed=seed)\n\n    if tokenizer is None or model is None:\n        model_or_generative_model, tokenizer = load_model(\n            model_config=model_config,\n            dataset_config=dataset_config,\n            benchmark_config=benchmark_config,\n        )\n        assert isinstance(model_or_generative_model, PreTrainedModel)\n        model = model_or_generative_model\n\n    compute_metrics = partial(compute_metrics, id2label=dataset_config.id2label)\n    early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n    trainer = trainer_class(\n        model=model,\n        args=training_args,\n        train_dataset=prepared_train,\n        eval_dataset=prepared_val,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics,\n        callbacks=[early_stopping],\n        data_collator=data_collator,\n        preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n    )\n\n    if not benchmark_config.verbose:\n\n        def no_logging(logs: dict[str, float]) -&gt; None:\n            return\n\n        trainer.log = no_logging\n\n    # Re-block terminal output, as it gets unblocked by the `transformers`\n    # package before training\n    block_terminal_output()\n\n    # Sort out callbacks. We remove the callbacks that are producing unnecessary\n    # output, to avoid cluttering the terminal output\n    if not benchmark_config.verbose:\n        trainer.remove_callback(PrinterCallback)\n    trainer.remove_callback(ProgressCallback)\n    if benchmark_config.progress_bar:\n        trainer.add_callback(NeverLeaveProgressCallback)\n\n    try:\n        trainer.train()\n\n        if benchmark_config.evaluate_train:\n            with torch.inference_mode():\n                evaluate_inputs = evaluate_inputs_fn(\n                    dataset=train,\n                    prepared_dataset=prepared_train,\n                    metric_key_prefix=\"train\",\n                )\n                train_scores = trainer.evaluate(**evaluate_inputs)\n            scores[\"train\"] = train_scores\n\n        with torch.inference_mode():\n            evaluate_inputs = evaluate_inputs_fn(\n                dataset=test, prepared_dataset=prepared_test, metric_key_prefix=\"test\"\n            )\n            test_scores = trainer.evaluate(**evaluate_inputs)\n        scores[\"test\"] = test_scores\n\n        return scores\n\n    except NaNValueInModelOutput as e:\n        del trainer\n        del model\n        del tokenizer\n        clear_memory()\n        raise e\n\n    except (RuntimeError, ValueError, IndexError) as e:\n        raise InvalidBenchmark(str(e))\n\n\ndef get_training_args(docs\n    benchmark_config: \"BenchmarkConfig\",\n    model_config: \"ModelConfig\",\n    iteration_idx: int,\n    dtype: DataType,\n    batch_size: int | None = None,\n) -&gt; TrainingArguments:\n    \"\"\"Get the training arguments for the current iteration.\n\n    Args:\n        benchmark_config:\n            The benchmark configuration.\n        model_config:\n            The model configuration.\n        iteration_idx:\n            The index of the current iteration. This is only used to generate a\n            unique random seed for the current iteration.\n        dtype:\n            The data type to use for the model weights.\n        batch_size:\n            The batch size to use for the current iteration, or None if the batch size\n            in the benchmark config should be used.\n\n    Returns:\n        The training arguments for the current iteration.\n    \"\"\"\n    # Set the logging strategy\n    if benchmark_config.verbose:\n        logging_strategy = IntervalStrategy.STEPS\n    else:\n        logging_strategy = IntervalStrategy.NO\n\n    # Set seed variable\n    seed = 4242 + iteration_idx\n\n    if batch_size is None:\n        batch_size = benchmark_config.batch_size\n\n    if (\n        benchmark_config.device == torch.device(\"cuda\")\n        and importlib.util.find_spec(\"bitsandbytes\") is not None\n    ):\n        optimizer = OptimizerNames.ADAMW_8BIT\n    else:\n        optimizer = OptimizerNames.ADAMW_TORCH\n\n    training_args = TrainingArguments(\n        output_dir=model_config.model_cache_dir,\n        evaluation_strategy=IntervalStrategy.STEPS,\n        logging_strategy=logging_strategy,\n        save_strategy=IntervalStrategy.STEPS,\n        eval_steps=30,\n        logging_steps=30,\n        save_steps=30,\n        max_steps=1 if hasattr(sys, \"_called_from_test\") else 10_000,\n        use_cpu=benchmark_config.device == torch.device(\"cpu\"),\n        report_to=[],\n        save_total_limit=1,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        learning_rate=2e-5,\n        warmup_ratio=0.01,\n        gradient_accumulation_steps=32 // batch_size,\n        load_best_model_at_end=True,\n        optim=optimizer,\n        seed=seed,\n        fp16=dtype == DataType.FP16,\n        bf16=dtype == DataType.BF16,\n        disable_tqdm=not benchmark_config.progress_bar,\n        ddp_find_unused_parameters=False,\n        save_safetensors=False,\n    )\n\n    # TEMP: Use only 1 GPU for now for finetuning\n    if benchmark_config.device == torch.device(\"cuda\"):\n        training_args._n_gpu = 1\n\n    return training_args\n</code></pre>"},{"location":"api/scandeval/generation/","title":"scandeval.generation","text":"scandeval.generation<p> source module scandeval.generation </p> <p>Functions related to text generation of models.</p> <p> Classes </p> <ul> <li> <p>StopWordCriteria \u2014 Stopping criteria for generation based on stop words.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>generate \u2014 Evaluate a model on a dataset through generation.</p> </li> <li> <p>generate_single_iteration \u2014 Evaluate a model on a dataset in a single iteration through generation.</p> </li> <li> <p>generate_batch \u2014 Evaluate a model on a single batch of examples through generation.</p> </li> <li> <p>extract_raw_predictions \u2014 Get the raw predictions from the generated sequences.</p> </li> <li> <p>get_generation_stopping_criteria \u2014 Get the stopping criteria for generation.</p> </li> <li> <p>debug_log \u2014 Log inputs and outputs for debugging purposes.</p> </li> </ul> <p> source generate(itr: tqdm, prepared_train: Dataset, prepared_tests: list[Dataset], model: GenerativeModel, model_config: ModelConfig, tokenizer: Tokenizer, data_collator: DataCollator, compute_metrics: Callable, extract_labels_fn: Callable[..., list[Any]], benchmark_config: BenchmarkConfig, dataset_config: DatasetConfig) \u2192 dict[str, list[dict[str, float]]] </p> <p>Evaluate a model on a dataset through generation.</p> <p> Parameters </p> <ul> <li> <p>itr :  tqdm \u2014</p> <p>The progress bar iterator.</p> </li> <li> <p>prepared_train :  Dataset \u2014</p> <p>The prepared training dataset.</p> </li> <li> <p>prepared_tests :  list[Dataset] \u2014</p> <p>The prepared bootstrapped test datasets.</p> </li> <li> <p>num_iter \u2014</p> <p>The number of iterations to run.</p> </li> <li> <p>rng \u2014</p> <p>The random number generator.</p> </li> <li> <p>model :  GenerativeModel \u2014</p> <p>The model to evaluate.</p> </li> <li> <p>model_config :  ModelConfig \u2014</p> <p>The configuration of the model.</p> </li> <li> <p>tokenizer :  Tokenizer \u2014</p> <p>The tokenizer to use for the model. If <code>None</code> then the model's tokenizer will be used.</p> </li> <li> <p>data_collator :  DataCollator \u2014</p> <p>The data collator to use for the model.</p> </li> <li> <p>compute_metrics :  Callable \u2014</p> <p>The function to use to compute the metrics.</p> </li> <li> <p>extract_labels_fn :  Callable[..., list[Any]] \u2014</p> <p>The function to use to extract the labels from the model output.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The configuration of the benchmark.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, list[dict[str, float]]] \u2014 A dictionary containing the scores, with keys \"test\" and maybe \"train\", with values being lists of dicts containing the scores for each metric for each iteration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source generate_single_iteration(prepared_dataset: Dataset, model: GenerativeModel, tokenizer: Tokenizer, data_collator: DataCollator, compute_metrics: Callable, extract_labels_fn: Callable[..., list[Any]], dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig, cache: ModelCache) \u2192 dict[str, float] </p> <p>Evaluate a model on a dataset in a single iteration through generation.</p> <p> Parameters </p> <ul> <li> <p>prepared_dataset :  Dataset \u2014</p> <p>The dataset to evaluate on.</p> </li> <li> <p>model :  GenerativeModel \u2014</p> <p>The model to evaluate.</p> </li> <li> <p>tokenizer :  Tokenizer \u2014</p> <p>The tokenizer to use for the model.</p> </li> <li> <p>data_collator :  DataCollator \u2014</p> <p>The data collator to use for the model.</p> </li> <li> <p>compute_metrics :  Callable \u2014</p> <p>The function to use to compute the metrics.</p> </li> <li> <p>extract_labels_fn :  Callable[..., list[Any]] \u2014</p> <p>The function to use to extract the labels from the dataset.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The configuration of the benchmark.</p> </li> <li> <p>cache :  ModelCache \u2014</p> <p>The model output cache.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] \u2014 A list of dictionaries containing the scores for each metric.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError</p> </li> </ul> <p> source class StopWordCriteria() </p> <p><p>Bases : StoppingCriteria</p></p> <p>Stopping criteria for generation based on stop words.</p> <p>Initialize the stopping criteria.</p> <p> Attributes </p> <ul> <li> <p>stop_word_id_lists \u2014</p> <p>A list of lists of token IDs that are used to determine whether generation should stop.</p> </li> <li> <p>indices_done :  list[int] \u2014</p> <p>A list of indices of the examples for which generation has already stopped. Resets every batch.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>stop_word_id_lists :  list[list[int]] \u2014</p> <p>A list of lists of token IDs that are used to determine whether generation should stop.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>clear \u2014 Clear the example indices for which generation has already stopped.</p> </li> </ul> <p> source method StopWordCriteria.clear() \u2192 None </p> <p>Clear the example indices for which generation has already stopped.</p> <p> source generate_batch(batch: dict[str, torch.Tensor], batch_idx: int, batch_size: int, non_cached_dataset: Dataset, model: GenerativeModel, tokenizer: Tokenizer, stopping_criteria: StopWordCriteria, generation_config: GenerationConfig, extract_labels_fn: Callable[..., list[str]], dataset_config: DatasetConfig) \u2192 tuple[ModelOutput, list[dict | str | list[str]]] </p> <p>Evaluate a model on a single batch of examples through generation.</p> <p> Parameters </p> <ul> <li> <p>batch :  dict[str, torch.Tensor] \u2014</p> <p>The batch of examples to evaluate on.</p> </li> <li> <p>batch_idx :  int \u2014</p> <p>The index of the batch.</p> </li> <li> <p>batch_size :  int \u2014</p> <p>The size of the batch.</p> </li> <li> <p>non_cached_dataset :  Dataset \u2014</p> <p>The dataset to evaluate on.</p> </li> <li> <p>model :  GenerativeModel \u2014</p> <p>The model to evaluate.</p> </li> <li> <p>tokenizer :  Tokenizer \u2014</p> <p>The tokenizer used to encode the examples.</p> </li> <li> <p>stopping_criteria :  StopWordCriteria \u2014</p> <p>The stopping criteria to use to stop generation.</p> </li> <li> <p>generation_config :  GenerationConfig \u2014</p> <p>The generation configuration to use.</p> </li> <li> <p>extract_labels_fn :  Callable[..., list[str]] \u2014</p> <p>The function to use to extract the labels from the model output.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[ModelOutput, list[dict | str | list[str]]] \u2014 The predictions generated so far, with the predictions for the current batch appended.</p> </li> </ul> <p> source extract_raw_predictions(generated_sequences: torch.Tensor, tokenizer: Tokenizer) \u2192 list[str] </p> <p>Get the raw predictions from the generated sequences.</p> <p> Parameters </p> <ul> <li> <p>generated_sequences :  torch.Tensor \u2014</p> <p>The generated sequences from the model. The outer-most list is the batch dimension, the inner-most list is the sequence dimension, consisting of token IDs.</p> </li> <li> <p>tokenizer :  Tokenizer \u2014</p> <p>The tokenizer used to generate the tokens.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 The candidate labels with the smallest edit distance to the predicted labels.</p> </li> </ul> <p> source get_generation_stopping_criteria(tokenizer: Tokenizer, model: GenerativeModel) \u2192 StopWordCriteria </p> <p>Get the stopping criteria for generation.</p> <p> Parameters </p> <ul> <li> <p>tokenizer :  Tokenizer \u2014</p> <p>The tokenizer used to tokenize the stop words.</p> </li> <li> <p>model :  GenerativeModel \u2014</p> <p>The generative model, which we use to ensure the tensors are on the same device, and also determine whether stop words are needed, based on the model type.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>StopWordCriteria \u2014 The stopping criteria for generation.</p> </li> </ul> <p> source debug_log(batch_idx: int, batch_size: int, non_cached_dataset: Dataset, extracted_labels: list[dict | str | list[str]], dataset_config: DatasetConfig) \u2192 None </p> <p>Log inputs and outputs for debugging purposes.</p> <p> Parameters </p> <ul> <li> <p>batch_idx :  int \u2014</p> <p>The index of the batch.</p> </li> <li> <p>batch_size :  int \u2014</p> <p>The size of the batch.</p> </li> <li> <p>non_cached_dataset :  Dataset \u2014</p> <p>The dataset to evaluate on.</p> </li> <li> <p>extracted_labels :  list[dict | str | list[str]] \u2014</p> <p>The extracted labels from the model output.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> </ul>"},{"location":"src/scandeval/generation/","title":"scandeval.generation","text":"scandeval.generation<p> docs module scandeval.generation </p> <pre><code>\"\"\"Functions related to text generation of models.\"\"\"\n\nimport logging\nimport sys\nimport warnings\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any, Callable\n\nimport torch\nfrom datasets import Dataset\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nfrom transformers import (\n    GenerationConfig,\n    PreTrainedTokenizerBase,\n    StoppingCriteria,\n    StoppingCriteriaList,\n)\nfrom transformers.modeling_utils import ModelOutput\n\nfrom .exceptions import InvalidBenchmark\nfrom .model_cache import (\n    ModelCache,\n    load_cached_model_outputs,\n    split_dataset_into_cached_and_non_cached,\n)\nfrom .openai_models import OpenAIModel\nfrom .structured_generation_utils import (\n    get_ner_logits_processors,\n    get_ner_prefix_allowed_tokens_fn,\n)\nfrom .tasks import LA, NER, RC, SENT, SUMM\nfrom .utils import SUPERTASKS_USING_LOGPROBS, clear_memory, get_end_of_chat_token_ids\nfrom .vllm_models import VLLMModel\n\nif TYPE_CHECKING:\n    from transformers import DataCollator\n\n    from .config import BenchmarkConfig, DatasetConfig, ModelConfig\n    from .protocols import GenerativeModel, Tokenizer\n\nlogger = logging.getLogger(__package__)\n\n\ndef generate(docs\n    itr: tqdm,\n    prepared_train: Dataset,\n    prepared_tests: list[Dataset],\n    model: \"GenerativeModel\",\n    model_config: \"ModelConfig\",\n    tokenizer: \"Tokenizer\",\n    data_collator: \"DataCollator\",\n    compute_metrics: Callable,\n    extract_labels_fn: Callable[..., list[Any]],\n    benchmark_config: \"BenchmarkConfig\",\n    dataset_config: \"DatasetConfig\",\n) -&gt; dict[str, list[dict[str, float]]]:\n    \"\"\"Evaluate a model on a dataset through generation.\n\n    Args:\n        itr:\n            The progress bar iterator.\n        prepared_train:\n            The prepared training dataset.\n        prepared_tests:\n            The prepared bootstrapped test datasets.\n        num_iter:\n            The number of iterations to run.\n        rng:\n            The random number generator.\n        model:\n            The model to evaluate.\n        model_config:\n            The configuration of the model.\n        tokenizer:\n            The tokenizer to use for the model. If `None` then the model's\n            tokenizer will be used.\n        data_collator:\n            The data collator to use for the model.\n        compute_metrics:\n            The function to use to compute the metrics.\n        extract_labels_fn:\n            The function to use to extract the labels from the model output.\n        benchmark_config:\n            The configuration of the benchmark.\n        dataset_config:\n            The configuration of the dataset.\n\n    Returns:\n        A dictionary containing the scores, with keys \"test\" and maybe \"train\", with\n        values being lists of dicts containing the scores for each metric for each\n        iteration.\n    \"\"\"\n    scores: dict[str, list[dict[str, float]]] = defaultdict(list)\n\n    # Set up the name of the model output cache. If we are testing then we save the\n    # model outputs to a different cache and ensure that that cache is deleted before\n    # the next test, to ensure that the tests are independent of each other\n    if benchmark_config.debug:\n        model_cache_dir = Path.cwd()\n    else:\n        model_cache_dir = Path(model_config.model_cache_dir)\n    if hasattr(sys, \"_called_from_test\"):\n        cache_name = f\"{dataset_config.name}-model-outputs-test.json\"\n        (model_cache_dir / cache_name).unlink(missing_ok=True)\n    elif benchmark_config.debug:\n        cache_name = f\"{model_config.model_id}-{dataset_config.name}-model-outputs.json\"\n    else:\n        cache_name = f\"{dataset_config.name}-model-outputs.json\"\n\n    cache = ModelCache(\n        model_cache_dir=model_cache_dir,\n        cache_name=cache_name,\n        max_generated_tokens=dataset_config.max_generated_tokens,\n    )\n\n    for idx in itr:\n        prepared_test = prepared_tests[idx]\n        assert isinstance(prepared_test, Dataset)\n\n        generation_kwargs = dict(\n            model=model,\n            tokenizer=tokenizer,\n            data_collator=data_collator,\n            compute_metrics=compute_metrics,\n            extract_labels_fn=extract_labels_fn,\n            dataset_config=dataset_config,\n            cache=cache,\n        )\n\n        def update_scores(\n            scores: dict[str, list[dict[str, float]]],\n            benchmark_config: \"BenchmarkConfig\",\n        ) -&gt; dict[str, list[dict[str, float]]]:\n            \"\"\"Perform a single iteration of generation and update the scores.\n\n            Args:\n                scores:\n                    The scores so far.\n                benchmark_config:\n                    The configuration of the benchmark.\n\n            Returns:\n                The updated scores.\n            \"\"\"\n            test_scores = generate_single_iteration(\n                prepared_dataset=prepared_test,\n                benchmark_config=benchmark_config,\n                **generation_kwargs,\n            )\n            logger.debug(f\"Test scores for iteration {idx}: {test_scores}\")\n            scores[\"test\"].append(test_scores)\n\n            if benchmark_config.evaluate_train:\n                train_scores = generate_single_iteration(\n                    prepared_dataset=prepared_train,\n                    benchmark_config=benchmark_config,\n                    **generation_kwargs,\n                )\n                logger.debug(f\"Train scores for iteration {idx}: {train_scores}\")\n                scores[\"train\"].append(train_scores)\n\n            clear_memory()\n            return scores\n\n        if isinstance(model, VLLMModel):\n            scores = update_scores(scores=scores, benchmark_config=benchmark_config)\n        else:\n            while True:\n                try:\n                    scores = update_scores(\n                        scores=scores, benchmark_config=benchmark_config\n                    )\n                    break\n                except Exception as e:\n                    oom_error = [\n                        \"CUDA out of memory\",\n                        \"CUDA error\",\n                        \"MPS backend out of memory\",\n                        \"Too many parallel completions requested.\",  # OpenAI specific\n                    ]\n                    if isinstance(model, VLLMModel) or all(\n                        error not in str(e) for error in oom_error\n                    ):\n                        raise InvalidBenchmark(str(e))\n                    clear_memory()\n                    benchmark_config.batch_size //= 2\n                    if benchmark_config.batch_size &lt; 1:\n                        raise InvalidBenchmark(\n                            \"GPU out of memory, even with a batch size of 1!\"\n                        )\n\n    if not benchmark_config.debug:\n        cache.remove()\n\n    return scores\n\n\ndef generate_single_iteration(docs\n    prepared_dataset: Dataset,\n    model: \"GenerativeModel\",\n    tokenizer: \"Tokenizer\",\n    data_collator: \"DataCollator\",\n    compute_metrics: Callable,\n    extract_labels_fn: Callable[..., list[Any]],\n    dataset_config: \"DatasetConfig\",\n    benchmark_config: \"BenchmarkConfig\",\n    cache: ModelCache,\n) -&gt; dict[str, float]:\n    \"\"\"Evaluate a model on a dataset in a single iteration through generation.\n\n    Args:\n        prepared_dataset:\n            The dataset to evaluate on.\n        model:\n            The model to evaluate.\n        tokenizer:\n            The tokenizer to use for the model.\n        data_collator:\n            The data collator to use for the model.\n        compute_metrics:\n            The function to use to compute the metrics.\n        extract_labels_fn:\n            The function to use to extract the labels from the dataset.\n        dataset_config:\n            The configuration of the dataset.\n        benchmark_config:\n            The configuration of the benchmark.\n        cache:\n            The model output cache.\n\n    Returns:\n        A list of dictionaries containing the scores for each metric.\n    \"\"\"\n    cache.load()\n\n    # Split up the prepared dataset into a cached and non-cached part\n    cached_dataset, non_cached_dataset = split_dataset_into_cached_and_non_cached(\n        dataset=prepared_dataset, cache=cache\n    )\n\n    all_preds: list[dict | str | list[str]] = list()\n\n    if len(non_cached_dataset) &gt; 0:\n        # Tokens used in generation to know when generation is finished\n        stopping_criteria = get_generation_stopping_criteria(\n            tokenizer=tokenizer, model=model\n        )\n\n        generation_config = GenerationConfig(\n            # What to output\n            max_new_tokens=dataset_config.max_generated_tokens,\n            output_scores=dataset_config.task.supertask in SUPERTASKS_USING_LOGPROBS,\n            return_dict_in_generate=True,\n            # How to sample\n            do_sample=False,  # Equivalent to greedy decoding (temperature=0)\n            # Special tokens\n            bos_token_id=tokenizer.bos_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n            pad_token_id=tokenizer.pad_token_id,\n        )\n\n        # Sort the non_cached dataset by the length of the text, to minimise the amount\n        # of padding that needs to be added, speeding up generation\n        non_cached_dataset = non_cached_dataset.add_column(\n            name=\"length\", column=[len(x) for x in non_cached_dataset[\"text\"]]\n        )\n        non_cached_dataset = non_cached_dataset.sort(\"length\", reverse=True)\n\n        # Enable batching by building a dataloader. The dataloader cannot deal with\n        # text columns, so we create a copy of the dataset without these\n        torch_dataset = non_cached_dataset.with_format(\"torch\").remove_columns(\n            [\n                column\n                for column in non_cached_dataset.column_names\n                if column != \"input_ids\"\n            ]\n        )\n\n        if isinstance(model, OpenAIModel):\n            batch_size = 1\n        elif isinstance(model, VLLMModel):\n            batch_size = len(torch_dataset)\n        else:\n            batch_size = benchmark_config.batch_size\n\n        dataloader = DataLoader(\n            dataset=torch_dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=4,\n            collate_fn=data_collator,\n        )\n\n        with warnings.catch_warnings():\n            # This ignores the following warning, which is out of our control:\n            #   \"os.fork() was called. os.fork() is incompatible with multithreaded\n            #   code, and JAX is multithreaded, so this will likely lead to a deadlock.\"\n            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n\n            itr = (\n                dataloader\n                if batch_size == len(torch_dataset) or hasattr(sys, \"_called_from_test\")\n                else tqdm(iterable=dataloader, leave=False)\n            )\n\n            # Generate the completions for the non-cached examples\n            for batch_idx, batch in enumerate(itr):\n                model_output, extracted_labels = generate_batch(\n                    batch=batch,\n                    batch_idx=batch_idx,\n                    batch_size=batch_size,\n                    non_cached_dataset=non_cached_dataset,\n                    model=model,\n                    tokenizer=tokenizer,\n                    stopping_criteria=stopping_criteria,\n                    generation_config=generation_config,\n                    extract_labels_fn=extract_labels_fn,\n                    dataset_config=dataset_config,\n                )\n\n                # Extended logging if we are running in debug mode\n                if benchmark_config.debug:\n                    debug_log(\n                        batch_idx=batch_idx,\n                        batch_size=batch_size,\n                        non_cached_dataset=non_cached_dataset,\n                        extracted_labels=extracted_labels,\n                        dataset_config=dataset_config,\n                    )\n\n                cache.add_to_cache(\n                    model_input=batch[\"input_ids\"],\n                    model_output=model_output,\n                    tokenizer=tokenizer,\n                )\n                all_preds.extend(extracted_labels)\n\n                # If we are debugging then we save the cache often, but since this makes\n                # evaluation slower, we do not do this by default\n                if benchmark_config.debug:\n                    cache.save()\n\n        if isinstance(itr, tqdm):\n            itr.close()\n\n        # Store the cache to disk\n        cache.save()\n\n    # Fetch the cached predictions for the cached examples\n    if len(cached_dataset) &gt; 0:\n        model_output = load_cached_model_outputs(\n            cached_dataset=cached_dataset, cache=cache, tokenizer=tokenizer\n        )\n        extracted_labels = extract_labels_fn(\n            input_batch=cached_dataset, model_output=model_output, tokenizer=tokenizer\n        )\n        all_preds.extend(extracted_labels)\n\n    if \"label\" in non_cached_dataset.column_names:\n        ground_truth = [\n            label.lower() if isinstance(label, str) else label\n            for label in non_cached_dataset[\"label\"] + cached_dataset[\"label\"]\n        ]\n    elif \"labels\" in non_cached_dataset.column_names:\n        ground_truth = [\n            [label.lower() if isinstance(label, str) else label for label in label_list]\n            for label_list in non_cached_dataset[\"labels\"] + cached_dataset[\"labels\"]\n        ]\n    elif \"target_text\" in non_cached_dataset.column_names:\n        ground_truth = non_cached_dataset[\"target_text\"] + cached_dataset[\"target_text\"]\n    else:\n        raise ValueError(\n            \"The dataset must have either a 'label', 'labels', or 'target_text' column\"\n        )\n\n    itr_scores: dict[str, float] = compute_metrics(\n        model_outputs_and_labels=(all_preds, ground_truth),\n        id2label=dataset_config.id2label,\n    )\n\n    return itr_scores\n\n\nclass StopWordCriteria(StoppingCriteria):docs\n    \"\"\"Stopping criteria for generation based on stop words.\n\n    Attributes:\n        stop_word_id_lists:\n            A list of lists of token IDs that are used to determine whether generation\n            should stop.\n        indices_done:\n            A list of indices of the examples for which generation has already stopped.\n            Resets every batch.\n    \"\"\"\n\n    def __init__(self, stop_word_id_lists: list[list[int]]):\n        \"\"\"Initialize the stopping criteria.\n\n        Args:\n            stop_word_id_lists:\n                A list of lists of token IDs that are used to determine whether\n                generation should stop.\n        \"\"\"\n        super().__init__()\n        self.stop_word_id_lists = stop_word_id_lists\n        self.indices_done: list[int] = list()\n\n    def clear(self) -&gt; None:docs\n        \"\"\"Clear the example indices for which generation has already stopped.\"\"\"\n        self.indices_done = list()\n\n    def __call__(\n        self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs\n    ) -&gt; bool:\n        \"\"\"Determine whether generation should stop.\n\n        Args:\n            input_ids:\n                The input IDs of the generated sequences.\n            scores:\n                The scores of the generated sequences. Not used.\n            **kwargs:\n                Additional keyword arguments. Not used.\n\n        Returns:\n            Whether generation should stop.\n        \"\"\"\n        for stop_word_id_list in self.stop_word_id_lists:\n            for batch_idx in range(input_ids.shape[0]):\n                inputs = input_ids[batch_idx].tolist()\n                sample_ends_with_stop_word = (\n                    inputs[-len(stop_word_id_list) :] == stop_word_id_list\n                )\n                if sample_ends_with_stop_word:\n                    self.indices_done.append(batch_idx)\n                if all(idx in self.indices_done for idx in range(input_ids.shape[0])):\n                    return True\n        return False\n\n\ndef generate_batch(docs\n    batch: dict[str, torch.Tensor],\n    batch_idx: int,\n    batch_size: int,\n    non_cached_dataset: Dataset,\n    model: \"GenerativeModel\",\n    tokenizer: \"Tokenizer\",\n    stopping_criteria: StopWordCriteria,\n    generation_config: GenerationConfig,\n    extract_labels_fn: Callable[..., list[str]],\n    dataset_config: \"DatasetConfig\",\n) -&gt; tuple[ModelOutput, list[dict | str | list[str]]]:\n    \"\"\"Evaluate a model on a single batch of examples through generation.\n\n    Args:\n        batch:\n            The batch of examples to evaluate on.\n        batch_idx:\n            The index of the batch.\n        batch_size:\n            The size of the batch.\n        non_cached_dataset:\n            The dataset to evaluate on.\n        model:\n            The model to evaluate.\n        tokenizer:\n            The tokenizer used to encode the examples.\n        stopping_criteria:\n            The stopping criteria to use to stop generation.\n        generation_config:\n            The generation configuration to use.\n        extract_labels_fn:\n            The function to use to extract the labels from the model output.\n        dataset_config:\n            The configuration of the dataset.\n\n    Returns:\n        The predictions generated so far, with the predictions for the current batch\n        appended.\n    \"\"\"\n    # Generate the completions of the documents in the batch\n    with torch.inference_mode():\n        inputs = batch[\"input_ids\"].to(model.device)\n        stopping_criteria.clear()\n\n        prefix_allowed_tokens_fn = None\n        logits_processors = None\n        if dataset_config.task == NER and isinstance(\n            tokenizer, PreTrainedTokenizerBase\n        ):\n            ner_tag_names = list(dataset_config.prompt_label_mapping.values())\n            prefix_allowed_tokens_fn = get_ner_prefix_allowed_tokens_fn(\n                ner_tag_names=ner_tag_names, tokenizer=tokenizer\n            )\n            if isinstance(model, VLLMModel):\n                logits_processors = get_ner_logits_processors(\n                    ner_tag_names=ner_tag_names, llm=model\n                )\n\n        model_output = model.generate(\n            inputs=inputs,\n            attention_mask=batch[\"attention_mask\"].to(model.device)\n            if \"attention_mask\" in batch\n            else None,\n            generation_config=generation_config,\n            stopping_criteria=StoppingCriteriaList([stopping_criteria]),\n            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n            logits_processors=logits_processors,\n        )\n        assert isinstance(model_output, ModelOutput)\n\n    # Some models include the input in the generated sequence, so we need to remove the\n    # input if it is present\n    inputs = inputs.detach().cpu()\n    model_output.sequences = model_output.sequences.detach().cpu()\n    if torch.equal(model_output.sequences[:, : inputs.shape[1]], inputs):\n        model_output.sequences = model_output.sequences[:, inputs.shape[1] :]\n\n    # Extract the labels from the model output and store them for metric computation\n    # later\n    batch_start = batch_idx * batch_size\n    batch_end = (batch_idx + 1) * batch_size\n    input_batch = non_cached_dataset[batch_start:batch_end]\n    extracted_labels: list = extract_labels_fn(\n        input_batch=input_batch, model_output=model_output, tokenizer=tokenizer\n    )\n\n    return model_output, extracted_labels\n\n\ndef extract_raw_predictions(docs\n    generated_sequences: torch.Tensor, tokenizer: \"Tokenizer\"\n) -&gt; list[str]:\n    \"\"\"Get the raw predictions from the generated sequences.\n\n    Args:\n        generated_sequences:\n            The generated sequences from the model. The outer-most list is the\n            batch dimension, the inner-most list is the sequence dimension,\n            consisting of token IDs.\n        tokenizer:\n            The tokenizer used to generate the tokens.\n\n    Returns:\n        The candidate labels with the smallest edit distance to the predicted labels.\n    \"\"\"\n    raw_predictions: list[str] = [\n        tokenizer.decode(completion_ids.tolist(), skip_special_tokens=True).split(\n            \"\\n\\n\"\n        )[0]\n        for completion_ids in generated_sequences.long()\n    ]\n\n    end_of_chat_token_ids = get_end_of_chat_token_ids(tokenizer=tokenizer)\n    if end_of_chat_token_ids is not None:\n        end_of_chat_token = tokenizer.decode(end_of_chat_token_ids).strip()\n        if end_of_chat_token:\n            raw_predictions = [\n                raw_prediction.split(end_of_chat_token)[0]\n                for raw_prediction in raw_predictions\n            ]\n\n    raw_predictions = [raw_prediction.strip() for raw_prediction in raw_predictions]\n\n    return raw_predictions\n\n\ndef get_generation_stopping_criteria(docs\n    tokenizer: \"Tokenizer\", model: \"GenerativeModel\"\n) -&gt; StopWordCriteria:\n    \"\"\"Get the stopping criteria for generation.\n\n    Args:\n        tokenizer:\n            The tokenizer used to tokenize the stop words.\n        model:\n            The generative model, which we use to ensure the tensors are on the\n            same device, and also determine whether stop words are needed, based on\n            the model type.\n\n    Returns:\n        The stopping criteria for generation.\n    \"\"\"\n    if isinstance(model, OpenAIModel):\n        return StopWordCriteria(stop_word_id_lists=[])\n\n    double_newline_ids: list[int] = tokenizer(\n        text=[\"\\n\\n\"], add_special_tokens=False\n    ).input_ids[0]\n    single_newline_ids: list[int] = tokenizer(\n        text=[\"\\n\"], add_special_tokens=False\n    ).input_ids[0]\n\n    stop_word_id_lists = [double_newline_ids, single_newline_ids + single_newline_ids]\n\n    end_chat_token_ids = get_end_of_chat_token_ids(tokenizer=tokenizer)\n    if end_chat_token_ids is not None:\n        stop_word_id_lists.append(end_chat_token_ids)\n\n    if tokenizer.bos_token is not None:\n        bos_token_ids: list[int] = tokenizer(\n            text=[tokenizer.bos_token], add_special_tokens=False\n        ).input_ids[0]\n        stop_word_id_lists.append(bos_token_ids)\n\n    if tokenizer.eos_token is not None:\n        eos_token_ids: list[int] = tokenizer(\n            text=[tokenizer.eos_token], add_special_tokens=False\n        ).input_ids[0]\n        stop_word_id_lists.append(eos_token_ids)\n\n    def remove_empty_tokens(token_id_list: list[int]) -&gt; list[int]:\n        return [\n            token_id for token_id in token_id_list if tokenizer.decode([token_id]) != \"\"\n        ]\n\n    stop_word_id_lists = [\n        remove_empty_tokens(token_id_list) for token_id_list in stop_word_id_lists\n    ]\n\n    return StopWordCriteria(stop_word_id_lists=stop_word_id_lists)\n\n\ndef debug_log(docs\n    batch_idx: int,\n    batch_size: int,\n    non_cached_dataset: Dataset,\n    extracted_labels: list[dict | str | list[str]],\n    dataset_config: \"DatasetConfig\",\n) -&gt; None:\n    \"\"\"Log inputs and outputs for debugging purposes.\n\n    Args:\n        batch_idx:\n            The index of the batch.\n        batch_size:\n            The size of the batch.\n        non_cached_dataset:\n            The dataset to evaluate on.\n        extracted_labels:\n            The extracted labels from the model output.\n        dataset_config:\n            The configuration of the dataset.\n    \"\"\"\n    sample_idxs = range(\n        batch_idx * batch_size,\n        min((batch_idx + 1) * batch_size, len(non_cached_dataset)),\n    )\n    samples = non_cached_dataset.select(sample_idxs)\n\n    if dataset_config.task == NER:\n        log_msgs = [\"\"]\n        for tokens, predictions, labels in zip(\n            samples[\"tokens\"], extracted_labels, samples[\"labels\"]\n        ):\n            predictions = [tag.upper() for tag in predictions]\n            sample = list(zip(tokens, predictions, labels))\n            log_batches = [\n                [(\"Tokens: \", \"Predictions: \", \"Labels: \")] + sample[i : i + 10]\n                for i in range(0, len(sample), 10)\n            ]\n            for log_batch in log_batches:\n                lengths = [len(max(triple, key=len)) for triple in log_batch]\n                log_batch = [\n                    [f\"{x:&lt;{length}}\" for x in triple]\n                    for triple, length in zip(log_batch, lengths)\n                ]\n                tokens = [triple[0] for triple in log_batch]\n                predictions = [triple[1] for triple in log_batch]\n                labels = [triple[2] for triple in log_batch]\n                log_msgs.append(\n                    \"\\t\".join(tokens)\n                    + \"\\n\"\n                    + \"\\t\".join(predictions)\n                    + \"\\n\"\n                    + \"\\t\".join(labels)\n                )\n        logger.info(\"\\n\\n\".join(log_msgs))\n\n    else:\n        # Define predictions\n        if dataset_config.task == RC:\n            extracted_labels = [\n                prediction[\"prediction_text\"]\n                for prediction in extracted_labels\n                if isinstance(prediction, dict)\n            ]\n\n        # Define labels\n        if dataset_config.task in {SENT, LA}:\n            labels = [\n                dataset_config.prompt_label_mapping.get(label, label).lower()\n                for label in samples[\"label\"]\n            ]\n        elif dataset_config.task == RC:\n            labels = [label[\"answers\"][\"text\"][0] for label in samples[\"label\"]]\n        elif dataset_config.task == SUMM:\n            labels = samples[\"target_text\"]\n        else:\n            labels = samples[\"label\"]\n\n        # Log inputs and outputs\n        for input_text, prediction, label in zip(\n            samples[\"text\"], extracted_labels, labels\n        ):\n            logger.info(\n                f\"Input: '{input_text}'\\n\"\n                f\"Prediction: '{prediction}'\\n\"\n                f\"Label: '{label}'\"\n            )\n</code></pre>"},{"location":"api/scandeval/human_evaluation/","title":"scandeval.human_evaluation","text":"scandeval.human_evaluation<p> source module scandeval.human_evaluation </p> <p>Gradio app for conducting human evaluation of the tasks.</p> <p> Classes </p> <ul> <li> <p>HumanEvaluator \u2014 An app for evaluating human performance on the ScandEval benchmark.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>main \u2014 Start the Gradio app for human evaluation.</p> </li> </ul> <p> source class HumanEvaluator(title: str, description: str, dummy_model_id: str = 'mistralai/Mistral-7B-v0.1') </p> <p>An app for evaluating human performance on the ScandEval benchmark.</p> <p>Initialize the HumanEvaluator.</p> <p> Parameters </p> <ul> <li> <p>annotator_id :  int \u2014</p> <p>The annotator ID for the evaluation.</p> </li> <li> <p>title :  str \u2014</p> <p>The title of the app.</p> </li> <li> <p>description :  str \u2014</p> <p>The description of the app.</p> </li> <li> <p>dummy_model_id :  str \u2014</p> <p>The model ID to use for generating prompts.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>create_app \u2014 Create the Gradio app for human evaluation.</p> </li> <li> <p>update_dataset_choices \u2014 Update the dataset choices based on the selected language and task.</p> </li> <li> <p>update_dataset \u2014 Update the dataset based on a selected dataset name.</p> </li> <li> <p>add_entity_to_answer \u2014 Add an entity to the answer.</p> </li> <li> <p>reset_entities \u2014 Reset the entities in the answer.</p> </li> <li> <p>submit_answer \u2014 Submit an answer to the dataset.</p> </li> <li> <p>example_to_markdown \u2014 Convert an example to a Markdown string.</p> </li> <li> <p>compute_and_log_scores \u2014 Computes and logs the scores for the dataset.</p> </li> </ul> <p> source method HumanEvaluator.create_app() \u2192 gr.Blocks </p> <p>Create the Gradio app for human evaluation.</p> <p> Returns </p> <ul> <li> <p>gr.Blocks \u2014 The Gradio app for human evaluation.</p> </li> </ul> <p> source method HumanEvaluator.update_dataset_choices(language: str, task: str) \u2192 gr.Dropdown </p> <p>Update the dataset choices based on the selected language and task.</p> <p> Parameters </p> <ul> <li> <p>language :  str \u2014</p> <p>The language selected by the user.</p> </li> <li> <p>task :  str \u2014</p> <p>The task selected by the user.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>gr.Dropdown \u2014 A list of dataset names that match the selected language and task.</p> </li> </ul> <p> source method HumanEvaluator.update_dataset(dataset_name: str, iteration: int) \u2192 tuple[gr.Markdown, gr.Markdown, gr.Dropdown, gr.Textbox, gr.Button, gr.Button, gr.Textbox, gr.Button] </p> <p>Update the dataset based on a selected dataset name.</p> <p> Parameters </p> <ul> <li> <p>dataset_name :  str \u2014</p> <p>The dataset name selected by the user.</p> </li> <li> <p>iteration :  int \u2014</p> <p>The iteration index of the datasets to evaluate.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[gr.Markdown, gr.Markdown, gr.Dropdown, gr.Textbox, gr.Button, gr.Button, gr.Textbox, gr.Button] \u2014 A tuple (task_examples, question, entity_type, entity, entity_add_button, entity_reset_button, answer, submit_button) for the selected dataset.</p> </li> </ul> <p> source method HumanEvaluator.add_entity_to_answer(question: str, entity_type: str, entity: str, answer: str) \u2192 tuple[gr.Textbox, gr.Textbox] </p> <p>Add an entity to the answer.</p> <p> Parameters </p> <ul> <li> <p>question :  str \u2014</p> <p>The current question.</p> </li> <li> <p>entity_type :  str \u2014</p> <p>The entity type selected by the user.</p> </li> <li> <p>entity :  str \u2014</p> <p>The entity provided by the user.</p> </li> <li> <p>answer :  str \u2014</p> <p>The current answer.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[gr.Textbox, gr.Textbox] \u2014 A tuple (entity, answer) with a (blank) entity and answer.</p> </li> </ul> <p> source method HumanEvaluator.reset_entities() \u2192 gr.Textbox </p> <p>Reset the entities in the answer.</p> <p> Returns </p> <ul> <li> <p>gr.Textbox \u2014 A blank answer.</p> </li> </ul> <p> source method HumanEvaluator.submit_answer(dataset_name: str, question: str, answer: str, annotator_id: int) \u2192 tuple[str, str] </p> <p>Submit an answer to the dataset.</p> <p> Parameters </p> <ul> <li> <p>dataset_name :  str \u2014</p> <p>The name of the dataset.</p> </li> <li> <p>question :  str \u2014</p> <p>The question for the dataset.</p> </li> <li> <p>answer :  str \u2014</p> <p>The answer to the question.</p> </li> <li> <p>annotator_id :  int \u2014</p> <p>The annotator ID for the evaluation.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[str, str] \u2014 A tuple (question, answer), with <code>question</code> being the next question, and <code>answer</code> being an empty string.</p> </li> </ul> <p> source method HumanEvaluator.example_to_markdown(example: dict) \u2192 tuple[str, str] </p> <p>Convert an example to a Markdown string.</p> <p> Parameters </p> <ul> <li> <p>example :  dict \u2014</p> <p>The example to convert.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[str, str] \u2014 A tuple (task_examples, question) for the example.</p> </li> </ul> <p> source method HumanEvaluator.compute_and_log_scores() \u2192 None </p> <p>Computes and logs the scores for the dataset.</p> <p> source main(annotator_id: int) \u2192 None </p> <p>Start the Gradio app for human evaluation.</p> <p> Raises </p> <ul> <li> <p>NeedsExtraInstalled</p> </li> </ul>"},{"location":"src/scandeval/human_evaluation/","title":"scandeval.human_evaluation","text":"scandeval.human_evaluation<p> docs module scandeval.human_evaluation </p> <pre><code>\"\"\"Gradio app for conducting human evaluation of the tasks.\"\"\"\n\nimport importlib.util\nimport json\nimport logging\nfrom functools import partial\nfrom pathlib import Path\n\nimport click\nfrom datasets import Dataset\nfrom transformers import AutoConfig, AutoTokenizer\nfrom transformers.utils import ModelOutput\n\nfrom .benchmark_config_factory import build_benchmark_config\nfrom .benchmark_dataset import BenchmarkDataset\nfrom .benchmarker import BenchmarkResult\nfrom .config import ModelConfig\nfrom .dataset_configs import SPEED_CONFIG, get_all_dataset_configs\nfrom .dataset_factory import DatasetFactory\nfrom .enums import Framework, ModelType\nfrom .exceptions import NeedsExtraInstalled\nfrom .scores import aggregate_scores\nfrom .tasks import NER\nfrom .types import ScoreDict\nfrom .utils import create_model_cache_dir, enforce_reproducibility\n\nif importlib.util.find_spec(\"gradio\") is not None:\n    import gradio as gr\n\n\nlogger = logging.getLogger(__package__)\n\n\nclass HumanEvaluator:docs\n    \"\"\"An app for evaluating human performance on the ScandEval benchmark.\"\"\"\n\n    def __init__(\n        self,\n        annotator_id: int,\n        title: str,\n        description: str,\n        dummy_model_id: str = \"mistralai/Mistral-7B-v0.1\",\n    ) -&gt; None:\n        \"\"\"Initialize the HumanEvaluator.\n\n        Args:\n            annotator_id:\n                The annotator ID for the evaluation.\n            title:\n                The title of the app.\n            description:\n                The description of the app.\n            dummy_model_id:\n                The model ID to use for generating prompts.\n        \"\"\"\n        self.annotator_id = annotator_id\n        self.title = title\n        self.description = description\n        self.dummy_model_id = dummy_model_id\n\n        self.sample_idx: int\n        self.benchmark_dataset: BenchmarkDataset\n        self.active_dataset: Dataset\n\n        self.dataset_configs = {\n            name: cfg\n            for name, cfg in get_all_dataset_configs().items()\n            if not cfg.unofficial\n        }\n        self.tasks = sorted(\n            {\n                cfg.task.name.replace(\"-\", \" \").title()\n                for cfg in self.dataset_configs.values()\n                if cfg != SPEED_CONFIG\n            }\n        )\n        self.languages = sorted(\n            {\n                language.name\n                for cfg in self.dataset_configs.values()\n                if cfg != SPEED_CONFIG\n                for language in cfg.languages\n                if language.name not in {\"Norwegian Bokm\u00e5l\", \"Norwegian Nynorsk\"}\n            }\n        )\n\n    def create_app(self) -&gt; \"gr.Blocks\":docs\n        \"\"\"Create the Gradio app for human evaluation.\n\n        Returns:\n            The Gradio app for human evaluation.\n        \"\"\"\n        with gr.Blocks(title=self.title, theme=gr.themes.Monochrome()) as app:\n            gr.components.HTML(f\"&lt;center&gt;&lt;h1&gt;{self.title}&lt;/h1&gt;&lt;/center&gt;\")\n            gr.components.Markdown(self.description)\n            with gr.Row(variant=\"panel\"):\n                language_dropdown = gr.Dropdown(\n                    label=\"Language\", choices=self.languages\n                )\n                task_dropdown = gr.Dropdown(label=\"Task\", choices=self.tasks)\n                dataset_dropdown = gr.Dropdown(label=\"Dataset\", choices=[\"\"])\n            with gr.Row(variant=\"panel\"):\n                with gr.Column():\n                    task_examples = gr.Markdown(\"Task Examples\", visible=False)\n                with gr.Column():\n                    question = gr.Markdown(label=\"Question\", visible=False)\n                    with gr.Row():\n                        ner_tag_dropdown = gr.Dropdown(\n                            label=\"Entity type\",\n                            choices=[\"\"],\n                            interactive=True,\n                            visible=False,\n                            scale=0.5,\n                        )\n                        ner_tag_answer = gr.Textbox(\n                            label=\"Entity\", interactive=True, visible=False, scale=1\n                        )\n                        with gr.Column(scale=0.2):\n                            ner_tag_add_button = gr.Button(\"Add entity\", visible=False)\n                            ner_tag_reset_button = gr.Button(\n                                \"Reset entities\", visible=False\n                            )\n                    answer = gr.Textbox(label=\"Answer\", visible=False)\n                    submit_button = gr.Button(\"Submit\", visible=False)\n\n            language_dropdown.change(\n                fn=self.update_dataset_choices,\n                inputs=[language_dropdown, task_dropdown],\n                outputs=dataset_dropdown,\n            )\n            task_dropdown.change(\n                fn=self.update_dataset_choices,\n                inputs=[language_dropdown, task_dropdown],\n                outputs=dataset_dropdown,\n            )\n            dataset_dropdown.change(\n                fn=partial(self.update_dataset, iteration=self.annotator_id),\n                inputs=dataset_dropdown,\n                outputs=[\n                    task_examples,\n                    question,\n                    ner_tag_dropdown,\n                    ner_tag_answer,\n                    ner_tag_add_button,\n                    ner_tag_reset_button,\n                    answer,\n                    submit_button,\n                ],\n            )\n            ner_tag_add_button.click(\n                fn=self.add_entity_to_answer,\n                inputs=[question, ner_tag_dropdown, ner_tag_answer, answer],\n                outputs=[ner_tag_answer, answer],\n            )\n            ner_tag_answer.submit(\n                fn=self.add_entity_to_answer,\n                inputs=[question, ner_tag_dropdown, ner_tag_answer, answer],\n                outputs=[ner_tag_answer, answer],\n            )\n            ner_tag_reset_button.click(fn=self.reset_entities, outputs=answer)\n            submit_button.click(\n                fn=partial(self.submit_answer, annotator_id=self.annotator_id),\n                inputs=[dataset_dropdown, question, answer],\n                outputs=[question, answer],\n            )\n            answer.submit(\n                fn=partial(self.submit_answer, annotator_id=self.annotator_id),\n                inputs=[dataset_dropdown, question, answer],\n                outputs=[question, answer],\n            )\n        return app\ndocs\n    def update_dataset_choices(self, language: str, task: str) -&gt; \"gr.Dropdown\":\n        \"\"\"Update the dataset choices based on the selected language and task.\n\n        Args:\n            language:\n                The language selected by the user.\n            task:\n                The task selected by the user.\n\n        Returns:\n            A list of dataset names that match the selected language and task.\n        \"\"\"\n        if language is None or task is None:\n            return gr.Dropdown(choices=[])\n\n        dataset_configs = [\n            cfg\n            for cfg in get_all_dataset_configs().values()\n            if language in {language.name for language in cfg.languages}\n            and task.lower().replace(\" \", \"-\") == cfg.task.name\n            and not cfg.unofficial\n        ]\n        assert len(dataset_configs) &gt; 0\n\n        choices = sorted([cfg.name for cfg in dataset_configs])\n\n        logger.info(\n            f\"User selected {language} and {task}, which resulted in the datasets \"\n            f\"{choices}, with {choices[0]!r} being chosen by default.\"\n        )\n\n        return gr.Dropdown(choices=choices, value=choices[0])\n\n    def update_dataset(docs\n        self, dataset_name: str, iteration: int\n    ) -&gt; \"tuple[gr.Markdown, gr.Markdown, gr.Dropdown, gr.Textbox, gr.Button, gr.Button, gr.Textbox, gr.Button]\":\n        \"\"\"Update the dataset based on a selected dataset name.\n\n        Args:\n            dataset_name:\n                The dataset name selected by the user.\n            iteration:\n                The iteration index of the datasets to evaluate.\n\n        Returns:\n            A tuple (task_examples, question, entity_type, entity, entity_add_button,\n            entity_reset_button, answer, submit_button) for the selected dataset.\n        \"\"\"\n        blank_answer = (\n            gr.Markdown(\"\", visible=False),\n            gr.Markdown(\"\", visible=False),\n            gr.Dropdown(visible=False),\n            gr.Textbox(visible=False),\n            gr.Button(visible=False),\n            gr.Button(visible=False),\n            gr.Textbox(\"\", visible=False),\n            gr.Button(visible=False),\n        )\n\n        if not dataset_name:\n            return blank_answer\n\n        logger.info(f\"User selected dataset {dataset_name} - loading dataset...\")\n        gr.Info(f\"Loading dataset {dataset_name}...\")\n\n        benchmark_config = build_benchmark_config(\n            progress_bar=False,\n            save_results=True,\n            task=None,\n            dataset=None,\n            language=[\n                language.code\n                for cfg in get_all_dataset_configs().values()\n                for language in cfg.languages\n                if not cfg.unofficial\n            ],\n            model_language=None,\n            dataset_language=None,\n            framework=None,\n            device=None,\n            batch_size=1,\n            evaluate_train=False,\n            raise_errors=False,\n            cache_dir=\".scandeval_cache\",\n            token=None,\n            openai_api_key=None,\n            prefer_azure=False,\n            azure_openai_api_key=None,\n            azure_openai_endpoint=None,\n            azure_openai_api_version=None,\n            force=False,\n            verbose=False,\n            trust_remote_code=False,\n            load_in_4bit=None,\n            use_flash_attention=None,\n            clear_model_cache=False,\n            only_validation_split=True,\n            few_shot=True,\n            num_iterations=iteration + 1,\n            debug=False,\n            run_with_cli=True,\n        )\n        dataset_factory = DatasetFactory(benchmark_config=benchmark_config)\n        dataset_config = get_all_dataset_configs()[dataset_name]\n\n        model_id = f\"human-{iteration}\"\n        model_config = ModelConfig(\n            model_id=model_id,\n            revision=\"main\",\n            framework=Framework.HUMAN,\n            task=\"text-generation\",\n            languages=dataset_config.languages,\n            model_type=ModelType.HUMAN,\n            model_cache_dir=create_model_cache_dir(\n                cache_dir=benchmark_config.cache_dir, model_id=model_id\n            ),\n            adapter_base_model_id=None,\n        )\n\n        self.benchmark_dataset = dataset_factory.build_dataset(dataset=dataset_config)\n        self.sample_idx = 0\n\n        dataset_path = (\n            Path(\".scandeval_cache\")\n            / \"human-evaluation\"\n            / dataset_name\n            / f\"human-{iteration}.csv\"\n        )\n        if dataset_path.exists():\n            self.active_dataset = Dataset.from_csv(str(dataset_path))\n            try:\n                while self.active_dataset[\"answer\"][self.sample_idx] is not None:\n                    self.sample_idx += 1\n            except IndexError:\n                self.compute_and_log_scores()\n                return blank_answer\n        else:\n            rng = enforce_reproducibility(framework=Framework.PYTORCH)\n            train, val, tests = self.benchmark_dataset._load_data(rng=rng)\n            _, _, tests = self.benchmark_dataset._load_prepared_data(\n                train=train,\n                val=val,\n                tests=tests,\n                model_config=model_config,\n                hf_model_config=AutoConfig.from_pretrained(self.dummy_model_id),\n                tokenizer=AutoTokenizer.from_pretrained(self.dummy_model_id),\n                benchmarking_generative_model=True,\n            )\n            self.active_dataset = (\n                tests[iteration]\n                .remove_columns(column_names=[\"input_ids\", \"attention_mask\"])\n                .add_column(name=\"answer\", column=[None] * len(tests[iteration]))\n            )\n\n        task_examples, question = self.example_to_markdown(\n            example=self.active_dataset[self.sample_idx]\n        )\n\n        logger.info(\n            f\"Loaded dataset {dataset_name}, with the following task examples:\\n\\n\"\n            f\"{task_examples}\"\n        )\n\n        if self.benchmark_dataset.dataset_config.task == NER:\n            ner_tags = list()\n            for ner_tag in dataset_config.prompt_label_mapping.values():\n                if ner_tag not in ner_tags:\n                    ner_tags.append(ner_tag)\n            return (\n                gr.Markdown(task_examples, visible=True),\n                gr.Markdown(question, visible=True),\n                gr.Dropdown(\n                    label=\"Entity type\",\n                    choices=ner_tags,\n                    value=ner_tags[0],\n                    visible=True,\n                ),\n                gr.Textbox(label=\"Entity\", interactive=True, visible=True),\n                gr.Button(\"Add entity\", visible=True),\n                gr.Button(\"Reset entities\", visible=True),\n                gr.Textbox(\n                    json.dumps({ner_tag: [] for ner_tag in ner_tags}),\n                    interactive=False,\n                    visible=True,\n                ),\n                gr.Button(\"Submit\", visible=True),\n            )\n        else:\n            return (\n                gr.Markdown(task_examples, visible=True),\n                gr.Markdown(question, visible=True),\n                gr.Dropdown(label=\"Entity type\", choices=[], visible=False),\n                gr.Textbox(label=\"Entity\", interactive=True, visible=False),\n                gr.Button(\"Add entity\", visible=False),\n                gr.Button(\"Reset entities\", visible=False),\n                gr.Textbox(\"\", interactive=True, visible=True),\n                gr.Button(\"Submit\", visible=True),\n            )\n\n    def add_entity_to_answer(docs\n        self, question: str, entity_type: str, entity: str, answer: str\n    ) -&gt; \"tuple[gr.Textbox, gr.Textbox]\":\n        \"\"\"Add an entity to the answer.\n\n        Args:\n            question:\n                The current question.\n            entity_type:\n                The entity type selected by the user.\n            entity:\n                The entity provided by the user.\n            answer:\n                The current answer.\n\n        Returns:\n            A tuple (entity, answer) with a (blank) entity and answer.\n        \"\"\"\n        if not entity_type or not entity:\n            return gr.Textbox(\"\"), gr.Textbox(\"\")\n\n        if entity not in question:\n            gr.Warning(\n                f\"The entity {entity!r} is not present in the question. Please \"\n                \"write it *exactly* as it appears in the question.\"\n            )\n            return gr.Textbox(entity), gr.Textbox(answer)\n\n        current_answer_obj = json.loads(answer)\n        if entity not in current_answer_obj[entity_type]:\n            current_answer_obj[entity_type].append(entity)\n\n        answer = json.dumps(current_answer_obj)\n        return gr.Textbox(\"\"), gr.Textbox(answer)\n\n    def reset_entities(self) -&gt; \"gr.Textbox\":docs\n        \"\"\"Reset the entities in the answer.\n\n        Returns:\n            A blank answer.\n        \"\"\"\n        ner_tags = list()\n        for (\n            ner_tag\n        ) in self.benchmark_dataset.dataset_config.prompt_label_mapping.values():\n            if ner_tag not in ner_tags:\n                ner_tags.append(ner_tag)\n        return gr.Textbox(json.dumps({ner_tag: [] for ner_tag in ner_tags}))\n\n    def submit_answer(docs\n        self, dataset_name: str, question: str, answer: str, annotator_id: int\n    ) -&gt; tuple[str, str]:\n        \"\"\"Submit an answer to the dataset.\n\n        Args:\n            dataset_name:\n                The name of the dataset.\n            question:\n                The question for the dataset.\n            answer:\n                The answer to the question.\n            annotator_id:\n                The annotator ID for the evaluation.\n\n        Returns:\n            A tuple (question, answer), with `question` being the next question, and\n            `answer` being an empty string.\n        \"\"\"\n        if not answer:\n            gr.Warning(\"Please provide an answer before submitting.\")\n            logger.info(\"User tried to submit without providing an answer.\")\n            return question, answer\n\n        # Custom NER validation\n        if self.benchmark_dataset.dataset_config.task == NER:\n            try:\n                json.loads(answer)\n            except json.JSONDecodeError:\n                gr.Warning(\"Please provide a valid JSON object as an answer.\")\n                logger.info(\"User tried to submit an invalid JSON object as an answer.\")\n                return question, answer\n\n            if not isinstance(json.loads(answer), dict):\n                gr.Warning(\n                    \"Please provide a JSON object with a dictionary as an answer.\"\n                )\n                logger.info(\n                    \"User tried to submit a JSON object without a dictionary as an answer.\"\n                )\n                return question, answer\n\n            ner_tags = list(\n                self.benchmark_dataset.dataset_config.prompt_label_mapping.values()\n            )\n            for ner_tag in ner_tags:\n                if ner_tag not in json.loads(answer).keys():\n                    gr.Warning(\n                        f\"Please provide a JSON object with the key {ner_tag!r}.\"\n                    )\n                    logger.info(\n                        \"User tried to submit a JSON object without the key \"\n                        f\"{ner_tag!r}.\"\n                    )\n                    return question, answer\n\n        samples_left = len(self.active_dataset) - self.sample_idx - 1\n        if samples_left:\n            gr.Info(f\"Submitted - {samples_left} to go!\")\n\n        # Store the user's answer\n        answers = self.active_dataset[\"answer\"]\n        answers[self.sample_idx] = answer\n        self.active_dataset = self.active_dataset.remove_columns(\"answer\").add_column(\n            name=\"answer\", column=answers\n        )\n        logger.info(\n            f\"User submitted the answer {answer!r} to the question {question!r}, with \"\n            f\"sample index {self.sample_idx}.\"\n        )\n\n        dataset_path = (\n            Path(\".scandeval_cache\")\n            / \"human-evaluation\"\n            / dataset_name\n            / f\"human-{annotator_id}.csv\"\n        )\n        dataset_path.parent.mkdir(parents=True, exist_ok=True)\n        self.active_dataset.to_csv(dataset_path)\n\n        # Attempt to get the next question\n        try:\n            self.sample_idx += 1\n            _, question = self.example_to_markdown(\n                example=self.active_dataset[self.sample_idx]\n            )\n\n            if self.benchmark_dataset.dataset_config.task == NER:\n                ner_tags = list()\n                for ner_tag in (\n                    self.benchmark_dataset.dataset_config.prompt_label_mapping.values()\n                ):\n                    if ner_tag not in ner_tags:\n                        ner_tags.append(ner_tag)\n                answer = json.dumps({ner_tag: [] for ner_tag in ner_tags})\n            else:\n                answer = \"\"\n\n        # If we fail to get the next question it means that the user has finished\n        # annotating the dataset, so we compute and log the scores\n        except IndexError:\n            self.compute_and_log_scores()\n            question = \"\"\n            answer = \"\"\n\n        return question, answer\n\n    def example_to_markdown(self, example: dict) -&gt; tuple[str, str]:docs\n        \"\"\"Convert an example to a Markdown string.\n\n        Args:\n            example:\n                The example to convert.\n\n        Returns:\n            A tuple (task_examples, question) for the example.\n        \"\"\"\n        task_examples: str | list[str] = [\n            sample.replace(\"\\n\", \"\\n\\n\")\n            for sample in example[\"text\"].split(\"\\n\\n\")[:-1]\n        ]\n        task_examples = \"\\n\\n**Example**\\n\\n\".join(task_examples)\n\n        question = \"**Question**\\n\\n\"\n        question += \"\\n\\n\".join(example[\"text\"].split(\"\\n\\n\")[-1].split(\"\\n\")[:-1])\n        question += \"\\n\\n\" + example[\"text\"].split(\"\\n\\n\")[-1].split(\"\\n\")[-1]\n\n        return task_examples, question\n\n    def compute_and_log_scores(self) -&gt; None:docs\n        \"\"\"Computes and logs the scores for the dataset.\"\"\"\n        tokenizer = AutoTokenizer.from_pretrained(self.dummy_model_id)\n        tokenizer.pad_token = tokenizer.eos_token\n        sequences = tokenizer(\n            self.active_dataset[\"answer\"],\n            add_special_tokens=False,\n            padding=True,\n            return_tensors=\"pt\",\n        ).input_ids\n        model_output = ModelOutput(sequences=sequences)\n        all_preds = self.benchmark_dataset._extract_labels_from_generation(\n            input_batch=self.active_dataset.to_dict(),\n            model_output=model_output,\n            tokenizer=tokenizer,\n        )\n        ground_truth = self.active_dataset[\"label\"]\n        itr_scores: dict[str, float] = self.benchmark_dataset._compute_metrics(\n            model_outputs_and_labels=(all_preds, ground_truth),\n            id2label=self.benchmark_dataset.dataset_config.id2label,\n        )\n\n        # We reverse the order, as the Info messages are printed in reverse order\n        scores = list(itr_scores.items())\n        scores.reverse()\n        gr.Info(\n            \"If you want to evaluate another dataset then please select a new \"\n            \"one from the menus.\"\n        )\n        for metric_name, score in scores:\n            gr.Info(f\"\\n\\n{metric_name}: {score:.2%}\")\n        gr.Info(\"You have completed this dataset! Here are your scores:\")\n        logger.info(\n            f\"User completed the dataset {self.benchmark_dataset.dataset_config.name!r}\"\n            f\", with the following scores: {itr_scores}\"\n        )\n\n        # Load previous human results, if any. We do this since the human evaluation is\n        # only a single iteration, so the results from the current annotation should be\n        # added to the previous results.\n        results_path = Path.cwd() / \"scandeval_benchmark_results.jsonl\"\n        results: ScoreDict = dict(raw=dict(test=list()))  # type: ignore[dict-item]\n        if results_path.exists():\n            all_results = [\n                json.loads(line.strip())\n                for line in results_path.read_text().strip().split(\"\\n\")\n                if line.strip()\n            ]\n            human_result_candidates = [\n                result\n                for result in all_results\n                if result[\"model\"] == \"human\"\n                and result[\"dataset\"] == self.benchmark_dataset.dataset_config.name\n            ]\n            if human_result_candidates:\n                results = human_result_candidates[0][\"results\"]\n\n        # Append to results\n        results[\"raw\"][\"test\"].append(  # type: ignore[union-attr]\n            {f\"test_{metric_name}\": score for metric_name, score in itr_scores.items()}\n        )\n\n        # Aggregate scores\n        total_dict: dict[str, float] = dict()\n        for metric_cfg in self.benchmark_dataset.dataset_config.task.metrics:\n            agg_scores = aggregate_scores(\n                scores=results[\"raw\"],  # type: ignore[arg-type]\n                metric_config=metric_cfg,\n            )\n            test_score, test_se = agg_scores[\"test\"]\n            test_score, _ = metric_cfg.postprocessing_fn(test_score)\n            test_se, _ = metric_cfg.postprocessing_fn(test_se)\n            total_dict[f\"test_{metric_cfg.name}\"] = test_score\n            total_dict[f\"test_{metric_cfg.name}_se\"] = test_se\n        results[\"total\"] = total_dict\n\n        benchmark_result = BenchmarkResult(\n            dataset=self.benchmark_dataset.dataset_config.name,\n            task=self.benchmark_dataset.dataset_config.task.name,\n            dataset_languages=[\n                language.code\n                for language in self.benchmark_dataset.dataset_config.languages\n            ],\n            model=\"human\",\n            results=results,\n            num_model_parameters=-1,\n            max_sequence_length=-1,\n            vocabulary_size=-1,\n            generative=True,\n            few_shot=True,\n            validation_split=True,\n        )\n        benchmark_result.append_to_results(results_path=results_path)\n\n\n@click.command()\n@click.option(\n    \"--annotator-id\",\n    \"-id\",\n    type=int,\n    required=True,\n    help=\"\"\"The annotator ID to use for the evaluation. Needs to be between 0 and 10,\n    inclusive.\"\"\",\n)\ndef main(annotator_id: int) -&gt; None:docs\n    \"\"\"Start the Gradio app for human evaluation.\"\"\"\n    if importlib.util.find_spec(\"gradio\") is None:\n        raise NeedsExtraInstalled(extra=\"human_evaluation\")\n\n    evaluator = HumanEvaluator(\n        annotator_id=annotator_id,\n        title=\"ScandEval Human Evaluation\",\n        description=\"\"\"\n        In this app we will evaluate your performance on a variety of tasks, with the\n        goal of comparing human performance to language model performance.\n\n        When you select a language and a task then you will be given a brief\n        description of the task, as well as examples of how to solve it. Please read\n        through these examples before proceeding with the task.\n\n        Please do not use any additional aids (such as search engines) when completing\n        these tasks.\n\n        Note that several examples appear more than once - this is intentional, as it\n        allows us to compare your performance across multiple examples.\n\n        Note that the Enter key will also submit your answer!\n        \"\"\",\n    )\n    evaluator.create_app().queue().launch()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"api/scandeval/languages/","title":"scandeval.languages","text":"scandeval.languages<p> source module scandeval.languages </p> <p>List of languages and their ISO 639-1 codes.</p> <p>Taken from https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes.</p> <p>Last updated 19 June 2022.</p> <p> Functions </p> <ul> <li> <p>get_all_languages \u2014 Get a list of all the languages.</p> </li> </ul> <p> source get_all_languages() \u2192 dict[str, Language] </p> <p>Get a list of all the languages.</p> <p> Returns </p> <ul> <li> <p>dict[str, Language] \u2014 A mapping between language codes and their configurations.</p> </li> </ul>"},{"location":"src/scandeval/languages/","title":"scandeval.languages","text":"scandeval.languages<p> docs module scandeval.languages </p> <pre><code>\"\"\"List of languages and their ISO 639-1 codes.\n\nTaken from https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes.\n\nLast updated 19 June 2022.\n\"\"\"\n\nfrom .config import Language\n\n\ndef get_all_languages() -&gt; dict[str, Language]:docs\n    \"\"\"Get a list of all the languages.\n\n    Returns:\n        A mapping between language codes and their configurations.\n    \"\"\"\n    return {cfg.code: cfg for cfg in globals().values() if isinstance(cfg, Language)}\n\n\nAB = Language(code=\"ab\", name=\"Abkhazian\")\nAA = Language(code=\"aa\", name=\"Afar\")\nAF = Language(code=\"af\", name=\"Afrikaans\")\nSQ = Language(code=\"sq\", name=\"Albanian\")\nAM = Language(code=\"am\", name=\"Amharic\")\nAR = Language(code=\"ar\", name=\"Arabic\")\nAN = Language(code=\"an\", name=\"Aragonese\")\nHY = Language(code=\"hy\", name=\"Armenian\")\nAS = Language(code=\"as\", name=\"Assamese\")\nAV = Language(code=\"av\", name=\"Avaric\")\nAE = Language(code=\"ae\", name=\"Avestan\")\nAY = Language(code=\"ay\", name=\"Aymara\")\nAZ = Language(code=\"az\", name=\"Azerbaijani\")\nBM = Language(code=\"bm\", name=\"Bambara\")\nBA = Language(code=\"ba\", name=\"Bashkir\")\nEU = Language(code=\"eu\", name=\"Basque\")\nBE = Language(code=\"be\", name=\"Belarusian\")\nBN = Language(code=\"bn\", name=\"Bengali\")\nBI = Language(code=\"bi\", name=\"Bislama\")\nBS = Language(code=\"bs\", name=\"Bosnian\")\nBR = Language(code=\"br\", name=\"Breton\")\nBG = Language(code=\"bg\", name=\"Bulgarian\")\nMY = Language(code=\"my\", name=\"Burmese\")\nCA = Language(code=\"ca\", name=\"Catalan\")\nCH = Language(code=\"ch\", name=\"Chamorro\")\nCE = Language(code=\"ce\", name=\"Chechen\")\nNY = Language(code=\"ny\", name=\"Chichewa\")\nZH = Language(code=\"zh\", name=\"Chinese\")\nCU = Language(code=\"cu\", name=\"Church Slavic\")\nCV = Language(code=\"cv\", name=\"Chuvash\")\nKW = Language(code=\"kw\", name=\"Cornish\")\nCO = Language(code=\"co\", name=\"Corsican\")\nCR = Language(code=\"cr\", name=\"Cree\")\nHR = Language(code=\"hr\", name=\"Croatian\")\nCS = Language(code=\"cs\", name=\"Czech\")\nDA = Language(code=\"da\", name=\"Danish\")\nDV = Language(code=\"dv\", name=\"Divehi\")\nNL = Language(code=\"nl\", name=\"Dutch\")\nDZ = Language(code=\"dz\", name=\"Dzongkha\")\nEN = Language(code=\"en\", name=\"English\")\nEO = Language(code=\"eo\", name=\"Esperanto\")\nET = Language(code=\"et\", name=\"Estonian\")\nEE = Language(code=\"ee\", name=\"Ewe\")\nFO = Language(code=\"fo\", name=\"Faroese\")\nFJ = Language(code=\"fj\", name=\"Fijian\")\nFI = Language(code=\"fi\", name=\"Finnish\")\nFR = Language(code=\"fr\", name=\"French\")\nFY = Language(code=\"fy\", name=\"Western Frisian\")\nFF = Language(code=\"ff\", name=\"Fulah\")\nGD = Language(code=\"gd\", name=\"Gaelic\")\nGL = Language(code=\"gl\", name=\"Galician\")\nLG = Language(code=\"lg\", name=\"Ganda\")\nKA = Language(code=\"ka\", name=\"Georgian\")\nDE = Language(code=\"de\", name=\"German\")\nEL = Language(code=\"el\", name=\"Greek\")\nKL = Language(code=\"kl\", name=\"Greenlandic\")\nGN = Language(code=\"gn\", name=\"Guarani\")\nGU = Language(code=\"gu\", name=\"Gujarati\")\nHT = Language(code=\"ht\", name=\"Haitian\")\nHA = Language(code=\"ha\", name=\"Hausa\")\nHE = Language(code=\"he\", name=\"Hebrew\")\nHZ = Language(code=\"hz\", name=\"Herero\")\nHI = Language(code=\"hi\", name=\"Hindi\")\nHO = Language(code=\"ho\", name=\"Hiri Motu\")\nHU = Language(code=\"hu\", name=\"Hungarian\")\nIS = Language(code=\"is\", name=\"Icelandic\")\nIO = Language(code=\"io\", name=\"Ido\")\nIG = Language(code=\"ig\", name=\"Igbo\")\nID = Language(code=\"id\", name=\"Indonesian\")\nIA = Language(code=\"ia\", name=\"Interlingua\")\nIE = Language(code=\"ie\", name=\"Interlingue\")\nIU = Language(code=\"iu\", name=\"Inuktitut\")\nIK = Language(code=\"ik\", name=\"Inupiaq\")\nGA = Language(code=\"ga\", name=\"Irish\")\nIT = Language(code=\"it\", name=\"Italian\")\nJA = Language(code=\"ja\", name=\"Japanese\")\nKN = Language(code=\"kn\", name=\"Kannada\")\nKR = Language(code=\"kr\", name=\"Kanuri\")\nKS = Language(code=\"ks\", name=\"Kashmiri\")\nKK = Language(code=\"kk\", name=\"Kazakh\")\nKM = Language(code=\"km\", name=\"Central Khmer\")\nKI = Language(code=\"ki\", name=\"Kikuyu\")\nRW = Language(code=\"rw\", name=\"Kinyarwanda\")\nKY = Language(code=\"ky\", name=\"Kirghiz\")\nKV = Language(code=\"kv\", name=\"Komi\")\nKG = Language(code=\"kg\", name=\"Kongo\")\nKO = Language(code=\"ko\", name=\"Korean\")\nKJ = Language(code=\"kj\", name=\"Kuanyama\")\nKU = Language(code=\"ku\", name=\"Kurdish\")\nLO = Language(code=\"lo\", name=\"Lao\")\nLA = Language(code=\"la\", name=\"Latin\")\nLV = Language(code=\"lv\", name=\"Latvian\")\nLI = Language(code=\"li\", name=\"Limburgan\")\nLN = Language(code=\"ln\", name=\"Lingala\")\nLT = Language(code=\"lt\", name=\"Lithuanian\")\nLU = Language(code=\"lu\", name=\"Luba-Katanga\")\nLB = Language(code=\"lb\", name=\"Luxembourgish\")\nMK = Language(code=\"mk\", name=\"Macedonian\")\nMG = Language(code=\"mg\", name=\"Malagasy\")\nMS = Language(code=\"ms\", name=\"Malay\")\nML = Language(code=\"ml\", name=\"Malayalam\")\nMT = Language(code=\"mt\", name=\"Maltese\")\nGV = Language(code=\"gv\", name=\"Manx\")\nMI = Language(code=\"mi\", name=\"Maori\")\nMR = Language(code=\"mr\", name=\"Marathi\")\nMH = Language(code=\"mh\", name=\"Marshallese\")\nMN = Language(code=\"mn\", name=\"Mongolian\")\nNA = Language(code=\"na\", name=\"Nauru\")\nNV = Language(code=\"nv\", name=\"Navajo\")\nND = Language(code=\"nd\", name=\"Northern Ndebele\")\nNR = Language(code=\"nr\", name=\"South Ndebele\")\nNG = Language(code=\"ng\", name=\"Ndonga\")\nNE = Language(code=\"ne\", name=\"Nepali\")\nNO = Language(code=\"no\", name=\"Norwegian\")\nNB = Language(code=\"nb\", name=\"Norwegian Bokm\u00e5l\")\nNN = Language(code=\"nn\", name=\"Norwegian Nynorsk\")\nII = Language(code=\"ii\", name=\"Sichuan Yi\")\nOC = Language(code=\"oc\", name=\"Occitan\")\nOJ = Language(code=\"oj\", name=\"Ojibwa\")\nOR = Language(code=\"or\", name=\"Oriya\")\nOM = Language(code=\"om\", name=\"Oromo\")\nOS = Language(code=\"os\", name=\"Ossetian\")\nPI = Language(code=\"pi\", name=\"Pali\")\nPS = Language(code=\"ps\", name=\"Pashto\")\nFA = Language(code=\"fa\", name=\"Persian\")\nPL = Language(code=\"pl\", name=\"Polish\")\nPT = Language(code=\"pt\", name=\"Portuguese\")\nPA = Language(code=\"pa\", name=\"Punjabi\")\nQU = Language(code=\"qu\", name=\"Quechua\")\nRO = Language(code=\"ro\", name=\"Romanian\")\nRM = Language(code=\"rm\", name=\"Romansh\")\nRN = Language(code=\"rn\", name=\"Rundi\")\nRU = Language(code=\"ru\", name=\"Russian\")\nSE = Language(code=\"se\", name=\"Northern Sami\")\nSM = Language(code=\"sm\", name=\"Samoan\")\nSG = Language(code=\"sg\", name=\"Sango\")\nSA = Language(code=\"sa\", name=\"Sanskrit\")\nSC = Language(code=\"sc\", name=\"Sardinian\")\nSR = Language(code=\"sr\", name=\"Serbian\")\nSN = Language(code=\"sn\", name=\"Shona\")\nSD = Language(code=\"sd\", name=\"Sindhi\")\nSI = Language(code=\"si\", name=\"Sinhala\")\nSK = Language(code=\"sk\", name=\"Slovak\")\nSL = Language(code=\"sl\", name=\"Slovenian\")\nSO = Language(code=\"so\", name=\"Somali\")\nST = Language(code=\"st\", name=\"Sotho\")\nES = Language(code=\"es\", name=\"Spanish\")\nSU = Language(code=\"su\", name=\"Sundanese\")\nSW = Language(code=\"sw\", name=\"Swahili\")\nSS = Language(code=\"ss\", name=\"Swati\")\nSV = Language(code=\"sv\", name=\"Swedish\")\nTL = Language(code=\"tl\", name=\"Tagalog\")\nTY = Language(code=\"ty\", name=\"Tahitian\")\nTG = Language(code=\"tg\", name=\"Tajik\")\nTA = Language(code=\"ta\", name=\"Tamil\")\nTT = Language(code=\"tt\", name=\"Tatar\")\nTE = Language(code=\"te\", name=\"Telugu\")\nTH = Language(code=\"th\", name=\"Thai\")\nBO = Language(code=\"bo\", name=\"Tibetan\")\nTI = Language(code=\"ti\", name=\"Tigrinya\")\nTO = Language(code=\"to\", name=\"Tonga\")\nTS = Language(code=\"ts\", name=\"Tsonga\")\nTN = Language(code=\"tn\", name=\"Tswana\")\nTR = Language(code=\"tr\", name=\"Turkish\")\nTK = Language(code=\"tk\", name=\"Turkmen\")\nTW = Language(code=\"tw\", name=\"Twi\")\nUG = Language(code=\"ug\", name=\"Uighur\")\nUK = Language(code=\"uk\", name=\"Ukrainian\")\nUR = Language(code=\"ur\", name=\"Urdu\")\nUZ = Language(code=\"uz\", name=\"Uzbek\")\nVE = Language(code=\"ve\", name=\"Venda\")\nVI = Language(code=\"vi\", name=\"Vietnamese\")\nVO = Language(code=\"vo\", name=\"Volap\u00fck\")\nWA = Language(code=\"wa\", name=\"Walloon\")\nCY = Language(code=\"cy\", name=\"Welsh\")\nWO = Language(code=\"wo\", name=\"Wolof\")\nXH = Language(code=\"xh\", name=\"Xhosa\")\nYI = Language(code=\"yi\", name=\"Yiddish\")\nYO = Language(code=\"yo\", name=\"Yoruba\")\nZA = Language(code=\"za\", name=\"Zhuang\")\nZU = Language(code=\"zu\", name=\"Zulu\")\n</code></pre>"},{"location":"api/scandeval/model_cache/","title":"scandeval.model_cache","text":"scandeval.model_cache<p> source module scandeval.model_cache </p> <p>ModelCache class for caching model outputs.</p> <p> Classes </p> <ul> <li> <p>GenerativeModelOutput \u2014 The output of a generative model.</p> </li> <li> <p>ModelCache \u2014 A cache for model outputs.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>split_dataset_into_cached_and_non_cached \u2014 Split a dataset into a cached and non-cached part.</p> </li> <li> <p>load_cached_model_outputs \u2014 Load the cached model outputs.</p> </li> </ul> <p> source dataclass GenerativeModelOutput(completion: str, top_score_indices: list[list[int]] | None = None, top_score_values: list[list[float]] | None = None, vocab_size: int | None = None) </p> <p>The output of a generative model.</p> <p> source class ModelCache(cache_name: str, max_generated_tokens: int) </p> <p>A cache for model outputs.</p> <p>Initialize the model output cache.</p> <p> Attributes </p> <ul> <li> <p>model_cache_dir \u2014</p> <p>The directory to store the cache in.</p> </li> <li> <p>cache_path \u2014</p> <p>The path to the cache file.</p> </li> <li> <p>cache \u2014</p> <p>The model output cache.</p> </li> <li> <p>max_generated_tokens \u2014</p> <p>The maximum number of tokens to generate for each example.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>model_cache_dir :  Path \u2014</p> <p>The directory to store the cache in.</p> </li> <li> <p>cache_name :  str \u2014</p> <p>The name of the cache file.</p> </li> <li> <p>max_generated_tokens :  int \u2014</p> <p>The maximum number of tokens to generate for each example.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>load \u2014 Load the model output cache.</p> </li> <li> <p>save \u2014 Save the model output cache to disk.</p> </li> <li> <p>remove \u2014 Remove the cache from memory and delete it from disk.</p> </li> <li> <p>cached_texts \u2014 Return the text inputs indexed in the cache.</p> </li> <li> <p>add_to_cache \u2014 Add the model input/output to the cache.</p> </li> </ul> <p> source method ModelCache.load() \u2192 None </p> <p>Load the model output cache.</p> <p> source method ModelCache.save() \u2192 None </p> <p>Save the model output cache to disk.</p> <p> source method ModelCache.remove() \u2192 None </p> <p>Remove the cache from memory and delete it from disk.</p> <p> source method ModelCache.cached_texts() \u2192 list[str] </p> <p>Return the text inputs indexed in the cache.</p> <p> source method ModelCache.add_to_cache(model_input: torch.Tensor, model_output: ModelOutput, tokenizer: Tokenizer) \u2192 None </p> <p>Add the model input/output to the cache.</p> <p> Parameters </p> <ul> <li> <p>model_input :  torch.Tensor \u2014</p> <p>The model input.</p> </li> <li> <p>model_output :  ModelOutput \u2014</p> <p>The model output.</p> </li> <li> <p>tokenizer :  Tokenizer \u2014</p> <p>The tokenizer used to generate the tokens.</p> </li> </ul> <p> source split_dataset_into_cached_and_non_cached(dataset: Dataset, cache: ModelCache) \u2192 tuple[Dataset, Dataset] </p> <p>Split a dataset into a cached and non-cached part.</p> <p> Parameters </p> <ul> <li> <p>dataset :  Dataset \u2014</p> <p>The dataset to split.</p> </li> <li> <p>cache :  ModelCache \u2014</p> <p>The model output cache.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[Dataset, Dataset] \u2014 The cached and non-cached parts of the dataset.</p> </li> </ul> <p> source load_cached_model_outputs(cached_dataset: Dataset, cache: ModelCache, tokenizer: Tokenizer) \u2192 ModelOutput </p> <p>Load the cached model outputs.</p> <p> Parameters </p> <ul> <li> <p>cached_dataset :  Dataset \u2014</p> <p>The dataset containing the cached examples.</p> </li> <li> <p>cache :  ModelCache \u2014</p> <p>The model output cache.</p> </li> <li> <p>tokenizer :  Tokenizer \u2014</p> <p>The tokenizer used to generate the tokens.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelOutput \u2014 The model output containing the cached sequences.</p> </li> </ul>"},{"location":"src/scandeval/model_cache/","title":"scandeval.model_cache","text":"scandeval.model_cache<p> docs module scandeval.model_cache </p> <pre><code>\"\"\"ModelCache class for caching model outputs.\"\"\"\n\nimport json\nimport logging\nimport math\nimport sys\nfrom collections import defaultdict\nfrom dataclasses import asdict, dataclass\nfrom typing import TYPE_CHECKING\n\nimport pandas as pd\nimport torch\nfrom tqdm.auto import tqdm\nfrom transformers.modeling_utils import ModelOutput\n\nif TYPE_CHECKING:\n    from pathlib import Path\n\n    from datasets import Dataset\n\n    from .protocols import Tokenizer\n\n\nlogger = logging.getLogger(__package__)\n\n\n@dataclass\nclass GenerativeModelOutput:docs\n    \"\"\"The output of a generative model.\"\"\"\n\n    completion: str\n    top_score_indices: list[list[int]] | None = None\n    top_score_values: list[list[float]] | None = None\n    vocab_size: int | None = None\n\n\nclass ModelCache:docs\n    \"\"\"A cache for model outputs.\n\n    Attributes:\n        model_cache_dir:\n            The directory to store the cache in.\n        cache_path:\n            The path to the cache file.\n        cache:\n            The model output cache.\n        max_generated_tokens:\n            The maximum number of tokens to generate for each example.\n    \"\"\"\n\n    def __init__(\n        self, model_cache_dir: \"Path\", cache_name: str, max_generated_tokens: int\n    ):\n        \"\"\"Initialize the model output cache.\n\n        Args:\n            model_cache_dir:\n                The directory to store the cache in.\n            cache_name:\n                The name of the cache file.\n            max_generated_tokens:\n                The maximum number of tokens to generate for each example.\n        \"\"\"\n        self.model_cache_dir = model_cache_dir\n        self.model_cache_dir.mkdir(parents=True, exist_ok=True)\n        self.cache_path = self.model_cache_dir / cache_name.replace(\"/\", \"--\")\n        self.max_generated_tokens = max_generated_tokens\n\n    def load(self) -&gt; None:docs\n        \"\"\"Load the model output cache.\"\"\"\n        if not self.cache_path.exists():\n            with self.cache_path.open(\"w\") as f:\n                json.dump(dict(), f)\n\n        with self.cache_path.open() as f:\n            json_cache = json.load(f)\n\n        cache: dict[str, GenerativeModelOutput] = dict()\n        for key in json_cache:\n            cache[key] = GenerativeModelOutput(**json_cache[key])\n\n        self.cache = cache\n\n    def save(self) -&gt; None:docs\n        \"\"\"Save the model output cache to disk.\"\"\"\n        dumpable_cache: dict[str, dict] = defaultdict(dict)\n        for key, value in self.cache.items():\n            dumpable_cache[key] = asdict(value)\n\n        with self.cache_path.open(\"w\") as f:\n            json.dump(dumpable_cache, f)\n\n    def __getitem__(self, key: str) -&gt; GenerativeModelOutput:\n        \"\"\"Get an item from the cache.\n\n        Args:\n            key:\n                The key to use to index the cache.\n\n        Returns:\n            The model output.\n        \"\"\"\n        return self.cache[key]\n\n    def __setitem__(self, key: str, value: GenerativeModelOutput) -&gt; None:\n        \"\"\"Set an item in the cache.\n\n        Args:\n            key:\n                The key to use to index the cache.\n            value:\n                The value to set in the cache.\n        \"\"\"\n        self.cache[key] = value\n\n    def remove(self) -&gt; None:docs\n        \"\"\"Remove the cache from memory and delete it from disk.\"\"\"\n        self.cache_path.unlink()\n        del self.cache\n\n    def cached_texts(self) -&gt; list[str]:docs\n        \"\"\"Return the text inputs indexed in the cache.\"\"\"\n        return [key for key in self.cache.keys()]\n\n    def add_to_cache(docs\n        self,\n        model_input: torch.Tensor,\n        model_output: ModelOutput,\n        tokenizer: \"Tokenizer\",\n    ) -&gt; None:\n        \"\"\"Add the model input/output to the cache.\n\n        Args:\n            model_input:\n                The model input.\n            model_output:\n                The model output.\n            tokenizer:\n                The tokenizer used to generate the tokens.\n        \"\"\"\n        model_input = model_input.detach().cpu()\n\n        # Extract the scores from the model output, to be cached. We only store the\n        # indices of the top scores, to save space. Further, we only store the scores\n        # if the generated sequence is shorter than the maximum length\n        store_scores = \"scores\" in model_output and self.max_generated_tokens &lt; 8\n        if store_scores:\n            scores = torch.stack(\n                tensors=[\n                    score_tensor.detach().cpu().float()\n                    for score_tensor in model_output.scores\n                ],\n                dim=1,\n            )\n            top_scores = torch.topk(scores, k=10)\n\n        # Store the generated sequences in the cache, one by one\n        # TODO: This is a bit slow, should be optimized\n        with tqdm(\n            iterable=model_input,\n            desc=\"Caching model outputs\",\n            leave=False,\n            disable=hasattr(sys, \"_called_from_test\"),\n        ) as pbar:\n            for sample_idx, sample in enumerate(pbar):\n                decoded_inputs = tokenizer.decode(\n                    token_ids=sample, skip_special_tokens=True\n                )\n                generated_ids = model_output.sequences[sample_idx].tolist()\n\n                # Set up the model output in a GenerativeModelOutput object\n                cached_model_output = GenerativeModelOutput(\n                    completion=tokenizer.decode(\n                        token_ids=generated_ids, skip_special_tokens=True\n                    )\n                )\n                if store_scores:\n                    cached_model_output.top_score_indices = top_scores.indices[\n                        sample_idx\n                    ].tolist()\n                    cached_model_output.top_score_values = top_scores.values[\n                        sample_idx\n                    ].tolist()\n                    cached_model_output.vocab_size = int(scores.shape[-1])\n\n                # Store the generated sequence in the cache\n                self[decoded_inputs] = cached_model_output\n\n\ndef split_dataset_into_cached_and_non_cached(docs\n    dataset: \"Dataset\", cache: ModelCache\n) -&gt; tuple[\"Dataset\", \"Dataset\"]:\n    \"\"\"Split a dataset into a cached and non-cached part.\n\n    Args:\n        dataset:\n            The dataset to split.\n        cache:\n            The model output cache.\n\n    Returns:\n        The cached and non-cached parts of the dataset.\n    \"\"\"\n    # Get the sample indices of the non-cached examples, which are unique with respect\n    # to the \"text\" column.\n    dataset_texts = pd.Series(dataset[\"text\"])\n    dataset_texts.drop_duplicates(inplace=True)\n    unique_non_cached_ids = set(\n        dataset_texts[~dataset_texts.isin(cache.cached_texts())].index.tolist()\n    )\n\n    # The cached examples are the ones that are not in the non-cached examples. This\n    # means that if the dataset has duplicates, only a single copy of the duplicate\n    # will be put in the non-cached part, and the rest in the cached part.\n    cached_ids = set(range(len(dataset))) - unique_non_cached_ids\n\n    cached = dataset.select(cached_ids)\n    non_cached = dataset.select(unique_non_cached_ids)\n    return cached, non_cached\n\n\ndef load_cached_model_outputs(docs\n    cached_dataset: \"Dataset\", cache: ModelCache, tokenizer: \"Tokenizer\"\n) -&gt; ModelOutput:\n    \"\"\"Load the cached model outputs.\n\n    Args:\n        cached_dataset:\n            The dataset containing the cached examples.\n        cache:\n            The model output cache.\n        tokenizer:\n            The tokenizer used to generate the tokens.\n\n    Returns:\n        The model output containing the cached sequences.\n    \"\"\"\n    # Load the raw model outputs from the cache\n    cached_model_outputs: list[GenerativeModelOutput] = [\n        cache[prompt] for prompt in cached_dataset[\"text\"]\n    ]\n\n    # Tokenize the cached sequences\n    tokenized_cached_sequences: list[torch.Tensor] = [\n        tokenizer(\n            text=cached_model_output.completion,\n            add_special_tokens=False,\n            return_tensors=\"pt\",\n        ).input_ids.squeeze(dim=0)\n        for cached_model_output in cached_model_outputs\n    ]\n\n    # Pad the cached completions to the same length\n    cached_sequences = torch.nn.utils.rnn.pad_sequence(\n        sequences=tokenized_cached_sequences,\n        batch_first=True,\n        padding_value=tokenizer.pad_token_id,\n    )\n\n    # If we do not have any cached scores, then wrap the padded cached sequences in a\n    # ModelOutput and return it\n    if (\n        cached_model_outputs[0].top_score_indices is None\n        or cached_model_outputs[0].top_score_values is None\n        or cached_model_outputs[0].vocab_size is None\n    ):\n        return ModelOutput(sequences=cached_sequences)\n\n    # Otherwise, we format the cached scores into a tensor of shape [batch_size,\n    # num_sequences, vocab_size], wrap it in a ModelOutput with the padded cached\n    # sequences, and return it\n    cached_scores = torch.full(\n        size=(\n            len(cached_model_outputs),\n            max(\n                len(cached_model_output.top_score_indices)\n                for cached_model_output in cached_model_outputs\n                if cached_model_output.top_score_indices is not None\n            ),\n            cached_model_outputs[0].vocab_size,\n        ),\n        fill_value=-math.inf,\n    )\n    top_score_indices = torch.nn.utils.rnn.pad_sequence(\n        sequences=[\n            torch.tensor(cached_model_output.top_score_indices)\n            for cached_model_output in cached_model_outputs\n        ],\n        batch_first=True,\n        padding_value=tokenizer.pad_token_id,\n    )\n    top_score_values = torch.nn.utils.rnn.pad_sequence(\n        sequences=[\n            torch.tensor(cached_model_output.top_score_values)\n            for cached_model_output in cached_model_outputs\n        ],\n        batch_first=True,\n        padding_value=-math.inf,\n    )\n    for batch_idx in range(cached_scores.shape[0]):\n        for sequence_idx in range(cached_scores.shape[1]):\n            top_indices = top_score_indices[batch_idx, sequence_idx]\n            top_values = top_score_values[batch_idx, sequence_idx]\n            cached_scores[batch_idx, sequence_idx, top_indices] = top_values\n    return ModelOutput(sequences=cached_sequences, scores=cached_scores)\n</code></pre>"},{"location":"api/scandeval/model_config/","title":"scandeval.model_config","text":"scandeval.model_config<p> source module scandeval.model_config </p> <p>Functions related to getting the model configuration.</p> <p> Functions </p> <ul> <li> <p>get_model_config \u2014 Fetches configuration for a model.</p> </li> </ul> <p> source get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetches configuration for a model.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The configuration of the benchmark.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel \u2014</p> <p>If all model setups can handle the model, but the model does not exist.</p> </li> <li> <p>ValueError</p> </li> <li> <p>NeedsExtraInstalled</p> </li> </ul>"},{"location":"src/scandeval/model_config/","title":"scandeval.model_config","text":"scandeval.model_config<p> docs module scandeval.model_config </p> <pre><code>\"\"\"Functions related to getting the model configuration.\"\"\"\n\nimport importlib.util\nfrom typing import TYPE_CHECKING\n\nfrom .enums import Framework\nfrom .exceptions import InvalidModel, NeedsExtraInstalled\nfrom .model_setups import MODEL_SETUP_CLASSES\n\nif TYPE_CHECKING:\n    from .config import BenchmarkConfig, ModelConfig\n\n\ndef get_model_config(docs\n    model_id: str, benchmark_config: \"BenchmarkConfig\"\n) -&gt; \"ModelConfig\":\n    \"\"\"Fetches configuration for a model.\n\n    Args:\n        model_id:\n            The model ID.\n        benchmark_config:\n            The configuration of the benchmark.\n\n    Returns:\n        The model configuration.\n\n    Raises:\n        InvalidModel:\n            If all model setups can handle the model, but the model does not exist.\n    \"\"\"\n    needs_extras: list[str] = list()\n    needs_env_vars: list[str] = list()\n    for setup_class in MODEL_SETUP_CLASSES:\n        setup = setup_class(benchmark_config=benchmark_config)\n\n        exists_or_dict = setup.model_exists(model_id=model_id)\n        if isinstance(exists_or_dict, dict):\n            if \"missing_extra\" in exists_or_dict:\n                needs_extras.append(exists_or_dict[\"missing_extra\"])\n            elif \"missing_env_var\" in exists_or_dict:\n                needs_env_vars.append(exists_or_dict[\"missing_env_var\"])\n            else:\n                raise ValueError(\n                    \"The dictionary returned by `model_exists` must contain either \"\n                    \"the key `missing_extra` or `missing_env_var`.\"\n                )\n        elif exists_or_dict:\n            model_config = setup.get_model_config(model_id=model_id)\n            if (\n                model_config.framework == Framework.JAX\n                and importlib.util.find_spec(\"jax\") is None\n            ):\n                raise NeedsExtraInstalled(extra=\"jax\")\n            return model_config\n    else:\n        msg = f\"Model {model_id} not found.\"\n        if needs_extras:\n            msg += (\n                \" However, it is possible that the model exists, but a package \"\n                \"needs to be installed to check if it exists. Please try running \"\n                f\"`pip install scandeval[{','.join(needs_extras)}]` or `pip install \"\n                \"scandeval[all]`, and try again.\"\n            )\n        elif needs_env_vars:\n            msg += (\n                \" However, it is possible that the model exists, but an environment \"\n                \"variable needs to be set to check if it exists. Please set the \"\n                f\"environment variables {','.join(needs_env_vars)} and try again.\"\n            )\n        raise InvalidModel(msg)\n</code></pre>"},{"location":"api/scandeval/model_loading/","title":"scandeval.model_loading","text":"scandeval.model_loading<p> source module scandeval.model_loading </p> <p>Functions related to the loading of models.</p> <p> Functions </p> <ul> <li> <p>load_model \u2014 Load a model.</p> </li> </ul> <p> source load_model(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 tuple[PreTrainedModel | GenerativeModel, Tokenizer] </p> <p>Load a model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[PreTrainedModel | GenerativeModel, Tokenizer] \u2014 The tokenizer and model.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul>"},{"location":"src/scandeval/model_loading/","title":"scandeval.model_loading","text":"scandeval.model_loading<p> docs module scandeval.model_loading </p> <pre><code>\"\"\"Functions related to the loading of models.\"\"\"\n\nfrom typing import TYPE_CHECKING, Type\n\nfrom .exceptions import InvalidBenchmark\nfrom .model_setups import (\n    FreshModelSetup,\n    HFModelSetup,\n    LocalModelSetup,\n    OpenAIModelSetup,\n)\nfrom .utils import (\n    GENERATIVE_DATASET_SUPERTASKS,\n    GENERATIVE_DATASET_TASKS,\n    model_is_generative,\n)\n\nif TYPE_CHECKING:\n    from transformers import PreTrainedModel\n\n    from .config import BenchmarkConfig, DatasetConfig, ModelConfig\n    from .protocols import GenerativeModel, ModelSetup, Tokenizer\n\n\ndef load_model(docs\n    model_config: \"ModelConfig\",\n    dataset_config: \"DatasetConfig\",\n    benchmark_config: \"BenchmarkConfig\",\n) -&gt; tuple[\"PreTrainedModel | GenerativeModel\", \"Tokenizer\"]:\n    \"\"\"Load a model.\n\n    Args:\n        model_config:\n            The model configuration.\n        dataset_config:\n            The dataset configuration.\n        benchmark_config:\n            The benchmark configuration.\n\n    Returns:\n        The tokenizer and model.\n    \"\"\"\n    model_type_to_model_setup_mapping: dict[str, Type[\"ModelSetup\"]] = dict(\n        fresh=FreshModelSetup,\n        hf=HFModelSetup,\n        local=LocalModelSetup,\n        openai=OpenAIModelSetup,\n    )\n    setup_class = model_type_to_model_setup_mapping[model_config.model_type]\n    setup = setup_class(benchmark_config=benchmark_config)\n\n    model, tokenizer = setup.load_model(\n        model_config=model_config, dataset_config=dataset_config\n    )\n\n    # Refuse to benchmark non-generative models on generative tasks\n    if (\n        (\n            dataset_config.task.supertask in GENERATIVE_DATASET_SUPERTASKS\n            or dataset_config.task.name in GENERATIVE_DATASET_TASKS\n        )\n        and model is not None\n        and not model_is_generative(model=model)\n    ):\n        raise InvalidBenchmark(\n            f\"Cannot benchmark non-generative model {model_config.model_id!r} on \"\n            f\"generative task {dataset_config.task.name!r}.\"\n        )\n\n    # TODO: XMOD model setup: https://huggingface.co/facebook/xmod-base#input-language\n\n    return model, tokenizer\n</code></pre>"},{"location":"api/scandeval/named_entity_recognition/","title":"scandeval.named_entity_recognition","text":"scandeval.named_entity_recognition<p> source module scandeval.named_entity_recognition </p> <p>Named entity recognition benchmark dataset.</p> <p> Classes </p> <ul> <li> <p>NamedEntityRecognition \u2014 Named entity recognition benchmark dataset.</p> </li> </ul> <p> source class NamedEntityRecognition() </p> <p><p>Bases : BenchmarkDataset</p></p> <p>Named entity recognition benchmark dataset.</p> <p> Parameters </p> <ul> <li> <p>dataset_config \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Attributes </p> <ul> <li> <p>dataset_config \u2014</p> <p>The configuration of the dataset.</p> </li> <li> <p>benchmark_config \u2014</p> <p>The configuration of the benchmark.</p> </li> </ul>"},{"location":"src/scandeval/named_entity_recognition/","title":"scandeval.named_entity_recognition","text":"scandeval.named_entity_recognition<p> docs module scandeval.named_entity_recognition </p> <pre><code>\"\"\"Named entity recognition benchmark dataset.\"\"\"\n\nimport importlib.util\nimport itertools as it\nimport json\nimport logging\nimport random\nimport re\nfrom copy import deepcopy\nfrom functools import partial\nfrom typing import TYPE_CHECKING, Any\n\nimport numpy as np\nfrom evaluate import EvaluationModule\nfrom transformers import DataCollatorWithPadding\nfrom transformers.data.data_collator import DataCollatorForTokenClassification\n\nfrom .benchmark_dataset import BenchmarkDataset\nfrom .exceptions import InvalidBenchmark, NeedsExtraInstalled\nfrom .generation import extract_raw_predictions\nfrom .utils import (\n    GENERATIVE_MODEL_TASKS,\n    convert_prompt_to_instruction,\n    model_is_generative,\n    raise_if_model_output_contains_nan_values,\n)\n\nif TYPE_CHECKING:\n    from datasets.arrow_dataset import Dataset\n    from datasets.dataset_dict import DatasetDict\n    from transformers import BatchEncoding, PreTrainedModel\n    from transformers.modeling_utils import ModelOutput\n\n    from .protocols import GenerativeModel, Tokenizer\n    from .types import Labels, Predictions\n\nif importlib.util.find_spec(\"demjson3\") is not None:\n    import demjson3\n\n\nlogger = logging.getLogger(__package__)\n\n\nclass NamedEntityRecognition(BenchmarkDataset):docs\n    \"\"\"Named entity recognition benchmark dataset.\n\n    Args:\n        dataset_config:\n            The dataset configuration.\n        benchmark_config:\n            The benchmark configuration.\n\n    Attributes:\n        dataset_config:\n            The configuration of the dataset.\n        benchmark_config:\n            The configuration of the benchmark.\n    \"\"\"\n\n    def _process_data(self, dataset_dict: \"DatasetDict\") -&gt; \"DatasetDict\":\n        \"\"\"Process the data.\n\n        Args:\n            dataset_dict:\n                The dataset dictionary.\n\n        Returns:\n            The processed dataset dictionary.\n        \"\"\"\n        # Check what labels are present in the dataset, and store if MISC tags are not\n        # present\n        labels_in_train: set[str] = {\n            tag for tag_list in dataset_dict[\"train\"][\"labels\"] for tag in tag_list\n        }\n        self.has_misc_tags = \"B-MISC\" in labels_in_train or \"I-MISC\" in labels_in_train\n\n        return dataset_dict\n\n    def _compute_metrics(\n        self,\n        model_outputs_and_labels: tuple[\"Predictions\", \"Labels\"],\n        id2label: dict[int, str],\n    ) -&gt; dict[str, float]:\n        \"\"\"Compute the metrics needed for evaluation.\n\n        Args:\n            model_outputs_and_labels:\n                The first array contains the probability predictions and the second\n                array contains the true labels.\n            id2label:\n                Conversion of indices to labels.\n\n        Returns:\n            A dictionary with the names of the metrics as keys and the metric values as\n            values.\n        \"\"\"\n        model_outputs, labels = model_outputs_and_labels\n\n        raise_if_model_output_contains_nan_values(model_output=model_outputs)\n\n        predictions: list[list[str]]\n        if not isinstance(model_outputs[0][0], str):\n            raw_predictions: list[list[int]] = np.argmax(\n                model_outputs, axis=-1\n            ).tolist()\n\n            # Remove ignored index (special tokens)\n            predictions = [\n                [\n                    id2label[pred_id]\n                    for pred_id, lbl_id in zip(pred, label)\n                    if lbl_id != -100\n                ]\n                for pred, label in zip(raw_predictions, labels)\n            ]\n            labels = [\n                [\n                    (\n                        id2label[int(lbl_id)]\n                        if isinstance(lbl_id, int) or isinstance(lbl_id, np.int_)\n                        else lbl_id\n                    )\n                    for lbl_id in label\n                    if lbl_id != -100\n                ]\n                for label in labels\n            ]\n\n        else:\n            predictions = model_outputs  # type: ignore[assignment]\n\n        # Replace predicted tag with either MISC or O tags if they are not part of the\n        # dataset\n        labels_without_misc = {\n            label\n            for label in self.dataset_config.id2label.values()\n            if label not in {\"b-misc\", \"i-misc\"}\n        }\n        ner_tag: str\n        for i, prediction_list in enumerate(predictions):\n            for j, ner_tag in enumerate(prediction_list):\n                if ner_tag not in labels_without_misc:\n                    if self.has_misc_tags and ner_tag[:2] == \"b-\":\n                        predictions[i][j] = \"b-misc\"\n                    elif self.has_misc_tags and ner_tag[:2] == \"i-\":\n                        predictions[i][j] = \"i-misc\"\n                    else:\n                        predictions[i][j] = \"o\"\n\n        # Remove MISC labels from predictions\n        predictions_no_misc = deepcopy(predictions)\n        for i, prediction_list in enumerate(predictions_no_misc):\n            for j, ner_tag in enumerate(prediction_list):\n                if ner_tag[-4:] == \"misc\":\n                    predictions_no_misc[i][j] = \"o\"\n\n        # Remove MISC labels from labels\n        labels_no_misc: list[list[str]] = deepcopy(labels)  # type: ignore[arg-type]\n        for i, label_list in enumerate(labels_no_misc):\n            for j, ner_tag in enumerate(label_list):\n                if (\n                    isinstance(ner_tag, str)\n                    and len(ner_tag) &gt;= 4\n                    and ner_tag[-4:] == \"misc\"\n                ):\n                    labels_no_misc[i][j] = \"o\"\n\n        # Compute the metrics\n        # We manually set the F1 metric to be 100% if both the labels and the models\n        # have no NER tags in them, since this causes an error with the `compute`\n        # method otherwise\n        predictions_all_zero = all(\n            all(ner_tag == \"o\" for ner_tag in prediction_list)\n            for prediction_list in predictions\n        )\n        labels_all_zero = all(\n            all(ner_tag == \"o\" for ner_tag in label_list) for label_list in labels\n        )\n        if predictions_all_zero and labels_all_zero:\n            results = dict(overall_f1=1.0)\n        else:\n            metric = self._metrics[\"micro_f1\"]\n            assert isinstance(metric, EvaluationModule)\n            results = metric.compute(predictions=predictions, references=labels)\n\n        # Compute the metrics without MISC tags\n        # We manually set the F1 metric to be 100% if both the labels and the models\n        # have no NER tags in them, since this causes an error with the `compute`\n        # method otherwise\n        predictions_no_misc_all_zero = all(\n            all(ner_tag == \"o\" for ner_tag in prediction_list)\n            for prediction_list in predictions_no_misc\n        )\n        labels_no_misc_all_zero = all(\n            all(ner_tag == \"o\" for ner_tag in label_list)\n            for label_list in labels_no_misc\n        )\n        if predictions_no_misc_all_zero and labels_no_misc_all_zero:\n            results_no_misc = dict(overall_f1=1.0)\n        else:\n            metric = self._metrics[\"micro_f1_no_misc\"]\n            assert isinstance(metric, EvaluationModule)\n            results_no_misc = metric.compute(\n                predictions=predictions_no_misc, references=labels_no_misc\n            )\n\n        # Raise error if the metrics are invalid\n        if results is None or results_no_misc is None:\n            raise InvalidBenchmark(\n                \"The predictions and labels are not of the same length.\"\n            )\n\n        return dict(\n            micro_f1_no_misc=results_no_misc[\"overall_f1\"],\n            micro_f1=results[\"overall_f1\"],\n        )\n\n    def _tokenize_and_align_labels(\n        self, examples: dict, tokenizer: \"Tokenizer\", label2id: dict[str, int]\n    ) -&gt; \"BatchEncoding\":\n        \"\"\"Tokenise all texts and align the labels with them.\n\n        Args:\n            examples:\n                The examples to be tokenised.\n            tokenizer:\n                A pretrained tokenizer.\n            label2id (dict):\n                A dictionary that converts NER tags to IDs.\n\n        Returns:\n            A dictionary containing the tokenized data as well as labels.\n        \"\"\"\n        # Tokenize the texts. We use the `is_split_into_words` argument here because\n        # the texts in our dataset are lists of words (with a label for each word)\n        tokenized_inputs = tokenizer(\n            examples[\"tokens\"], is_split_into_words=True, truncation=True, padding=True\n        )\n\n        # Extract a mapping between all the tokens and their corresponding word. If the\n        # tokenizer is of a \"fast\" variant then this can be accessed through the\n        # `word_ids` method. Otherwise, we have to extract it manually.\n        all_labels: list[list[int]] = list()\n        labels: list[str]\n        word_ids: list[int | None]\n        for i, labels in enumerate(examples[\"labels\"]):\n            # Try to get the word IDs from the tokenizer\n            try:\n                word_ids = tokenized_inputs.word_ids(batch_index=i)\n\n            # If the tokenizer is not of a \"fast\" variant, we have to extract the word\n            # IDs manually\n            except ValueError:\n                # Get the list of words in the document\n                words: list[str] = examples[\"tokens\"][i]\n\n                # Get the list of token IDs in the document\n                tok_ids: list[int] = tokenized_inputs.input_ids[i]\n\n                # Decode the token IDs\n                tokens = tokenizer.convert_ids_to_tokens(tok_ids)\n                assert isinstance(tokens, list)\n\n                # Remove prefixes from the tokens\n                prefixes_to_remove = [\"\u2581\", \"##\"]\n                for tok_idx, tok in enumerate(tokens):\n                    if tok:\n                        for prefix in prefixes_to_remove:\n                            if tok.startswith(prefix):\n                                tokens[tok_idx] = tok[len(prefix) :]\n\n                # Replace UNK tokens with the correct word\n                tokens = self._handle_unk_tokens(\n                    tokenizer=tokenizer, tokens=tokens, words=words\n                )\n\n                # Get list of special tokens. Some tokenizers do not record these\n                # properly, which is why we convert the values to their indices and\n                # then back to strings\n                sp_toks = [\n                    tokenizer.convert_ids_to_tokens(\n                        tokenizer.convert_tokens_to_ids(sp_tok)\n                    )\n                    for sp_tok in tokenizer.special_tokens_map.values()\n                ]\n\n                # Replace special tokens with `None`\n                tokens_with_none = [None if tok in sp_toks else tok for tok in tokens]\n\n                # Get the alignment between the words and the tokens, on a character\n                # level\n                word_idxs = [\n                    word_idx for word_idx, word in enumerate(words) for _ in str(word)\n                ]\n                token_idxs = [\n                    tok_idx\n                    for tok_idx, tok_or_none in enumerate(tokens_with_none)\n                    for _ in str(tok_or_none)\n                    if tok_or_none is not None\n                ]\n                alignment = list(zip(word_idxs, token_idxs))\n\n                # Raise error if there are not as many characters in the words as in\n                # the tokens. This can be due to the use of a different prefix.\n                if len(word_idxs) != len(token_idxs):\n                    raise InvalidBenchmark(\n                        \"The tokens could not be aligned with the words during manual \"\n                        \"word-token alignment. It seems that the tokenizer is neither \"\n                        \"of the fast variant nor of a SentencePiece/WordPiece variant.\"\n                    )\n\n                # Get the aligned word IDs\n                word_ids = list()\n                for tok_idx, tok_or_none in enumerate(tokens_with_none):\n                    if tok_or_none is None or tok_or_none == \"\":\n                        word_ids.append(None)\n                    else:\n                        word_idx = [\n                            word_idx\n                            for word_idx, token_idx in alignment\n                            if token_idx == tok_idx\n                        ][0]\n                        word_ids.append(word_idx)\n\n            previous_word_idx: int | None = None\n            label_ids: list[int] = list()\n            for word_id in word_ids:\n                # Special tokens have a word id that is None. We set the label to -100\n                # so they are automatically ignored in the loss function\n                if word_id is None:\n                    label_ids.append(-100)\n\n                # We set the label for the first token of each word\n                elif word_id != previous_word_idx:\n                    label = labels[word_id]\n                    try:\n                        label_id = label2id[label.lower()]\n                    except KeyError:\n                        msg = f\"The label {label} was not found in the model's config.\"\n                        raise InvalidBenchmark(msg)\n                    label_ids.append(label_id)\n\n                # For the other tokens in a word, we set the label to -100\n                else:\n                    label_ids.append(-100)\n\n                previous_word_idx = word_id\n\n            all_labels.append(label_ids)\n        tokenized_inputs[\"labels\"] = all_labels\n        return tokenized_inputs\n\n    def _handle_unk_tokens(\n        self, tokenizer: \"Tokenizer\", tokens: list[str], words: list[str]\n    ) -&gt; list[str]:\n        \"\"\"Replace unknown tokens in the tokens with the corresponding word.\n\n        Args:\n            tokenizer:\n                The tokenizer used to tokenize the words.\n            tokens:\n                The list of tokens.\n            words:\n                The list of words.\n\n        Returns:\n            The list of tokens with unknown tokens replaced by the corresponding word.\n        \"\"\"\n        # Locate the token indices of the unknown tokens\n        token_unk_idxs = [\n            i for i, tok in enumerate(tokens) if tok == tokenizer.unk_token\n        ]\n\n        # Locate the word indices of the words which contain an unknown token\n        word_unk_idxs = [\n            i\n            for i, word in enumerate(words)\n            if tokenizer.unk_token\n            in tokenizer.convert_ids_to_tokens(\n                tokenizer.encode(word, add_special_tokens=False)\n            )\n        ]\n\n        # Iterate over the token index and word index pairs\n        for tok_idx, word_idx in zip(token_unk_idxs, word_unk_idxs):\n            # Fetch the word\n            word = words[word_idx]\n\n            # Tokenize the word, which is now a list containing at least one UNK token\n            tokens_with_unk = tokenizer.convert_ids_to_tokens(\n                tokenizer.encode(word, add_special_tokens=False)\n            )\n\n            # Iterate over the tokens in the word\n            for possible_unk_token in tokens_with_unk:\n                # If the token is not an UNK token then we remove the first occurence\n                # of the content of this token from the word. The result of the `word`\n                # variable will be the content of the UNK token.\n                # NOTE: This is a bit hacky and not bulletproof. For instance, if the\n                # word is \"1925-1950\" and the tokenizer splits it into [\"[UNK]\", \"-\",\n                # \"19\", \"50\"], then the result will be 2519 instead of 1925. This\n                # happens almost never, however, so we can live with it.\n                if possible_unk_token != tokenizer.unk_token:\n                    word = word.replace(possible_unk_token, \"\", 1)\n\n            # Replace the token with the word\n            tokens[tok_idx] = word\n\n        # Return the tokens\n        return tokens\n\n    def _preprocess_data(self, dataset: \"Dataset\", **kwargs) -&gt; \"Dataset\":\n        \"\"\"Preprocess a dataset by tokenizing and aligning the labels.\n\n        Args:\n            dataset:\n                The dataset to preprocess.\n            kwargs:\n                Extra keyword arguments containing objects used in preprocessing the\n                dataset.\n\n        Returns:\n            Hugging Face dataset:\n                The preprocessed dataset.\n        \"\"\"\n        if kwargs[\"model_config\"].task in GENERATIVE_MODEL_TASKS:\n            if \"few_shot_examples\" in kwargs:\n                few_shot_examples = kwargs[\"few_shot_examples\"]\n                few_shot_fn = partial(\n                    self._apply_few_shot_prompt, few_shot_examples=few_shot_examples\n                )\n                dataset = dataset.map(\n                    few_shot_fn,\n                    batched=True,\n                    load_from_cache_file=False,\n                    keep_in_memory=True,\n                )\n\n            def tokenise(examples: dict) -&gt; \"BatchEncoding\":\n                return kwargs[\"tokenizer\"](\n                    text=examples[\"text\"], truncation=True, padding=False\n                )\n\n            tokenised_dataset = dataset.map(\n                tokenise, batched=True, load_from_cache_file=False, keep_in_memory=True\n            )\n\n        else:\n            map_fn = partial(\n                self._tokenize_and_align_labels,\n                tokenizer=kwargs[\"tokenizer\"],\n                label2id=kwargs[\"hf_model_config\"].label2id,\n            )\n            tokenised_dataset = dataset.map(\n                map_fn, batched=True, load_from_cache_file=False, keep_in_memory=True\n            )\n\n        return tokenised_dataset\n\n    def _load_data_collator(\n        self,\n        tokenizer: \"Tokenizer | None\" = None,\n        model: \"PreTrainedModel | GenerativeModel | None\" = None,\n    ):\n        \"\"\"Load the data collator used to prepare samples during finetuning.\n\n        Args:\n            tokenizer:\n                A pretrained tokenizer. Can be None if the tokenizer is not used in the\n                initialisation of the data collator. Defaults to None.\n            model:\n                A pretrained model. Can be None if the model is not used in the\n                initialisation of the data collator. Defaults to None.\n\n        Returns:\n            Hugging Face data collator:\n                The data collator.\n        \"\"\"\n        if model_is_generative(model=model):\n            return DataCollatorWithPadding(tokenizer=tokenizer)\n        else:\n            return DataCollatorForTokenClassification(\n                tokenizer=tokenizer, label_pad_token_id=-100\n            )\n\n    def _extract_few_shot_examples(\n        self, train_dataset: \"Dataset\", random_seed: int\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"Extract few-shot examples from the training dataset.\n\n        Args:\n            train_dataset:\n                The training dataset.\n            random_seed:\n                The random seed to use when extracting the few-shot examples.\n\n        Returns:\n            list[dict[str, Any]]:\n                The few-shot examples.\n        \"\"\"\n        shuffled_train = train_dataset.shuffle(seed=random_seed)\n        num_few_shots = self.dataset_config.num_few_shot_examples\n        labels = it.cycle(\n            [\n                label.lower()\n                for label in self.dataset_config.task.labels\n                if label.lower().startswith(\"b-\")\n            ]\n        )\n        few_shot_examples: list[dict[str, Any]] = list()\n\n        # We pick the few-shot examples one at a time rather than all at once since\n        # we're working with a bootstrapped training dataset, meaning that it will have\n        # duplicates. This ensures that we don't have any duplicates in the few-shot\n        # examples\n        while len(few_shot_examples) &lt; num_few_shots:\n            label = next(labels)\n            possible_examples = shuffled_train.filter(\n                lambda x: label in [tag.lower() for tag in x[\"labels\"]]\n            )\n            if len(possible_examples) == 0:\n                continue\n            example = possible_examples.select(range(1))[0]\n            few_shot_examples.append(example)\n            shuffled_train = shuffled_train.filter(\n                lambda x: x[\"text\"] != example[\"text\"]\n            )\n\n        random.seed(random_seed)\n        random.shuffle(few_shot_examples)\n        return few_shot_examples\n\n    def _apply_few_shot_prompt(\n        self, examples: dict, few_shot_examples: list[dict], tokenizer: \"Tokenizer\"\n    ) -&gt; dict:\n        \"\"\"Apply a few-shot prompt to the examples.\n\n        Args:\n            examples:\n                The examples to apply the prompt to.\n            few_shot_examples:\n                The examples to be included in the few-shot prompt.\n            tokenizer:\n                The tokenizer to use to encode the few-shot prompt.\n\n        Returns:\n            The examples with the few-shot prompt applied.\n        \"\"\"\n        prompt_labels = self.dataset_config.prompt_label_mapping.values()\n\n        def create_label(example: dict) -&gt; str:\n            labels: dict[str, list[str]] = {\n                prompt_label: list() for prompt_label in prompt_labels\n            }\n            for token, label in zip(example[\"tokens\"], example[\"labels\"]):\n                label = label.lower()\n                if label == \"o\":\n                    continue\n                prompt_label = self.dataset_config.prompt_label_mapping[label]\n                if label.startswith(\"b-\"):\n                    labels[prompt_label].append(token)\n                elif label.startswith(\"i-\"):\n                    labels[prompt_label][-1] += \" \" + token\n            return json.dumps(labels, ensure_ascii=False)\n\n        # Build the few-shot part of the prompt\n        few_shot_prompts = [\n            self.dataset_config.prompt_template.format(\n                text=\" \".join(example[\"tokens\"]).replace(\"\\n\", \" \").strip(),\n                label=create_label(example),\n            )\n            for example in few_shot_examples\n        ]\n        prompt_prefix = \"\"\n        if self.dataset_config.prompt_prefix:\n            prompt_prefix = self.dataset_config.prompt_prefix + \"\\n\\n\"\n        few_shot_prompt = prompt_prefix + \"\\n\\n\".join(few_shot_prompts)\n\n        # Add the texts from the examples to the prompts\n        new_prompts = [\n            self.dataset_config.prompt_template.format(\n                text=\" \".join(tokens).replace(\"\\n\", \" \").strip(), label=\"\"\n            )\n            for tokens in examples[\"tokens\"]\n        ]\n        examples[\"text\"] = [\n            few_shot_prompt + \"\\n\\n\" + new_prompt for new_prompt in new_prompts\n        ]\n\n        return examples\n\n    def _apply_instruction_prompt(self, examples: dict, tokenizer: \"Tokenizer\") -&gt; dict:\n        \"\"\"Apply an instruction prompt to the examples.\n\n        Args:\n            examples:\n                The examples to apply the prompt to.\n            tokenizer:\n                The tokenizer to use to encode the instruction prompt.\n\n        Returns:\n            The examples with the instruction prompt applied.\n        \"\"\"\n        prompts = [\n            self.dataset_config.instruction_prompt.format(\n                text=re.sub(r\"\\n+\", \"\\n\", text).strip()\n            )\n            for text in examples[\"text\"]\n        ]\n        prompts = [\n            convert_prompt_to_instruction(prompt=prompt, tokenizer=tokenizer)\n            for prompt in prompts\n        ]\n        examples[\"text\"] = prompts\n        return examples\n\n    def _extract_labels_from_generation(\n        self,\n        input_batch: dict[str, list],\n        model_output: \"ModelOutput\",\n        tokenizer: \"Tokenizer\",\n    ) -&gt; list[Any]:\n        \"\"\"Extract the predicted labels from the generated output.\n\n        Args:\n            input_batch:\n                The input batch, where the keys are the feature names and the values\n                are lists with the feature values.\n            model_output:\n                The raw generated output of the model.\n            tokenizer:\n                The tokenizer used together with the model.\n\n        Returns:\n            list:\n                The predicted labels.\n        \"\"\"\n        if importlib.util.find_spec(\"demjson3\") is None:\n            raise NeedsExtraInstalled(extra=\"generative\")\n\n        raw_predictions = extract_raw_predictions(\n            generated_sequences=model_output[\"sequences\"], tokenizer=tokenizer\n        )\n\n        # Attempt to extract the JSON dictionary from the predictions\n        json_regex = r\"\\{.+?\\}\"\n        json_matches = [\n            re.search(pattern=json_regex, string=raw_prediction, flags=re.DOTALL)\n            or raw_prediction\n            for raw_prediction in raw_predictions\n        ]\n        raw_predictions = [\n            json_match.group() if isinstance(json_match, re.Match) else json_match\n            for json_match in json_matches\n        ]\n\n        tokens = input_batch[\"tokens\"]\n        predicted_labels: list[list[str]] = [\n            [\"o\"] * len(token_ids) for token_ids in tokens\n        ]\n        for idx, raw_prediction in enumerate(raw_predictions):\n            try:\n                json_output = demjson3.decode(txt=raw_prediction)\n                if not isinstance(json_output, dict):\n                    logger.debug(\n                        \"The model output is not a JSON dictionary, so cannot parse \"\n                        f\"it. Skipping. Here is the output: {raw_prediction}\"\n                    )\n                    continue\n                elif not all(isinstance(key, str) for key in json_output.keys()):\n                    logger.debug(\n                        \"The model output is not a JSON dictionary with string keys, \"\n                        \"so cannot parse it. Skipping. Here is the output: \"\n                        f\"{raw_prediction}\"\n                    )\n                    continue\n                elif not all(isinstance(value, list) for value in json_output.values()):\n                    logger.debug(\n                        \"The model output is not a JSON dictionary with list values, \"\n                        \"so cannot parse it. Skipping. Here is the output: \"\n                        f\"{raw_prediction}\"\n                    )\n                    continue\n                prediction_dict: dict[str, list[str]] = json_output\n            except demjson3.JSONDecodeError:\n                logger.debug(\n                    \"The model output is not valid JSON, so cannot parse it. Skipping. \"\n                    f\"Here is the output: {raw_prediction!r}\"\n                )\n                continue\n\n            prompt_label_mapping = self.dataset_config.prompt_label_mapping\n            for prompt_tag_name, named_entities in prediction_dict.items():\n                try:\n                    tag_name = [\n                        tag[2:]\n                        for tag, prompt_tag in prompt_label_mapping.items()\n                        if prompt_tag == prompt_tag_name\n                    ][0]\n                except IndexError:\n                    logger.debug(\n                        \"The model produced an invalid prompt tag name, \"\n                        f\"{prompt_tag_name}. Skipping.\"\n                    )\n                    continue\n\n                named_entities = [str(named_entity) for named_entity in named_entities]\n                for named_entity in named_entities:\n                    for ne_idx, named_entity_word in enumerate(named_entity.split()):\n                        for token_idx, token in enumerate(tokens[idx]):\n                            if named_entity_word in token:\n                                if ne_idx == 0:\n                                    predicted_labels[idx][token_idx] = f\"b-{tag_name}\"\n                                elif (\n                                    predicted_labels[idx][token_idx] == \"o\"\n                                    and predicted_labels[idx][token_idx - 1][2:]\n                                    == tag_name\n                                ):\n                                    predicted_labels[idx][token_idx] = f\"i-{tag_name}\"\n        return predicted_labels\n</code></pre>"},{"location":"api/scandeval/openai_models/","title":"scandeval.openai_models","text":"scandeval.openai_models<p> source module scandeval.openai_models </p> <p>Model and tokenizer wrapper for OpenAI models.</p> <p> Classes </p> <ul> <li> <p>OpenAITokenizer \u2014 An OpenAI tokenizer.</p> </li> <li> <p>OpenAIModel \u2014 An OpenAI model.</p> </li> </ul> <p> source class OpenAITokenizer(hf_model_config: PretrainedConfig) </p> <p>An OpenAI tokenizer.</p> <p>Initialize the tokenizer.</p> <p> Attributes </p> <ul> <li> <p>model_config \u2014</p> <p>The model configuration.</p> </li> <li> <p>hf_model_config \u2014</p> <p>The Hugging Face model configuration.</p> </li> <li> <p>encoding :  tiktoken.Encoding \u2014</p> <p>The encoding.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>hf_model_config :  PretrainedConfig \u2014</p> <p>The Hugging Face model configuration.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>decode \u2014 Decode token IDs.</p> </li> <li> <p>batch_decode \u2014 Decode batched token IDs.</p> </li> <li> <p>encode \u2014 Encode text.</p> </li> <li> <p>convert_ids_to_tokens \u2014 Convert token IDs to tokens.</p> </li> <li> <p>convert_tokens_to_ids \u2014 Convert tokens to token IDs.</p> </li> <li> <p>pad \u2014 Pad encoded inputs.</p> </li> <li> <p>apply_chat_template \u2014 Apply a chat template to a conversation.</p> </li> </ul> <p> source property OpenAITokenizer.encoding: tiktoken.Encoding </p> <p>Return the underlying tiktoken encoding.</p> <p> source method OpenAITokenizer.decode(token_ids: list[int], **kwargs) \u2192 str </p> <p>Decode token IDs.</p> <p> Parameters </p> <ul> <li> <p>token_ids :  list[int] \u2014</p> <p>The token IDs to decode.</p> </li> <li> <p>**kwargs \u2014</p> <p>Additional keyword arguments.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The decoded text.</p> </li> </ul> <p> source method OpenAITokenizer.batch_decode(sequences: list[list[int]], **kwargs) \u2192 list[str] </p> <p>Decode batched token IDs.</p> <p> Parameters </p> <ul> <li> <p>sequences :  list[list[int]] \u2014</p> <p>The token IDs to decode.</p> </li> <li> <p>**kwargs \u2014</p> <p>Additional keyword arguments.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 The decoded text.</p> </li> </ul> <p> source method OpenAITokenizer.encode(text: str, **kwargs) \u2192 list[int] </p> <p>Encode text.</p> <p> Parameters </p> <ul> <li> <p>text :  str \u2014</p> <p>The text to encode.</p> </li> <li> <p>**kwargs \u2014</p> <p>Additional keyword arguments.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[int] \u2014 The encoded text.</p> </li> </ul> <p> source method OpenAITokenizer.convert_ids_to_tokens(ids: int | list[int], skip_special_tokens: bool = False) \u2192 str | list[str] </p> <p>Convert token IDs to tokens.</p> <p> Parameters </p> <ul> <li> <p>ids :  int | list[int] \u2014</p> <p>The token IDs to convert.</p> </li> <li> <p>skip_special_tokens :  bool \u2014</p> <p>Whether to skip special tokens. Defaults to False.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str | list[str] \u2014 The tokens.</p> </li> </ul> <p> source method OpenAITokenizer.convert_tokens_to_ids(tokens: str | list[str]) \u2192 int | list[int] </p> <p>Convert tokens to token IDs.</p> <p> Parameters </p> <ul> <li> <p>tokens :  str | list[str] \u2014</p> <p>The tokens to convert.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>int | list[int] \u2014 The token IDs.</p> </li> </ul> <p> source method OpenAITokenizer.pad(encoded_inputs: BatchEncoding | list[BatchEncoding] | dict[str, list[int]] | dict[str, list[list[int]]] | list[dict[str, list[int]]], **kwargs) \u2192 BatchEncoding </p> <p>Pad encoded inputs.</p> <p> Parameters </p> <ul> <li> <p>encoded_inputs :  BatchEncoding | list[BatchEncoding] | dict[str, list[int]] | dict[str, list[list[int]]] | list[dict[str, list[int]]] \u2014</p> <p>Tokenized inputs. Can represent one input (BatchEncoding or Dict[str, List[int]]) or a batch of tokenized inputs (list of BatchEncoding, Dict[str, List[List[int]]] or List[Dict[str, List[int]]]) so you can use this method during preprocessing as well as in a PyTorch Dataloader collate function.</p> </li> <li> <p>**kwargs \u2014</p> <p>Additional keyword arguments.</p> </li> </ul> <p> source method OpenAITokenizer.apply_chat_template(conversation: list[dict[Literal[role, content], str]], **kwargs) \u2192 str | list[int] </p> <p>Apply a chat template to a conversation.</p> <p> Parameters </p> <ul> <li> <p>conversation :  list[dict[Literal[role, content], str]] \u2014</p> <p>The conversation to apply the chat template to.</p> </li> <li> <p>**kwargs \u2014</p> <p>Additional keyword arguments.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str | list[int] \u2014 The chat template.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source class OpenAIModel(hf_model_config: PretrainedConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig, tokenizer: OpenAITokenizer) </p> <p>An OpenAI model.</p> <p>Initialize the model.</p> <p> Attributes </p> <ul> <li> <p>model_config \u2014</p> <p>The model configuration.</p> </li> <li> <p>config \u2014</p> <p>The Hugging Face model configuration.</p> </li> <li> <p>dataset_config \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config \u2014</p> <p>The benchmark configuration.</p> </li> <li> <p>tokenizer \u2014</p> <p>The tokenizer.</p> </li> <li> <p>device \u2014</p> <p>The device to use, is always CPU.</p> </li> <li> <p>is_chat_model \u2014</p> <p>Whether the model is a chat model.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>hf_model_config :  PretrainedConfig \u2014</p> <p>The Hugging Face model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> <li> <p>tokenizer :  OpenAITokenizer \u2014</p> <p>The tokenizer.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>generate \u2014 Generate text using the model.</p> </li> <li> <p>to \u2014 Move the model to a device.</p> </li> <li> <p>eval \u2014 Put the model in evaluation mode.</p> </li> </ul> <p> source method OpenAIModel.generate(inputs: Tensor, generation_config: GenerationConfig | None = None, **generation_kwargs) \u2192 Tensor | LongTensor | ModelOutput </p> <p>Generate text using the model.</p> <p> Parameters </p> <ul> <li> <p>inputs :  Tensor \u2014</p> <p>The input IDs, of shape (batch_size, sequence_length).</p> </li> <li> <p>generation_config :  GenerationConfig | None \u2014</p> <p>The generation configuration. If None then a default GenerationConfig will be used. Defaults to None.</p> </li> <li> <p>**generation_kwargs \u2014</p> <p>Additional keyword arguments. Can also be used to override generation configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Tensor | LongTensor | ModelOutput \u2014 The model output.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError</p> </li> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source method OpenAIModel.to(device: torch.device) \u2192 OpenAIModel </p> <p>Move the model to a device.</p> <p> Parameters </p> <ul> <li> <p>device :  torch.device \u2014</p> <p>The device to move the model to.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>OpenAIModel \u2014 The model.</p> </li> </ul> <p> source method OpenAIModel.eval() \u2192 OpenAIModel </p> <p>Put the model in evaluation mode.</p> <p> Returns </p> <ul> <li> <p>OpenAIModel \u2014 The model.</p> </li> </ul>"},{"location":"src/scandeval/openai_models/","title":"scandeval.openai_models","text":"scandeval.openai_models<p> docs module scandeval.openai_models </p> <pre><code>\"\"\"Model and tokenizer wrapper for OpenAI models.\"\"\"\n\nimport importlib.util\nimport logging\nimport math\nfrom typing import TYPE_CHECKING, Literal\n\nimport torch\nfrom torch import LongTensor\nfrom torch.nn.utils.rnn import pad_sequence\nfrom transformers import BatchEncoding, GenerationConfig\nfrom transformers.modeling_utils import ModelOutput\n\nfrom .config import BenchmarkConfig, DatasetConfig, ModelConfig\nfrom .exceptions import InvalidBenchmark, InvalidModel, NeedsExtraInstalled\nfrom .tasks import NER\nfrom .types import is_list_of_int, is_list_of_list_of_int\n\nif TYPE_CHECKING:\n    import tiktoken\n    from torch import Tensor\n    from transformers import PretrainedConfig\n\nif importlib.util.find_spec(\"tiktoken\") is not None:\n    import tiktoken\n\nif importlib.util.find_spec(\"openai\") is not None:\n    from openai import AzureOpenAI, BadRequestError, NotFoundError, OpenAI\n    from openai.types.chat import ChatCompletionUserMessageParam\n    from openai.types.chat.chat_completion_token_logprob import TopLogprob\n    from openai.types.shared.response_format_json_object import ResponseFormatJSONObject\n\nlogger = logging.getLogger(__package__)\n\n\nclass OpenAITokenizer:docs\n    \"\"\"An OpenAI tokenizer.\n\n    Attributes:\n        model_config:\n            The model configuration.\n        hf_model_config:\n            The Hugging Face model configuration.\n        encoding:\n            The encoding.\n    \"\"\"\n\n    unk_token: str | None = \"&lt;unk&gt;\"\n    unk_token_id = -1\n    pad_token: str | None = \"&lt;pad&gt;\"\n    padding_side: Literal[\"right\", \"left\"] = \"left\"\n    is_fast = False\n    chat_template: str | None = None\n\n    def __init__(\n        self, model_config: ModelConfig, hf_model_config: \"PretrainedConfig\"\n    ) -&gt; None:\n        \"\"\"Initialize the tokenizer.\n\n        Args:\n            model_config:\n                The model configuration.\n            hf_model_config:\n                The Hugging Face model configuration.\n        \"\"\"\n        self.model_config = model_config\n        self.hf_model_config = hf_model_config\n        self.model_max_length = self.hf_model_config.model_max_length\n\n        self.bos_token_id: int = self.hf_model_config.bos_token_id or -1\n        self.cls_token_id: int = self.bos_token_id\n        self.eos_token_id: int = self.hf_model_config.eos_token_id or -1\n        self.sep_token_id: int = self.eos_token_id\n        self.pad_token_id: int = self.hf_model_config.pad_token_id or -1\n\n        self.bos_token: str | None = self.encoding.decode([self.bos_token_id])\n        self.cls_token: str | None = self.bos_token\n        self.eos_token: str | None = self.encoding.decode([self.eos_token_id])\n        self.sep_token: str | None = self.eos_token\n\n        assert self.bos_token is not None\n        assert self.cls_token is not None\n        assert self.eos_token is not None\n        assert self.sep_token is not None\n        assert self.pad_token is not None\n\n        self.vocab_size = self.encoding.max_token_value + 1\n        self.special_tokens_map: dict[str, str] = dict(\n            bos_token=self.bos_token,\n            cls_token=self.cls_token,\n            eos_token=self.eos_token,\n            sep_token=self.sep_token,\n            pad_token=self.pad_token,\n        )\n\n    @property\n    def encoding(self) -&gt; \"tiktoken.Encoding\":docs\n        \"\"\"Return the underlying tiktoken encoding.\"\"\"\n        try:\n            return tiktoken.encoding_for_model(model_name=self.model_config.model_id)\n        except KeyError:\n            # For Azure, the model_id is the deployment name. I do not know how to\n            # dynamically get the currently deployed model so assuming Azure only\n            # supports the latest models.\n            # TODO: This needs to be fixed, as gpt-4o uses a different encoding\n            return tiktoken.get_encoding(\"cl100k_base\")\n\n    def __call__(self, text: str | list[str], **kwargs) -&gt; BatchEncoding:\n        \"\"\"Tokenize text.\n\n        Args:\n            text:\n                The text to tokenize.\n            **kwargs:\n                Additional keyword arguments.\n\n        Returns:\n            The tokenized text.\n        \"\"\"\n        truncation = kwargs.get(\"truncation\", False)\n        start_idx = -self.model_max_length if truncation else 0\n\n        text_list = [text] if isinstance(text, str) else text\n        encoded_inputs = [\n            BatchEncoding(\n                dict(\n                    input_ids=self.encoding.encode(\n                        text,\n                        allowed_special={\n                            self.bos_token,\n                            self.eos_token,\n                            self.cls_token,\n                            self.sep_token,\n                            self.pad_token,\n                        },\n                    )[start_idx:]\n                )\n            )\n            for text in text_list\n        ]\n        return self.pad(encoded_inputs=encoded_inputs)\n\n    def decode(self, token_ids: list[int], **kwargs) -&gt; str:docs\n        \"\"\"Decode token IDs.\n\n        Args:\n            token_ids:\n                The token IDs to decode.\n            **kwargs:\n                Additional keyword arguments.\n\n        Returns:\n            The decoded text.\n        \"\"\"\n        token_ids = [\n            token_id for token_id in token_ids if token_id != self.pad_token_id\n        ]\n        return self.encoding.decode(tokens=token_ids)\ndocs\n    def batch_decode(self, sequences: list[list[int]], **kwargs) -&gt; list[str]:\n        \"\"\"Decode batched token IDs.\n\n        Args:\n            sequences:\n                The token IDs to decode.\n            **kwargs:\n                Additional keyword arguments.\n\n        Returns:\n            The decoded text.\n        \"\"\"\n        return [self.decode(token_ids=sequence) for sequence in sequences]\n\n    def encode(self, text: str, **kwargs) -&gt; list[int]:docs\n        \"\"\"Encode text.\n\n        Args:\n            text:\n                The text to encode.\n            **kwargs:\n                Additional keyword arguments.\n\n        Returns:\n            The encoded text.\n        \"\"\"\n        return self(text, **kwargs).input_ids.tolist()[0]\n\n    def convert_ids_to_tokens(docs\n        self, ids: int | list[int], skip_special_tokens: bool = False\n    ) -&gt; str | list[str]:\n        \"\"\"Convert token IDs to tokens.\n\n        Args:\n            ids:\n                The token IDs to convert.\n            skip_special_tokens:\n                Whether to skip special tokens. Defaults to False.\n\n        Returns:\n            The tokens.\n        \"\"\"\n        ids_list = [ids] if isinstance(ids, int) else ids\n        tokens = [self.decode(token_ids=[i]) for i in ids_list]\n        if skip_special_tokens:\n            tokens = [\n                token\n                for token in tokens\n                if token not in self.special_tokens_map.values()\n            ]\n        if len(tokens) == 1:\n            return tokens[0]\n        return tokens\ndocs\n    def convert_tokens_to_ids(self, tokens: str | list[str]) -&gt; int | list[int]:\n        \"\"\"Convert tokens to token IDs.\n\n        Args:\n            tokens:\n                The tokens to convert.\n\n        Returns:\n            The token IDs.\n        \"\"\"\n        if isinstance(tokens, str):\n            tokens = [tokens]\n        ids = [self.encode(text=token)[0] for token in tokens]\n        if len(ids) == 1:\n            return ids[0]\n        return ids\n\n    def pad(docs\n        self,\n        encoded_inputs: (\n            BatchEncoding\n            | list[BatchEncoding]\n            | dict[str, list[int]]\n            | dict[str, list[list[int]]]\n            | list[dict[str, list[int]]]\n        ),\n        **kwargs,\n    ) -&gt; BatchEncoding:\n        \"\"\"Pad encoded inputs.\n\n        Args:\n            encoded_inputs:\n                Tokenized inputs. Can represent one input (BatchEncoding or Dict[str,\n                List[int]]) or a batch of tokenized inputs (list of BatchEncoding,\n                Dict[str, List[List[int]]] or List[Dict[str, List[int]]]) so you can\n                use this method during preprocessing as well as in a PyTorch Dataloader\n                collate function.\n            **kwargs:\n                Additional keyword arguments.\n        \"\"\"\n        # Single example\n        if isinstance(encoded_inputs, BatchEncoding):\n            return encoded_inputs\n        elif isinstance(encoded_inputs, dict) and is_list_of_int(\n            encoded_inputs[\"input_ids\"]\n        ):\n            return BatchEncoding(data=encoded_inputs)\n\n        # Batch of examples\n        if isinstance(encoded_inputs, dict) and is_list_of_list_of_int(\n            encoded_inputs[\"input_ids\"]\n        ):\n            input_ids = encoded_inputs[\"input_ids\"]\n        else:\n            assert isinstance(encoded_inputs, list)\n            input_ids = [list(example[\"input_ids\"]) for example in encoded_inputs]\n\n        # Flip the token IDs in the lists, since `pad_sequence` pads to the right by\n        # default, and we want padding to the left\n        flipped_input_ids: list[\"Tensor\"] = []\n        for input_id_list in input_ids:\n            input_id_list.reverse()\n            flipped_input_ids.append(LongTensor(input_id_list))\n\n        padded_input_ids = (\n            pad_sequence(\n                sequences=flipped_input_ids,\n                batch_first=True,\n                padding_value=self.pad_token_id,\n            )\n            .flip(dims=[1])\n            .long()\n        )\n\n        return BatchEncoding(dict(input_ids=padded_input_ids))\n\n    def apply_chat_template(docs\n        self, conversation: list[dict[Literal[\"role\", \"content\"], str]], **kwargs\n    ) -&gt; str | list[int]:\n        \"\"\"Apply a chat template to a conversation.\n\n        Args:\n            conversation:\n                The conversation to apply the chat template to.\n            **kwargs:\n                Additional keyword arguments.\n\n        Returns:\n            The chat template.\n        \"\"\"\n        raise NotImplementedError(\"Chat templates are not supported for OpenAI models.\")\n\n\nclass OpenAIModel:docs\n    \"\"\"An OpenAI model.\n\n    Attributes:\n        model_config:\n            The model configuration.\n        config:\n            The Hugging Face model configuration.\n        dataset_config:\n            The dataset configuration.\n        benchmark_config:\n            The benchmark configuration.\n        tokenizer:\n            The tokenizer.\n        device:\n            The device to use, is always CPU.\n        is_chat_model:\n            Whether the model is a chat model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        hf_model_config: \"PretrainedConfig\",\n        dataset_config: DatasetConfig,\n        benchmark_config: BenchmarkConfig,\n        tokenizer: OpenAITokenizer,\n    ) -&gt; None:\n        \"\"\"Initialize the model.\n\n        Args:\n            model_config:\n                The model configuration.\n            hf_model_config:\n                The Hugging Face model configuration.\n            dataset_config:\n                The dataset configuration.\n            benchmark_config:\n                The benchmark configuration.\n            tokenizer:\n                The tokenizer.\n        \"\"\"\n        if importlib.util.find_spec(\"openai\") is None:\n            raise NeedsExtraInstalled(extra=\"openai\")\n\n        self.model_config = model_config\n        self.config = hf_model_config\n        self.dataset_config = dataset_config\n        self.benchmark_config = benchmark_config\n        self.tokenizer = tokenizer\n        self.device = torch.device(\"cpu\")\n        self.client = self._initialize_openai_client()\n        self.is_chat_model = self._is_chat_model()\n        self.supports_json_mode = self._supports_json_mode()\n\n    def _initialize_openai_client(self) -&gt; \"OpenAI | AzureOpenAI\":\n        \"\"\"Initialize and return the OpenAI client.\n\n        Returns:\n            The OpenAI client.\n\n        Raises:\n            InvalidBenchmark:\n                If the OpenAI API key is not specified.\n        \"\"\"\n        if self.benchmark_config.openai_api_key is not None:\n            return OpenAI(api_key=self.benchmark_config.openai_api_key, max_retries=60)\n        elif self.benchmark_config.azure_openai_api_key is not None:\n            if (\n                self.benchmark_config.azure_openai_endpoint is None\n                or self.benchmark_config.azure_openai_api_version is None\n            ):\n                if self.benchmark_config.run_with_cli:\n                    argument_message = (\n                        \"`--azure-openai-endpoint` and `--azure_openai_api_version` \"\n                        \"flags\"\n                    )\n                else:\n                    argument_message = (\n                        \"`azure_openai_endpoint` and `azure_openai_api_version` \"\n                        \"arguments in the `Benchmarker` class\"\n                    )\n                raise InvalidModel(\n                    \"Azure OpenAI models require an endpoint and API version to be \"\n                    \"specified. Please specify the endpoint with the \"\n                    f\"{argument_message}, or specify the environment variables \"\n                    \"`AZURE_OPENAI_ENDPOINT` and `AZURE_OPENAI_API_VERSION`.\"\n                )\n            return AzureOpenAI(\n                api_key=self.benchmark_config.azure_openai_api_key,\n                azure_endpoint=self.benchmark_config.azure_openai_endpoint,\n                api_version=self.benchmark_config.azure_openai_api_version,\n                max_retries=60,\n            )\n        elif self.benchmark_config.run_with_cli:\n            raise InvalidModel(\n                \"OpenAI models require an API key to be specified. Please specify the \"\n                \"`--openai-api-key` argument (or `--azure-openai-api-key` and \"\n                \"`--azure-openai-endpoint` arguments) or specify the environment \"\n                \"variables `OPENAI_API_KEY` (or `AZURE_OPENAI_API_KEY` and \"\n                \"`AZURE_OPENAI_ENDPOINT`).\"\n            )\n        else:\n            raise InvalidModel(\n                \"OpenAI models require an API key to be specified. Please specify the \"\n                \"`openai_api_key` argument to the `Benchmarker` class (or \"\n                \"`azure_openai_api_key` and `azure_openai_endpoint` arguments) or \"\n                \"specify the environment variables `OPENAI_API_KEY` (or \"\n                \"`AZURE_OPENAI_API_KEY` and `AZURE_OPENAI_ENDPOINT`).\"\n            )\n\n    def _is_chat_model(self) -&gt; bool:\n        \"\"\"Returns whether the model is a chat model.\"\"\"\n        try:\n            self.client.completions.create(\n                model=self.model_config.model_id, prompt=\"Test\", max_tokens=1\n            )\n            return False\n        except (NotFoundError, BadRequestError) as e:\n            chat_model_strings = [\n                \"This is a chat model\",\n                \"The completion operation does not work with the specified model\",\n            ]\n            if any(string in str(e) for string in chat_model_strings):\n                return True\n            raise e\n\n    def _supports_json_mode(self) -&gt; bool:\n        \"\"\"Returns whether the model supports JSON mode.\"\"\"\n        if not self.is_chat_model:\n            return False\n\n        try:\n            self.client.chat.completions.create(  # type: ignore[call-overload]\n                model=self.model_config.model_id,\n                messages=[\n                    ChatCompletionUserMessageParam(role=\"user\", content=\"Test json\")\n                ],\n                max_tokens=1,\n                response_format=ResponseFormatJSONObject(type=\"json_object\"),\n            )\n            return True\n        except BadRequestError as e:\n            no_json_mode_strings = [\"not supported with this model\"]\n            if any(string in str(e) for string in no_json_mode_strings):\n                return False\n            raise e\n\n    def generate(docs\n        self,\n        inputs: \"Tensor\",\n        generation_config: GenerationConfig | None = None,\n        **generation_kwargs,\n    ) -&gt; \"Tensor | LongTensor | ModelOutput\":\n        \"\"\"Generate text using the model.\n\n        Args:\n            inputs:\n                The input IDs, of shape (batch_size, sequence_length).\n            generation_config:\n                The generation configuration. If None then a default GenerationConfig\n                will be used. Defaults to None.\n            **generation_kwargs:\n                Additional keyword arguments. Can also be used to override\n                generation configuration.\n\n        Returns:\n            The model output.\n        \"\"\"\n        if generation_config is None:\n            generation_config = GenerationConfig(**generation_kwargs)\n        else:\n            for key, value in generation_kwargs.items():\n                setattr(generation_config, key, value)\n\n        multiple_inputs = inputs.size(dim=0) &gt; 1\n        if multiple_inputs:\n            raise ValueError(\n                \"OpenAI models do not support multiple inputs. Please use a batch \"\n                \"size of 1.\"\n            )\n\n        two_dimensional_input = len(inputs.size()) == 2\n        if two_dimensional_input:\n            inputs = inputs[0]\n\n        prompt = self.tokenizer.decode(\n            [\n                token_id\n                for token_id in inputs.tolist()\n                if token_id != self.config.pad_token_id\n            ]\n        )\n\n        model_id = self.model_config.model_id\n        max_tokens: int = generation_config.max_new_tokens or 1\n        temperature = (\n            0.0 if not generation_config.do_sample else generation_config.temperature\n        )\n        generation_kwargs = dict(\n            model=model_id,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            top_p=generation_config.top_p,\n            n=generation_config.num_return_sequences,\n            frequency_penalty=generation_config.repetition_penalty - 1.0,\n            stop=[\"\\n\\n\", self.tokenizer.eos_token, self.tokenizer.pad_token],\n            seed=4242,\n        )\n\n        if generation_config.output_scores:\n            generation_kwargs[\"logprobs\"] = True\n            generation_kwargs[\"top_logprobs\"] = 10\n\n        if (\n            self.dataset_config.task == NER\n            and self.supports_json_mode\n            and \"json\" in prompt.lower()\n        ):\n            generation_kwargs[\"response_format\"] = dict(type=\"json_object\")\n\n        try:\n            if not self.is_chat_model:\n                model_output = self.client.completions.create(\n                    prompt=prompt, **generation_kwargs\n                )\n                generation_output = model_output.choices[0].text.strip()\n            else:\n                model_output = self.client.chat.completions.create(\n                    messages=[dict(role=\"system\", content=prompt)], **generation_kwargs\n                )\n                generation_output = model_output.choices[0].message.content.strip()\n        except BadRequestError as e:\n            raise InvalidBenchmark(\n                f\"Encountered error during OpenAI generation: {str(e)!r}\\n\"\n                f\"The prompt causing it was {prompt!r}.\"\n            )\n\n        completion_ids = self.tokenizer([generation_output]).input_ids.tolist()\n        output = LongTensor(completion_ids)\n\n        if generation_config.return_dict_in_generate:\n            output_dict: dict[str, Tensor | tuple] = dict(sequences=output)\n\n            # Add logprobs scores to the output\n            if generation_config.output_scores:\n                # Create a list with placeholder logprobs for every token generated.\n                # Each tensor in the list will be of shape (batch_size, vocab_size)\n                batch_size = 1\n                vocab_size = self.config.vocab_size\n                logprobs_list = model_output.choices[0].logprobs.content\n                seq_len = len(logprobs_list)\n                scores = [\n                    torch.full(size=(batch_size, vocab_size), fill_value=-math.inf)\n                    for _ in range(seq_len)\n                ]\n\n                # Fill in the logprobs for each generated token. The logprobs from the\n                # OpenAI output only contain the logprobs for the top-k tokens, so we\n                # only fill in these and leave the rest at ~0% probability\n                for gen_token_idx, logprobs in enumerate(logprobs_list):\n                    # Remove duplicate logprobs, since, e.g., \"negative\" and \" negative\"\n                    # can both be included in the list, and we only want the one with\n                    # the highest logprob\n                    top_logprobs: list[TopLogprob] = list()\n                    for logprob_obj in logprobs.top_logprobs:\n                        token = logprob_obj.token.strip()\n                        if token in {lg.token.strip() for lg in top_logprobs}:\n                            continue\n                        top_logprobs.append(logprob_obj)\n\n                    for logprob_obj in top_logprobs:\n                        logprob = logprob_obj.logprob\n                        token = logprob_obj.token.strip()\n                        if token:\n                            token_idx = self.tokenizer.encode(text=token)[0]\n                            scores[gen_token_idx][0, token_idx] = logprob\n\n                output_dict[\"scores\"] = tuple(scores)\n\n            return ModelOutput(output_dict)\n\n        return output\n\n    def to(self, device: torch.device) -&gt; \"OpenAIModel\":docs\n        \"\"\"Move the model to a device.\n\n        Args:\n            device:\n                The device to move the model to.\n\n        Returns:\n            The model.\n        \"\"\"\n        return self\n\n    def eval(self) -&gt; \"OpenAIModel\":docs\n        \"\"\"Put the model in evaluation mode.\n\n        Returns:\n            The model.\n        \"\"\"\n        return self\n\n    def __call__(self, *args, **kwargs):\n        \"\"\"Call the model.\"\"\"\n        raise NotImplementedError(\"OpenAI models do not support the `__call__` method.\")\n</code></pre>"},{"location":"api/scandeval/protocols/","title":"scandeval.protocols","text":"scandeval.protocols<p> source module scandeval.protocols </p> <p>Protocols used throughout the project.</p> <p> Classes </p> <ul> <li> <p>Tokenizer \u2014 A protocol for a tokenizer.</p> </li> <li> <p>GenerativeModel \u2014 A protocol for a generative model.</p> </li> <li> <p>ModelSetup \u2014 A protocol for a general model setup.</p> </li> </ul> <p> source class Tokenizer() </p> <p><p>Bases : Protocol</p></p> <p>A protocol for a tokenizer.</p> <p> Methods </p> <ul> <li> <p>decode \u2014 Decode a list of token IDs.</p> </li> <li> <p>batch_decode \u2014 Decode a batch of token IDs.</p> </li> <li> <p>encode \u2014 Encode one or more texts.</p> </li> <li> <p>convert_ids_to_tokens \u2014 Convert a list of token IDs to tokens.</p> </li> <li> <p>convert_tokens_to_ids \u2014 Convert a list of tokens to token IDs.</p> </li> <li> <p>pad \u2014 Pad a batch of encoded inputs.</p> </li> <li> <p>apply_chat_template \u2014 Apply a chat template to a conversation.</p> </li> </ul> <p> source method Tokenizer.decode(token_ids: list[int], **kwargs) \u2192 str </p> <p>Decode a list of token IDs.</p> <p> Parameters </p> <ul> <li> <p>token_ids :  list[int] \u2014</p> <p>The token IDs to decode.</p> </li> <li> <p>**kwargs \u2014</p> <p>Keyword arguments to pass to the tokenizer.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The decoded string.</p> </li> </ul> <p> source method Tokenizer.batch_decode(sequences: list[list[int]], **kwargs) \u2192 list[str] </p> <p>Decode a batch of token IDs.</p> <p> Parameters </p> <ul> <li> <p>sequences :  list[list[int]] \u2014</p> <p>The token IDs to decode.</p> </li> <li> <p>**kwargs \u2014</p> <p>Keyword arguments to pass to the tokenizer.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 The decoded strings.</p> </li> </ul> <p> source method Tokenizer.encode(text: str, **kwargs) \u2192 list[int] </p> <p>Encode one or more texts.</p> <p> Parameters </p> <ul> <li> <p>text :  str \u2014</p> <p>The text to encode.</p> </li> <li> <p>**kwargs \u2014</p> <p>Keyword arguments to pass to the tokenizer.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[int] \u2014 The encoded token IDs.</p> </li> </ul> <p> source method Tokenizer.convert_ids_to_tokens(ids: int | list[int], skip_special_tokens: bool = False) \u2192 str | list[str] </p> <p>Convert a list of token IDs to tokens.</p> <p> Parameters </p> <ul> <li> <p>ids :  int | list[int] \u2014</p> <p>The token IDs to convert.</p> </li> <li> <p>skip_special_tokens :  bool \u2014</p> <p>Whether to skip special tokens.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str | list[str] \u2014 The tokens.</p> </li> </ul> <p> source method Tokenizer.convert_tokens_to_ids(tokens: str | list[str]) \u2192 int | list[int] </p> <p>Convert a list of tokens to token IDs.</p> <p> Parameters </p> <ul> <li> <p>tokens :  str | list[str] \u2014</p> <p>The tokens to convert.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>int | list[int] \u2014 The token IDs.</p> </li> </ul> <p> source method Tokenizer.pad(encoded_inputs: Union[BatchEncoding, list[BatchEncoding], dict[str, list[int]], dict[str, list[list[int]]], list[dict[str, list[int]]]], **kwargs) \u2192 BatchEncoding </p> <p>Pad a batch of encoded inputs.</p> <p> Parameters </p> <ul> <li> <p>encoded_inputs :  Union[BatchEncoding, list[BatchEncoding], dict[str, list[int]], dict[str, list[list[int]]], list[dict[str, list[int]]]] \u2014</p> <p>The encoded inputs to pad.</p> </li> <li> <p>**kwargs \u2014</p> <p>Keyword arguments to pass to the tokenizer.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BatchEncoding \u2014 The padded encoded inputs.</p> </li> </ul> <p> source method Tokenizer.apply_chat_template(conversation: list[dict[Literal[role, content], str]], **kwargs) \u2192 str | list[int] </p> <p>Apply a chat template to a conversation.</p> <p> Parameters </p> <ul> <li> <p>conversation :  list[dict[Literal[role, content], str]] \u2014</p> <p>The conversation.</p> </li> <li> <p>**kwargs \u2014</p> <p>Keyword arguments to pass to the tokenizer.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str | list[int] \u2014 The conversation as a string.</p> </li> </ul> <p> source class GenerativeModel() </p> <p><p>Bases : Protocol</p></p> <p>A protocol for a generative model.</p> <p> Methods </p> <ul> <li> <p>to \u2014 Move the model to a device.</p> </li> <li> <p>eval \u2014 Put the model in evaluation mode.</p> </li> <li> <p>generate \u2014 Generate text.</p> </li> </ul> <p> source method GenerativeModel.to(device: torch.device) \u2192 GenerativeModel </p> <p>Move the model to a device.</p> <p> Parameters </p> <ul> <li> <p>device :  torch.device \u2014</p> <p>The device to move the model to.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>GenerativeModel \u2014 The model.</p> </li> </ul> <p> source method GenerativeModel.eval() \u2192 GenerativeModel </p> <p>Put the model in evaluation mode.</p> <p> Returns </p> <ul> <li> <p>GenerativeModel \u2014 The model.</p> </li> </ul> <p> source method GenerativeModel.generate(inputs: torch.Tensor, generation_config: GenerationConfig | None = None, **generation_kwargs) \u2192 ModelOutput | torch.Tensor </p> <p>Generate text.</p> <p> Parameters </p> <ul> <li> <p>inputs :  torch.Tensor \u2014</p> <p>The input IDs.</p> </li> <li> <p>generation_config :  GenerationConfig | None \u2014</p> <p>The generation configuration.</p> </li> <li> <p>**generation_kwargs \u2014</p> <p>Keyword arguments to pass to the generation method.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelOutput | torch.Tensor \u2014 The generated text.</p> </li> </ul> <p> source class ModelSetup() </p> <p><p>Bases : Protocol</p></p> <p>A protocol for a general model setup.</p> <p>Initialize the model setup.</p> <p> Parameters </p> <ul> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>model_exists \u2014 Check whether a model exists.</p> </li> <li> <p>get_model_config \u2014 Get the model configuration.</p> </li> <li> <p>load_model \u2014 Load a model.</p> </li> </ul> <p> source method ModelSetup.model_exists(model_id: str) \u2192 bool | dict[str, str] </p> <p>Check whether a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | dict[str, str] \u2014 Whether the model exist, or a dictionary explaining why we cannot check whether the model exists.</p> </li> </ul> <p> source method ModelSetup.get_model_config(model_id: str) \u2192 ModelConfig </p> <p>Get the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> source method ModelSetup.load_model(model_config: ModelConfig, dataset_config: DatasetConfig) \u2192 tuple[PreTrainedModel | GenerativeModel, Tokenizer] </p> <p>Load a model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[PreTrainedModel | GenerativeModel, Tokenizer] \u2014 The tokenizer and model.</p> </li> </ul>"},{"location":"src/scandeval/protocols/","title":"scandeval.protocols","text":"scandeval.protocols<p> docs module scandeval.protocols </p> <pre><code>\"\"\"Protocols used throughout the project.\"\"\"\n\nfrom typing import TYPE_CHECKING, Literal, Protocol, Union, runtime_checkable\n\nif TYPE_CHECKING:\n    import torch\n    from transformers import (\n        BatchEncoding,\n        GenerationConfig,\n        PretrainedConfig,\n        PreTrainedModel,\n    )\n    from transformers.utils import ModelOutput\n\n    from .config import BenchmarkConfig, DatasetConfig, ModelConfig\n\n\n@runtime_checkable\nclass Tokenizer(Protocol):docs\n    \"\"\"A protocol for a tokenizer.\"\"\"\n\n    cls_token: str | None\n    sep_token: str | None\n    bos_token: str | None\n    eos_token: str | None\n    pad_token: str | None\n    unk_token: str | None\n    cls_token_id: int\n    sep_token_id: int\n    bos_token_id: int\n    eos_token_id: int\n    pad_token_id: int\n    unk_token_id: int\n    is_fast: bool\n    chat_template: str | None\n    special_tokens_map: dict[str, str]\n    model_max_length: int\n    vocab_size: int\n    padding_side: Literal[\"right\", \"left\"]\n\n    def __call__(self, text: str | list[str], **kwargs) -&gt; \"BatchEncoding\":\n        \"\"\"Call the tokenizer.\n\n        Args:\n            text:\n                The text to tokenize.\n            **kwargs:\n                Keyword arguments to pass to the tokenizer.\n\n        Returns:\n            The encoded inputs.\n        \"\"\"\n        ...\n\n    def decode(self, token_ids: list[int], **kwargs) -&gt; str:docs\n        \"\"\"Decode a list of token IDs.\n\n        Args:\n            token_ids:\n                The token IDs to decode.\n            **kwargs:\n                Keyword arguments to pass to the tokenizer.\n\n        Returns:\n            The decoded string.\n        \"\"\"\n        ...\ndocs\n    def batch_decode(self, sequences: list[list[int]], **kwargs) -&gt; list[str]:\n        \"\"\"Decode a batch of token IDs.\n\n        Args:\n            sequences:\n                The token IDs to decode.\n            **kwargs:\n                Keyword arguments to pass to the tokenizer.\n\n        Returns:\n            The decoded strings.\n        \"\"\"\n        ...\n\n    def encode(self, text: str, **kwargs) -&gt; list[int]:docs\n        \"\"\"Encode one or more texts.\n\n        Args:\n            text:\n                The text to encode.\n            **kwargs:\n                Keyword arguments to pass to the tokenizer.\n\n        Returns:\n            The encoded token IDs.\n        \"\"\"\n        ...\n\n    def convert_ids_to_tokens(docs\n        self, ids: int | list[int], skip_special_tokens: bool = False\n    ) -&gt; str | list[str]:\n        \"\"\"Convert a list of token IDs to tokens.\n\n        Args:\n            ids:\n                The token IDs to convert.\n            skip_special_tokens:\n                Whether to skip special tokens.\n\n        Returns:\n            The tokens.\n        \"\"\"\n        ...\ndocs\n    def convert_tokens_to_ids(self, tokens: str | list[str]) -&gt; int | list[int]:\n        \"\"\"Convert a list of tokens to token IDs.\n\n        Args:\n            tokens:\n                The tokens to convert.\n\n        Returns:\n            The token IDs.\n        \"\"\"\n        ...\n\n    def pad(docs\n        self,\n        encoded_inputs: Union[\n            \"BatchEncoding\",\n            list[\"BatchEncoding\"],\n            dict[str, list[int]],\n            dict[str, list[list[int]]],\n            list[dict[str, list[int]]],\n        ],\n        **kwargs,\n    ) -&gt; \"BatchEncoding\":\n        \"\"\"Pad a batch of encoded inputs.\n\n        Args:\n            encoded_inputs:\n                The encoded inputs to pad.\n            **kwargs:\n                Keyword arguments to pass to the tokenizer.\n\n        Returns:\n            The padded encoded inputs.\n        \"\"\"\n        ...\n\n    def apply_chat_template(docs\n        self, conversation: list[dict[Literal[\"role\", \"content\"], str]], **kwargs\n    ) -&gt; str | list[int]:\n        \"\"\"Apply a chat template to a conversation.\n\n        Args:\n            conversation:\n                The conversation.\n            **kwargs:\n                Keyword arguments to pass to the tokenizer.\n\n        Returns:\n            The conversation as a string.\n        \"\"\"\n        ...\n\n\n@runtime_checkable\nclass GenerativeModel(Protocol):docs\n    \"\"\"A protocol for a generative model.\"\"\"\n\n    config: \"PretrainedConfig\"\n    device: \"torch.device\"\n\n    def to(self, device: \"torch.device\") -&gt; \"GenerativeModel\":docs\n        \"\"\"Move the model to a device.\n\n        Args:\n            device:\n                The device to move the model to.\n\n        Returns:\n            The model.\n        \"\"\"\n        ...\n\n    def eval(self) -&gt; \"GenerativeModel\":docs\n        \"\"\"Put the model in evaluation mode.\n\n        Returns:\n            The model.\n        \"\"\"\n        ...\n\n    def generate(docs\n        self,\n        inputs: \"torch.Tensor\",\n        generation_config: \"GenerationConfig | None\" = None,\n        **generation_kwargs,\n    ) -&gt; \"ModelOutput | torch.Tensor\":\n        \"\"\"Generate text.\n\n        Args:\n            inputs:\n                The input IDs.\n            generation_config:\n                The generation configuration.\n            **generation_kwargs:\n                Keyword arguments to pass to the generation method.\n\n        Returns:\n            The generated text.\n        \"\"\"\n        ...\n\n    def __call__(\n        self, input_ids: \"torch.Tensor\", labels: \"torch.Tensor | None\" = None\n    ) -&gt; \"ModelOutput\":\n        \"\"\"Call the model.\n\n        Args:\n            input_ids:\n                The input IDs.\n            labels:\n                The labels.\n\n        Returns:\n            The model output.\n        \"\"\"\n        ...\n\n\nclass ModelSetup(Protocol):docs\n    \"\"\"A protocol for a general model setup.\"\"\"\n\n    def __init__(self, benchmark_config: \"BenchmarkConfig\") -&gt; None:\n        \"\"\"Initialize the model setup.\n\n        Args:\n            benchmark_config:\n                The benchmark configuration.\n        \"\"\"\n        ...\n\n    def model_exists(self, model_id: str) -&gt; bool | dict[str, str]:docs\n        \"\"\"Check whether a model exists.\n\n        Args:\n            model_id:\n                The model ID.\n\n        Returns:\n            Whether the model exist, or a dictionary explaining why we cannot check\n            whether the model exists.\n        \"\"\"\n        ...\n\n    def get_model_config(self, model_id: str) -&gt; \"ModelConfig\":docs\n        \"\"\"Get the model configuration.\n\n        Args:\n            model_id:\n                The model ID.\n\n        Returns:\n            The model configuration.\n        \"\"\"\n        ...\n\n    def load_model(docs\n        self, model_config: \"ModelConfig\", dataset_config: \"DatasetConfig\"\n    ) -&gt; tuple[\"PreTrainedModel | GenerativeModel\", \"Tokenizer\"]:\n        \"\"\"Load a model.\n\n        Args:\n            model_config:\n                The model configuration.\n            dataset_config:\n                The dataset configuration.\n\n        Returns:\n            The tokenizer and model.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/scandeval/question_answering/","title":"scandeval.question_answering","text":"scandeval.question_answering<p> source module scandeval.question_answering </p> <p>Question-answering benchmark dataset.</p> <p> Classes </p> <ul> <li> <p>QuestionAnswering \u2014 Question-answering benchmark dataset.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>prepare_train_examples \u2014 Prepare the features for training.</p> </li> <li> <p>prepare_test_examples \u2014 Prepare test examples.</p> </li> <li> <p>prepare_examples_for_generation \u2014 Prepare test examples.</p> </li> </ul> <p> source class QuestionAnswering() </p> <p><p>Bases : BenchmarkDataset</p></p> <p>Question-answering benchmark dataset.</p> <p> Parameters </p> <ul> <li> <p>dataset_config \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Attributes </p> <ul> <li> <p>dataset_config \u2014</p> <p>The configuration of the dataset.</p> </li> <li> <p>benchmark_config \u2014</p> <p>The configuration of the benchmark.</p> </li> </ul> <p> source prepare_train_examples(examples: BatchEncoding, tokenizer: Tokenizer) \u2192 BatchEncoding </p> <p>Prepare the features for training.</p> <p> Parameters </p> <ul> <li> <p>examples :  BatchEncoding \u2014</p> <p>The examples to prepare.</p> </li> <li> <p>tokenizer :  Tokenizer \u2014</p> <p>The tokenizer to use to prepare the examples.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BatchEncoding \u2014 The prepared examples.</p> </li> </ul> <p> source prepare_test_examples(examples: BatchEncoding, tokenizer: Tokenizer) \u2192 BatchEncoding </p> <p>Prepare test examples.</p> <p> Parameters </p> <ul> <li> <p>examples :  BatchEncoding \u2014</p> <p>Dictionary of test examples.</p> </li> <li> <p>tokenizer :  Tokenizer \u2014</p> <p>The tokenizer used to preprocess the examples.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BatchEncoding \u2014 The prepared test examples.</p> </li> </ul> <p> source prepare_examples_for_generation(examples: BatchEncoding, tokenizer: Tokenizer) \u2192 BatchEncoding </p> <p>Prepare test examples.</p> <p> Parameters </p> <ul> <li> <p>examples :  BatchEncoding \u2014</p> <p>Dictionary of test examples.</p> </li> <li> <p>tokenizer :  Tokenizer \u2014</p> <p>The tokenizer used to preprocess the examples.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BatchEncoding \u2014 The prepared test examples.</p> </li> </ul>"},{"location":"src/scandeval/question_answering/","title":"scandeval.question_answering","text":"scandeval.question_answering<p> docs module scandeval.question_answering </p> <pre><code>\"\"\"Question-answering benchmark dataset.\"\"\"\n\nimport logging\nimport re\nfrom functools import partial\nfrom typing import TYPE_CHECKING, Any, Type\n\nimport numpy as np\nfrom evaluate import EvaluationModule\nfrom transformers.data.data_collator import DataCollatorWithPadding\n\nfrom .benchmark_dataset import BenchmarkDataset\nfrom .exceptions import InvalidBenchmark\nfrom .generation import extract_raw_predictions\nfrom .question_answering_trainer import QuestionAnsweringTrainer\nfrom .utils import (\n    convert_prompt_to_instruction,\n    get_special_token_metadata,\n    raise_if_model_output_contains_nan_values,\n)\n\nif TYPE_CHECKING:\n    from datasets.arrow_dataset import Dataset\n    from transformers.modeling_utils import ModelOutput, PreTrainedModel\n    from transformers.tokenization_utils_base import BatchEncoding\n    from transformers.trainer import Trainer\n\n    from .protocols import GenerativeModel, Tokenizer\n    from .types import Labels, Predictions\n\nlogger = logging.getLogger(__package__)\n\n\nclass QuestionAnswering(BenchmarkDataset):docs\n    \"\"\"Question-answering benchmark dataset.\n\n    Args:\n        dataset_config:\n            The dataset configuration.\n        benchmark_config:\n            The benchmark configuration.\n\n    Attributes:\n        dataset_config:\n            The configuration of the dataset.\n        benchmark_config:\n            The configuration of the benchmark.\n    \"\"\"\n\n    def _preprocess_data(self, dataset: \"Dataset\", **kwargs) -&gt; \"Dataset\":\n        \"\"\"Preprocess a dataset by tokenizing and aligning the labels.\n\n        Args:\n            dataset:\n                The dataset to preprocess.\n            kwargs:\n                Extra keyword arguments containing objects used in preprocessing the\n                dataset.\n\n        Returns:\n            The preprocessed dataset.\n        \"\"\"\n        split: str = kwargs.pop(\"split\")\n        tokenizer: \"Tokenizer\" = kwargs.pop(\"tokenizer\")\n        generative_model: bool = kwargs.pop(\"generative_model\")\n\n        # If the tokenizer is not a fast variant then raise an error\n        if not tokenizer.is_fast and not generative_model:\n            raise InvalidBenchmark(\n                \"Question-answering benchmarks require a fast tokenizer.\"\n            )\n\n        if generative_model:\n            preprocess_fn = partial(\n                prepare_examples_for_generation, tokenizer=tokenizer\n            )\n        elif split == \"test\":\n            preprocess_fn = partial(prepare_test_examples, tokenizer=tokenizer)\n        else:\n            preprocess_fn = partial(prepare_train_examples, tokenizer=tokenizer)\n\n        # Preprocess the data and return it\n        try:\n            if generative_model:\n                cols_to_remove = [\n                    col for col in dataset.column_names if col not in [\"id\", \"text\"]\n                ]\n            else:\n                cols_to_remove = dataset.column_names\n            preprocessed = dataset.map(\n                preprocess_fn,\n                batched=True,\n                batch_size=10,\n                remove_columns=cols_to_remove,\n                load_from_cache_file=False,\n                keep_in_memory=True,\n            )\n\n        except NotImplementedError as e:\n            raise InvalidBenchmark(str(e))\n\n        # The Trainer hides the columns that are not used by the model (here `id` and\n        # `offset_mapping` which we will need for our post-processing), so we put them\n        # back\n        preprocessed.set_format(\n            type=preprocessed.format[\"type\"], columns=list(preprocessed.features.keys())\n        )\n\n        # Return the preprocessed dataset\n        return preprocessed\n\n    def _get_trainer_class(self) -&gt; Type[\"Trainer\"]:\n        return QuestionAnsweringTrainer\n\n    def _get_evaluate_inputs(\n        self, dataset: \"Dataset\", prepared_dataset: \"Dataset\", metric_key_prefix: str\n    ):\n        return dict(\n            orig_eval_dataset=dataset,\n            eval_dataset=prepared_dataset,\n            metric_key_prefix=metric_key_prefix,\n        )\n\n    def _load_data_collator(\n        self,\n        tokenizer: \"Tokenizer | None\" = None,\n        model: \"PreTrainedModel | GenerativeModel | None\" = None,\n    ):\n        \"\"\"Load the data collator used to prepare samples during finetuning.\n\n        Args:\n            tokenizer:\n                A pretrained tokenizer. Can be None if the tokenizer is not used in the\n                initialisation of the data collator. Defaults to None.\n            model:\n                A pretrained model. Can be None if the model is not used in the\n                initialisation of the data collator. Defaults to None.\n\n        Returns:\n            The data collator.\n        \"\"\"\n        return DataCollatorWithPadding(tokenizer=tokenizer)\n\n    def _compute_metrics(\n        self,\n        model_outputs_and_labels: tuple[\"Predictions\", \"Labels\"],\n        id2label: dict[int, str],\n    ) -&gt; dict[str, float]:\n        \"\"\"Compute the metrics needed for evaluation.\n\n        Args:\n            model_outputs_and_labels:\n                The first sequence contains the model outputs and the second sequence\n                contains the true labels.\n            id2label:\n                Conversion of indices to labels.\n\n        Returns:\n            A dictionary with the names of the metrics as keys and the metric values as\n            values.\n        \"\"\"\n        model_outputs, labels = model_outputs_and_labels\n\n        raise_if_model_output_contains_nan_values(model_output=model_outputs)\n\n        model_output_dtype = np.asarray(model_outputs).dtype\n        if model_output_dtype in [np.float16, np.float32, np.float64]:\n            predictions = np.asarray(model_outputs).argmax(axis=-1)\n        else:\n            predictions = model_outputs\n\n        results: dict[str, float] = dict()\n        for cfg in self.dataset_config.task.metrics:\n            metric = self._metrics[cfg.name]\n            assert isinstance(metric, EvaluationModule)\n            score_dict: dict[str, float] | None = metric.compute(\n                predictions=predictions, references=labels, **cfg.compute_kwargs\n            )\n\n            # The metric returns None if we are running on multi-GPU and the current\n            # process is not the main process\n            if score_dict is not None:\n                scores = score_dict[cfg.results_key]\n                if isinstance(scores, list):\n                    scores = sum(scores) / len(scores)\n                results[cfg.name] = scores\n\n        return results\n\n    def _extract_few_shot_examples(\n        self, train_dataset: \"Dataset\", random_seed: int\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"Extract few-shot examples from the training dataset.\n\n        Args:\n            train_dataset:\n                The training dataset.\n            random_seed:\n                The random seed to use when extracting the few-shot examples.\n\n        Returns:\n            The few-shot examples.\n        \"\"\"\n        # Locate the maximum number of tokens that constitutes a short example. We\n        # start with 512 tokens and double it until there is at least `num_few_shots`\n        # many examples that are shorter than the maximum number of tokens.\n        max_num_tokens = 512\n        while True:\n            train_with_short_examples = train_dataset.filter(\n                lambda example: len(example[\"context\"]) &lt; max_num_tokens\n            )\n            num_short_examples = len(train_with_short_examples)\n            if num_short_examples &gt;= self.dataset_config.num_few_shot_examples:\n                break\n            max_num_tokens *= 2\n\n        train_with_short_examples = train_dataset.filter(\n            lambda example: len(example[\"context\"]) &lt; max_num_tokens\n        )\n        shuffled_train = train_with_short_examples.shuffle(seed=random_seed)\n        num_few_shots = self.dataset_config.num_few_shot_examples\n        few_shot_examples: list[dict[str, Any]] = list()\n\n        # We pick the few-shot examples one at a time rather than all at once since\n        # we're working with a bootstrapped training dataset, meaning that it will have\n        # duplicates. This ensures that we don't have any duplicates in the few-shot\n        # examples\n        while len(few_shot_examples) &lt; num_few_shots:\n            example = shuffled_train.select(range(1))[0]\n            few_shot_examples.append(example)\n            shuffled_train = shuffled_train.filter(\n                lambda x: x[\"context\"] != example[\"context\"]\n            )\n\n        return few_shot_examples\n\n    def _apply_few_shot_prompt(\n        self, examples: dict, few_shot_examples: list[dict], tokenizer: \"Tokenizer\"\n    ) -&gt; dict:\n        \"\"\"Apply a few-shot prompt to the examples.\n\n        Args:\n            examples:\n                The examples to apply the prompt to.\n            few_shot_examples:\n                The examples to be included in the few-shot prompt.\n            tokenizer:\n                The tokenizer to use to encode the few-shot prompt.\n\n        Returns:\n            The examples with the few-shot prompt applied.\n        \"\"\"\n        # Build the few-shot part of the prompt\n        few_shot_prompts = [\n            self.dataset_config.prompt_template.format(\n                text=example[\"context\"].replace(\"\\n\", \" \").strip(),\n                question=example[\"question\"].strip(),\n                label=example[\"answers\"][\"text\"][0],\n            )\n            for example in few_shot_examples\n        ]\n        prompt_prefix = \"\"\n        if self.dataset_config.prompt_prefix:\n            prompt_prefix = self.dataset_config.prompt_prefix + \"\\n\\n\"\n        few_shot_prompt = prompt_prefix + \"\\n\\n\".join(few_shot_prompts)\n\n        # Add the texts from the examples to the prompts\n        new_prompts = [\n            self.dataset_config.prompt_template.format(\n                text=context.replace(\"\\n\", \" \").strip(), question=question, label=\"\"\n            )\n            for context, question in zip(examples[\"context\"], examples[\"question\"])\n        ]\n        examples[\"text\"] = [\n            few_shot_prompt + \"\\n\\n\" + new_prompt for new_prompt in new_prompts\n        ]\n\n        return examples\n\n    def _apply_instruction_prompt(self, examples: dict, tokenizer: \"Tokenizer\") -&gt; dict:\n        \"\"\"Apply an instruction prompt to the examples.\n\n        Args:\n            examples:\n                The examples to apply the prompt to.\n            tokenizer:\n                The tokenizer to use to encode the instruction prompt.\n\n        Returns:\n            The examples with the instruction prompt applied.\n        \"\"\"\n        prompts = [\n            self.dataset_config.instruction_prompt.format(\n                text=re.sub(r\"\\n+\", \"\\n\", text).strip(), question=question.strip()\n            )\n            for (text, question) in zip(examples[\"context\"], examples[\"question\"])\n        ]\n        prompts = [\n            convert_prompt_to_instruction(prompt=prompt, tokenizer=tokenizer)\n            for prompt in prompts\n        ]\n        examples[\"text\"] = prompts\n        return examples\n\n    def _extract_labels_from_generation(\n        self,\n        input_batch: dict[str, list],\n        model_output: \"ModelOutput\",\n        tokenizer: \"Tokenizer\",\n    ) -&gt; list[Any]:\n        \"\"\"Extract the predicted labels from the generated output.\n\n        Args:\n            input_batch:\n                The input batch, where the keys are the feature names and the values\n                are lists with the feature values.\n            model_output:\n                The raw generated output of the model.\n            tokenizer:\n                The tokenizer used together with the model.\n\n        Returns:\n            The predicted labels.\n        \"\"\"\n        raw_predictions = extract_raw_predictions(\n            generated_sequences=model_output[\"sequences\"], tokenizer=tokenizer\n        )\n\n        predictions = [\n            dict(\n                id=id,\n                prediction_text=predicted_answer.lower(),\n                no_answer_probability=0.0,\n            )\n            for id, predicted_answer in zip(input_batch[\"id\"], raw_predictions)\n        ]\n\n        return predictions\n\n\ndef prepare_train_examples(docs\n    examples: \"BatchEncoding\", tokenizer: \"Tokenizer\"\n) -&gt; \"BatchEncoding\":\n    \"\"\"Prepare the features for training.\n\n    Args:\n        examples:\n            The examples to prepare.\n        tokenizer:\n            The tokenizer to use to prepare the examples.\n\n    Returns:\n        The prepared examples.\n    \"\"\"\n    # Some of the questions have lots of whitespace on the left, which is not useful\n    # and will make the truncation of the context fail (the tokenized question will\n    # take a lots of space). So we remove that left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Extract special token metadata from the tokenizer\n    special_token_metadata = get_special_token_metadata(tokenizer=tokenizer)\n    has_cls_token = special_token_metadata[\"has_cls_token\"]\n    has_sep_token = special_token_metadata[\"has_sep_token\"]\n    cls_token_id = special_token_metadata[\"cls_token_id\"]\n    cls_token = special_token_metadata[\"cls_token\"]\n    sep_token = special_token_metadata[\"sep_token\"]\n\n    # If the tokenizer is not adding special tokens, then we add them manually\n    if not has_cls_token and not has_sep_token:\n        examples[\"question\"] = [\n            f\"{cls_token}{q}{sep_token}\" for q in examples[\"question\"]\n        ]\n        examples[\"context\"] = [f\"{c}{sep_token}\" for c in examples[\"context\"]]\n\n    # Set the stride used during tokenization, when the context is long enough to be\n    # split into several features. Since we are always keeping the question tokens, we\n    # need to make sure that the stride does not exceed the resulting maximum context\n    # length.\n    max_question_tokens = max(len(tokenizer(q).input_ids) for q in examples[\"question\"])\n    num_special_tokens = int(has_cls_token) + int(has_sep_token)\n    stride = tokenizer.model_max_length // 4\n    max_length = tokenizer.model_max_length - stride\n    stride = min(stride, max_length - max_question_tokens - num_special_tokens)\n    max_length = tokenizer.model_max_length - stride\n\n    # Tokenize our examples with truncation and padding, but keep the overflows using a\n    # stride. This results in one example possible giving several features when a\n    # context is long, each of those features having a context that overlaps a bit the\n    # context of the previous feature.\n    tokenized_examples = tokenizer(\n        text=examples[\"question\"],\n        text_pair=examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we\n    # need a map from a feature to its corresponding example. This key gives us just\n    # that\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n    # The offset mappings will give us a map from token to character position in the\n    # original context. This will help us compute the start_positions and\n    # end_positions.\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n\n    # Initialise the start- and end positions of the answers\n    tokenized_examples[\"start_positions\"] = list()\n    tokenized_examples[\"end_positions\"] = list()\n\n    for i, offsets in enumerate(offset_mapping):\n        # Get the input IDs for the current example\n        input_ids = tokenized_examples.input_ids[i]\n\n        # We will label impossible answers with the index of the CLS token\n        cls_index = input_ids.index(cls_token_id)\n\n        # Grab the sequence corresponding to that example (to know what is the context\n        # and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n\n        # Manually ensure that the special tokens are set to None in `sequence_ids`\n        for special_token in tokenizer.special_tokens_map.keys():\n            if hasattr(tokenizer, f\"{special_token}_id\"):\n                special_token_id = getattr(tokenizer, f\"{special_token}_id\")\n                if special_token_id is not None:\n                    sequence_ids = [\n                        None if token_id == special_token_id else seq_id\n                        for token_id, seq_id in zip(input_ids, sequence_ids)\n                    ]\n\n        # One example can give several spans, this is the index of the example\n        # containing this span of text.\n        sample_index = sample_mapping[i]\n        answers = examples[\"answers\"][sample_index]\n\n        # If no answers are given, set the cls_index as answer.\n        if len(answers[\"answer_start\"]) == 0:\n            tokenized_examples.start_positions.append(cls_index)\n            tokenized_examples.end_positions.append(cls_index)\n\n        else:\n            # Start/end character index of the answer in the text.\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            # Start token index of the current span in the text.\n            token_start_index = 0\n            while sequence_ids[token_start_index] != 1:\n                token_start_index += 1\n\n            # End token index of the current span in the text.\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != 1:\n                token_end_index -= 1\n\n            # Detect if the answer is out of the span (in which case this feature is\n            # labeled with the CLS index).\n            if not (\n                offsets[token_start_index][0] &lt;= start_char\n                and offsets[token_end_index][1] &gt;= end_char\n            ):\n                tokenized_examples.start_positions.append(cls_index)\n                tokenized_examples.end_positions.append(cls_index)\n\n            # Otherwise move the token_start_index and token_end_index to the two ends\n            # of the answer. Note: we could go after the last offset if the answer is\n            # the last word (edge case).\n            else:\n                while (\n                    token_start_index &lt;= token_end_index\n                    and offsets[token_start_index][0] &lt;= start_char\n                ):\n                    token_start_index += 1\n                token_start_index -= 1\n                tokenized_examples.start_positions.append(token_start_index)\n                while (\n                    token_start_index &lt;= token_end_index\n                    and offsets[token_end_index][1] &gt;= end_char\n                ):\n                    token_end_index -= 1\n                token_end_index += 1\n                tokenized_examples.end_positions.append(token_end_index)\n                assert token_end_index &gt;= token_start_index\n\n    return tokenized_examples\n\n\ndef prepare_test_examples(docs\n    examples: \"BatchEncoding\", tokenizer: \"Tokenizer\"\n) -&gt; \"BatchEncoding\":\n    \"\"\"Prepare test examples.\n\n    Args:\n        examples:\n            Dictionary of test examples.\n        tokenizer:\n            The tokenizer used to preprocess the examples.\n\n    Returns:\n        The prepared test examples.\n    \"\"\"\n    # Some of the questions have lots of whitespace on the left, which is not useful\n    # and will make the truncation of the context fail (the tokenized question will\n    # take a lots of space). So we remove that left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Extract special token metadata from the tokenizer\n    special_token_metadata = get_special_token_metadata(tokenizer=tokenizer)\n    has_cls_token = special_token_metadata[\"has_cls_token\"]\n    has_sep_token = special_token_metadata[\"has_sep_token\"]\n    cls_token = special_token_metadata[\"cls_token\"]\n    sep_token = special_token_metadata[\"sep_token\"]\n\n    # If the tokenizer is not adding special tokens, then we add them manually\n    if not has_cls_token and not has_sep_token:\n        examples[\"question\"] = [\n            f\"{cls_token}{q}{sep_token}\" for q in examples[\"question\"]\n        ]\n        examples[\"context\"] = [f\"{c}{sep_token}\" for c in examples[\"context\"]]\n\n    # Set the stride used during tokenization, when the context is long enough to be\n    # split into several features. Since we are always keeping the question tokens, we\n    # need to make sure that the stride does not exceed the resulting maximum context\n    # length.\n    max_question_tokens = max(len(tokenizer(q).input_ids) for q in examples[\"question\"])\n    num_special_tokens = int(has_cls_token) + int(has_sep_token)\n    stride = tokenizer.model_max_length // 4\n    max_length = tokenizer.model_max_length - stride\n    stride = min(stride, max_length - max_question_tokens - num_special_tokens)\n    max_length = tokenizer.model_max_length - stride\n\n    # Tokenize our examples with truncation and maybe padding, but keep the overflows\n    # using a stride. This results in one example possible giving several features when\n    # a context is long, each of those features having a context that overlaps a bit\n    # the context of the previous feature.\n    tokenized_examples = tokenizer(\n        text=examples[\"question\"],\n        text_pair=examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we\n    # need a map from a feature to its corresponding example. This key gives us just\n    # that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n    # We keep the id that gave us this feature and we will store the offset mappings.\n    tokenized_examples[\"id\"] = list()\n\n    for i in range(len(tokenized_examples.input_ids)):\n        # Grab the sequence corresponding to that example (to know what is the context\n        # and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1\n\n        # One example can give several spans, this is the index of the example\n        # containing this span of text.\n        sample_index = sample_mapping[i]\n        tokenized_examples.id.append(examples[\"id\"][sample_index])\n\n        # Set to (-1, -1) the offset_mapping that are not part of the context so it's\n        # easy to determine if a token position is part of the context or not.\n        tokenized_examples.offset_mapping[i] = [\n            (o if sequence_ids[k] == context_index else (-1, -1))\n            for k, o in enumerate(tokenized_examples.offset_mapping[i])\n        ]\n\n    return tokenized_examples\n\n\ndef prepare_examples_for_generation(docs\n    examples: \"BatchEncoding\", tokenizer: \"Tokenizer\"\n) -&gt; \"BatchEncoding\":\n    \"\"\"Prepare test examples.\n\n    Args:\n        examples:\n            Dictionary of test examples.\n        tokenizer:\n            The tokenizer used to preprocess the examples.\n\n    Returns:\n        The prepared test examples.\n    \"\"\"\n    tokenized_examples = tokenizer(text=examples[\"text\"], truncation=True)\n    tokenized_examples[\"label\"] = [\n        dict(\n            id=id,\n            answers=dict(\n                answer_start=answer_dct[\"answer_start\"],\n                text=[answer_text.lower() for answer_text in answer_dct[\"text\"]],\n            ),\n        )\n        for id, answer_dct in zip(examples[\"id\"], examples[\"answers\"])\n    ]\n    return tokenized_examples\n</code></pre>"},{"location":"api/scandeval/question_answering_trainer/","title":"scandeval.question_answering_trainer","text":"scandeval.question_answering_trainer<p> source module scandeval.question_answering_trainer </p> <p>Question answering Trainer subclass.</p> <p> Classes </p> <ul> <li> <p>QuestionAnsweringTrainer \u2014 Trainer subclass for question answering tasks.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>postprocess_predictions_and_labels \u2014 Postprocess the predictions and labels, to allow easier metric computation.</p> </li> <li> <p>find_best_answer \u2014 Find the best answer for a given example.</p> </li> <li> <p>find_valid_answers \u2014 Find the valid answers from the start and end indexes.</p> </li> </ul> <p> source class QuestionAnsweringTrainer(**kwargs) </p> <p><p>Bases : Trainer</p></p> <p>Trainer subclass for question answering tasks.</p> <p>Initialize the trainer.</p> <p> Methods </p> <ul> <li> <p>evaluate \u2014 Evaluate the model on the given dataset.</p> </li> </ul> <p> source method QuestionAnsweringTrainer.evaluate(eval_dataset: Dataset | None = None, orig_eval_dataset: Dataset | None = None, ignore_keys: list[str] | None = None, metric_key_prefix: str = 'eval') \u2192 dict[str, float] | None </p> <p>Evaluate the model on the given dataset.</p> <p> Parameters </p> <ul> <li> <p>eval_dataset :  Dataset | None \u2014</p> <p>The dataset to evaluate on. If None, then use the stored evaluation dataset.</p> </li> <li> <p>orig_eval_dataset :  Dataset | None \u2014</p> <p>The original evaluation dataset, before any postprocessing. If None, then use the stored original evaluation dataset.</p> </li> <li> <p>ignore_keys :  list[str] | None \u2014</p> <p>The keys to ignore when computing the metrics.</p> </li> <li> <p>metric_key_prefix :  str \u2014</p> <p>The prefix to use for the metric keys.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] | None \u2014 The metrics computed on the evaluation dataset.</p> </li> </ul> <p> source postprocess_predictions_and_labels(predictions: list, dataset: Dataset, prepared_dataset: Dataset, cls_token_index: int) \u2192 tuple[list[dict], list[dict]] </p> <p>Postprocess the predictions and labels, to allow easier metric computation.</p> <p> Parameters </p> <ul> <li> <p>predictions :  list \u2014</p> <p>A pair of (start_logits, end_logits) predictions.</p> </li> <li> <p>dataset :  Dataset \u2014</p> <p>The dataset containing the examples.</p> </li> <li> <p>prepared_dataset :  Dataset \u2014</p> <p>The dataset containing the prepared examples.</p> </li> <li> <p>cls_token_index :  int \u2014</p> <p>The index of the CLS token.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[list[dict], list[dict]] \u2014 The postprocessed predictions and labels.</p> </li> </ul> <p> source find_best_answer(all_start_logits: np.ndarray, all_end_logits: np.ndarray, prepared_dataset: Dataset, feature_indices: list[int], context: str, max_answer_length: int, num_best_logits: int, min_null_score: float, cls_token_index: int) \u2192 str </p> <p>Find the best answer for a given example.</p> <p> Parameters </p> <ul> <li> <p>all_start_logits :  np.ndarray \u2014</p> <p>The start logits for all the features.</p> </li> <li> <p>all_end_logits :  np.ndarray \u2014</p> <p>The end logits for all the features.</p> </li> <li> <p>prepared_dataset :  Dataset \u2014</p> <p>The dataset containing the prepared examples.</p> </li> <li> <p>feature_indices :  list[int] \u2014</p> <p>The indices of the features associated with the current example.</p> </li> <li> <p>context :  str \u2014</p> <p>The context of the example.</p> </li> <li> <p>max_answer_length :  int \u2014</p> <p>The maximum length of the answer.</p> </li> <li> <p>num_best_logits :  int \u2014</p> <p>The number of best logits to consider.</p> </li> <li> <p>min_null_score :  float \u2014</p> <p>The minimum score an answer can have.</p> </li> <li> <p>cls_token_index :  int \u2014</p> <p>The index of the CLS token.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The best answer for the example.</p> </li> </ul> <p> source find_valid_answers(start_logits: np.ndarray, end_logits: np.ndarray, offset_mapping: list[tuple[int, int]], context: str, max_answer_length: int, num_best_logits: int, min_null_score: float) \u2192 list[dict] </p> <p>Find the valid answers from the start and end indexes.</p> <p> Parameters </p> <ul> <li> <p>start_logits :  np.ndarray \u2014</p> <p>The logits for the start of the answer.</p> </li> <li> <p>end_logits :  np.ndarray \u2014</p> <p>The logits for the end of the answer.</p> </li> <li> <p>offset_mapping :  list[tuple[int, int]] \u2014</p> <p>The offset mapping, being a list of pairs of integers for each token index, containing the start and end character index in the original context.</p> </li> <li> <p>context :  str \u2014</p> <p>The context of the example.</p> </li> <li> <p>max_answer_length :  int \u2014</p> <p>The maximum length of the answer.</p> </li> <li> <p>num_best_logits :  int \u2014</p> <p>The number of best logits to consider. Note that this function will run in O(<code>num_best_logits</code> ^ 2) time.</p> </li> <li> <p>min_null_score :  float \u2014</p> <p>The minimum score an answer can have.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[dict] \u2014 A list of the valid answers, each being a dictionary with keys \"text\" and \"score\", the score being the sum of the start and end logits.</p> </li> </ul>"},{"location":"src/scandeval/question_answering_trainer/","title":"scandeval.question_answering_trainer","text":"scandeval.question_answering_trainer<p> docs module scandeval.question_answering_trainer </p> <pre><code>\"\"\"Question answering Trainer subclass.\"\"\"\n\nfrom collections import defaultdict\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom transformers.trainer import Trainer\n\nfrom .utils import get_special_token_metadata\n\nif TYPE_CHECKING:\n    from datasets.arrow_dataset import Dataset\n\n\nclass QuestionAnsweringTrainer(Trainer):docs\n    \"\"\"Trainer subclass for question answering tasks.\"\"\"\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        \"\"\"Initialize the trainer.\"\"\"\n        super().__init__(*args, **kwargs)\n\n        # Get the CLS token id for the tokenizer\n        special_token_metadata = get_special_token_metadata(self.tokenizer)\n        self.cls_token_id = special_token_metadata[\"cls_token_id\"]\n\n        # Set the label names\n        self.label_names = [\"start_positions\", \"end_positions\"]\n\n    def evaluate(docs\n        self,\n        eval_dataset: \"Dataset | None\" = None,\n        orig_eval_dataset: \"Dataset | None\" = None,\n        ignore_keys: list[str] | None = None,\n        metric_key_prefix: str = \"eval\",\n    ) -&gt; dict[str, float] | None:\n        \"\"\"Evaluate the model on the given dataset.\n\n        Args:\n            eval_dataset:\n                The dataset to evaluate on. If None, then use the stored evaluation\n                dataset.\n            orig_eval_dataset:\n                The original evaluation dataset, before any postprocessing. If None,\n                then use the stored original evaluation dataset.\n            ignore_keys:\n                The keys to ignore when computing the metrics.\n            metric_key_prefix:\n                The prefix to use for the metric keys.\n\n        Returns:\n            The metrics computed on the evaluation dataset.\n        \"\"\"\n        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n\n        # Temporarily disable metric computation, we will do it in the loop here.\n        compute_metrics = self.compute_metrics  # type: ignore[has-type]\n        self.compute_metrics = None\n        eval_loop = (\n            self.prediction_loop\n            if self.args.use_legacy_prediction_loop\n            else self.evaluation_loop\n        )\n        try:\n            output = eval_loop(\n                eval_dataloader,\n                description=\"Evaluation\",\n                prediction_loss_only=True if compute_metrics is None else None,\n                ignore_keys=ignore_keys,\n                metric_key_prefix=metric_key_prefix,\n            )\n        finally:\n            self.compute_metrics = compute_metrics\n\n        if orig_eval_dataset is not None:\n            preds_and_labels = postprocess_predictions_and_labels(\n                predictions=output.predictions,\n                dataset=orig_eval_dataset,\n                prepared_dataset=eval_dataset,\n                cls_token_index=self.cls_token_id,\n            )\n            output.metrics.update(self.compute_metrics(preds_and_labels))\n\n            # Prefix all keys with metric_key_prefix + '_'\n            for key in list(output.metrics.keys()):\n                if not key.startswith(f\"{metric_key_prefix}_\"):\n                    output.metrics[f\"{metric_key_prefix}_{key}\"] = output.metrics.pop(\n                        key\n                    )\n\n        # Only the main node log the results by default\n        if self.args.should_log:\n            self.log(output.metrics)\n\n        self.control = self.callback_handler.on_evaluate(\n            self.args,\n            self.state,\n            self.control,  # type: ignore[has-type]\n            output.metrics,\n        )\n        return output.metrics\n\n\ndef postprocess_predictions_and_labels(docs\n    predictions: list,\n    dataset: \"Dataset\",\n    prepared_dataset: \"Dataset\",\n    cls_token_index: int,\n) -&gt; tuple[list[dict], list[dict]]:\n    \"\"\"Postprocess the predictions and labels, to allow easier metric computation.\n\n    Args:\n        predictions:\n            A pair of (start_logits, end_logits) predictions.\n        dataset:\n            The dataset containing the examples.\n        prepared_dataset:\n            The dataset containing the prepared examples.\n        cls_token_index:\n            The index of the CLS token.\n\n    Returns:\n        The postprocessed predictions and labels.\n    \"\"\"\n    # Extract the logits from the predictions\n    all_start_logits = predictions[0]\n    all_end_logits = predictions[1]\n\n    # Build a map from an example to its corresponding features, being the blocks of\n    # text from the context that we're feeding into the model. An example can have\n    # multiple features/blocks if it has a long context.\n    id_to_index = {k: i for i, k in enumerate(dataset[\"id\"])}\n    features_per_example = defaultdict(list)\n    for i, feature in enumerate(prepared_dataset):\n        id = feature[\"id\"]\n        example_index = id_to_index[id]\n        features_per_example[example_index].append(i)\n\n    # Loop over all the examples\n    predictions = list()\n    labels = list()\n    for example_index, example in enumerate(dataset):\n        # Extract the best valid answer associated with the current example\n        best_answer = find_best_answer(\n            all_start_logits=all_start_logits,\n            all_end_logits=all_end_logits,\n            prepared_dataset=prepared_dataset,\n            feature_indices=features_per_example[example_index],\n            context=example[\"context\"],\n            max_answer_length=30,\n            num_best_logits=20,\n            min_null_score=0.0,\n            cls_token_index=cls_token_index,\n        )\n\n        # Create the final prediction dictionary, to be added to the list of\n        # predictions\n        prediction = dict(\n            id=example[\"id\"], prediction_text=best_answer, no_answer_probability=0.0\n        )\n\n        # Add the answer to the list of predictions\n        predictions.append(prediction)\n\n        # Create the associated reference dictionary, to be added to the list of\n        # references\n        label = dict(\n            id=example[\"id\"],\n            answers=dict(\n                text=example[\"answers\"][\"text\"],\n                answer_start=example[\"answers\"][\"answer_start\"],\n            ),\n        )\n\n        # Add the answer and label to the list of predictions and labels, respectively\n        labels.append(label)\n\n    return predictions, labels\n\n\ndef find_best_answer(docs\n    all_start_logits: np.ndarray,\n    all_end_logits: np.ndarray,\n    prepared_dataset: \"Dataset\",\n    feature_indices: list[int],\n    context: str,\n    max_answer_length: int,\n    num_best_logits: int,\n    min_null_score: float,\n    cls_token_index: int,\n) -&gt; str:\n    \"\"\"Find the best answer for a given example.\n\n    Args:\n        all_start_logits:\n            The start logits for all the features.\n        all_end_logits:\n            The end logits for all the features.\n        prepared_dataset:\n            The dataset containing the prepared examples.\n        feature_indices:\n            The indices of the features associated with the current example.\n        context:\n            The context of the example.\n        max_answer_length:\n            The maximum length of the answer.\n        num_best_logits:\n            The number of best logits to consider.\n        min_null_score:\n            The minimum score an answer can have.\n        cls_token_index:\n            The index of the CLS token.\n\n    Returns:\n        The best answer for the example.\n    \"\"\"\n    # Loop through all the features associated to the current example\n    valid_answers = list()\n    for feature_index in feature_indices:\n        # Get the features associated with the current example\n        features = prepared_dataset[feature_index]\n\n        # Get the predictions of the model for this feature\n        start_logits = all_start_logits[feature_index]\n        end_logits = all_end_logits[feature_index]\n\n        # Update minimum null prediction\n        cls_index = features[\"input_ids\"].index(cls_token_index)\n        feature_null_score = (start_logits[cls_index] + end_logits[cls_index]).item()\n        if min_null_score &lt; feature_null_score:\n            min_null_score = feature_null_score\n\n        # Find the valid answers for the feature\n        valid_answers_for_feature = find_valid_answers(\n            start_logits=start_logits,\n            end_logits=end_logits,\n            offset_mapping=features[\"offset_mapping\"],\n            context=context,\n            max_answer_length=max_answer_length,\n            num_best_logits=num_best_logits,\n            min_null_score=min_null_score,\n        )\n        valid_answers.extend(valid_answers_for_feature)\n\n    # In the very rare edge case we have not a single non-null prediction, we create a\n    # fake prediction to avoid failure\n    if not valid_answers:\n        return \"\"\n\n    # Otherwise, we select the answer with the largest score as the best answer, and\n    # return it\n    best_answer_dict = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n    return best_answer_dict[\"text\"]\n\n\ndef find_valid_answers(docs\n    start_logits: np.ndarray,\n    end_logits: np.ndarray,\n    offset_mapping: list[tuple[int, int]],\n    context: str,\n    max_answer_length: int,\n    num_best_logits: int,\n    min_null_score: float,\n) -&gt; list[dict]:\n    \"\"\"Find the valid answers from the start and end indexes.\n\n    Args:\n        start_logits:\n            The logits for the start of the answer.\n        end_logits:\n            The logits for the end of the answer.\n        offset_mapping:\n            The offset mapping, being a list of pairs of integers for each token index,\n            containing the start and end character index in the original context.\n        context:\n            The context of the example.\n        max_answer_length:\n            The maximum length of the answer.\n        num_best_logits:\n            The number of best logits to consider. Note that this function will run in\n            O(`num_best_logits` ^ 2) time.\n        min_null_score:\n            The minimum score an answer can have.\n\n    Returns:\n        A list of the valid answers, each being a dictionary with keys \"text\" and\n        \"score\", the score being the sum of the start and end logits.\n    \"\"\"\n    # Fetch the top-k predictions for the start- and end token indices\n    start_indexes = np.argsort(start_logits)[-1 : -num_best_logits - 1 : -1].tolist()\n    end_indexes = np.argsort(end_logits)[-1 : -num_best_logits - 1 : -1].tolist()\n\n    # We loop over all combinations of starting and ending indexes for valid answers\n    valid_answers = list()\n    for start_index in start_indexes:\n        for end_index in end_indexes:\n            # If the starting or ending index is out-of-scope, meaning that they are\n            # either out of bounds or correspond to part of the input_ids that are not\n            # in the context, then we skip this index\n            if (\n                start_index &gt;= len(offset_mapping)\n                or end_index &gt;= len(offset_mapping)\n                or tuple(offset_mapping[start_index]) == (-1, -1)\n                or tuple(offset_mapping[end_index]) == (-1, -1)\n            ):\n                continue\n\n            # Do not consider answers with a length that is either negative or greater\n            # than the context length\n            max_val = max_answer_length + start_index - 1\n            if end_index &lt; start_index or end_index &gt; max_val:\n                continue\n\n            # If we got to this point then the answer is valid, so we store the\n            # corresponding start- and end character indices in the original context,\n            # and from these extract the answer\n            start_char = offset_mapping[start_index][0]\n            end_char = offset_mapping[end_index][1]\n            text = context[start_char:end_char]\n\n            # Compute the score of the answer, being the sum of the start and end\n            # logits. Intuitively, this indicates how likely the answer is to be\n            # correct, and allows us to pick the best valid answer.\n            score = start_logits[start_index] + end_logits[end_index]\n\n            # Add the answer to the list of valid answers, if the score is greater\n            # than the minimum null score\n            if score &gt; min_null_score:\n                valid_answers.append(dict(score=score, text=text))\n\n    return valid_answers\n</code></pre>"},{"location":"api/scandeval/scores/","title":"scandeval.scores","text":"scandeval.scores<p> source module scandeval.scores </p> <p>Aggregation of raw scores into the mean and a confidence interval.</p> <p> Functions </p> <ul> <li> <p>log_scores \u2014 Log the scores.</p> </li> <li> <p>aggregate_scores \u2014 Helper function to compute the mean with confidence intervals.</p> </li> </ul> <p> source log_scores(dataset_name: str, metric_configs: list[MetricConfig], scores: dict[str, list[dict[str, float]]], model_id: str) \u2192 ScoreDict </p> <p>Log the scores.</p> <p> Parameters </p> <ul> <li> <p>dataset_name :  str \u2014</p> <p>Name of the dataset.</p> </li> <li> <p>metric_configs :  list[MetricConfig] \u2014</p> <p>List of metrics to log.</p> </li> <li> <p>scores :  dict[str, list[dict[str, float]]] \u2014</p> <p>The scores that are to be logged. This is a dict with keys 'train' and 'test', with values being lists of dictionaries full of scores.</p> </li> <li> <p>model_id :  str \u2014</p> <p>The full Hugging Face Hub path to the pretrained transformer model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ScoreDict \u2014 A dictionary with keys 'raw_scores' and 'total', with 'raw_scores' being identical to <code>scores</code> and 'total' being a dictionary with the aggregated scores (means and standard errors).</p> </li> </ul> <p> source aggregate_scores(scores: dict[str, list[dict[str, float]]], metric_config: MetricConfig) \u2192 dict[str, tuple[float, float]] </p> <p>Helper function to compute the mean with confidence intervals.</p> <p> Parameters </p> <ul> <li> <p>scores :  dict[str, list[dict[str, float]]] \u2014</p> <p>Dictionary with the names of the metrics as keys, of the form \"_\", such as \"val_f1\", and values the metric values. <li> <p>metric_config :  MetricConfig \u2014</p> <p>The configuration of the metric, which is used to collect the correct metric from <code>scores</code>.</p> </li> <p> Returns </p> <ul> <li> <p>dict[str, tuple[float, float]] \u2014 Dictionary with keys among 'train' and 'test', with corresponding values being a pair of floats, containing the score and the radius of its 95% confidence interval.</p> </li> </ul>"},{"location":"src/scandeval/scores/","title":"scandeval.scores","text":"scandeval.scores<p> docs module scandeval.scores </p> <pre><code>\"\"\"Aggregation of raw scores into the mean and a confidence interval.\"\"\"\n\nimport logging\nimport warnings\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\n\nif TYPE_CHECKING:\n    from .config import MetricConfig\n    from .types import ScoreDict\n\nlogger = logging.getLogger(__package__)\n\n\ndef log_scores(docs\n    dataset_name: str,\n    metric_configs: list[\"MetricConfig\"],\n    scores: dict[str, list[dict[str, float]]],\n    model_id: str,\n) -&gt; \"ScoreDict\":\n    \"\"\"Log the scores.\n\n    Args:\n        dataset_name:\n            Name of the dataset.\n        metric_configs:\n            List of metrics to log.\n        scores:\n            The scores that are to be logged. This is a dict with keys 'train' and\n            'test', with values being lists of dictionaries full of scores.\n        model_id:\n            The full Hugging Face Hub path to the pretrained transformer model.\n\n    Returns:\n        A dictionary with keys 'raw_scores' and 'total', with 'raw_scores' being\n        identical to `scores` and 'total' being a dictionary with the aggregated scores\n        (means and standard errors).\n    \"\"\"\n    logger.info(f\"Finished evaluation of {model_id} on {dataset_name}.\")\n\n    total_dict: dict[str, float] = dict()\n\n    # Logging of the aggregated scores\n    for metric_cfg in metric_configs:\n        agg_scores = aggregate_scores(scores=scores, metric_config=metric_cfg)\n        test_score, test_se = agg_scores[\"test\"]\n        test_score, test_score_str = metric_cfg.postprocessing_fn(test_score)\n        test_se, test_se_str = metric_cfg.postprocessing_fn(test_se)\n        msg = f\"{metric_cfg.pretty_name}:\"\n\n        if \"train\" in agg_scores.keys():\n            train_score, train_se = agg_scores[\"train\"]\n            train_score, train_score_str = metric_cfg.postprocessing_fn(train_score)\n            train_se, train_se_str = metric_cfg.postprocessing_fn(train_se)\n            msg += f\"\\n  - Test: {test_score_str} \u00b1 {test_se_str}\"\n            msg += f\"\\n  - Train: {train_score_str} \u00b1 {train_se_str}\"\n\n            # Store the aggregated train scores\n            total_dict[f\"train_{metric_cfg.name}\"] = train_score\n            total_dict[f\"train_{metric_cfg.name}_se\"] = train_se\n\n        else:\n            msg += f\" {test_score_str} \u00b1 {test_se_str}\"\n\n        # Store the aggregated test scores\n        total_dict[f\"test_{metric_cfg.name}\"] = test_score\n        total_dict[f\"test_{metric_cfg.name}_se\"] = test_se\n\n        # Log the scores\n        logger.info(msg)\n\n    # Define a dict with both the raw scores and the aggregated scores\n    all_scores: dict[str, dict[str, float] | dict[str, list[dict[str, float]]]]\n    all_scores = dict(raw=scores, total=total_dict)\n\n    # Return the extended scores\n    return all_scores\n\n\ndef aggregate_scores(docs\n    scores: dict[str, list[dict[str, float]]], metric_config: \"MetricConfig\"\n) -&gt; dict[str, tuple[float, float]]:\n    \"\"\"Helper function to compute the mean with confidence intervals.\n\n    Args:\n        scores:\n            Dictionary with the names of the metrics as keys, of the form\n            \"&lt;split&gt;_&lt;metric_name&gt;\", such as \"val_f1\", and values the metric values.\n        metric_config:\n            The configuration of the metric, which is used to collect the correct\n            metric from `scores`.\n\n    Returns:\n        Dictionary with keys among 'train' and 'test', with corresponding values being\n        a pair of floats, containing the score and the radius of its 95% confidence\n        interval.\n    \"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n\n        results: dict[str, tuple[float, float]] = dict()\n\n        if \"train\" in scores.keys():\n            train_scores = [\n                (\n                    dct[metric_config.name]\n                    if metric_config.name in dct\n                    else dct[f\"train_{metric_config.name}\"]\n                )\n                for dct in scores[\"train\"]\n            ]\n            train_score = np.mean(train_scores).item()\n\n            if len(train_scores) &gt; 1:\n                sample_std = np.std(train_scores, ddof=1)\n                train_se = sample_std / np.sqrt(len(train_scores))\n            else:\n                train_se = np.nan\n\n            results[\"train\"] = (train_score, 1.96 * train_se)\n\n        if \"test\" in scores.keys():\n            test_scores = [\n                (\n                    dct[metric_config.name]\n                    if metric_config.name in dct\n                    else dct[f\"test_{metric_config.name}\"]\n                )\n                for dct in scores[\"test\"]\n            ]\n            test_score = np.mean(test_scores).item()\n\n            if len(test_scores) &gt; 1:\n                sample_std = np.std(test_scores, ddof=1)\n                test_se = sample_std / np.sqrt(len(test_scores))\n            else:\n                test_se = np.nan\n\n            results[\"test\"] = (test_score, 1.96 * test_se)\n\n        return results\n</code></pre>"},{"location":"api/scandeval/sequence_classification/","title":"scandeval.sequence_classification","text":"scandeval.sequence_classification<p> source module scandeval.sequence_classification </p> <p>Sequence classification benchmark dataset.</p> <p> Classes </p> <ul> <li> <p>SequenceClassification \u2014 Sequence classification benchmark dataset.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>get_closest_logprobs_labels \u2014 Get the labels with the highest predicted logprob value.</p> </li> <li> <p>get_closest_word_edit_labels \u2014 Get the labels with the smallest edit distance to the predicted labels.</p> </li> </ul> <p> source class SequenceClassification() </p> <p><p>Bases : BenchmarkDataset</p></p> <p>Sequence classification benchmark dataset.</p> <p> Parameters </p> <ul> <li> <p>dataset_config \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Attributes </p> <ul> <li> <p>dataset_config \u2014</p> <p>The configuration of the dataset.</p> </li> <li> <p>benchmark_config \u2014</p> <p>The configuration of the benchmark.</p> </li> </ul> <p> source get_closest_logprobs_labels(generation_logprobs: torch.Tensor, tokenizer: Tokenizer, dataset_config: DatasetConfig) \u2192 list[str] </p> <p>Get the labels with the highest predicted logprob value.</p> <p>In case a candidate label is split into multiple tokens, we only use the first token to compute the logprob value. E.g., if the candidate label \"positive\" is tokenised as [\"pos\", \"itive\"], we only use the logprob value of \"pos\" to represent the logprob value of the entire label.</p> <p> Parameters </p> <ul> <li> <p>generation_logprobs :  torch.Tensor \u2014</p> <p>The logprobs of the generated tokens.</p> </li> <li> <p>tokenizer :  Tokenizer \u2014</p> <p>The tokenizer used to generate the tokens.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> <li> <p>benchmark_config \u2014</p> <p>The configuration of the benchmark.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 The predicted labels.</p> </li> </ul> <p> source get_closest_word_edit_labels(generated_sequences: torch.Tensor, tokenizer: Tokenizer, dataset_config: DatasetConfig) \u2192 list[str] </p> <p>Get the labels with the smallest edit distance to the predicted labels.</p> <p> Parameters </p> <ul> <li> <p>generated_sequences :  torch.Tensor \u2014</p> <p>The generated sequences from the model. The outer-most list is the batch dimension, the inner-most list is the sequence dimension, consisting of token IDs.</p> </li> <li> <p>tokenizer :  Tokenizer \u2014</p> <p>The tokenizer used to generate the tokens.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 The candidate labels with the smallest edit distance to the predicted labels.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NeedsExtraInstalled</p> </li> </ul>"},{"location":"src/scandeval/sequence_classification/","title":"scandeval.sequence_classification","text":"scandeval.sequence_classification<p> docs module scandeval.sequence_classification </p> <pre><code>\"\"\"Sequence classification benchmark dataset.\"\"\"\n\nimport importlib.util\nimport itertools as it\nimport logging\nimport random\nimport re\nfrom functools import partial\nfrom typing import TYPE_CHECKING, Any\n\nimport numpy as np\nimport torch\nfrom evaluate import EvaluationModule\nfrom transformers.data.data_collator import DataCollatorWithPadding\n\nfrom .benchmark_dataset import BenchmarkDataset\nfrom .exceptions import InvalidBenchmark, NeedsExtraInstalled\nfrom .generation import extract_raw_predictions\nfrom .utils import (\n    GENERATIVE_MODEL_TASKS,\n    convert_prompt_to_instruction,\n    get_special_token_metadata,\n    raise_if_model_output_contains_nan_values,\n    should_prefix_space_be_added_to_labels,\n)\n\nif TYPE_CHECKING:\n    from datasets.arrow_dataset import Dataset\n    from transformers import BatchEncoding, PreTrainedModel\n    from transformers.modeling_utils import ModelOutput\n\n    from .config import DatasetConfig\n    from .protocols import GenerativeModel, Tokenizer\n    from .types import Labels, Predictions\n\nif importlib.util.find_spec(\"Levenshtein\") is not None:\n    import Levenshtein\n\n\nlogger = logging.getLogger(__package__)\n\n\nclass SequenceClassification(BenchmarkDataset):docs\n    \"\"\"Sequence classification benchmark dataset.\n\n    Args:\n        dataset_config:\n            The dataset configuration.\n        benchmark_config:\n            The benchmark configuration.\n\n    Attributes:\n        dataset_config:\n            The configuration of the dataset.\n        benchmark_config:\n            The configuration of the benchmark.\n    \"\"\"\n\n    def _preprocess_data(self, dataset: \"Dataset\", **kwargs) -&gt; \"Dataset\":\n        \"\"\"Preprocess a dataset by tokenizing and aligning the labels.\n\n        Args:\n            dataset:\n                The dataset to preprocess.\n            kwargs:\n                Extra keyword arguments containing objects used in preprocessing the\n                dataset.\n\n        Returns:\n            The preprocessed dataset.\n        \"\"\"\n        tokenizer: \"Tokenizer\" = kwargs[\"tokenizer\"]\n\n        # Extract special token metadata from the tokenizer\n        special_token_metadata = get_special_token_metadata(tokenizer=tokenizer)\n        has_cls_token = special_token_metadata[\"has_cls_token\"]\n        has_sep_token = special_token_metadata[\"has_sep_token\"]\n        cls_token = special_token_metadata[\"cls_token\"]\n        sep_token = special_token_metadata[\"sep_token\"]\n\n        def tokenise(examples: dict) -&gt; \"BatchEncoding\":\n            # If the tokenizer is not adding special tokens, then we add them manually.\n            # We don't need this when performing few-shot evaluations, so in that case\n            # we don't add the special tokens.\n            if (\n                not has_cls_token\n                and not has_sep_token\n                and cls_token is not None\n                and sep_token is not None\n                and kwargs[\"model_config\"].task not in GENERATIVE_MODEL_TASKS\n            ):\n                examples[\"text\"] = [\n                    f\"{cls_token}{doc}{sep_token}\" for doc in examples[\"text\"]\n                ]\n\n            return tokenizer(text=examples[\"text\"], truncation=True, padding=False)\n\n        tokenised = dataset.map(tokenise, batched=True, load_from_cache_file=False)\n\n        if kwargs[\"model_config\"].task not in GENERATIVE_MODEL_TASKS:\n            numericalise = partial(\n                self._create_numerical_labels,\n                label2id=kwargs[\"hf_model_config\"].label2id,\n            )\n            return tokenised.map(\n                numericalise,\n                batched=True,\n                load_from_cache_file=False,\n                keep_in_memory=True,\n            ).remove_columns([\"text\"])\n        else:\n            return tokenised\n\n    def _create_numerical_labels(self, examples: dict, label2id: dict) -&gt; dict:\n        try:\n            examples[\"label\"] = [label2id[lbl.lower()] for lbl in examples[\"label\"]]\n        except KeyError:\n            raise InvalidBenchmark(\n                f\"One of the labels in the dataset, {examples['label'].lower()}, does \"\n                f\"not occur in the label2id dictionary {label2id}.\"\n            )\n        return examples\n\n    def _load_data_collator(\n        self,\n        tokenizer: \"Tokenizer | None\" = None,\n        model: \"PreTrainedModel | GenerativeModel | None\" = None,\n    ):\n        \"\"\"Load the data collator used to prepare samples during finetuning.\n\n        Args:\n            tokenizer:\n                A pretrained tokenizer. Can be None if the tokenizer is not used in the\n                initialisation of the data collator. Defaults to None.\n            model:\n                A pretrained model. Can be None if the model is not used in the\n                initialisation of the data collator. Defaults to None.\n\n        Returns:\n            The data collator.\n        \"\"\"\n        return DataCollatorWithPadding(tokenizer, padding=\"longest\")\n\n    def _compute_metrics(\n        self,\n        model_outputs_and_labels: tuple[\"Predictions\", \"Labels\"],\n        id2label: dict[int, str],\n    ) -&gt; dict[str, float]:\n        \"\"\"Compute the metrics needed for evaluation.\n\n        Args:\n            model_outputs_and_labels:\n                The first sequence contains the model outputs and the second sequence\n                contains the true labels.\n            id2label:\n                Conversion of indices to labels.\n\n        Returns:\n            A dictionary with the names of the metrics as keys and the metric values as\n            values.\n        \"\"\"\n        model_outputs, labels = model_outputs_and_labels\n        label2id = {label: idx for idx, label in id2label.items()}\n\n        raise_if_model_output_contains_nan_values(model_output=model_outputs)\n\n        model_output_dtype = np.asarray(model_outputs).dtype\n        if model_output_dtype in [np.float16, np.float32, np.float64]:\n            predictions = np.asarray(model_outputs).argmax(axis=-1)\n        else:\n            predictions = model_outputs\n\n        prompt_label_to_label_mapping = {\n            prompt_label: label\n            for label, prompt_label in self.dataset_config.prompt_label_mapping.items()\n        }\n        predictions = [\n            (\n                label2id[prompt_label_to_label_mapping[pred.lower()]]\n                if isinstance(pred, str)\n                else pred\n            )\n            for pred in predictions\n        ]\n\n        label_ids = [\n            label2id[label.lower()] if isinstance(label, str) else label\n            for label in labels\n        ]\n\n        results: dict[str, float] = dict()\n        for cfg in self.dataset_config.task.metrics:\n            metric = self._metrics[cfg.name]\n            assert isinstance(metric, EvaluationModule)\n            score_dict: dict[str, float] | None = metric.compute(\n                predictions=predictions, references=label_ids, **cfg.compute_kwargs\n            )\n\n            # The metric returns None if we are running on multi-GPU and the current\n            # process is not the main process\n            if score_dict is not None:\n                scores = score_dict[cfg.results_key]\n                if isinstance(scores, list):\n                    scores = sum(scores) / len(scores)\n                results[cfg.name] = scores\n\n        return results\n\n    def _extract_few_shot_examples(\n        self, train_dataset: \"Dataset\", random_seed: int\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"Extract few-shot examples from the training dataset.\n\n        Args:\n            train_dataset:\n                The training dataset.\n            random_seed:\n                The random seed to use when extracting the few-shot examples.\n\n        Returns:\n            The few-shot examples.\n        \"\"\"\n        shuffled_train = train_dataset.shuffle(seed=random_seed)\n        num_few_shots = self.dataset_config.num_few_shot_examples\n        labels = it.cycle(self.dataset_config.task.labels)\n        few_shot_examples: list[dict[str, Any]] = list()\n\n        # We pick the few-shot examples one at a time rather than all at once since\n        # we're working with a bootstrapped training dataset, meaning that it will have\n        # duplicates. This ensures that we don't have any duplicates in the few-shot\n        # examples\n        while len(few_shot_examples) &lt; num_few_shots:\n            label = next(labels)\n            possible_examples = shuffled_train.filter(\n                lambda x: x[\"label\"].lower() == label.lower()\n            )\n            if len(possible_examples) == 0:\n                continue\n            example = possible_examples.select(range(1))[0]\n            few_shot_examples.append(example)\n            shuffled_train = shuffled_train.filter(\n                lambda x: x[\"text\"] != example[\"text\"]\n            )\n\n        random.seed(random_seed)\n        random.shuffle(few_shot_examples)\n        return few_shot_examples\n\n    def _apply_few_shot_prompt(\n        self, examples: dict, few_shot_examples: list[dict], tokenizer: \"Tokenizer\"\n    ) -&gt; dict:\n        \"\"\"Apply a few-shot prompt to the examples.\n\n        Args:\n            examples:\n                The examples to apply the prompt to.\n            few_shot_examples:\n                The examples to be included in the few-shot prompt.\n            tokenizer:\n                The tokenizer to use to encode the few-shot prompt.\n\n        Returns:\n            The examples with the few-shot prompt applied.\n        \"\"\"\n        # Build the few-shot part of the prompt\n        label_mapping = self.dataset_config.prompt_label_mapping\n        few_shot_prompts = [\n            self.dataset_config.prompt_template.format(\n                text=re.sub(r\"\\n+\", \"\\n\", example[\"text\"]).strip(),\n                label=label_mapping[example[\"label\"].lower()],\n            )\n            for example in few_shot_examples\n        ]\n        prompt_prefix = \"\"\n        if self.dataset_config.prompt_prefix:\n            prompt_prefix = self.dataset_config.prompt_prefix + \"\\n\\n\"\n        few_shot_prompt = prompt_prefix + \"\\n\\n\".join(few_shot_prompts)\n\n        # Add the texts from the examples to the prompts. We remove newlines from the\n        # examples as they have the special function to separate the few-shot examples\n        # from one another\n        new_prompts = [\n            self.dataset_config.prompt_template.format(\n                text=re.sub(r\"\\n+\", \"\\n\", text).strip(), label=\"\"\n            )\n            for text in examples[\"text\"]\n        ]\n\n        final_prompts = [\n            few_shot_prompt + \"\\n\\n\" + new_prompt for new_prompt in new_prompts\n        ]\n\n        examples[\"text\"] = final_prompts\n\n        return examples\n\n    def _apply_instruction_prompt(self, examples: dict, tokenizer: \"Tokenizer\") -&gt; dict:\n        \"\"\"Apply an instruction prompt to the examples.\n\n        Args:\n            examples:\n                The examples to apply the prompt to.\n            tokenizer:\n                The tokenizer to use to encode the instruction prompt.\n\n        Returns:\n            The examples with the instruction prompt applied.\n        \"\"\"\n        prompts = [\n            self.dataset_config.instruction_prompt.format(\n                text=re.sub(r\"\\n+\", \"\\n\", text).strip()\n            )\n            for text in examples[\"text\"]\n        ]\n        prompts = [\n            convert_prompt_to_instruction(prompt=prompt, tokenizer=tokenizer)\n            for prompt in prompts\n        ]\n        examples[\"text\"] = prompts\n        return examples\n\n    def _extract_labels_from_generation(\n        self,\n        input_batch: dict[str, list],\n        model_output: \"ModelOutput\",\n        tokenizer: \"Tokenizer\",\n    ) -&gt; list[Any]:\n        \"\"\"Extract the predicted labels from the generated output.\n\n        Args:\n            input_batch:\n                The input batch, where the keys are the feature names and the values\n                are lists with the feature values.\n            model_output:\n                The raw generated output of the model.\n            tokenizer:\n                The tokenizer used together with the model.\n\n        Returns:\n            The predicted labels.\n        \"\"\"\n        if \"scores\" in model_output:\n            if isinstance(model_output[\"scores\"], tuple):\n                all_logprobs = torch.stack(model_output[\"scores\"], dim=1)\n            else:\n                all_logprobs = model_output[\"scores\"]\n            return get_closest_logprobs_labels(\n                generation_logprobs=all_logprobs,\n                tokenizer=tokenizer,\n                dataset_config=self.dataset_config,\n            )\n        else:\n            return get_closest_word_edit_labels(\n                generated_sequences=model_output[\"sequences\"],\n                tokenizer=tokenizer,\n                dataset_config=self.dataset_config,\n            )\n\n\ndef get_closest_logprobs_labels(docs\n    generation_logprobs: torch.Tensor,\n    tokenizer: \"Tokenizer\",\n    dataset_config: \"DatasetConfig\",\n) -&gt; list[str]:\n    \"\"\"Get the labels with the highest predicted logprob value.\n\n    In case a candidate label is split into multiple tokens, we only use the first\n    token to compute the logprob value. E.g., if the candidate label \"positive\" is\n    tokenised as [\"pos\", \"itive\"], we only use the logprob value of \"pos\" to\n    represent the logprob value of the entire label.\n\n    Args:\n        generation_logprobs:\n            The logprobs of the generated tokens.\n        tokenizer:\n            The tokenizer used to generate the tokens.\n        dataset_config:\n            The configuration of the dataset.\n        benchmark_config:\n            The configuration of the benchmark.\n\n    Returns:\n        The predicted labels.\n    \"\"\"\n    candidate_labels = [\n        dataset_config.prompt_label_mapping[lbl]\n        for lbl in dataset_config.id2label.values()\n    ]\n\n    add_prefix_space_to_labels = should_prefix_space_be_added_to_labels(\n        labels_to_be_generated=candidate_labels, tokenizer=tokenizer\n    )\n\n    # Shape: [batch_size, num_candidate_labels]\n    pred_logprobs = torch.empty(\n        generation_logprobs.shape[0],\n        len(candidate_labels),\n        device=generation_logprobs.device,\n    )\n\n    for idx, candidate_label in enumerate(candidate_labels):\n        # We only use the first token to represent the logprob value of the entire\n        # label.\n        label_ready_for_tokenization = candidate_label.lower()\n        if add_prefix_space_to_labels:\n            label_ready_for_tokenization = \" \" + label_ready_for_tokenization\n        candidate_label_ids: list[list[int]] = tokenizer(\n            [label_ready_for_tokenization], add_special_tokens=False\n        )[\"input_ids\"]\n        candidate_label_id: int = candidate_label_ids[0][0]\n        pred_logprobs[:, idx] = generation_logprobs[:, 0, candidate_label_id]\n\n    # Shape: [batch_size,]\n    predicted_label_ids = pred_logprobs.argmax(dim=1)\n\n    predicted_labels = [candidate_labels[idx] for idx in predicted_label_ids]\n\n    return predicted_labels\n\n\ndef get_closest_word_edit_labels(docs\n    generated_sequences: torch.Tensor,\n    tokenizer: \"Tokenizer\",\n    dataset_config: \"DatasetConfig\",\n) -&gt; list[str]:\n    \"\"\"Get the labels with the smallest edit distance to the predicted labels.\n\n    Args:\n        generated_sequences:\n            The generated sequences from the model. The outer-most list is the\n            batch dimension, the inner-most list is the sequence dimension,\n            consisting of token IDs.\n        tokenizer:\n            The tokenizer used to generate the tokens.\n        dataset_config:\n            The configuration of the dataset.\n\n    Returns:\n        The candidate labels with the smallest edit distance to the predicted labels.\n    \"\"\"\n    if importlib.util.find_spec(\"Levenshtein\") is None:\n        raise NeedsExtraInstalled(extra=\"openai\")\n\n    raw_predictions = extract_raw_predictions(\n        generated_sequences=generated_sequences, tokenizer=tokenizer\n    )\n\n    candidate_labels = [\n        dataset_config.prompt_label_mapping[lbl]\n        for lbl in dataset_config.id2label.values()\n    ]\n    new_predicted_labels: list[str] = list()\n    for predicted_label in raw_predictions:\n        edit_distances = [\n            Levenshtein.distance(s1=predicted_label.lower(), s2=candidate_label.lower())\n            for candidate_label in candidate_labels\n        ]\n        closest_label = candidate_labels[np.argmin(edit_distances).item()]\n        new_predicted_labels.append(closest_label)\n    return new_predicted_labels\n</code></pre>"},{"location":"api/scandeval/speed_benchmark/","title":"scandeval.speed_benchmark","text":"scandeval.speed_benchmark<p> source module scandeval.speed_benchmark </p> <p>Benchmarking model inference speed.</p> <p> Functions </p> <ul> <li> <p>benchmark_speed \u2014 Benchmark model inference speed.</p> </li> <li> <p>benchmark_speed_single_iteration \u2014 Run a single iteration of the speed benchmark.</p> </li> </ul> <p> source benchmark_speed(itr: tqdm, tokenizer: PreTrainedTokenizer, model: PreTrainedModel, model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 dict[str, list[dict[str, float]]] </p> <p>Benchmark model inference speed.</p> <p> Parameters </p> <ul> <li> <p>itr :  tqdm \u2014</p> <p>tqdm iterator.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer \u2014</p> <p>Tokenizer to use.</p> </li> <li> <p>model :  PreTrainedModel \u2014</p> <p>Model to use.</p> </li> <li> <p>model_config :  ModelConfig \u2014</p> <p>Model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>Dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>Benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, list[dict[str, float]]] \u2014 Dictionary of scores.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source benchmark_speed_single_iteration(tokenizer: Tokenizer, model: PreTrainedModel | GenerativeModel, itr_idx: int, model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 dict[str, dict[str, float]] | Exception </p> <p>Run a single iteration of the speed benchmark.</p> <p> Parameters </p> <ul> <li> <p>tokenizer :  Tokenizer \u2014</p> <p>The tokenizer to use in the benchmark.</p> </li> <li> <p>model :  PreTrainedModel | GenerativeModel \u2014</p> <p>The model to use in the benchmark.</p> </li> <li> <p>itr_idx :  int \u2014</p> <p>The index of the iteration.</p> </li> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, dict[str, float]] | Exception \u2014 A dictionary containing the scores for the current iteration, with keys <code>train</code> and <code>test</code>. If an exception is raised, then the exception is returned.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError</p> </li> </ul>"},{"location":"src/scandeval/speed_benchmark/","title":"scandeval.speed_benchmark","text":"scandeval.speed_benchmark<p> docs module scandeval.speed_benchmark </p> <pre><code>\"\"\"Benchmarking model inference speed.\"\"\"\n\nimport logging\nfrom collections import defaultdict\nfrom typing import TYPE_CHECKING\n\nimport pyinfer\nimport torch\nfrom tqdm.auto import tqdm\nfrom transformers.modeling_utils import GenerationConfig, PreTrainedModel\n\nfrom .exceptions import InvalidBenchmark\nfrom .model_loading import load_model\nfrom .utils import clear_memory, model_is_generative\n\nif TYPE_CHECKING:\n    from transformers.tokenization_utils import PreTrainedTokenizer\n\n    from .config import BenchmarkConfig, DatasetConfig, ModelConfig\n    from .protocols import GenerativeModel, Tokenizer\n\n\nlogger = logging.getLogger(__package__)\n\n\ndef benchmark_speed(docs\n    itr: tqdm,\n    tokenizer: \"PreTrainedTokenizer\",\n    model: PreTrainedModel,\n    model_config: \"ModelConfig\",\n    dataset_config: \"DatasetConfig\",\n    benchmark_config: \"BenchmarkConfig\",\n) -&gt; dict[str, list[dict[str, float]]]:\n    \"\"\"Benchmark model inference speed.\n\n    Args:\n        itr:\n            tqdm iterator.\n        tokenizer:\n            Tokenizer to use.\n        model:\n            Model to use.\n        model_config:\n            Model configuration.\n        dataset_config:\n            Dataset configuration.\n        benchmark_config:\n            Benchmark configuration.\n\n    Returns:\n        Dictionary of scores.\n    \"\"\"\n    scores: dict[str, list[dict[str, float]]] = defaultdict(list)\n\n    for itr_idx in itr:\n        itr_scores = benchmark_speed_single_iteration(\n            tokenizer=tokenizer,\n            model=model,\n            itr_idx=itr_idx,\n            model_config=model_config,\n            dataset_config=dataset_config,\n            benchmark_config=benchmark_config,\n        )\n        clear_memory()\n\n        if isinstance(itr_scores, Exception):\n            raise InvalidBenchmark(f\"Speed benchmark failed with error: {itr_scores!r}\")\n        else:\n            scores[\"test\"].append(itr_scores[\"test\"])\n            if benchmark_config.evaluate_train:\n                scores[\"train\"].append(itr_scores[\"train\"])\n            logger.debug(f\"Scores for iteration {itr_idx}: {itr_scores}\")\n\n    return scores\n\n\ndef benchmark_speed_single_iteration(docs\n    tokenizer: \"Tokenizer\",\n    model: \"PreTrainedModel | GenerativeModel\",\n    itr_idx: int,\n    model_config: \"ModelConfig\",\n    dataset_config: \"DatasetConfig\",\n    benchmark_config: \"BenchmarkConfig\",\n) -&gt; dict[str, dict[str, float]] | Exception:\n    \"\"\"Run a single iteration of the speed benchmark.\n\n    Args:\n        tokenizer:\n            The tokenizer to use in the benchmark.\n        model:\n            The model to use in the benchmark.\n        itr_idx:\n            The index of the iteration.\n        model_config:\n            The model configuration.\n        dataset_config:\n            The dataset configuration.\n        benchmark_config:\n            The benchmark configuration.\n\n    Returns:\n        A dictionary containing the scores for the current iteration, with keys `train`\n        and `test`. If an exception is raised, then the exception is returned.\n    \"\"\"\n    is_generative = model_is_generative(model=model)\n\n    scores: dict[str, dict[str, float]] = dict()\n    try:\n        # Reinitialise a new model\n        if tokenizer is None or model is None:\n            model, tokenizer = load_model(\n                model_config=model_config,\n                dataset_config=dataset_config,\n                benchmark_config=benchmark_config,\n            )\n\n        def predict(doc: str) -&gt; None:\n            \"\"\"Function used to benchmark inference speed of the model.\"\"\"\n            # Raise an error if the tokenizer or model is undefined\n            if tokenizer is None or model is None:\n                raise ValueError(\"Tokenizer and model must not be None.\")\n\n            # Tokenize the document\n            inputs = tokenizer(doc, padding=True, truncation=True, return_tensors=\"pt\")\n\n            inputs = {name: tensor.to(model.device) for name, tensor in inputs.items()}\n\n            # Run inference with the model\n            with torch.inference_mode():\n                if is_generative:\n                    model.generate(\n                        inputs=inputs[\"input_ids\"],\n                        generation_config=GenerationConfig(\n                            max_new_tokens=1,\n                            pad_token_id=model.config.pad_token_id,\n                            eos_token_id=model.config.eos_token_id,\n                            do_sample=False,\n                        ),\n                    )\n                else:\n                    assert isinstance(model, PreTrainedModel)\n                    model(**inputs)\n\n        base_doc = \"Document which contains roughly 10 tokens. \"\n        multiplier = 10 * (1 + itr_idx)\n        doc = base_doc * multiplier\n        short_multiplier = 1.25 * (1 + itr_idx)\n        short_doc = base_doc * round(short_multiplier)\n\n        # Do a warmup run, as the first run is always slower\n        pyinfer.InferenceReport(model=predict, inputs=base_doc, n_seconds=1).run(\n            print_report=False\n        )\n\n        speed_scores = pyinfer.InferenceReport(\n            model=predict, inputs=doc, n_seconds=3\n        ).run(print_report=False)\n        num_tokens = len(tokenizer([doc], truncation=True)[\"input_ids\"][0])\n        tokens_per_second = speed_scores[\"Infer(p/sec)\"] * num_tokens\n\n        speed_scores_short = pyinfer.InferenceReport(\n            model=predict, inputs=short_doc, n_seconds=3\n        ).run(print_report=False)\n        num_tokens_short = len(tokenizer([short_doc], truncation=True)[\"input_ids\"][0])\n        tokens_per_second_short = speed_scores_short[\"Infer(p/sec)\"] * num_tokens_short\n\n        scores[\"test\"] = dict(\n            test_speed=tokens_per_second, test_speed_short=tokens_per_second_short\n        )\n        if benchmark_config.evaluate_train:\n            scores[\"train\"] = dict(\n                train_speed=tokens_per_second, train_speed_short=tokens_per_second_short\n            )\n\n        # Return the scores\n        return scores\n\n    except (RuntimeError, ValueError, IndexError) as e:\n        return e\n</code></pre>"},{"location":"api/scandeval/structured_generation_utils/","title":"scandeval.structured_generation_utils","text":"scandeval.structured_generation_utils<p> source module scandeval.structured_generation_utils </p> <p>Utility functions related to structured generation.</p> <p> Functions </p> <ul> <li> <p>get_ner_schema \u2014 Get the schema for the NER answer format, used for structured generation.</p> </li> <li> <p>get_ner_prefix_allowed_tokens_fn \u2014 Get the prefix allowed tokens function for the NER task, used in <code>transformers</code>.</p> </li> <li> <p>get_ner_logits_processors \u2014 Get the logits processors for the NER task, used in vLLM.</p> </li> </ul> <p> source get_ner_schema(ner_tag_names: list[str]) \u2192 type[BaseModel] </p> <p>Get the schema for the NER answer format, used for structured generation.</p> <p> Parameters </p> <ul> <li> <p>ner_tag_names :  list[str] \u2014</p> <p>The NER tag names.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>type[BaseModel] \u2014 The schema for the NER answer format.</p> </li> </ul> <p> source get_ner_prefix_allowed_tokens_fn(ner_tag_names: list[str], tokenizer: PreTrainedTokenizerBase) \u2192 JSONPrefixAllowedTokens </p> <p>Get the prefix allowed tokens function for the NER task, used in <code>transformers</code>.</p> <p> Parameters </p> <ul> <li> <p>ner_tag_names :  list[str] \u2014</p> <p>The NER tag names.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizerBase \u2014</p> <p>The tokenizer to use for tokenizing the JSON Schema.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>JSONPrefixAllowedTokens \u2014 The prefix allowed tokens function for the NER task.</p> </li> </ul> <p> source get_ner_logits_processors(ner_tag_names: list[str], llm: LLM) \u2192 list[Callable[[list[int], torch.Tensor], torch.Tensor]] </p> <p>Get the logits processors for the NER task, used in vLLM.</p> <p> Parameters </p> <ul> <li> <p>ner_tag_names :  list[str] \u2014</p> <p>The NER tag names.</p> </li> <li> <p>llm :  LLM \u2014</p> <p>The vLLM model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[Callable[[list[int], torch.Tensor], torch.Tensor]] \u2014 The logit processors for the NER task.</p> </li> </ul>"},{"location":"src/scandeval/structured_generation_utils/","title":"scandeval.structured_generation_utils","text":"scandeval.structured_generation_utils<p> docs module scandeval.structured_generation_utils </p> <pre><code>\"\"\"Utility functions related to structured generation.\"\"\"\n\nimport importlib.util\nfrom typing import TYPE_CHECKING, Any, Callable\n\nimport torch\nfrom pydantic import BaseModel, conlist, create_model\n\nif importlib.util.find_spec(\"outlines\") is not None:\n    from outlines.integrations.transformers import JSONPrefixAllowedTokens\n    from outlines.integrations.vllm import JSONLogitsProcessor\n\nif TYPE_CHECKING:\n    from outlines.integrations.transformers import JSONPrefixAllowedTokens\n    from transformers import PreTrainedTokenizerBase\n    from vllm import LLM\n\n\ndef get_ner_schema(ner_tag_names: list[str]) -&gt; type[BaseModel]:docs\n    \"\"\"Get the schema for the NER answer format, used for structured generation.\n\n    Args:\n        ner_tag_names:\n            The NER tag names.\n\n    Returns:\n        The schema for the NER answer format.\n    \"\"\"\n    keys_and_their_types: dict[str, Any] = {\n        tag_name: (conlist(str, max_length=5), ...) for tag_name in ner_tag_names\n    }\n    schema = create_model(\"AnswerFormat\", **keys_and_their_types)\n    return schema\n\n\ndef get_ner_prefix_allowed_tokens_fn(docs\n    ner_tag_names: list[str], tokenizer: \"PreTrainedTokenizerBase\"\n) -&gt; \"JSONPrefixAllowedTokens\":\n    \"\"\"Get the prefix allowed tokens function for the NER task, used in `transformers`.\n\n    Args:\n        ner_tag_names:\n            The NER tag names.\n        tokenizer:\n            The tokenizer to use for tokenizing the JSON Schema.\n\n    Returns:\n        The prefix allowed tokens function for the NER task.\n    \"\"\"\n    return JSONPrefixAllowedTokens(\n        schema=get_ner_schema(ner_tag_names=ner_tag_names),\n        tokenizer_or_pipe=tokenizer,\n        whitespace_pattern=r\" ?\",\n    )\n\n\ndef get_ner_logits_processors(docs\n    ner_tag_names: list[str], llm: \"LLM\"\n) -&gt; list[Callable[[list[int], torch.Tensor], torch.Tensor]]:\n    \"\"\"Get the logits processors for the NER task, used in vLLM.\n\n    Args:\n        ner_tag_names:\n            The NER tag names.\n        llm:\n            The vLLM model.\n\n    Returns:\n        The logit processors for the NER task.\n    \"\"\"\n    logits_processor = JSONLogitsProcessor(\n        schema=get_ner_schema(ner_tag_names=ner_tag_names),\n        llm=llm,\n        whitespace_pattern=r\" ?\",\n    )\n    return [logits_processor]\n</code></pre>"},{"location":"api/scandeval/tasks/","title":"scandeval.tasks","text":"scandeval.tasks<p> source module scandeval.tasks </p> <p>All benchmarks tasks used in ScandEval.</p> <p> Functions </p> <ul> <li> <p>get_all_tasks \u2014 Get a list of all the dataset tasks.</p> </li> </ul> <p> source get_all_tasks() \u2192 dict[str, Task] </p> <p>Get a list of all the dataset tasks.</p> <p> Returns </p> <ul> <li> <p>dict[str, Task] \u2014 A mapping between names of dataset tasks and their configurations.</p> </li> </ul>"},{"location":"src/scandeval/tasks/","title":"scandeval.tasks","text":"scandeval.tasks<p> docs module scandeval.tasks </p> <pre><code>\"\"\"All benchmarks tasks used in ScandEval.\"\"\"\n\nfrom .config import MetricConfig, Task\n\n\ndef get_all_tasks() -&gt; dict[str, Task]:docs\n    \"\"\"Get a list of all the dataset tasks.\n\n    Returns:\n        A mapping between names of dataset tasks and their configurations.\n    \"\"\"\n    return {cfg.name: cfg for cfg in globals().values() if isinstance(cfg, Task)}\n\n\nLA = Task(\n    name=\"linguistic-acceptability\",\n    supertask=\"sequence-classification\",\n    metrics=[\n        MetricConfig(\n            name=\"mcc\",\n            pretty_name=\"Matthew's Correlation Coefficient\",\n            huggingface_id=\"matthews_correlation\",\n            results_key=\"matthews_correlation\",\n        ),\n        MetricConfig(\n            name=\"macro_f1\",\n            pretty_name=\"Macro-average F1-score\",\n            huggingface_id=\"f1\",\n            results_key=\"f1\",\n            compute_kwargs=dict(average=\"macro\"),\n        ),\n    ],\n    labels=[\"incorrect\", \"correct\"],\n)\n\n\nNER = Task(\n    name=\"named-entity-recognition\",\n    supertask=\"token-classification\",\n    metrics=[\n        MetricConfig(\n            name=\"micro_f1_no_misc\",\n            pretty_name=\"Micro-average F1-score without MISC tags\",\n            huggingface_id=\"seqeval\",\n            results_key=\"overall_f1\",\n        ),\n        MetricConfig(\n            name=\"micro_f1\",\n            pretty_name=\"Micro-average F1-score with MISC tags\",\n            huggingface_id=\"seqeval\",\n            results_key=\"overall_f1\",\n        ),\n    ],\n    labels=[\n        \"o\",\n        \"b-loc\",\n        \"i-loc\",\n        \"b-org\",\n        \"i-org\",\n        \"b-per\",\n        \"i-per\",\n        \"b-misc\",\n        \"i-misc\",\n    ],\n)\n\n\nRC = Task(\n    name=\"reading-comprehension\",\n    supertask=\"question-answering\",\n    metrics=[\n        MetricConfig(\n            name=\"em\",\n            pretty_name=\"Exact Match\",\n            huggingface_id=\"squad_v2\",\n            results_key=\"exact\",\n            postprocessing_fn=lambda raw_score: (raw_score, f\"{raw_score:.2f}%\"),\n        ),\n        MetricConfig(\n            name=\"f1\",\n            pretty_name=\"F1-score\",\n            huggingface_id=\"squad_v2\",\n            results_key=\"f1\",\n            postprocessing_fn=lambda raw_score: (raw_score, f\"{raw_score:.2f}%\"),\n        ),\n    ],\n    labels=[\"start_positions\", \"end_positions\"],\n)\n\n\nSENT = Task(\n    name=\"sentiment-classification\",\n    supertask=\"sequence-classification\",\n    metrics=[\n        MetricConfig(\n            name=\"mcc\",\n            pretty_name=\"Matthew's Correlation Coefficient\",\n            huggingface_id=\"matthews_correlation\",\n            results_key=\"matthews_correlation\",\n        ),\n        MetricConfig(\n            name=\"macro_f1\",\n            pretty_name=\"Macro-average F1-score\",\n            huggingface_id=\"f1\",\n            results_key=\"f1\",\n            compute_kwargs=dict(average=\"macro\"),\n        ),\n    ],\n    labels=[\"negative\", \"neutral\", \"positive\"],\n)\n\n\nSUMM = Task(\n    name=\"summarization\",\n    supertask=\"text-to-text\",\n    metrics=[\n        MetricConfig(\n            name=\"bertscore\",\n            pretty_name=\"BERTScore\",\n            huggingface_id=\"bertscore\",\n            results_key=\"f1\",\n            compute_kwargs=dict(\n                model_type=\"microsoft/mdeberta-v3-base\", device=\"auto\", batch_size=32\n            ),\n        ),\n        MetricConfig(\n            name=\"rouge_l\",\n            pretty_name=\"ROUGE-L\",\n            huggingface_id=\"rouge\",\n            results_key=\"rougeL\",\n        ),\n    ],\n    labels=[],\n)\n\n\nKNOW = Task(\n    name=\"knowledge\",\n    supertask=\"sequence-classification\",\n    metrics=[\n        MetricConfig(\n            name=\"mcc\",\n            pretty_name=\"Matthew's Correlation Coefficient\",\n            huggingface_id=\"matthews_correlation\",\n            results_key=\"matthews_correlation\",\n        ),\n        MetricConfig(\n            name=\"accuracy\",\n            pretty_name=\"Accuracy\",\n            huggingface_id=\"accuracy\",\n            results_key=\"accuracy\",\n        ),\n    ],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n)\n\n\nMCRC = Task(\n    name=\"multiple-choice-reading-comprehension\",\n    supertask=\"sequence-classification\",\n    metrics=[\n        MetricConfig(\n            name=\"mcc\",\n            pretty_name=\"Matthew's Correlation Coefficient\",\n            huggingface_id=\"matthews_correlation\",\n            results_key=\"matthews_correlation\",\n        ),\n        MetricConfig(\n            name=\"accuracy\",\n            pretty_name=\"Accuracy\",\n            huggingface_id=\"accuracy\",\n            results_key=\"accuracy\",\n        ),\n    ],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n)\n\n\nCOMMON_SENSE = Task(\n    name=\"common-sense-reasoning\",\n    supertask=\"sequence-classification\",\n    metrics=[\n        MetricConfig(\n            name=\"mcc\",\n            pretty_name=\"Matthew's Correlation Coefficient\",\n            huggingface_id=\"matthews_correlation\",\n            results_key=\"matthews_correlation\",\n        ),\n        MetricConfig(\n            name=\"accuracy\",\n            pretty_name=\"Accuracy\",\n            huggingface_id=\"accuracy\",\n            results_key=\"accuracy\",\n        ),\n    ],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n)\n\n\nTEXT_MODELLING = Task(\n    name=\"text-modelling\",\n    supertask=\"text-modelling\",\n    metrics=[\n        MetricConfig(\n            name=\"perplexity\",\n            pretty_name=\"Perplexity\",\n            huggingface_id=\"perplexity\",\n            results_key=\"mean_perplexity\",\n        )\n    ],\n    labels=[],\n)\n\n\nSPEED = Task(\n    name=\"speed\",\n    supertask=\"sequence-classification\",\n    metrics=[\n        MetricConfig(\n            name=\"speed\",\n            pretty_name=\"Tokens per second\",\n            huggingface_id=\"\",\n            results_key=\"speed\",\n            postprocessing_fn=lambda raw_score: (raw_score, f\"{raw_score:,.0f}\"),\n        ),\n        MetricConfig(\n            name=\"speed_short\",\n            pretty_name=\"Tokens per second on short documents\",\n            huggingface_id=\"\",\n            results_key=\"speed\",\n            postprocessing_fn=lambda raw_score: (raw_score, f\"{raw_score:,.0f}\"),\n        ),\n    ],\n    labels=[],\n)\n</code></pre>"},{"location":"api/scandeval/text_to_text/","title":"scandeval.text_to_text","text":"scandeval.text_to_text<p> source module scandeval.text_to_text </p> <p>Text-to-text generation benchmark dataset.</p> <p> Classes </p> <ul> <li> <p>TextToText \u2014 Text-to-text benchmark dataset.</p> </li> </ul> <p> source class TextToText() </p> <p><p>Bases : BenchmarkDataset</p></p> <p>Text-to-text benchmark dataset.</p> <p> Parameters </p> <ul> <li> <p>dataset_config \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Attributes </p> <ul> <li> <p>dataset_config \u2014</p> <p>The configuration of the dataset.</p> </li> <li> <p>benchmark_config \u2014</p> <p>The configuration of the benchmark.</p> </li> </ul>"},{"location":"src/scandeval/text_to_text/","title":"scandeval.text_to_text","text":"scandeval.text_to_text<p> docs module scandeval.text_to_text </p> <pre><code>\"\"\"Text-to-text generation benchmark dataset.\"\"\"\n\nimport logging\nimport random\nimport re\nfrom typing import TYPE_CHECKING, Any\n\nimport numpy as np\nfrom evaluate import EvaluationModule\nfrom transformers.data.data_collator import DataCollatorWithPadding\n\nfrom .benchmark_dataset import BenchmarkDataset\nfrom .exceptions import InvalidBenchmark\nfrom .generation import extract_raw_predictions\nfrom .utils import (\n    METRIC_ATTRIBUTES_TAKING_UP_MEMORY,\n    HiddenPrints,\n    clear_memory,\n    convert_prompt_to_instruction,\n    raise_if_model_output_contains_nan_values,\n)\n\nif TYPE_CHECKING:\n    from datasets.arrow_dataset import Dataset\n    from transformers import BatchEncoding, PreTrainedModel\n    from transformers.utils import ModelOutput\n\n    from .protocols import GenerativeModel, Tokenizer\n    from .types import Labels, Predictions\n\n\nlogger = logging.getLogger(__package__)\n\n\nclass TextToText(BenchmarkDataset):docs\n    \"\"\"Text-to-text benchmark dataset.\n\n    Args:\n        dataset_config:\n            The dataset configuration.\n        benchmark_config:\n            The benchmark configuration.\n\n    Attributes:\n        dataset_config:\n            The configuration of the dataset.\n        benchmark_config:\n            The configuration of the benchmark.\n    \"\"\"\n\n    def _preprocess_data(self, dataset: \"Dataset\", **kwargs) -&gt; \"Dataset\":\n        \"\"\"Preprocess a dataset by tokenizing and aligning the labels.\n\n        Args:\n            dataset:\n                The dataset to preprocess.\n            kwargs:\n                Extra keyword arguments containing objects used in preprocessing the\n                dataset.\n\n        Returns:\n            The preprocessed dataset.\n        \"\"\"\n        tokenizer: \"Tokenizer\" = kwargs[\"tokenizer\"]\n\n        def tokenise(examples: dict) -&gt; \"BatchEncoding\":\n            return tokenizer(text=examples[\"text\"], truncation=True, padding=False)\n\n        tokenised = dataset.map(\n            tokenise, batched=True, load_from_cache_file=False, keep_in_memory=True\n        )\n\n        return tokenised\n\n    def _load_data_collator(\n        self,\n        tokenizer: \"Tokenizer | None\" = None,\n        model: \"PreTrainedModel | GenerativeModel | None\" = None,\n    ):\n        \"\"\"Load the data collator used to prepare samples during finetuning.\n\n        Args:\n            tokenizer:\n                A pretrained tokenizer. Can be None if the tokenizer is not used in the\n                initialisation of the data collator. Defaults to None.\n            model:\n                A pretrained model. Can be None if the model is not used. Defaults to\n                None.\n\n        Returns:\n            The data collator.\n        \"\"\"\n        return DataCollatorWithPadding(tokenizer, padding=\"longest\")\n\n    def _compute_metrics(\n        self,\n        model_outputs_and_labels: tuple[\"Predictions\", \"Labels\"],\n        id2label: dict[int, str],\n    ) -&gt; dict[str, float]:\n        \"\"\"Compute the metrics needed for evaluation.\n\n        Args:\n            model_outputs_and_labels:\n                The first sequence contains the model outputs and the second sequence\n                contains the true labels.\n            id2label:\n                Conversion of indices to labels.\n\n        Returns:\n            A dictionary with the names of the metrics as keys and the metric values as\n            values.\n        \"\"\"\n        model_outputs, labels = model_outputs_and_labels\n\n        raise_if_model_output_contains_nan_values(model_output=model_outputs)\n\n        model_output_dtype = np.asarray(model_outputs).dtype\n        output_is_prob = model_output_dtype in [np.float16, np.float32, np.float64]\n        if output_is_prob:\n            predictions = np.asarray(model_outputs).argmax(axis=-1)\n        else:\n            predictions = model_outputs\n\n        results: dict[str, float] = dict()\n        for cfg in self.dataset_config.task.metrics:\n            metric = self._metrics[cfg.name]\n            assert isinstance(metric, EvaluationModule)\n\n            # Some metrics can be computed on hardware accelerators. In this case we\n            # start by setting the device to the same device as the model\n            if cfg.compute_kwargs.get(\"device\", None) == \"auto\":\n                cfg.compute_kwargs[\"device\"] = self.benchmark_config.device.type\n\n            while True:\n                try:\n                    with HiddenPrints():\n                        score_dict: dict[str, float] | None = metric.compute(\n                            predictions=predictions,\n                            references=labels,\n                            **cfg.compute_kwargs,\n                        )\n\n                    # Clear the cache of the BERTScorer to avoid memory leaks\n                    for attribute in METRIC_ATTRIBUTES_TAKING_UP_MEMORY:\n                        if hasattr(metric, attribute):\n                            delattr(metric, attribute)\n\n                    clear_memory()\n                    break\n                except Exception as e:\n                    # Clear the cache of the BERTScorer to avoid memory leaks\n                    if hasattr(metric, \"cached_bertscorer\"):\n                        del metric.cached_bertscorer\n                        clear_memory()\n\n                    oom_error = [\n                        \"CUDA out of memory\",\n                        \"CUDA error\",\n                        \"MPS backend out of memory\",\n                    ]\n                    if not any(error in str(e) for error in oom_error):\n                        raise InvalidBenchmark(str(e))\n\n                    if cfg.compute_kwargs.get(\"batch_size\", 1) &gt; 1:\n                        batch_size = cfg.compute_kwargs[\"batch_size\"]\n                        cfg.compute_kwargs[\"batch_size\"] = batch_size // 2\n                        logger.debug(\n                            \"Out of memory error occurred during the computation of \"\n                            f\"the metric {cfg.pretty_name}. Reducing the batch size to \"\n                            f\"{cfg.compute_kwargs['batch_size']}.\"\n                        )\n                    elif cfg.compute_kwargs.get(\"device\", \"cpu\") != \"cpu\":\n                        cfg.compute_kwargs[\"batch_size\"] = 32\n                        cfg.compute_kwargs[\"device\"] = \"cpu\"\n                        logger.debug(\n                            \"Out of memory error occurred during the computation of \"\n                            f\"the metric {cfg.pretty_name}. Moving the computation to \"\n                            \"the CPU.\"\n                        )\n                    else:\n                        raise InvalidBenchmark(str(e))\n\n            # The metric returns None if we are running on multi-GPU and the current\n            # process is not the main process\n            if score_dict is not None:\n                scores = score_dict[cfg.results_key]\n                if isinstance(scores, list):\n                    scores = sum(scores) / len(scores)\n                results[cfg.name] = scores\n\n        return results\n\n    def _extract_few_shot_examples(\n        self, train_dataset: \"Dataset\", random_seed: int\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"Extract few-shot examples from the training dataset.\n\n        Args:\n            train_dataset:\n                The training dataset.\n            random_seed:\n                The random seed to use when extracting the few-shot examples.\n\n        Returns:\n            The few-shot examples.\n        \"\"\"\n        shuffled_train = train_dataset.shuffle(seed=random_seed)\n        num_few_shots = self.dataset_config.num_few_shot_examples\n        few_shot_examples: list[dict[str, Any]] = list()\n\n        # We pick the few-shot examples one at a time rather than all at once since\n        # we're working with a bootstrapped training dataset, meaning that it will have\n        # duplicates. This ensures that we don't have any duplicates in the few-shot\n        # examples\n        while len(few_shot_examples) &lt; num_few_shots:\n            example = shuffled_train.select(range(1))[0]\n            few_shot_examples.append(example)\n            shuffled_train = shuffled_train.filter(\n                lambda x: x[\"text\"] != example[\"text\"]\n            )\n\n        random.seed(random_seed)\n        random.shuffle(few_shot_examples)\n        return few_shot_examples\n\n    def _apply_few_shot_prompt(\n        self, examples: dict, few_shot_examples: list[dict], tokenizer: \"Tokenizer\"\n    ) -&gt; dict:\n        \"\"\"Apply a few-shot prompt to the examples.\n\n        Args:\n            examples:\n                The examples to apply the prompt to.\n            few_shot_examples:\n                The examples to be included in the few-shot prompt.\n            tokenizer:\n                The tokenizer to use to encode the few-shot prompt.\n\n        Returns:\n            The examples with the few-shot prompt applied.\n        \"\"\"\n        # Build the few-shot part of the prompt\n        few_shot_prompts = [\n            self.dataset_config.prompt_template.format(\n                text=example[\"text\"].replace(\"\\n\", \" \").strip(),\n                target_text=example[\"target_text\"].replace(\"\\n\", \" \").strip(),\n            )\n            for example in few_shot_examples\n        ]\n        prompt_prefix = \"\"\n        if self.dataset_config.prompt_prefix:\n            prompt_prefix = self.dataset_config.prompt_prefix + \"\\n\\n\"\n        few_shot_prompt = prompt_prefix + \"\\n\\n\".join(few_shot_prompts)\n\n        # Add the texts from the examples to the prompts. We remove newlines from the\n        # examples as they have the special function to separate the few-shot examples\n        # from one another\n        new_prompts = [\n            self.dataset_config.prompt_template.format(\n                text=text.replace(\"\\n\", \" \").strip(), target_text=\"\"\n            ).strip()\n            for text in examples[\"text\"]\n        ]\n\n        final_prompts = [\n            few_shot_prompt + \"\\n\\n\" + new_prompt for new_prompt in new_prompts\n        ]\n\n        examples[\"text\"] = final_prompts\n\n        return examples\n\n    def _apply_instruction_prompt(self, examples: dict, tokenizer: \"Tokenizer\") -&gt; dict:\n        \"\"\"Apply an instruction prompt to the examples.\n\n        Args:\n            examples:\n                The examples to apply the prompt to.\n            tokenizer:\n                The tokenizer to use to encode the instruction prompt.\n\n        Returns:\n            The examples with the instruction prompt applied.\n        \"\"\"\n        prompts = [\n            self.dataset_config.instruction_prompt.format(\n                text=re.sub(r\"\\n+\", \"\\n\", text).strip()\n            )\n            for text in examples[\"text\"]\n        ]\n        prompts = [\n            convert_prompt_to_instruction(prompt=prompt, tokenizer=tokenizer)\n            for prompt in prompts\n        ]\n        examples[\"text\"] = prompts\n        return examples\n\n    def _extract_labels_from_generation(\n        self,\n        input_batch: dict[str, list],\n        model_output: \"ModelOutput\",\n        tokenizer: \"Tokenizer\",\n    ) -&gt; list[Any]:\n        \"\"\"Extract the predicted labels from the generated output.\n\n        Args:\n            input_batch:\n                The input batch, where the keys are the feature names and the values\n                are lists with the feature values.\n            model_output:\n                The raw generated output of the model.\n            tokenizer:\n                The tokenizer used together with the model.\n\n        Returns:\n            The predicted labels.\n        \"\"\"\n        raw_predictions = extract_raw_predictions(\n            generated_sequences=model_output.sequences, tokenizer=tokenizer\n        )\n        return raw_predictions\n</code></pre>"},{"location":"api/scandeval/types/","title":"scandeval.types","text":"scandeval.types<p> source module scandeval.types </p> <p>Types used throughout the project.</p> <p> Functions </p> <ul> <li> <p>is_list_of_int \u2014 Check if an object is a list of integers.</p> </li> <li> <p>is_list_of_list_of_int \u2014 Check if an object is a list of list of integers.</p> </li> <li> <p>is_list_of_str \u2014 Check if an object is a list of integers.</p> </li> </ul> <p> source is_list_of_int(x: Any) \u2192 TypeGuard[list[int]] </p> <p>Check if an object is a list of integers.</p> <p> Parameters </p> <ul> <li> <p>x :  Any \u2014</p> <p>The object to check.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>TypeGuard[list[int]] \u2014 Whether the object is a list of integers.</p> </li> </ul> <p> source is_list_of_list_of_int(x: Any) \u2192 TypeGuard[list[list[int]]] </p> <p>Check if an object is a list of list of integers.</p> <p> Parameters </p> <ul> <li> <p>x :  Any \u2014</p> <p>The object to check.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>TypeGuard[list[list[int]]] \u2014 Whether the object is a list of list of integers.</p> </li> </ul> <p> source is_list_of_str(x: Any) \u2192 TypeGuard[list[str]] </p> <p>Check if an object is a list of integers.</p> <p> Parameters </p> <ul> <li> <p>x :  Any \u2014</p> <p>The object to check.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>TypeGuard[list[str]] \u2014 Whether the object is a list of strings.</p> </li> </ul>"},{"location":"src/scandeval/types/","title":"scandeval.types","text":"scandeval.types<p> docs module scandeval.types </p> <pre><code>\"\"\"Types used throughout the project.\"\"\"\n\nfrom typing import Any, TypeGuard\n\nimport numpy as np\n\nScoreDict = dict[str, dict[str, float] | dict[str, list[dict[str, float]]]]\nPredictions = np.ndarray | list[str] | list[list[str]]\nLabels = np.ndarray | list[str] | list[list[str]]\n\n\ndef is_list_of_int(x: Any) -&gt; TypeGuard[list[int]]:docs\n    \"\"\"Check if an object is a list of integers.\n\n    Args:\n        x:\n            The object to check.\n\n    Returns:\n        Whether the object is a list of integers.\n    \"\"\"\n    return isinstance(x, list) and all(isinstance(i, int) for i in x)\n\n\ndef is_list_of_list_of_int(x: Any) -&gt; TypeGuard[list[list[int]]]:docs\n    \"\"\"Check if an object is a list of list of integers.\n\n    Args:\n        x:\n            The object to check.\n\n    Returns:\n        Whether the object is a list of list of integers.\n    \"\"\"\n    return (\n        isinstance(x, list)\n        and all(isinstance(i, list) for i in x)\n        and all(isinstance(j, int) for i in x for j in i)\n    )\n\n\ndef is_list_of_str(x: Any) -&gt; TypeGuard[list[str]]:docs\n    \"\"\"Check if an object is a list of integers.\n\n    Args:\n        x:\n            The object to check.\n\n    Returns:\n        Whether the object is a list of strings.\n    \"\"\"\n    return isinstance(x, list) and all(isinstance(i, str) for i in x)\n</code></pre>"},{"location":"api/scandeval/utils/","title":"scandeval.utils","text":"scandeval.utils<p> source module scandeval.utils </p> <p>Utility functions to be used in other scripts.</p> <p> Classes </p> <ul> <li> <p>HiddenPrints \u2014 Context manager which removes all terminal output.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>create_model_cache_dir \u2014 Create cache directory for a model.</p> </li> <li> <p>clear_memory \u2014 Clears the memory of unused items.</p> </li> <li> <p>enforce_reproducibility \u2014 Ensures reproducibility of experiments.</p> </li> <li> <p>is_module_installed \u2014 Check if a module is installed.</p> </li> <li> <p>block_terminal_output \u2014 Blocks libraries from writing output to the terminal.</p> </li> <li> <p>get_class_by_name \u2014 Get a class by its name.</p> </li> <li> <p>kebab_to_pascal \u2014 Converts a kebab-case string to PascalCase.</p> </li> <li> <p>internet_connection_available \u2014 Checks if internet connection is available by pinging google.com.</p> </li> <li> <p>get_special_token_metadata \u2014 Get the special token metadata for a tokenizer.</p> </li> <li> <p>get_huggingface_model_lists \u2014 Fetches up-to-date model lists from the Hugging Face Hub.</p> </li> <li> <p>model_is_generative \u2014 Check if a model is generative or not.</p> </li> <li> <p>raise_if_model_output_contains_nan_values \u2014 Raise an exception if the model output contains NaN values.</p> </li> <li> <p>should_prompts_be_stripped \u2014 Determine if we should strip the prompts for few-shot evaluation.</p> </li> <li> <p>should_prefix_space_be_added_to_labels \u2014 Determine if we should add a prefix space to the labels.</p> </li> <li> <p>get_end_of_chat_token_ids \u2014 Get the end token ID for chat models.</p> </li> <li> <p>convert_prompt_to_instruction \u2014 Make an instruction prompt conform to a chat template.</p> </li> <li> <p>scramble \u2014 Scramble a string in a bijective manner.</p> </li> <li> <p>unscramble \u2014 Unscramble a string in a bijective manner.</p> </li> <li> <p>get_model_max_length \u2014 Get the maximum context length of a model.</p> </li> </ul> <p> source create_model_cache_dir(cache_dir: str, model_id: str) \u2192 str </p> <p>Create cache directory for a model.</p> <p> Parameters </p> <ul> <li> <p>cache_dir :  str \u2014</p> <p>The cache directory.</p> </li> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The path to the cache directory.</p> </li> </ul> <p> source clear_memory() </p> <p>Clears the memory of unused items.</p> <p> source enforce_reproducibility(framework: Framework | str, seed: int = 4242) </p> <p>Ensures reproducibility of experiments.</p> <p> Parameters </p> <ul> <li> <p>framework :  Framework | str \u2014</p> <p>The framework used for the benchmarking.</p> </li> <li> <p>seed :  int \u2014</p> <p>Seed for the random number generator.</p> </li> </ul> <p> source is_module_installed(module: str) \u2192 bool </p> <p>Check if a module is installed.</p> <p>This is used when dealing with spaCy models, as these are installed as separate Python packages.</p> <p> Parameters </p> <ul> <li> <p>module :  str \u2014</p> <p>The name of the module.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool \u2014 Whether the module is installed or not.</p> </li> </ul> <p> source block_terminal_output() </p> <p>Blocks libraries from writing output to the terminal.</p> <p>This filters warnings from some libraries, sets the logging level to ERROR for some libraries, disabled tokeniser progress bars when using Hugging Face tokenisers, and disables most of the logging from the <code>transformers</code> library.</p> <p> source get_class_by_name(class_name: str | list[str], module_name: str | None = None) \u2192 Type | None </p> <p>Get a class by its name.</p> <p> Parameters </p> <ul> <li> <p>class_name :  str | list[str] \u2014</p> <p>The name of the class, written in kebab-case. The corresponding class name must be the same, but written in PascalCase, and lying in a module with the same name, but written in snake_case. If a list of strings is passed, the first class that is found is returned.</p> </li> <li> <p>module_name :  str | None \u2014</p> <p>The name of the module where the class is located. If None then the module name is assumed to be the same as the class name, but written in snake_case. Defaults to None.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Type | None \u2014 The class. If the class is not found, None is returned.</p> </li> </ul> <p> source kebab_to_pascal(kebab_string: str) \u2192 str </p> <p>Converts a kebab-case string to PascalCase.</p> <p> Parameters </p> <ul> <li> <p>kebab_string :  str \u2014</p> <p>The kebab-case string.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The PascalCase string.</p> </li> </ul> <p> source internet_connection_available() \u2192 bool </p> <p>Checks if internet connection is available by pinging google.com.</p> <p> Returns </p> <ul> <li> <p>bool \u2014 Whether or not internet connection is available.</p> </li> </ul> <p> source get_special_token_metadata(tokenizer: Tokenizer) \u2192 dict </p> <p>Get the special token metadata for a tokenizer.</p> <p> Parameters </p> <ul> <li> <p>tokenizer :  Tokenizer \u2014</p> <p>The tokenizer.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict \u2014 The special token metadata.</p> </li> </ul> <p> source get_huggingface_model_lists(languages: list[Language] | None, token: bool | str | None) \u2192 dict[str, list[str]] </p> <p>Fetches up-to-date model lists from the Hugging Face Hub.</p> <p> Parameters </p> <ul> <li> <p>languages :  list[Language] | None \u2014</p> <p>The language codes of the language to consider. If None then the models will not be filtered on language.</p> </li> <li> <p>token :  bool | str | None \u2014</p> <p>The authentication token for the Hugging Face Hub. If a boolean value is specified then the token will be fetched from the Hugging Face CLI, where the user has logged in through <code>huggingface-cli login</code>. If a string is specified then it will be used as the token.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, list[str]] \u2014 The keys are filterings of the list, which includes all language codes, including 'multilingual', as well as 'all'. The values are lists of model IDs.</p> </li> </ul> <p> source class HiddenPrints() </p> <p>Context manager which removes all terminal output.</p> <p> source model_is_generative(model: PreTrainedModel | GenerativeModel) \u2192 bool </p> <p>Check if a model is generative or not.</p> <p> Parameters </p> <ul> <li> <p>model :  PreTrainedModel | GenerativeModel \u2014</p> <p>The model to check.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool \u2014 Whether the model is generative or not.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> <li> <p>e</p> </li> </ul> <p> source raise_if_model_output_contains_nan_values(model_output: Predictions) \u2192 None </p> <p>Raise an exception if the model output contains NaN values.</p> <p> Parameters </p> <ul> <li> <p>model_output :  Predictions \u2014</p> <p>The model output to check.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>If the model output contains NaN values.</p> </li> <li> <p>NaNValueInModelOutput</p> </li> </ul> <p> source should_prompts_be_stripped(labels_to_be_generated: list[str], tokenizer: Tokenizer) \u2192 bool </p> <p>Determine if we should strip the prompts for few-shot evaluation.</p> <p>This is the case if the tokenizer needs to include the space as part of the label token. The strategy is thus to tokenize a label with a preceeding colon (as in the prompts), i.e., \": positive\", and check if the tokenization starts with the tokens of \": \". If this is the case, then we should not strip the prompts, since the tokenizer produces the whitespace token separately.</p> <p> Parameters </p> <ul> <li> <p>labels_to_be_generated :  list[str] \u2014</p> <p>The labels that are to be generated.</p> </li> <li> <p>tokenizer :  Tokenizer \u2014</p> <p>The tokenizer used to tokenize the labels.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool \u2014 Whether we should strip the prompts.</p> </li> </ul> <p> source should_prefix_space_be_added_to_labels(labels_to_be_generated: list[str], tokenizer: Tokenizer) \u2192 bool </p> <p>Determine if we should add a prefix space to the labels.</p> <p>This is the case if the prompts are stripped and the tokenizer doesn't automatically add prefix whitespaces to the labels.</p> <p> Parameters </p> <ul> <li> <p>labels_to_be_generated :  list[str] \u2014</p> <p>The labels that are to be generated.</p> </li> <li> <p>tokenizer :  Tokenizer \u2014</p> <p>The tokenizer used to tokenize the labels.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool \u2014 Whether we should add a prefix space to the labels.</p> </li> </ul> <p> source get_end_of_chat_token_ids(tokenizer: Tokenizer) \u2192 list[int] | None </p> <p>Get the end token ID for chat models.</p> <p>This is only relevant for tokenizers with a chat template.</p> <p> Parameters </p> <ul> <li> <p>tokenizer :  Tokenizer \u2014</p> <p>The tokenizer.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[int] | None \u2014 The token IDs used to end chats, or None if the tokenizer does not have a chat template.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014</p> <p>If the end-of-chat token could not be located.</p> </li> </ul> <p> source convert_prompt_to_instruction(prompt: str, tokenizer: Tokenizer) \u2192 str </p> <p>Make an instruction prompt conform to a chat template.</p> <p>Note that it is expected that the prompt has the following format:</p> <p> Parameters </p> <ul> <li> <p>prompt :  str \u2014</p> <p>The prompt.</p> </li> <li> <p>tokenizer :  Tokenizer \u2014</p> <p>The tokenizer.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The chat template formatted prompt.</p> </li> </ul> <p> source scramble(text: str) \u2192 str </p> <p>Scramble a string in a bijective manner.</p> <p> Parameters </p> <ul> <li> <p>text :  str \u2014</p> <p>The string to scramble.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The scrambled string.</p> </li> </ul> <p> source unscramble(scrambled_text: str) \u2192 str </p> <p>Unscramble a string in a bijective manner.</p> <p> Parameters </p> <ul> <li> <p>scrambled_text :  str \u2014</p> <p>The scrambled string to unscramble.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The unscrambled string.</p> </li> </ul> <p> source get_model_max_length(model: PreTrainedModel | GenerativeModel, tokenizer: Tokenizer | None = None) \u2192 int </p> <p>Get the maximum context length of a model.</p> <p> Parameters </p> <ul> <li> <p>model :  PreTrainedModel | GenerativeModel \u2014</p> <p>The model.</p> </li> <li> <p>tokenizer :  Tokenizer | None \u2014</p> <p>The tokenizer, or None if the tokenizer is not available.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum context length.</p> </li> </ul>"},{"location":"src/scandeval/utils/","title":"scandeval.utils","text":"scandeval.utils<p> docs module scandeval.utils </p> <pre><code>\"\"\"Utility functions to be used in other scripts.\"\"\"\n\nimport gc\nimport importlib\nimport importlib.util\nimport logging\nimport os\nimport random\nimport re\nimport sys\nimport warnings\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Literal, Type\n\nimport numpy as np\nimport pkg_resources\nimport requests\nimport torch\nfrom datasets.utils import disable_progress_bar\nfrom huggingface_hub import HfApi\nfrom requests.exceptions import RequestException\nfrom transformers import GenerationConfig\nfrom transformers import logging as tf_logging\n\nfrom .enums import Framework\nfrom .exceptions import InvalidModel, NaNValueInModelOutput\nfrom .languages import DA, NB, NN, NO, SV, get_all_languages\nfrom .openai_models import OpenAIModel, OpenAITokenizer\n\nif TYPE_CHECKING:\n    from huggingface_hub.hf_api import ModelInfo\n    from transformers import PreTrainedModel\n\n    from .config import Language\n    from .protocols import GenerativeModel, Tokenizer\n    from .types import Predictions\n\nlogger = logging.getLogger(__package__)\n\n\nif importlib.util.find_spec(\"ray\") is not None:\n    import ray\n\n\n# This is used as input to generative models; it cannot be a special token\nDUMMY_FILL_VALUE = 100\n\n\nGENERATIVE_MODEL_TASKS = [\n    \"text-generation\",\n    \"conversational\",\n    # \"text2text-generation\",  # TODO: Add this when we support it\n]\n\n\nGENERATIVE_DATASET_TASKS = [\n    \"knowledge\",\n    \"common-sense-reasoning\",\n    \"multiple-choice-reading-comprehension\",\n]\n\n\nGENERATIVE_DATASET_SUPERTASKS = [\"text-to-text\", \"text-modelling\"]\n\n\nSUPERTASKS_USING_LOGPROBS = [\"sequence-classification\"]\n\n\nMETRIC_ATTRIBUTES_TAKING_UP_MEMORY = [\"cached_bertscorer\"]\n\n\ndef create_model_cache_dir(cache_dir: str, model_id: str) -&gt; str:docs\n    \"\"\"Create cache directory for a model.\n\n    Args:\n        cache_dir:\n            The cache directory.\n        model_id:\n            The model ID.\n\n    Returns:\n        The path to the cache directory.\n    \"\"\"\n    # to avoid nesting due to models name containing '/'\n    _model_id = model_id.replace(\"/\", \"--\")\n    cache_dir_path = Path(cache_dir) / \"model_cache\" / _model_id\n    return str(cache_dir_path)\n\n\ndef clear_memory():docs\n    \"\"\"Clears the memory of unused items.\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    if torch.backends.mps.is_available():\n        torch.mps.empty_cache()\n\ndocs\ndef enforce_reproducibility(framework: Framework | str, seed: int = 4242):\n    \"\"\"Ensures reproducibility of experiments.\n\n    Args:\n        framework:\n            The framework used for the benchmarking.\n        seed:\n            Seed for the random number generator.\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    rng = np.random.default_rng(seed)\n    if framework in (Framework.PYTORCH, Framework.JAX):\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cudnn.deterministic = True\n        torch.use_deterministic_algorithms(True, warn_only=True)\n    return rng\n\n\ndef is_module_installed(module: str) -&gt; bool:docs\n    \"\"\"Check if a module is installed.\n\n    This is used when dealing with spaCy models, as these are installed as separate\n    Python packages.\n\n    Args:\n        module:\n            The name of the module.\n\n    Returns:\n        Whether the module is installed or not.\n    \"\"\"\n    # Get list of all modules, including their versions\n    installed_modules_with_versions = list(pkg_resources.working_set)\n\n    # Strip the module versions from the list of modules. Also make the modules lower\n    # case and replace dashes with underscores\n    installed_modules = [\n        re.sub(\"[0-9. ]\", \"\", str(module)).lower().replace(\"-\", \"_\")\n        for module in installed_modules_with_versions\n    ]\n\n    # Check if the module is installed by checking if the module name is in the list\n    return module.lower() in installed_modules\n\n\ndef block_terminal_output():docs\n    \"\"\"Blocks libraries from writing output to the terminal.\n\n    This filters warnings from some libraries, sets the logging level to ERROR for some\n    libraries, disabled tokeniser progress bars when using Hugging Face tokenisers, and\n    disables most of the logging from the `transformers` library.\n    \"\"\"\n    # Ignore miscellaneous warnings\n    warnings.filterwarnings(\"ignore\", category=UserWarning)\n    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n    warnings.filterwarnings(\n        \"ignore\",\n        module=\"torch.nn.parallel*\",\n        message=\"Was asked to gather along dimension 0, but all input tensors were \"\n        \"scalars; will instead unsqueeze and return a vector.\",\n    )\n    warnings.filterwarnings(\"ignore\", module=\"seqeval*\")\n\n    # Up the logging level, to disable outputs\n    logging.getLogger(\"filelock\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"absl\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"datasets\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"openai\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"torch.distributed.distributed_c10d\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"torch.nn.parallel.distributed\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"vllm\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"vllm.engine.llm_engine\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"vllm.transformers_utils.tokenizer\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"vllm.core.scheduler\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"vllm.model_executor.weight_utils\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"httpx\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"ray._private.worker\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"matplotlib.font_manager\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"accelerate\").setLevel(logging.CRITICAL)\n\n    # This suppresses vLLM logging\n    os.environ[\"LOG_LEVEL\"] = \"CRITICAL\"\n    os.environ[\"VLLM_CONFIGURE_LOGGING\"] = \"0\"\n\n    if importlib.util.find_spec(\"ray\") is not None:\n        ray._private.worker._worker_logs_enabled = False\n\n    # Disable the tokeniser progress bars\n    disable_progress_bar()\n\n    # Disable most of the `transformers` logging\n    tf_logging._default_log_level = logging.CRITICAL\n    tf_logging.set_verbosity(logging.CRITICAL)\n    logging.getLogger(\"transformers.trainer\").setLevel(logging.CRITICAL)\n\n\ndef get_class_by_name(docs\n    class_name: str | list[str], module_name: str | None = None\n) -&gt; Type | None:\n    \"\"\"Get a class by its name.\n\n    Args:\n        class_name:\n            The name of the class, written in kebab-case. The corresponding class name\n            must be the same, but written in PascalCase, and lying in a module with the\n            same name, but written in snake_case. If a list of strings is passed, the\n            first class that is found is returned.\n        module_name:\n            The name of the module where the class is located. If None then the module\n            name is assumed to be the same as the class name, but written in\n            snake_case. Defaults to None.\n\n    Returns:\n        The class. If the class is not found, None is returned.\n    \"\"\"\n    # Ensure that `class_name` is a sequence\n    if isinstance(class_name, str):\n        class_name = [class_name]\n\n    # Loop over the class names\n    error_messages = list()\n    for name in class_name:\n        # Get the snake_case and PascalCase version of the class name\n        name_snake = name.replace(\"-\", \"_\")\n        name_pascal = kebab_to_pascal(kebab_string=name)\n\n        # Import the module\n        try:\n            if not module_name:\n                module_name = f\"scandeval.{name_snake}\"\n            module = importlib.import_module(name=module_name)\n        except ModuleNotFoundError as e:\n            error_messages.append(str(e))\n            module_name = None\n            continue\n\n        # Get the class from the module\n        try:\n            class_: Type = getattr(module, name_pascal)\n        except AttributeError:\n            module_name = None\n            continue\n\n        # Return the class\n        return class_\n\n    if error_messages:\n        errors = \"\\n- \" + \"\\n- \".join(error_messages)\n        logger.debug(\n            f\"Could not find the class with the name(s) {', '.join(class_name)}. The \"\n            f\"following error messages were raised: {errors}\"\n        )\n\n    # If the class could not be found, return None\n    return None\n\n\ndef kebab_to_pascal(kebab_string: str) -&gt; str:docs\n    \"\"\"Converts a kebab-case string to PascalCase.\n\n    Args:\n        kebab_string:\n            The kebab-case string.\n\n    Returns:\n        The PascalCase string.\n    \"\"\"\n    return \"\".join(word.title() for word in kebab_string.split(\"-\"))\n\n\ndef internet_connection_available() -&gt; bool:docs\n    \"\"\"Checks if internet connection is available by pinging google.com.\n\n    Returns:\n        Whether or not internet connection is available.\n    \"\"\"\n    try:\n        requests.get(\"https://www.google.com\")\n        return True\n    except RequestException:\n        return False\n\n\ndef get_special_token_metadata(tokenizer: \"Tokenizer\") -&gt; dict:docs\n    \"\"\"Get the special token metadata for a tokenizer.\n\n    Args:\n        tokenizer:\n            The tokenizer.\n\n    Returns:\n        The special token metadata.\n    \"\"\"\n    # Create some test input IDs, to check if the tokenizer is adding special tokens\n    test_input_ids = tokenizer(\"Test\").input_ids\n\n    # Extract the CLS token IDs from the tokenizer, if it's using them\n    has_cls_token = True\n    if tokenizer.cls_token_id in test_input_ids:\n        cls_token_id = tokenizer.cls_token_id\n        cls_token = tokenizer.cls_token\n    elif tokenizer.bos_token_id in test_input_ids:\n        cls_token_id = tokenizer.bos_token_id\n        cls_token = tokenizer.bos_token\n    elif tokenizer.cls_token is not None:\n        cls_token_id = tokenizer.cls_token_id\n        cls_token = tokenizer.cls_token\n        has_cls_token = False\n    else:\n        cls_token_id = tokenizer.bos_token_id\n        cls_token = tokenizer.bos_token\n        has_cls_token = False\n\n    # Extract the SEP token IDs from the tokenizer, if it's using them\n    has_sep_token = True\n    if tokenizer.sep_token_id in test_input_ids:\n        sep_token = tokenizer.sep_token\n    elif tokenizer.eos_token_id in test_input_ids:\n        sep_token = tokenizer.eos_token\n    elif tokenizer.sep_token is not None:\n        sep_token = tokenizer.sep_token\n        has_sep_token = False\n    else:\n        sep_token = tokenizer.eos_token\n        has_sep_token = False\n\n    return dict(\n        cls_token_id=cls_token_id,\n        cls_token=cls_token,\n        sep_token=sep_token,\n        has_cls_token=has_cls_token,\n        has_sep_token=has_sep_token,\n    )\n\n\ndef get_huggingface_model_lists(docs\n    languages: list[\"Language\"] | None, token: bool | str | None\n) -&gt; dict[str, list[str]]:\n    \"\"\"Fetches up-to-date model lists from the Hugging Face Hub.\n\n    Args:\n        languages:\n            The language codes of the language to consider. If None then the models\n            will not be filtered on language.\n        token:\n            The authentication token for the Hugging Face Hub. If a boolean value is\n            specified then the token will be fetched from the Hugging Face CLI, where\n            the user has logged in through `huggingface-cli login`. If a string is\n            specified then it will be used as the token.\n\n    Returns:\n        The keys are filterings of the list, which includes all language codes,\n        including 'multilingual', as well as 'all'. The values are lists of model IDs.\n    \"\"\"\n    # Get list of all languages\n    all_languages = list(get_all_languages().values())\n\n    # If no languages are specified, then include all languages\n    language_list = all_languages if languages is None else languages\n\n    # Form string of languages\n    if len(language_list) == 1:\n        language_string = language_list[0].name\n    else:\n        language_list = sorted(language_list, key=lambda x: x.name)\n        if {lang.code for lang in language_list} == {\n            lang.code for lang in all_languages\n        }:\n            language_string = \"all\"\n        else:\n            # Remove generic 'Norwegian' from the list of languages if both 'Bokm\u00e5l'\n            # and 'Nynorsk' already exist in the list\n            if all([lang in language_list for lang in [NO, NB, NN]]):\n                language_list = [lang for lang in language_list if lang != NO]\n\n            language_string = (\n                f\"{', '.join(lang.name for lang in language_list[:-1])} and \"\n                f\"{language_list[-1].name}\"\n            )\n\n    # Log fetching message\n    logger.info(f\"Fetching list of {language_string} models from the Hugging Face Hub.\")\n\n    # Initialise the API\n    api: HfApi = HfApi()\n\n    # Initialise model lists\n    model_lists = defaultdict(list)\n\n    # Do not iterate over all the languages if we are not filtering on language\n    language_itr: list[\"Language | None\"]\n    if {lang.code for lang in language_list} == {lang.code for lang in all_languages}:\n        language_itr = [None]\n    else:\n        language_itr = deepcopy(language_list)  # type: ignore[arg-type]\n\n    for language in language_itr:\n        # Extract the language code\n        language_str: str | None\n        if language is not None:\n            language_str = language.code\n        else:\n            language_str = None\n\n        # Fetch the model list\n        models: list[\"ModelInfo\"] = list(\n            api.list_models(language=language_str, token=token)\n        )\n\n        # Filter the models to only keep the ones with the specified language\n        models = [\n            model\n            for model in models\n            if (language is None or language.code in model.tags)\n        ]\n\n        # Only keep the models which are not finetuned\n        models = [\n            model\n            for model in models\n            if model.pipeline_tag is None\n            or model.pipeline_tag\n            in {\"fill-mask\", \"sentence-similarity\", \"feature-extraction\"}\n            | set(GENERATIVE_MODEL_TASKS)\n        ]\n\n        # Extract the model IDs\n        model_ids: list[str] = [model.modelId for model in models if model.modelId]\n\n        # Remove models that have \"finetuned\" in their name\n        model_ids = [\n            model_id for model_id in model_ids if \"finetuned\" not in model_id.lower()\n        ]\n\n        # Store the model IDs\n        model_lists[\"all\"].extend(model_ids)\n        if language is not None:\n            model_lists[language.code].extend(model_ids)\n\n    # Add multilingual models manually\n    multi_models = [\n        \"google-bert/bert-base-multilingual-cased\",\n        \"google-bert/bert-base-multilingual-uncased\",\n        \"distilbert-base-multilingual-cased\",\n        \"distilbert/cardiffnlp/twitter-xlm-roberta-base\",\n        \"microsoft/infoxlm-base\",\n        \"microsoft/infoxlm-large\",\n        \"microsoft/xlm-align-base\",\n        \"microsoft/mdeberta-v3-base\",\n        \"setu4993/LaBSE\",\n        \"sentence-transformers/distilbert-multilingual-nli-stsb-quora-ranking\",\n        \"sentence-transformers/distiluse-base-multilingual-cased\",\n        \"sentence-transformers/distiluse-base-multilingual-cased-v1\",\n        \"sentence-transformers/distiluse-base-multilingual-cased-v2\",\n        \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n        \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n        \"sentence-transformers/paraphrase-xlm-r-multilingual-v1\",\n        \"sentence-transformers/quora-distilbert-multilingual\",\n        \"sentence-transformers/stsb-xlm-r-multilingual\",\n        \"sentence-transformers/use-cmlm-multilingual\",\n        \"studio-ousia/mluke-base\",\n        \"studio-ousia/mluke-large\",\n        \"FacebookAI/xlm-roberta-base\",\n        \"FacebookAI/xlm-roberta-large\",\n        \"dbmdz/bert-tiny-historic-multilingual-cased\",\n        \"dbmdz/bert-mini-historic-multilingual-cased\",\n        \"dbmdz/bert-base-historic-multilingual-cased\",\n        \"dbmdz/bert-medium-historic-multilingual-cased\",\n    ]\n    model_lists[\"multilingual\"] = multi_models\n    model_lists[\"all\"].extend(multi_models)\n\n    # Add fresh models\n    fresh_models = [\"fresh-xlm-roberta-base\", \"fresh-electra-small\"]\n    model_lists[\"fresh\"].extend(fresh_models)\n    model_lists[\"all\"].extend(fresh_models)\n\n    # Add some multilingual Danish models manually that have not marked 'da' as their\n    # language\n    if DA in language_itr:\n        multi_da_models: list[str] = [\n            \"Geotrend/bert-base-en-da-cased\",\n            \"Geotrend/bert-base-25lang-cased\",\n            \"Geotrend/bert-base-en-fr-de-no-da-cased\",\n            \"Geotrend/distilbert-base-en-da-cased\",\n            \"Geotrend/distilbert-base-25lang-cased\",\n            \"Geotrend/distilbert-base-en-fr-de-no-da-cased\",\n        ]\n        model_lists[\"da\"].extend(multi_da_models)\n        model_lists[\"all\"].extend(multi_da_models)\n\n    # Add some multilingual Swedish models manually that have not marked 'sv' as their\n    # language\n    if SV in language_itr:\n        multi_sv_models: list[str] = []\n        model_lists[\"sv\"].extend(multi_sv_models)\n        model_lists[\"all\"].extend(multi_sv_models)\n\n    # Add some multilingual Norwegian models manually that have not marked 'no', 'nb'\n    # or 'nn' as their language\n    if any(lang in language_itr for lang in [NO, NB, NN]):\n        multi_no_models: list[str] = [\n            \"Geotrend/bert-base-en-no-cased\",\n            \"Geotrend/bert-base-25lang-cased\",\n            \"Geotrend/bert-base-en-fr-de-no-da-cased\",\n            \"Geotrend/distilbert-base-en-no-cased\",\n            \"Geotrend/distilbert-base-25lang-cased\",\n            \"Geotrend/distilbert-base-en-fr-de-no-da-cased\",\n        ]\n        model_lists[\"no\"].extend(multi_no_models)\n        model_lists[\"all\"].extend(multi_no_models)\n\n    # Remove duplicates from the lists\n    for lang, model_list in model_lists.items():\n        model_lists[lang] = list(set(model_list))\n\n    # Remove banned models\n    BANNED_MODELS = [\n        r\"TransQuest/siamesetransquest-da.*\",\n        r\"M-CLIP/.*\",\n        r\".*/.*CTRL.*\",  # TEMP\n    ]\n    for lang, model_list in model_lists.items():\n        model_lists[lang] = [\n            model\n            for model in model_list\n            if not any(re.search(regex, model) is not None for regex in BANNED_MODELS)\n        ]\n\n    return dict(model_lists)\n\n\nclass HiddenPrints:docs\n    \"\"\"Context manager which removes all terminal output.\"\"\"\n\n    def __enter__(self):\n        \"\"\"Enter the context manager.\"\"\"\n        self._original_stdout = sys.stdout\n        self._original_stderr = sys.stderr\n        sys.stdout = open(os.devnull, \"w\")\n        sys.stderr = open(os.devnull, \"w\")\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Exit the context manager.\"\"\"\n        sys.stdout.close()\n        sys.stderr.close()\n        sys.stdout = self._original_stdout\n        sys.stderr = self._original_stderr\n\ndocs\ndef model_is_generative(model: \"PreTrainedModel | GenerativeModel\") -&gt; bool:\n    \"\"\"Check if a model is generative or not.\n\n    Args:\n        model:\n            The model to check.\n\n    Returns:\n        Whether the model is generative or not.\n    \"\"\"\n    known_generative_models = [\"VLLMModel\", \"OpenAIModel\"]\n    if any(model.__class__.__name__ == name for name in known_generative_models):\n        return True\n\n    try:\n        dummy_inputs = torch.tensor(\n            [[DUMMY_FILL_VALUE]], device=model.device, dtype=torch.long\n        )\n        generation_config = GenerationConfig(\n            max_new_tokens=1,\n            pad_token_id=model.config.pad_token_id,\n            eos_token_id=model.config.eos_token_id,\n        )\n        try:\n            model.generate(\n                inputs=dummy_inputs,\n                attention_mask=torch.ones_like(dummy_inputs),\n                generation_config=generation_config,\n            )\n        except ValueError as e:\n            if \"attention_mask\" not in str(e):\n                raise e\n            model.generate(inputs=dummy_inputs, generation_config=generation_config)\n        return True\n    except (NotImplementedError, TypeError) as e:\n        if \"PYTORCH_ENABLE_MPS_FALLBACK\" in str(e):\n            raise InvalidModel(\n                \"The benchmark failed because the environment variable \"\n                \"`PYTORCH_ENABLE_MPS_FALLBACK` is not set. Please set this \"\n                \"environment variable to `1` and try again.\"\n            )\n        logger.debug(\n            f\"The model was found not to be generative, as an error occurred: {e}\"\n        )\n        return False\n\ndocs\ndef raise_if_model_output_contains_nan_values(model_output: \"Predictions\") -&gt; None:\n    \"\"\"Raise an exception if the model output contains NaN values.\n\n    Args:\n        model_output:\n            The model output to check.\n\n    Raises:\n        If the model output contains NaN values.\n    \"\"\"\n    if isinstance(model_output, np.ndarray):\n        if model_output.dtype == np.float32 and np.isnan(model_output).any():\n            raise NaNValueInModelOutput()\n    elif len(model_output) &gt; 0:\n        if isinstance(model_output[0], str):\n            if any(x != x for x in model_output):\n                raise NaNValueInModelOutput()\n        elif len(model_output[0]) &gt; 0:\n            if any(x != x for sublist in model_output for x in sublist):\n                raise NaNValueInModelOutput()\n\n\ndef should_prompts_be_stripped(docs\n    labels_to_be_generated: list[str], tokenizer: \"Tokenizer\"\n) -&gt; bool:\n    \"\"\"Determine if we should strip the prompts for few-shot evaluation.\n\n    This is the case if the tokenizer needs to include the space as part of the label\n    token. The strategy is thus to tokenize a label with a preceeding colon (as in the\n    prompts), i.e., \": positive\", and check if the tokenization starts with the tokens\n    of \": \". If this is the case, then we should not strip the prompts, since the\n    tokenizer produces the whitespace token separately.\n\n    Args:\n        labels_to_be_generated:\n            The labels that are to be generated.\n        tokenizer:\n            The tokenizer used to tokenize the labels.\n\n    Returns:\n        Whether we should strip the prompts.\n    \"\"\"\n    strip_prompts = True\n    for label in labels_to_be_generated:\n        colon_tokens = tokenizer(\": \", add_special_tokens=False).input_ids\n        label_tokens = tokenizer(\": \" + label, add_special_tokens=False).input_ids\n\n        if isinstance(colon_tokens, torch.Tensor):\n            colon_tokens = list(colon_tokens.squeeze(0))\n        if isinstance(label_tokens, torch.Tensor):\n            label_tokens = list(label_tokens.squeeze(0))\n\n        label_tokens_start_with_colon_tokens = (\n            label_tokens[: len(colon_tokens)] == colon_tokens\n        )\n        if label_tokens_start_with_colon_tokens:\n            strip_prompts = False\n\n    return strip_prompts\n\n\ndef should_prefix_space_be_added_to_labels(docs\n    labels_to_be_generated: list[str], tokenizer: \"Tokenizer\"\n) -&gt; bool:\n    \"\"\"Determine if we should add a prefix space to the labels.\n\n    This is the case if the prompts are stripped and the tokenizer doesn't\n    automatically add prefix whitespaces to the labels.\n\n    Args:\n        labels_to_be_generated:\n            The labels that are to be generated.\n        tokenizer:\n            The tokenizer used to tokenize the labels.\n\n    Returns:\n        Whether we should add a prefix space to the labels.\n    \"\"\"\n    # We don't add a prefix space to OpenAI models, since they output strings directly,\n    # and we always strip these for token ID consistency\n    if isinstance(tokenizer, OpenAITokenizer):\n        return False\n\n    if not should_prompts_be_stripped(\n        labels_to_be_generated=labels_to_be_generated, tokenizer=tokenizer\n    ):\n        return False\n\n    whitespace_token = tokenizer.convert_ids_to_tokens(\n        ids=tokenizer(\" \", add_special_tokens=False).input_ids[0]\n    )[0]\n\n    add_prefix_space = True\n    for label in labels_to_be_generated:\n        label_tokens = tokenizer(label, add_special_tokens=False).input_ids\n        if isinstance(label_tokens, torch.Tensor):\n            label_tokens = list(label_tokens.squeeze(0))\n        first_label_token: int = int(label_tokens[0])\n        first_character_of_label = tokenizer.convert_ids_to_tokens(first_label_token)[0]\n        has_prefix_space = first_character_of_label == whitespace_token\n        if has_prefix_space:\n            add_prefix_space = False\n            break\n\n    return add_prefix_space\n\ndocs\ndef get_end_of_chat_token_ids(tokenizer: \"Tokenizer\") -&gt; list[int] | None:\n    \"\"\"Get the end token ID for chat models.\n\n    This is only relevant for tokenizers with a chat template.\n\n    Args:\n        tokenizer:\n            The tokenizer.\n\n    Returns:\n        The token IDs used to end chats, or None if the tokenizer does not have a chat\n        template.\n\n    Raises:\n        ValueError:\n            If the end-of-chat token could not be located.\n    \"\"\"\n    if tokenizer.chat_template is None:\n        return None\n\n    user_message: dict[Literal[\"role\", \"content\"], str] = dict()\n    user_message[\"role\"] = \"user\"\n    user_message[\"content\"] = \"X\"\n    token_ids = tokenizer.apply_chat_template(conversation=[user_message])\n    assert isinstance(token_ids, list)\n\n    for idx, token in enumerate(tokenizer.convert_ids_to_tokens(token_ids)):\n        token_id = tokenizer.convert_tokens_to_ids(token)\n        assert isinstance(token_id, int)\n        token = tokenizer.decode([token_id])\n        if \"X\" in token:\n            x_token_index = idx\n            break\n    else:\n        raise ValueError(\"Could not locate the end-of-chat token for the model.\")\n\n    end_of_chat_tokens = token_ids[x_token_index + 1 :]\n    if len(end_of_chat_tokens) == 0:\n        return None\n    return end_of_chat_tokens\n\ndocs\ndef convert_prompt_to_instruction(prompt: str, tokenizer: \"Tokenizer\") -&gt; str:\n    \"\"\"Make an instruction prompt conform to a chat template.\n\n    Note that it is expected that the prompt has the following format:\n\n    Args:\n        prompt:\n            The prompt.\n        tokenizer:\n            The tokenizer.\n\n    Returns:\n        The chat template formatted prompt.\n    \"\"\"\n    if tokenizer.chat_template is None:\n        return prompt\n\n    user_message: dict[Literal[\"role\"] | Literal[\"content\"], str] = dict()\n    user_message[\"role\"] = \"user\"\n    user_message[\"content\"] = prompt\n\n    instruction_prompt = tokenizer.apply_chat_template(\n        conversation=[user_message],\n        chat_template=tokenizer.chat_template,\n        add_generation_prompt=True,\n        tokenize=False,\n    )\n    assert isinstance(instruction_prompt, str)\n\n    return instruction_prompt\n\n\ndef scramble(text: str) -&gt; str:docs\n    \"\"\"Scramble a string in a bijective manner.\n\n    Args:\n        text:\n            The string to scramble.\n\n    Returns:\n        The scrambled string.\n    \"\"\"\n    rng = np.random.default_rng(seed=4242)\n    permutation = rng.permutation(x=len(text))\n    scrambled = \"\".join(text[i] for i in permutation)\n    return scrambled\n\n\ndef unscramble(scrambled_text: str) -&gt; str:docs\n    \"\"\"Unscramble a string in a bijective manner.\n\n    Args:\n        scrambled_text:\n            The scrambled string to unscramble.\n\n    Returns:\n        The unscrambled string.\n    \"\"\"\n    rng = np.random.default_rng(seed=4242)\n    permutation = rng.permutation(x=len(scrambled_text))\n    inverse_permutation = np.argsort(permutation)\n    unscrambled = \"\".join(scrambled_text[i] for i in inverse_permutation)\n    return unscrambled\n\n\ndef get_model_max_length(docs\n    model: \"PreTrainedModel | GenerativeModel\", tokenizer: \"Tokenizer | None\" = None\n) -&gt; int:\n    \"\"\"Get the maximum context length of a model.\n\n    Args:\n        model:\n            The model.\n        tokenizer:\n            The tokenizer, or None if the tokenizer is not available.\n\n    Returns:\n        The maximum context length.\n    \"\"\"\n    all_max_lengths: list[int] = list()\n\n    if tokenizer is not None:\n        # Add the registered max length of the tokenizer\n        if hasattr(tokenizer, \"model_max_length\") and tokenizer.model_max_length &lt; int(\n            1e30\n        ):\n            all_max_lengths.append(tokenizer.model_max_length)\n\n        # Add the max length derived from the model's input sizes\n        if hasattr(tokenizer, \"max_model_input_sizes\"):\n            all_max_lengths.extend(\n                [\n                    size\n                    for size in tokenizer.max_model_input_sizes.values()\n                    if size is not None\n                ]\n            )\n\n    # Add max length candidates from the model's configuration\n    candidate_config_max_lengths = [\n        \"max_position_embeddings\",\n        \"model_max_length\",\n        \"max_sequence_length\",\n        \"sliding_window\",\n        \"sliding_window_size\",\n    ]\n    for candidate_config_max_length in candidate_config_max_lengths:\n        if (\n            hasattr(model.config, candidate_config_max_length)\n            and (value := getattr(model.config, candidate_config_max_length))\n            is not None\n        ):\n            all_max_lengths.append(value)\n\n    # To avoid models having artificially low max lengths, we remove any max lengths\n    # that are less than 128\n    all_max_lengths = [\n        max_length for max_length in all_max_lengths if max_length &gt;= 128\n    ]\n\n    if len(list(all_max_lengths)) &gt; 0:\n        model_max_length = min(list(all_max_lengths))\n\n        # If the model is an OpenAI chat model then we add on 7 extra tokens, as\n        # these are part of the chat prompt and was removed from the sequence\n        # length\n        if (\n            model_max_length &gt;= 0\n            and isinstance(model, OpenAIModel)\n            and model.is_chat_model\n        ):\n            model_max_length += 7\n    else:\n        model_max_length = -1\n\n    return model_max_length\n</code></pre>"},{"location":"api/scandeval/vllm_models/","title":"scandeval.vllm_models","text":"scandeval.vllm_models<p> source module scandeval.vllm_models </p> <p>A wrapper for vLLM models.</p> <p> Classes </p> <ul> <li> <p>VLLMModel \u2014 A wrapper for vLLM models.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>clear_vllm \u2014 Clear the GPU memory used by the vLLM model, enabling re-initialisation.</p> </li> </ul> <p> source class VLLMModel(hf_model_config: PretrainedConfig, model_cache_dir: str | Path, trust_remote_code: bool, tokenizer: PreTrainedTokenizerBase | None = None, adapter_base_model_id: str | None = None) </p> <p>A wrapper for vLLM models.</p> <p>Initialize a vLLM model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>A model configuration.</p> </li> <li> <p>hf_model_config :  PretrainedConfig \u2014</p> <p>A Hugging Face model configuration.</p> </li> <li> <p>model_cache_dir :  str | Path \u2014</p> <p>The directory to cache the model in.</p> </li> <li> <p>trust_remote_code :  bool \u2014</p> <p>Whether to trust remote code, e.g., from Hugging Face.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizerBase | None \u2014</p> <p>A Hugging Face tokenizer. If None, the tokenizer will need to be loaded separately.</p> </li> <li> <p>adapter_base_model_id :  str | None \u2014</p> <p>The base model ID of the adapter model, if the model is an adapter. None otherwise.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>generate \u2014 Generate sequences using the model.</p> </li> <li> <p>set_tokenizer \u2014 Set the tokenizer to use for generation.</p> </li> <li> <p>to \u2014 Dummy method to make the model compatible with the benchmarking script.</p> </li> <li> <p>eval \u2014 Dummy method to make the model compatible with the benchmarking script.</p> </li> <li> <p>children \u2014 Dummy method to make the model compatible with the benchmarking script.</p> </li> </ul> <p> source method VLLMModel.generate(inputs: torch.Tensor, generation_config: GenerationConfig | None = None, **generation_kwargs) \u2192 torch.Tensor | torch.LongTensor | ModelOutput </p> <p>Generate sequences using the model.</p> <p> Parameters </p> <ul> <li> <p>inputs :  torch.Tensor \u2014</p> <p>The input batch of sequences to generate from.</p> </li> <li> <p>generation_config :  GenerationConfig | None \u2014</p> <p>The generation config to use for generation.</p> </li> <li> <p>**generation_kwargs \u2014</p> <p>Additional generation kwargs to pass to the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>torch.Tensor | torch.LongTensor | ModelOutput \u2014 The generated sequences.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError</p> </li> </ul> <p> source method VLLMModel.set_tokenizer(tokenizer: Tokenizer) \u2192 None </p> <p>Set the tokenizer to use for generation.</p> <p> Parameters </p> <ul> <li> <p>tokenizer :  Tokenizer \u2014</p> <p>The tokenizer to use for generation.</p> </li> </ul> <p> source method VLLMModel.to(_: torch.device) \u2192 None </p> <p>Dummy method to make the model compatible with the benchmarking script.</p> <p> source method VLLMModel.eval() \u2192 None </p> <p>Dummy method to make the model compatible with the benchmarking script.</p> <p> source method VLLMModel.children() \u2192 list </p> <p>Dummy method to make the model compatible with the benchmarking script.</p> <p> source clear_vllm() \u2192 None </p> <p>Clear the GPU memory used by the vLLM model, enabling re-initialisation.</p>"},{"location":"src/scandeval/vllm_models/","title":"scandeval.vllm_models","text":"scandeval.vllm_models<p> docs module scandeval.vllm_models </p> <pre><code>\"\"\"A wrapper for vLLM models.\"\"\"\n\nimport importlib.util\nimport logging\nimport math\nimport sys\nfrom pathlib import Path\nfrom types import MethodType\nfrom typing import TYPE_CHECKING\n\nimport torch\nfrom huggingface_hub import snapshot_download\nfrom tqdm import tqdm\nfrom transformers import GenerationConfig\nfrom transformers.utils import ModelOutput\n\nfrom .exceptions import NeedsExtraInstalled\nfrom .protocols import Tokenizer\nfrom .utils import clear_memory, get_end_of_chat_token_ids\n\nif TYPE_CHECKING:\n    from transformers import PretrainedConfig, PreTrainedTokenizerBase\n    from vllm import LLM, RequestOutput\n    from vllm.lora.request import LoRARequest\n\n    from .config import ModelConfig\n\nif importlib.util.find_spec(\"ray\") is not None:\n    import ray\n\nif importlib.util.find_spec(\"vllm\") is not None:\n    from vllm import LLM, SamplingParams\n    from vllm.lora.request import LoRARequest\n\n    try:\n        from vllm.model_executor.parallel_utils.parallel_state import (\n            destroy_model_parallel,\n        )\n    except ImportError:\n        from vllm.distributed.parallel_state import destroy_model_parallel\n\n\nlogger = logging.getLogger(__package__)\n\n\nclass VLLMModel:docs\n    \"\"\"A wrapper for vLLM models.\"\"\"\n\n    def __init__(\n        self,\n        model_config: \"ModelConfig\",\n        hf_model_config: \"PretrainedConfig\",\n        model_cache_dir: \"str | Path\",\n        trust_remote_code: bool,\n        tokenizer: \"PreTrainedTokenizerBase | None\" = None,\n        adapter_base_model_id: str | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize a vLLM model.\n\n        Args:\n            model_config:\n                A model configuration.\n            hf_model_config:\n                A Hugging Face model configuration.\n            model_cache_dir:\n                The directory to cache the model in.\n            trust_remote_code:\n                Whether to trust remote code, e.g., from Hugging Face.\n            tokenizer:\n                A Hugging Face tokenizer. If None, the tokenizer will need to be\n                loaded separately.\n            adapter_base_model_id:\n                The base model ID of the adapter model, if the model is an adapter.\n                None otherwise.\n        \"\"\"\n        self.model_config = model_config\n        self.config = hf_model_config\n        self.model_cache_dir = model_cache_dir\n        self.trust_remote_code = trust_remote_code\n        self.device = torch.device(\"cuda\")\n        self.tokenizer = tokenizer\n        self.adapter_base_model_id = adapter_base_model_id\n        self.adapter_path: str | None = None\n        self.lora_request: \"LoRARequest | None\" = None\n\n        self.max_model_len = 5_000\n        potential_max_model_length_config_names = [\n            \"max_position_embeddings\",\n            \"max_sequence_length\",\n            \"model_max_length\",\n            \"sliding_window\",\n            \"sliding_window_size\",\n            \"n_positions\",\n        ]\n        for config_name in potential_max_model_length_config_names:\n            if hasattr(hf_model_config, config_name):\n                model_len = getattr(hf_model_config, config_name)\n                if model_len is not None:\n                    self.max_model_len = min(self.max_model_len, model_len)\n\n        quantization = None\n        if hasattr(self.config, \"quantization_config\"):\n            quantization = self.config.quantization_config.get(\"quant_method\", None)\n\n        # The quantised models require extra dependencies\n        if quantization == \"gptq\" and (\n            importlib.util.find_spec(\"auto_gptq\") is None\n            or importlib.util.find_spec(\"optimum\") is None\n        ):\n            raise NeedsExtraInstalled(extra=\"quantization\")\n        if quantization == \"awq\" and importlib.util.find_spec(\"awq\") is None:\n            raise NeedsExtraInstalled(extra=\"quantization\")\n\n        dtype: str | torch.dtype = \"auto\"\n        if quantization is not None and self.config.torch_dtype != torch.float16:\n            logger.info(\n                \"You are loading a quantized model with dtype \"\n                f\"{self.config.torch_dtype}, which vLLM does not support. Setting \"\n                \"dtype to float16 instead.\"\n            )\n            dtype = torch.float16\n\n        if self.adapter_base_model_id is not None:\n            download_dir = str(Path(self.model_cache_dir) / \"base_model\")\n        else:\n            download_dir = str(self.model_cache_dir)\n\n        vllm_kwargs = dict(\n            model=self.adapter_base_model_id or self.model_config.model_id,\n            tokenizer=self.adapter_base_model_id or self.model_config.model_id,\n            gpu_memory_utilization=0.95,\n            max_model_len=self.max_model_len,\n            download_dir=download_dir,\n            trust_remote_code=self.trust_remote_code,\n            revision=self.model_config.revision,\n            seed=4242,\n            distributed_executor_backend=\"ray\",\n            tensor_parallel_size=torch.cuda.device_count(),\n            disable_custom_all_reduce=True,\n            quantization=quantization,\n            dtype=dtype,\n            enforce_eager=True,\n            max_logprobs=10,\n            # TEMP: Prefix caching isn't supported with sliding window in vLLM yet, so\n            # we disable it for now\n            enable_prefix_caching=False,\n            enable_lora=self.adapter_base_model_id is not None,\n            max_lora_rank=256,\n        )\n\n        self._model = self._initialise(vllm_kwargs=vllm_kwargs)\n\n    def _initialise(self, vllm_kwargs: dict) -&gt; \"LLM\":\n        \"\"\"Initialise the vLLM model.\n\n        Args:\n            vllm_kwargs:\n                The keyword arguments to pass to the vLLM\n\n        Returns:\n            The initialised vLLM model.\n        \"\"\"\n        clear_vllm()\n        model = LLM(**vllm_kwargs)\n        model._run_engine = MethodType(_run_engine_with_fixed_progress_bars, model)\n\n        if self.adapter_base_model_id is not None:\n            self.adapter_path = snapshot_download(\n                repo_id=self.model_config.model_id, cache_dir=Path(self.model_cache_dir)\n            )\n            self.lora_request = LoRARequest(\n                lora_name=\"adapter\", lora_int_id=1, lora_path=self.adapter_path\n            )\n\n        return model\n\n    def __del__(self) -&gt; None:\n        \"\"\"Clear the GPU memory used by the model, and remove the model itself.\"\"\"\n        if hasattr(self, \"_model\"):\n            del self._model\n        del self\n        clear_vllm()\n\n    def generate(docs\n        self,\n        inputs: torch.Tensor,\n        generation_config: GenerationConfig | None = None,\n        **generation_kwargs,\n    ) -&gt; torch.Tensor | torch.LongTensor | ModelOutput:\n        \"\"\"Generate sequences using the model.\n\n        Args:\n            inputs:\n                The input batch of sequences to generate from.\n            generation_config:\n                The generation config to use for generation.\n            **generation_kwargs:\n                Additional generation kwargs to pass to the model.\n\n        Returns:\n            The generated sequences.\n        \"\"\"\n        if self.tokenizer is None:\n            raise ValueError(\"Tokenizer must be loaded to generate sequences.\")\n\n        if generation_config is None:\n            generation_config = GenerationConfig(**generation_kwargs)\n        else:\n            for key, value in generation_kwargs.items():\n                setattr(generation_config, key, value)\n\n        # Define which tokens to use as stopping criteria. We want to use the padding\n        # token, end-of-sentence token, and a double newline (since these separate the\n        # few-shot examples in the input)\n        stop_tokens: list[str] = [\"\\n\\n\"]\n        if self.tokenizer.pad_token_id is not None:\n            stop_tokens.append(self.tokenizer.pad_token)\n        if self.tokenizer.eos_token_id is not None:\n            stop_tokens.append(self.tokenizer.eos_token)\n            if self.tokenizer.pad_token_id is None:\n                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n        if (\n            self.tokenizer.bos_token_id is not None\n            and self.tokenizer.pad_token_id is None\n        ):\n            self.tokenizer.pad_token_id = self.tokenizer.bos_token_id\n            self.tokenizer.pad_token = self.tokenizer.bos_token\n        assert self.tokenizer.pad_token_id is not None\n\n        # Add end of chat token as a stopping token, if it exists\n        end_of_chat_token_ids = get_end_of_chat_token_ids(tokenizer=self.tokenizer)\n        if end_of_chat_token_ids is not None:\n            end_of_chat_token = self.tokenizer.decode(end_of_chat_token_ids).strip()\n            if end_of_chat_token:\n                stop_tokens.append(end_of_chat_token)\n\n        # Define the parameters used for vLLM generation\n        max_tokens: int = generation_config.max_new_tokens or 1\n        temperature = (\n            0.0 if not generation_config.do_sample else generation_config.temperature\n        )\n        sampling_params = SamplingParams(\n            # What to output\n            max_tokens=max_tokens,\n            logprobs=10 if generation_config.output_scores else None,\n            n=generation_config.num_return_sequences,\n            # How to sample\n            temperature=temperature,\n            top_p=generation_config.top_p,\n            top_k=generation_config.top_k,\n            stop=[stop_token for stop_token in stop_tokens if stop_token],\n            repetition_penalty=generation_config.repetition_penalty,\n            frequency_penalty=generation_config.repetition_penalty - 1.0,\n            logits_processors=generation_kwargs.get(\"logits_processors\"),\n        )\n\n        # The inputs are tokenised, so we decode them to get the original text, which\n        # is the input to the vLLM model\n        prompts = self.tokenizer.batch_decode(\n            sequences=inputs, skip_special_tokens=True\n        )\n\n        # If any of the prompts are empty then we need to replace them with a BOS token\n        # so that the vLLM model can generate from them\n        if any(len(prompt) == 0 for prompt in prompts):\n            logger.debug(\"Found empty prompts, replacing with BOS token.\")\n            prompts = [\n                prompt if len(prompt) &gt; 0 else self.tokenizer.bos_token\n                for prompt in prompts\n            ]\n\n        # Generate sequences using vLLM\n        input_is_a_test = len(prompts) == 1 and len(set(prompts[0])) == 1\n        raw_outputs = self._model.generate(\n            prompts=prompts,\n            sampling_params=sampling_params,\n            use_tqdm=(not input_is_a_test),\n            lora_request=self.lora_request,\n        )\n\n        # Collect the generated sequences into a single tensor of shape\n        # (batch_size, generated_sequence_length)\n        output = torch.nn.utils.rnn.pad_sequence(\n            sequences=[\n                torch.LongTensor(output.outputs[0].token_ids) for output in raw_outputs\n            ],\n            batch_first=True,\n            padding_value=float(self.tokenizer.pad_token_id),\n        )\n\n        if generation_config.return_dict_in_generate:\n            # Add logprobs scores to the output\n            if generation_config.output_scores:\n                # Create a list with placeholder logprobs for every token generated.\n                # Each tensor in the list will be of shape (batch_size, vocab_size)\n                batch_size = len(raw_outputs)\n                vocab_size = len(self.tokenizer.get_vocab())\n                max_seq_len = max(\n                    len(raw_output.outputs[0].logprobs) for raw_output in raw_outputs\n                )\n                scores = [\n                    torch.full(size=(batch_size, vocab_size), fill_value=-math.inf)\n                    for _ in range(max_seq_len)\n                ]\n\n                # Fill in the logprobs for each generated token. The logprobs from the\n                # vLLM output only contain the logprobs for the top-k tokens, so we\n                # only fill in these and leave the rest at ~0% probability\n                for sample_idx, raw_output in enumerate(raw_outputs):\n                    assert raw_output.outputs[0].logprobs is not None\n                    seq_len = len(raw_output.outputs[0].logprobs)\n                    for gen_token_idx in range(seq_len):\n                        logprobs_dict = raw_output.outputs[0].logprobs[gen_token_idx]\n                        for token_idx, logprob_obj in logprobs_dict.items():\n                            logprob = logprob_obj.logprob\n                            scores[gen_token_idx][sample_idx, token_idx] = logprob\n\n                output = ModelOutput(dict(sequences=output, scores=tuple(scores)))\n            else:\n                output = ModelOutput(dict(sequences=output))\n\n        return output\n\n    def __call__(\n        self,\n        inputs: torch.Tensor,\n        generation_config: GenerationConfig | None = None,\n        **generation_kwargs,\n    ) -&gt; torch.Tensor | torch.LongTensor | ModelOutput:\n        \"\"\"Generate sequences using the model.\n\n        Args:\n            inputs:\n                The input batch of sequences to generate from.\n            generation_config:\n                The generation config to use for generation.\n            **generation_kwargs:\n                Additional generation kwargs to pass to the model.\n\n        Returns:\n            The generated sequences.\n        \"\"\"\n        return self.generate(\n            inputs=inputs, generation_config=generation_config, **generation_kwargs\n        )\n\n    def set_tokenizer(self, tokenizer: \"Tokenizer\") -&gt; None:docs\n        \"\"\"Set the tokenizer to use for generation.\n\n        Args:\n            tokenizer:\n                The tokenizer to use for generation.\n        \"\"\"\n        self.tokenizer = tokenizer\n        self._model.set_tokenizer(tokenizer)\n\n    def to(self, _: torch.device) -&gt; None:docs\n        \"\"\"Dummy method to make the model compatible with the benchmarking script.\"\"\"\n        pass\n\n    def eval(self) -&gt; None:docs\n        \"\"\"Dummy method to make the model compatible with the benchmarking script.\"\"\"\n        pass\n\n    def children(self) -&gt; list:docs\n        \"\"\"Dummy method to make the model compatible with the benchmarking script.\"\"\"\n        return []\n\n\ndef _run_engine_with_fixed_progress_bars(\n    self: \"LLM\", use_tqdm: bool\n) -&gt; list[\"RequestOutput\"]:\n    if use_tqdm:\n        num_requests = self.llm_engine.get_num_unfinished_requests()\n        pbar = tqdm(\n            total=num_requests, leave=False, disable=hasattr(sys, \"_called_from_test\")\n        )\n\n    # Run the engine.\n    outputs: list[\"RequestOutput\"] = list()\n    while self.llm_engine.has_unfinished_requests():\n        step_outputs = self.llm_engine.step()\n        for output in step_outputs:\n            if output.finished:\n                outputs.append(output)\n                if use_tqdm:\n                    pbar.update(1)\n\n    if use_tqdm:\n        pbar.close()\n\n    # Sort the outputs by request ID. This is necessary because some requests may be\n    # finished earlier than its previous requests.\n    outputs = sorted(outputs, key=lambda x: int(x.request_id))\n\n    return outputs\n\n\ndef clear_vllm() -&gt; None:docs\n    \"\"\"Clear the GPU memory used by the vLLM model, enabling re-initialisation.\"\"\"\n    try:\n        destroy_model_parallel()\n    except ImportError:\n        pass\n    clear_memory()\n    ray.shutdown()\n</code></pre>"}]}
# üáµüáπ Portuguese

This is an overview of all the datasets used in the European Portuguese part of EuroEval. The
datasets are grouped by their task - see the [task overview](/tasks) for more
information about what these constitute.

## Sentiment Classification

### Unofficial: SST-2 PT

This dataset was published in [this paper](https://arxiv.org/abs/2404.05333) and is part of the extraglue dataset. It is created by taking the original SST-2 dataset and using machine translation (DeepL) to translate it.

The original dataset contains 67,300 training, 872 validation, and 1,820 test samples. We use 1,024 / 256 / 2,048 samples for train / val / test respectively. Given that the original validation dataset only has 1,820 sample for testing, we derive that split from the training split, while ensuring no overlaps occur. This dataset only includes positive and negative labels, no neutrals.

Here are a few examples from the training split:

```json
{
  "text": "um drama psicol√≥gico absorvente e inquietante .",
  "label": "positive"
}
```

```json
{
  "text": "tudo o que n√£o se pode suportar",
  "label": "negative"
}
```

```json
{
  "text": "m√° escrita",
  "label": "negative"
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 12
- Prefix prompt:
  ```
  Abaixo encontras documentos e os seus sentimentos correspondentes, que podem ser 'positivo' ou 'negativo'.
  ```
- Base prompt template:
  ```
  Documento: {text}
  Sentimento: {label}
  ```
- Instruction-tuned prompt template:

  ```
  Texto: {text}

  Clasifica o sentimento do documento. Responde apenas com 'positivo' ou 'negativo'.
  ```

You can evaluate this dataset directly as follows:

```bash
$ euroeval --model <model-id> --dataset sst2-pt
```

## Named Entity Recognition

## HAREM-pt

This dataset is based on the [Primeiro HAREM](https://www.linguateca.pt/harem/) evaluation campaign for **Portuguese from Portugal**, using the manually annotated **Colec√ß√£o Dourada**.

We extract only documents where `<ORIGEM>` is `PT`, i.e., of **Portuguese origin**. The raw XML annotations are parsed and converted to token-level BIO labels. Tags are mapped to standard CoNLL categories:

- `PER` (pessoa)
- `LOC` (local)
- `ORG` (organiza√ß√£o)
- `MISC` (diverso)

Labels follow the standard CoNLL BIO scheme with numeric encoding:

```python
{
  "O": 0,
  "B-PER": 1,
  "I-PER": 2,
  "B-ORG": 3,
  "I-ORG": 4,
  "B-LOC": 5,
  "I-LOC": 6,
  "B-MISC": 7,
  "I-MISC": 8
}
```

In addition to tokenization and label alignment, each document is split into individual sentences, using punctuation-based heuristics. This makes the dataset better suited for sentence-level inference and generation.

Due to the limited number of PT-origin documents (1,965 examples total), we couldn‚Äôt reach the target of 2,304 (1,024 + 256 + 1,024). The final split is:

- Train: 873 examples
- Validation: 218 examples
- Test: 874 examples


```json
{
  "tokens": array(["Na", "Covilh√£", "ainda", "n√£o", "havia", "liceu", "nessa", "altura", "."], dtype=object),
  "labels": array([0, 5, 0, 0, 0, 0, 0, 0, 0], dtype=object)
}
```
```json
{
 "tokens": array(["Por", "exemplo", ",", "em", "Filosofia", "est√°", "muito", "boa", "."], dtype=object),
  "labels": array([0, 0, 0, 0, 7, 0, 0, 0, 0], dtype=object)
}
```
```json
{
  "tokens": array(["Sabe", "qual", "a", "origem", "da", "sua", "fam√≠lia", "?"], dtype=object),
  "labels": array([0, 0, 0, 0, 0, 0, 0, 0], dtype=object)
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 8
- Prefix prompt:
  ```
  Seguem-se frases e dicion√°rios JSON com as entidades mencionadas presentes na frase indicada.
  ```
- Base prompt template:
  ```
  Frase: {text}
  Entidades mencionadas: {label}
  ```
- Instruction-tuned prompt template:
  ```
  Frase: {text}

  Identifica as entidades mencionadas na frase. Deves devolver um dicion√°rio JSON com as chaves 'pessoa', 'organiza√ß√£o', 'local' e 'diverso' . Os valores devem ser listas contendo as entidades mencionadas desse tipo, tal como ocorrem na frase.
  ```
- Label mapping:
    - `B-PER` ‚û°Ô∏è `pessoa`
    - `I-PER` ‚û°Ô∏è `pessoa`
    - `B-LOC` ‚û°Ô∏è `local`
    - `I-LOC` ‚û°Ô∏è `local`
    - `B-ORG` ‚û°Ô∏è `organiza√ß√£o`
    - `I-ORG` ‚û°Ô∏è `organiza√ß√£o`
    - `B-MISC` ‚û°Ô∏è `diverso`
    - `I-MISC` ‚û°Ô∏è `diverso`

You can evaluate this dataset directly as follows:

```bash
$ euroeval --model <model-id> --dataset harem-pt
```

## Linguistic Acceptability

### ScaLA-pt

This dataset is a Portuguese version of ScaLA, created by corrupting grammatically correct sentences from the [Universal Dependencies Portuguese-Bosque treebank](https://github.com/UniversalDependencies/UD_Portuguese-Bosque), filtered to only include samples from the European Portuguese source *CETEMP√∫blico*. The treebank is based on the Constraint Grammar conversion of the Bosque corpus, part of the Floresta Sint√°(c)tica treebank.

Corruptions were applied by either **removing a word** from the sentence or **swapping two neighbouring words**. Rules based on part-of-speech tags were used to ensure that these corruptions lead to grammatical errors.

The final dataset contains:

- **Training set**: 1,024 examples
- **Validation set**: 256 examples
- **Test set**: 2,048 examples

These splits are used as-is in the framework.

Here are a few examples from the training split:

```json
{
    "text": "Nos Em os mercados orientais, T√≥quio foi a excep√ß√£o e, ao o meio da de a manh√£, a bolsa tendia para uma alta marginal, com o √≠ndice Nikkei a marcar 12,07 pontos no em o fim da de a sess√£o da de a manh√£.",
    "label": "incorrect"
}
```
```json
{
    "text": "A equipa est√° a mostrar progressos, mas ainda h√° muito para fazer.",
    "label": "correct"
}
```
```json
{
    "text": "V√°rios estudos t√™m mostrado que estes linfomas regridem depois de tratamentos dirigidos √† a HP a, o que sugere uma rela√ß√£o entre os dois.",
    "label": "incorrect"
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 12
- Prefix prompt:
  ```
  Seguem-se abaixo textos e se s√£o gramaticalmente corretos.
  ```
- Base prompt template:
  ```
    Texto: {text}
    Gramaticalmente correcto: {label}
  ```
- Instruction-tuned prompt template:
  ```
    Texto: {text}

    Determina se o texto √© gramaticalmente correcto ou n√£o. Responde com 'sim' ou 'n√£o', e nada mais.
  ```
- Label mapping:
    - `correct` ‚û°Ô∏è `sim`
    - `incorrect` ‚û°Ô∏è `n√£o`

You can evaluate this dataset directly as follows:

```bash
$ euroeval --model <model-id> --dataset scala-pt

## Reading Comprehension

### Unofficial: BoolQ-PT

This dataset was published in [this paper](https://arxiv.org/abs/2404.05333) and is part of the extraglue dataset. It is created by taking the original BoolQ dataset and using machine translation (DeepL) to translate it.

The original dataset has a passage, question, and yes/no label. We adapt this dataset by taking the original passage, question, and yes/no options, and turning it into a Q/A style question where the model can answer yes or no.

The original dataset contains 9,430 training, 3,270 validation, and 3,250 test samples. We use 1,024 / 256 / 2,048 samples for train / val / test respectively. We've observed some overlap in the splits, so decided to concatenate all splits into a single dataset, shuffling it, and extract splits.

Here are a few examples from the training split:

```json
{
  "text": "Texto: Animais Fant√°sticos e Onde Encontr√°-los -- Fantastic Beasts and Where to Find Them √© um livro de 2001 escrito pela autora brit√¢nica J.K. Rowling (sob o pseud√≥nimo do autor fict√≠cio Newt Scamander) sobre as criaturas m√°gicas do universo Harry Potter. A vers√£o original, ilustrada pela pr√≥pria autora, pretende ser a c√≥pia de Harry Potter do livro did√°tico com o mesmo nome mencionado em Harry Potter e a Pedra Filosofal (ou Harry Potter and the Sorcerer's Stone nos EUA), o primeiro romance da s√©rie Harry Potter. Inclui v√°rias notas no seu interior, supostamente escritas √† m√£o por Harry, Ron Weasley e Hermione Granger, detalhando as suas pr√≥prias experi√™ncias com algumas das bestas descritas e incluindo piadas relacionadas com a s√©rie original.\nPergunta: Animais fant√°sticos e onde encontr√°-los est√° relacionado com Harry Potter?\nOp√ß√µes:\na. sim\nb. n√£o",
  "label": "a"
}
```

```json
{
  "text": "Texto: Oceano Ant√°rtico -- O Oceano Ant√°rtico, tamb√©m conhecido como Oceano Ant√°rtico ou Oceano Austral, compreende as √°guas mais a sul do Oceano Mundial, geralmente consideradas a sul de 60¬∞ de latitude sul e circundando a Ant√°rctida. Como tal, √© considerado como a quarta maior das cinco principais divis√µes oce√¢nicas: mais pequeno do que os oceanos Pac√≠fico, Atl√¢ntico e √çndico, mas maior do que o oceano √Årtico. Esta zona oce√¢nica √© o local onde as √°guas frias da Ant√°rctida, que fluem para norte, se misturam com as √°guas subant√°rcticas, mais quentes.\nPergunta: Existe um oceano chamado oceano Austral?\nOp√ß√µes:\na. sim\nb. n√£o",
  "label": "a"
}
```

```json
{
  "text": "Texto: Lista dos votos de desempate dos vice-presidentes dos Estados Unidos -- O vice-presidente dos Estados Unidos √© o presidente ex officio do Senado, como previsto no artigo I, sec√ß√£o 3, cl√°usula 4, da Constitui√ß√£o dos Estados Unidos, mas s√≥ pode votar para desempatar. De acordo com o Senado dos Estados Unidos, at√© 28 de fevereiro de 2018, o voto de desempate foi dado 264 vezes por 36 vice-presidentes.\nPergunta: O vice-presidente j√° desempatou alguma vez no Senado?\nOp√ß√µes:\na. sim\nb. n√£o"
  "label": "a"
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 5
- Prefix prompt:
  ```
  As seguintes s√£o perguntas de escolha m√∫ltipla (com respostas).
  ```
- Base prompt template:
  ```
  Pergunta: {text}
  Op√ß√µes:
  a. {option_a}
  b. {option_b}
  Resposta: {label}
  ```
- Instruction-tuned prompt template:

  ```
  Pergunta: {text}
  Op√ß√µes:
  a. {option_a}
  b. {option_b}

  Responde √† pergunta acima usando s√≥ 'a' ou 'b', e nada mais.
  ```

You can evaluate this dataset directly as follows:

```bash
$ euroeval --model <model-id> --dataset boolq-pt
```

## Knowledge

### MMLU-pt

This dataset is a subset of [MMLUx](https://huggingface.co/datasets/LumiOpen/opengpt-x_mmlux). Originally from openGPT-X/mmlux.

The original full dataset consists of 270 / 1,439 / 14,774 samples for training, validation, and testing, respectively. As this is not expected from EuroEval, we merged them, removed any duplicates, and then created new splits with 256 / 2048 / 1024 samples for validation, test, and training, respectively.

Here are a few examples from the training split:

```json
{
  "text": "De que tipo de direitos gozam os Estados costeiros sobre a sua plataforma continental?\nOp√ß√µes:\na. O Estado costeiro goza ipso facto e ab initio de direitos soberanos sobre a sua plataforma continental para efeitos de explora√ß√£o e aproveitamento dos seus recursos naturais\nb. O Estado costeiro s√≥ pode exercer direitos soberanos sobre a sua plataforma continental mediante declara√ß√£o\nc. O Estado costeiro exerce direitos soberanos sobre a sua plataforma continental para efeitos de explora√ß√£o dos seus recursos hali√™uticos\nd. O Estado costeiro s√≥ pode exercer direitos limitados sobre a sua plataforma continental e apenas com o consentimento dos Estados vizinhos",
  "label": "a"
}
```

```json
{
  "text": "Qual delas n√£o √© uma compet√™ncia-chave reconhecida da gest√£o?\nOp√ß√µes:\na. Compet√™ncias conceptuais\nb. Compet√™ncias humanas\nc. Compet√™ncias t√©cnicas\nd. Compet√™ncias de reda√ß√£o",
  "label": "d"
}
```

```json
{
    "text": "O presidente executa um "veto de bolso" fazendo qual das seguintes op√ß√µes?\nOp√ß√µes:\na. Manifestando publicamente a rejei√ß√£o de um projeto de lei\nb. Emitindo uma ordem executiva que invalida um projeto de lei recentemente aprovado\nc. N√£o assinando um projeto de lei ap√≥s o encerramento do Congresso\nd. Retirando embaixadores de uma negocia√ß√£o de paz",
    "label": "c",
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 5
- Prefix prompt:
  ```
  Las siguientes son preguntas de opci√≥n m√∫ltiple (con respuestas).
  ```
- Base prompt template:
  ```
  Pergunta: {text}
  Op√ß√µes:
  a. {option_a}
  b. {option_b}
  c. {option_c}
  d. {option_d}
  Resposta: {label}
  ```
- Instruction-tuned prompt template:

  ```
  Pergunta: {text}
  Op√ß√µes:
  a. {option_a}
  b. {option_b}
  c. {option_c}
  d. {option_d}

  Responde √† pergunta acima usando s√≥ 'a' ou 'b', 'c' ou 'd', e nada mais.
  ```

You can evaluate this dataset directly as follows:

```bash
$ euroeval --model <model-id> --dataset mmlu-pt
```

## Common-sense Reasoning

### HellaSwag-pt

This dataset is a subset of [HellaswagX](https://huggingface.co/datasets/LumiOpen/opengpt-x_goldenswagx). Originally from openGPT-X/hellaswagx.

The original full dataset consists of 1530 / 1530 samples for training and validation, respectively. However, they are exactly equal. For EuroEval, we use a split of 660 / 256 / 2.05k samples for training, validation, and testing, respectively.

Here are a few examples from the training split:

```json
{
  "text": "Como fazer com que o seu namorado √† dist√¢ncia se sinta especial. Escreva uma carta de amor √† moda antiga para enviar por correio normal. Embora seja poss√≠vel enviar um e-mail instantaneamente, receber um pacote ou uma carta pelo correio √© um esfor√ßo muito mais √≠ntimo e sincero. As cartas tamb√©m criam uma recorda√ß√£o que n√£o pode ser feita por correio eletr√≥nico.\nOp√ß√µes:\na. N√£o se preocupe em escrever o poema perfeito ou algo profundo, o facto de se ter esfor√ßado por escrever √© suficiente. Pode fazer um desenho, encontrar um cart√£o pr√©-fabricado ou at√© enviar um postal de um local especial.\nb. Considere a possibilidade de criar um √°lbum de recortes com as notas do seu casamento como forma de surpreender o seu namorado com flores, um colar sentido ou at√© uma caixa com os brinquedos favoritos dele. A carta ir√° acompanhar a maioria dos filmes favoritos dele, dos quais voc√™ e o seu homem gostam de falar.\nc. Numa carta, escrevem-se palavras que v√£o at√© ao cora√ß√£o da pessoa. Se quiser enganar algu√©m para que lhe conte um pequeno segredo que lhe contou, tem de ter cuidado.\nd. Escreva-o em sil√™ncio, n√£o em voz alta e clara, e pe√ßa ao destinat√°rio que o leia duas vezes. Utilize a linha de assunto para explicar a raz√£o pela qual est√° a escrever ao seu namorado.",
  "label": "a"
}
```

```json
{
  "text": "Como cultivar inhame. Comece a cultivar os rebentos. Os inhames n√£o s√£o cultivados a partir de sementes como a maioria dos outros vegetais - eles crescem a partir de estacas, que s√£o derivadas dos rebentos de inhames adultos. Para fazer crescer os rebentos, corte um inhame ao meio e mergulhe uma das partes num copo de √°gua fria.\nOp√ß√µes:\na. Mesmo antes de as plantas come√ßarem a brotar, escave um peda√ßo do caule e coloque-o debaixo da √°gua para que fique nivelado com o fundo do copo. Repita este processo at√© ter cerca de 5 cm de caule.\nb. A meio do processo de imers√£o, feche a outra metade num balde de √°gua comercial. Pense em usar latas, baldes tupperware e outros recipientes que sejam grandes o suficiente para conter v√°rios inhames de uma vez.\nc. Voc√™ deve ver as sementes brotarem. Se n√£o conseguir, corte pequenas sec√ß√µes e mantenha os rebentos no copo de √°gua fria.\nd. Insira palitos de dentes em tr√™s pontos √† volta do meio do inhame e suspenda-o sobre o recipiente, meio submerso na √°gua. Certifique-se de que o inhame escolhido tem um aspeto saud√°vel.",
  "label": "d"
}
```

```json
{
"text": "Como detetar o pl√°gio. Utilize aplica√ß√µes online gratuitas que n√£o requerem subscri√ß√µes ou inscri√ß√µes para verificar documentos electr√≥nicos. Pesquise no Google "verificador de pl√°gio" para encontrar uma s√©rie de aplica√ß√µes Web gratuitas que cont√™m caixas onde pode colar o texto suspeito. Carregue no bot√£o verificar e deixe que a aplica√ß√£o analise a Internet em busca de inst√¢ncias de texto duplicado.\nOp√ß√µes:\na. Qualquer coisa que apare√ßa indica que est√° a utilizar uma destas aplica√ß√µes gratuitas. Normalmente, √© necess√°rio iniciar sess√£o no in√≠cio da aplica√ß√£o.\nb. Cuidado! Utilizar os motores de busca para descobrir alguns sites oficiais de educa√ß√£o e classific√°-los como "falsos". Exemplo: ' math problem manuscript for mr.\nc. Se quiser converter pdfs em texto, pode faz√™-lo. Algu√©m que entregue um documento pdf, embora n√£o seja inerentemente suspeito, pode ser um sinal de que est√° a tentar evitar ser apanhado.\nd. Aparecer√° uma janela de teste a perguntar se precisa de uma aplica√ß√£o de pesquisa. Se n√£o precisar, escolha google ' anti-pasteuriza√ß√£o.",
"label": "c",
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 5
- Prefix prompt:
  ```
  Las siguientes son preguntas de opci√≥n m√∫ltiple (con respuestas).
  ```
- Base prompt template:
  ```
  Pergunta: {text}
  Op√ß√µes:
  a. {option_a}
  b. {option_b}
  c. {option_c}
  d. {option_d}
  Resposta: {label}
  ```
- Instruction-tuned prompt template:

  ```
  Pergunta: {text}
  Op√ß√µes:
  a. {option_a}
  b. {option_b}
  c. {option_c}
  d. {option_d}

  Responde √† pergunta acima usando s√≥ 'a' ou 'b', 'c' ou 'd', e nada mais.
  ```

You can evaluate this dataset directly as follows:

```bash
$ euroeval --model <model-id> --dataset hellaswag-pt
```

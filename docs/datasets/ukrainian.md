# üá∫üá¶ Ukrainian

This is an overview of all the datasets used in the Ukrainian part of EuroEval. The
datasets are grouped by their task - see the [task overview](/tasks) for more
information about what these constitute.

## Sentiment Classification

### Cross-Domain UK Reviews

The dataset can be found [here](https://huggingface.co/datasets/vkovenko/cross_domain_uk_reviews).
The data is scrapped from [Tripadvisor](https://www.tripadvisor.com/) and [Rozetka](https://rozetka.com.ua/).

The [original dataset](https://huggingface.co/datasets/vkovenko/cross_domain_uk_reviews/blob/main/processed_data.csv)
contains 611,863 samples. We use 1,024 / 256 / 2,048 samples for our training,
validation and test splits, respectively.

Here are a few examples from the training split:

```json
{
    "text": "—è–∫ —ñ –≤—Å—ñ Mc Donalds, —è–∫—ñ—Å—Ç—å –¥—É–∂–µ –Ω–∏–∑—å–∫–∞, –∞–ª–µ —Ä–∞—Ö—É–Ω–æ–∫ –≤–∏—Å–æ–∫–∏–π –∑–∞ —Ç–µ, —â–æ –≤–∏ —ó—Å—Ç–µ. . —à–∫–æ–¥–∞, –∞–ª–µ –Ω–µ –¥–æ—Ö–æ–¥–∏—Ç—å –¥–æ –¥–æ—Å—Ç–∞—Ç–Ω–æ—Å—Ç—ñ",
    "label": "negative"
}
```

```json
{
    "text": "–ü–æ—Å—É–¥–æ–º–∏–π–Ω–æ—é –º–∞—à–∏–Ω–æ—é –∫–æ—Ä–∏—Å—Ç—É—é—Å—å –¥–∞–≤–Ω–æ, —Ä–æ–±–æ—Ç–æ—é —Ü—ñ–ª–∫–æ–º –∑–∞–æ–≤–æ–ª–µ–Ω–∞. –ü—Ä–∞—Ü—é—î –¥—É–∂–µ —Ç–∏—Ö–æ —ñ –ø—Ä–µ–∫—Ä–∞—Å–Ω–æ —Å–ø—Ä–∞–≤–ª—è—î—Ç—å—Å—è –∑ –∑–∞–±—Ä—É–¥–Ω–µ–Ω–∏–º –ø–æ—Å—É–¥–æ–º. –í–º—ñ—â–∞—î –≤ —Å–µ–±–µ 12 –∫–æ–º–ø–ª–µ–∫—Ç—ñ–≤ –ø–æ—Å—É–¥—É.",
    "label": "positive"
}
```

```json
{
    "text": "–ó—É–ø–∏–Ω–∏–ª–∏—Å—è –≤ –≥–æ—Ç–µ–ª—ñ –≤ –ª–∏–ø–Ω—ñ 2021 —Ä–æ–∫—É –∑ —Å—ñ–º'—î—é ( 4 –ª—é–¥–∏–Ω–∏ ) , –Ω–æ–º–µ—Ä –±—É–≤ –æ–±—Ä–∞–Ω–∏–π –∑–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º–∏ –ª—é–∫—Å . –£ –Ω–æ–º–µ—Ä—ñ –ø—Ä–æ—Å—Ç–æ—Ä–æ —ñ —á–∏—Å—Ç–æ . –ü—Ä–∏ –±—Ä–æ–Ω—é–≤–∞–Ω–Ω—ñ –≤–æ–Ω–∏ –ø–æ–ø—Ä–æ—Å–∏–ª–∏ –≤–∏–∫–ª–∞—Å—Ç–∏ –¥–∏–≤–∞–Ω , —â–æ —ñ –±—É–ª–æ –∑—Ä–æ–±–ª–µ–Ω–æ . –£ –≤–∞–Ω–Ω—ñ–π –∫—ñ–º–Ω–∞—Ç—ñ –±—É–ª–∏ –≤—Å—ñ –≤–∏—Ç—Ä–∞—Ç–Ω—ñ –º–∞—Ç–µ—Ä—ñ–∞–ª–∏ —Ç–∞ —Ä—É—à–Ω–∏–∫–∏ , –≤ –¥–æ—Å—Ç–∞—Ç–Ω—ñ–π –∫—ñ–ª—å–∫–æ—Å—Ç—ñ . –£ –Ω–æ–º–µ—Ä—ñ —î –Ω–µ–≤–µ–ª–∏–∫–∏–π —Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫ , —Å–µ–π—Ñ . –ê–ª–µ —Ä–æ–∑—á–∞—Ä–æ–≤–∞–Ω–∏–π –°–ù–Ü–î–ê–ù–û–ö . –û–≥–æ–ª–æ—à–µ–Ω–∏–π —Å–Ω—ñ–¥–∞–Ω–æ–∫ —à–≤–µ–¥—Å—å–∫–∏–π —Å—Ç—ñ–ª –±—É–≤ , –∞–ª–µ —Ü–µ –±—É–ª–æ –ø–æ–≤–Ω–µ —Ä–æ–∑—á–∞—Ä—É–≤–∞–Ω–Ω—è . –í–∏–±–∞—á—Ç–µ , –∞–ª–µ –º–æ–∂–Ω–∞ –∑—Ä–æ–±–∏—Ç–∏ –±—ñ–ª—å—à —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–∏–º —ñ –∫–æ—Ä–∏—Å–Ω–∏–º , –±–µ–∑ –º–∞–π–æ–Ω–µ–∑—É –Ω–∞ –æ–≤–æ—á–∞—Ö ? ? . –º—É—Ö–∏ –ª—ñ—Ç–∞–ª–∏ —ñ –±—É–ª–æ –Ω–µ–ø—Ä–∏—î–º–Ω–æ –ø–µ—Ä–µ–±—É–≤–∞—Ç–∏ –≤ –ø—Ä–∏–º—ñ—â–µ–Ω–Ω—ñ .",
    "label": "neutral"
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 12
- Prefix prompt:

  ```text
  –ù–∏–∂—á–µ –Ω–∞–≤–µ–¥–µ–Ω—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏ —ñ —ó—Ö –Ω–∞—Å—Ç—Ä—ñ–π, —è–∫–∏–π –º–æ–∂–µ –±—É—Ç–∏ '–ø–æ–∑–∏—Ç–∏–≤–Ω–∏–π', '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∏–π' –∞–±–æ '–Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π'.
  ```

- Base prompt template:

  ```text
  –î–æ–∫—É–º–µ–Ω—Ç: {text}
  –ù–∞—Å—Ç—Ä—ñ–π: {label}
  ```

- Instruction-tuned prompt template:

  ```text
  –î–æ–∫—É–º–µ–Ω—Ç: {text}

  –ö–ª–∞—Å–∏—Ñ—ñ–∫—É–π—Ç–µ –Ω–∞—Å—Ç—Ä—ñ–π —É –¥–æ–∫—É–º–µ–Ω—Ç—ñ. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π—Ç–µ '–ø–æ–∑–∏—Ç–∏–≤–Ω–∏–π', '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∏–π', –∞–±–æ '–Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π', —ñ –Ω—ñ—á–æ–≥–æ –±—ñ–ª—å—à–µ.
  ```

- Label mapping:
  - `positive` ‚û°Ô∏è `–ø–æ–∑–∏—Ç–∏–≤–Ω–∏–π`
  - `neutral` ‚û°Ô∏è `–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∏–π`
  - `negative` ‚û°Ô∏è `–Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π`

You can evaluate this dataset directly as follows:

```bash
euroeval --model <model-id> --dataset cross-domain-uk-reviews
```

## Named Entity Recognition

### NER-uk

The dataset can be found [here](https://github.com/lang-uk/ner-uk).
The dataset primarily consists of text from the
[Open Corpus of Ukrainian Texts](https://github.com/brown-uk/corpus).

The original dataset consists of 10,833 / 668 / 1,307 samples for the
training, validation, and test splits, respectively. We use 1,024 / 256 / 2,048
samples for our training, validation and test splits, respectively. The train and
validation splits are subsets of the original splits, while the test split is
created using additional samples from the train split.

Here are a few examples from the training split:

```json
{
  "tokens": ["–•–æ—á–∞", "–Ω–µ–ø—Ä–æ—Å—Ç–æ", "–ø—Ä–æ", "–Ω–µ—ó", "—Ä–æ–∑–ø–æ–≤—ñ—Å—Ç–∏", "¬ª", ".", "–í–µ–¥–º—ñ–¥—å", "–∑–∞–º–æ–≤–∫", ",", "–ø–æ–¥–∏–≤–∏–≤—Å—è", "–Ω–∞", "–¥—Ä—É–∑—ñ–≤", ",", "—è–∫—ñ", "—É–≤–∞–∂–Ω–æ", "–π–æ–≥–æ", "—Å–ª—É—Ö–∞–ª–∏", ",", "—ñ", "–∑–∞–ø–∏—Ç–∞–≤", ":"],
  "labels": ["O", "O", "O", "O", "O", "O", "O", "B-PER", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"]
}
```

```json
{
  "tokens": ["–ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∏–π", "–º–∞—Ç–µ—Ä—ñ–∞–ª", "–±—É–ª–æ", "–æ–±—Ä–æ–±–ª–µ–Ω–æ", "—Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–æ", ".", "–ú–µ—Ç–æ—é", "–∑–∞–ø—Ä–æ–ø–æ–Ω–æ–≤–∞–Ω–æ—ó", "—Å—Ç–∞—Ç—Ç—ñ", "—î", "–∞–Ω–∞–ª—ñ–∑", "—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ-–∑–º—ñ—Å—Ç–æ–≤–∏—Ö", "–æ—Å–æ–±–ª–∏–≤–æ—Å—Ç–µ–π", "–ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω—å", "—É", "—Ä–∞–π–æ–Ω–Ω—ñ–π", "–ø—Ä–µ—Å—ñ", "–¢–µ—Ä–Ω–æ–ø—ñ–ª—å—â–∏–Ω–∏", "–æ–∑–Ω–∞—á–µ–Ω–æ–≥–æ", "–ø–µ—Ä—ñ–æ–¥—É", "."],
  "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-LOC", "O", "O", "O"]
}
```

```json
{
  "tokens": ["–Ø–∫", "–≤—ñ–¥–æ–º–æ", ",", "—Ä—ñ—à–µ–Ω–Ω—è", "¬´", "–ü—Ä–æ", "–≤–∏—Ö—ñ–¥", "–∑—ñ", "—Å–∫–ª–∞–¥—É", "–∑–∞—Å–Ω–æ–≤–Ω–∏–∫—ñ–≤", "—Ä–µ–¥–∞–∫—Ü—ñ—ó", "–≥–∞–∑–µ—Ç–∏", "¬´", "–ñ–∏—Ç–æ–º–∏—Ä—â–∏–Ω–∞", "¬ª", "–∑", "—ñ–Ω—ñ—Ü—ñ–∞—Ç–∏–≤–∏", "–≥–æ–ª–æ–≤–∏", "–æ–±–ª–∞—Å–Ω–æ—ó", "—Ä–∞–¥–∏", "–±—É–ª–æ", "–ø—Ä–∏–π–Ω—è—Ç–æ", "–Ω–∞", "–¥—Ä—É–≥—ñ–π", "—Å–µ—Å—ñ—ó", "–æ–±–ª–∞—Å–Ω–æ—ó", "—Ä–∞–¥–∏", "24", "–≥—Ä—É–¥–Ω—è", "–º–∏–Ω—É–ª–æ–≥–æ", "—Ä–æ–∫—É", "‚Äî", "—Å–∞–º–µ", "—Ç–æ–≥–æ", "–¥–Ω—è", ",", "–∫–æ–ª–∏", "–í–µ—Ä—Ö–æ–≤–Ω–∞", "–†–∞–¥–∞", "—É—Ö–≤–∞–ª–∏–ª–∞", "–≤", "–æ—Å—Ç–∞—Ç–æ—á–Ω—ñ–π", "—Ä–µ–¥–∞–∫—Ü—ñ—ó", "–ó–∞–∫–æ–Ω", "–ø—Ä–æ", "—Ä–µ—Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—è", "–ø—Ä–µ—Å–∏", "."],
  "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-ORG", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-ORG", "I-ORG", "O", "O", "O", "O", "O", "O", "O", "O", "O"]
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 8
- Prefix prompt:

  ```text
  –ù–∏–∂—á–µ –Ω–∞–≤–µ–¥–µ–Ω—ñ —Ä–µ—á–µ–Ω–Ω—è —Ç–∞ JSON-—Å–ª–æ–≤–Ω–∏–∫–∏ –∑ —ñ–º–µ–Ω–æ–≤–∞–Ω–∏–º–∏ —Å—É—Ç–Ω–æ—Å—Ç—è–º–∏, —è–∫—ñ –ø—Ä–∏—Å—É—Ç–Ω—ñ —É –¥–∞–Ω–æ–º—É —Ä–µ—á–µ–Ω–Ω—ñ.
  ```

- Base prompt template:

  ```text
  –†–µ—á–µ–Ω–Ω—è: {text}
  –Ü–º–µ–Ω–æ–≤–∞–Ω—ñ —Å—É—Ç–Ω–æ—Å—Ç—ñ: {label}
  ```

- Instruction-tuned prompt template:

  ```text
  –†–µ—á–µ–Ω–Ω—è: {text}

  –Ü–¥–µ–Ω—Ç–∏—Ñ—ñ–∫—É–π—Ç–µ —ñ–º–µ–Ω–æ–≤–∞–Ω—ñ —Å—É—Ç–Ω–æ—Å—Ç—ñ —É —Ä–µ—á–µ–Ω–Ω—ñ. –í–∏ –ø–æ–≤–∏–Ω–Ω—ñ –≤–∏–≤–µ—Å—Ç–∏ —Ü–µ —è–∫ JSON-—Å–ª–æ–≤–Ω–∏–∫ –∑ –∫–ª—é—á–∞–º–∏ '–æ—Å–æ–±–∞', '–º—ñ—Å—Ü–µ', '–æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—è' —Ç–∞ '—Ä—ñ–∑–Ω–µ'. –ó–Ω–∞—á–µ–Ω–Ω—è –º–∞—é—Ç—å –±—É—Ç–∏ —Å–ø–∏—Å–∫–∞–º–∏ —ñ–º–µ–Ω–æ–≤–∞–Ω–∏—Ö —Å—É—Ç–Ω–æ—Å—Ç–µ–π —Ü—å–æ–≥–æ —Ç–∏–ø—É, —Ç–æ—á–Ω–æ —Ç–∞–∫–∏–º–∏, —è–∫ –≤–æ–Ω–∏ –∑'—è–≤–ª—è—é—Ç—å—Å—è —É —Ä–µ—á–µ–Ω–Ω—ñ.
  ```

- Label mapping:
  - `B-PER` ‚û°Ô∏è `–æ—Å–æ–±–∞`
  - `I-PER` ‚û°Ô∏è `–æ—Å–æ–±–∞`
  - `B-LOC` ‚û°Ô∏è `–º—ñ—Å—Ü–µ`
  - `I-LOC` ‚û°Ô∏è `–º—ñ—Å—Ü–µ`
  - `B-ORG` ‚û°Ô∏è `–æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—è`
  - `I-ORG` ‚û°Ô∏è `–æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—è`
  - `B-MISC` ‚û°Ô∏è `—Ä—ñ–∑–Ω–µ`
  - `I-MISC` ‚û°Ô∏è `—Ä—ñ–∑–Ω–µ`

You can evaluate this dataset directly as follows:

```bash
euroeval --model <model-id> --dataset ner-uk-mini
```

## Linguistic Acceptability

### ScaLA-uk

This dataset was published in [this paper](https://aclanthology.org/2023.nodalida-1.20/)
and was automatically created from the [Ukrainian Universal Dependencies
treebank](https://github.com/UniversalDependencies/UD_Ukrainian-ParlaMint) by assuming that
the documents in the treebank are correct, and corrupting the samples to create
grammatically incorrect samples. The corruptions were done by either removing a word
from a sentence, or by swapping two neighbouring words in a sentence. To ensure that
this does indeed break the grammaticality of the sentence, a set of rules were used on
the part-of-speech tags of the words in the sentence.

The original full dataset consists of 1,024 / 256 / 2,048 samples for training,
validation and testing, respectively (so 3,328 samples used in total). These splits are
used as-is in the framework.

Here are a few examples from the training split:

```json
{
  "text": "–ü—ñ–¥ –ø–∞—Ç—Ä–æ–Ω–∞—Ç–æ–º –ü—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞ –£–∫—Ä–∞—ó–Ω–∏ –≤ —Ü—å–æ–º—É —Ä–æ—Ü—ñ –ø—Ä–æ–≤–µ–¥–µ–Ω–æ –Ü–Ü –í—Å–µ—É–∫—Ä–∞—ó–Ω—Å—å–∫—ñ –ª—ñ—Ç–Ω—ñ —Å–ø–æ—Ä—Ç–∏–≤–Ω—ñ —ñ–≥—Ä–∏, —è–∫—ñ —Å—Ç–∞–ª–∏ –≤–∞–∂–ª–∏–≤–∏–º –µ—Ç–∞–ø–æ–º —É –ø—ñ–¥–≥–æ—Ç–æ–≤—Ü—ñ –¥–æ –∫–≤–∞–ª—ñ—Ñ—ñ–∫–∞—Ü—ñ–π–Ω–∏—Ö –∑–º–∞–≥–∞–Ω—å –ø–æ –≤—ñ–¥–±–æ—Ä—É –¥–æ –õ—ñ—Ç–Ω—å–æ—ó –æ–ª—ñ–º–ø—ñ–∞–¥–∏ –≤ –ê—Ñ—ñ–Ω–∞—Ö, —Å–ø—Ä–∏—è–ª–∏ –∑–º—ñ—Ü–Ω–µ–Ω–Ω—é —Ñ—ñ–∑–∫—É–ª—å—Ç—É—Ä–Ω–æ-—Å–ø–æ—Ä—Ç–∏–≤–Ω–æ–≥–æ —Ä—É—Ö—É, –æ—Ö–æ–ø–∏–≤—à–∏ –≤—Å—ñ –≤–µ—Ä—Å—Ç–≤–∏ –Ω–∞—Å–µ–ª–µ–Ω–Ω—è.",
  "label": "correct"
}
```

```json
{
  "text": "–Ü –ø—Ä–æ—à—É, –¥–∞–≤–∞–π—Ç–µ –ø–æ–¥—è–∫—É—î–º–æ –∑–∞ –¥–æ–ø–æ–º–æ–≥—É –Ω–∞—à–∏–º –±—ñ–ª–æ—Ä—É—Å—å–∫–∏–º —Å—É—Å—ñ–¥–∞–º.",
  "label": "correct"
}
```

```json
{
  "text": "–®–∞–Ω–æ–≤–Ω—ñ –∫–æ–ª–µ–≥–∏, —Ç–µ–ø–µ—Ä –ø–µ—Ä–µ—Ö–æ–¥–∏–º–æ –¥–æ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ.",
  "label": "incorrect"
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 12
- Prefix prompt:

  ```text
  –ù–∏–∂—á–µ –Ω–∞–≤–µ–¥–µ–Ω—ñ —Ä–µ—á–µ–Ω–Ω—è —ñ —ó—Ö–Ω—è –≥—Ä–∞–º–∞—Ç–∏—á–Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ñ—Å—Ç—å.
  ```

- Base prompt template:

  ```text
  –†–µ—á–µ–Ω–Ω—è: {text}
  –ì—Ä–∞–º–∞—Ç–∏—á–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ: {label}
  ```

- Instruction-tuned prompt template:

  ```text
  –†–µ—á–µ–Ω–Ω—è: {text}

  –í–∏–∑–Ω–∞—á—Ç–µ, —á–∏ —Ä–µ—á–µ–Ω–Ω—è –≥—Ä–∞–º–∞—Ç–∏—á–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω–µ —á–∏ –Ω—ñ. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π—Ç–µ '—Ç–∞–∫', —è–∫—â–æ —Ä–µ—á–µ–Ω–Ω—è –ø—Ä–∞–≤–∏–ª—å–Ω–µ, —ñ '–Ω—ñ', —è–∫—â–æ –Ω—ñ. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π—Ç–µ –ª–∏—à–µ —Ü–∏–º —Å–ª–æ–≤–æ–º, —ñ –Ω—ñ—á–∏–º –±—ñ–ª—å—à–µ.
  ```

- Label mapping:
  - `correct` ‚û°Ô∏è `—Ç–∞–∫`
  - `incorrect` ‚û°Ô∏è `–Ω—ñ`

You can evaluate this dataset directly as follows:

```bash
euroeval --model <model-id> --dataset scala-uk
```

## Reading Comprehension

### MultiWikiQA-uk

This dataset was published in [this paper](https://doi.org/10.48550/arXiv.2509.04111)
and contains Wikipedia articles with LLM-generated questions and answers in 300+
languages.

The original full dataset consists of 5,000 samples in a single split. We use a 1,024 /
256 / 2,048 split for training, validation and testing, respectively, sampled randomly.

Here are a few examples from the training split:

```json
{
  "context": "Thalassema thalassema\xa0‚Äî –≤–∏–¥ –µ—Ö—ñ—É—Ä —Ä–æ–¥–∏–Ω–∏ Thalassematidae.\n\n–ü–æ—à–∏—Ä–µ–Ω–Ω—è \n–í–∏–¥ –ø–æ—à–∏—Ä–µ–Ω–∏–π —É –ø—Ä–∏–ø–ª–∏–≤–Ω—ñ–π –∑–æ–Ω—ñ –≤–∑–¥–æ–≤–∂ —î–≤—Ä–æ–ø–µ–π—Å—å–∫–æ–≥–æ —É–∑–±–µ—Ä–µ–∂–∂—è –ê—Ç–ª–∞–Ω—Ç–∏—á–Ω–æ–≥–æ –æ–∫–µ–∞–Ω—É —Ç–∞ –°–µ—Ä–µ–¥–∑–µ–º–Ω–æ–≥–æ –º–æ—Ä—è.\n\n–û–ø–∏—Å \n–ß–µ—Ä–≤'—è–∫ –∑–∞–≤–¥–æ–≤–∂–∫–∏ 2-7\xa0—Å–º. –ù–∞ –ø–µ—Ä–µ–¥–Ω—å–æ–º—É –∫—ñ–Ω—Ü—ñ —Ç—ñ–ª–∞ —Ä–æ–∑—Ç–∞—à–æ–≤–∞–Ω–∏–π —á–µ—Ä–≤–æ–Ω–∏–π –º'—è–∑–∏—Å—Ç–∏–π —Ö–æ–±–æ—Ç–æ–∫, —è–∫–∏–π –º–æ–∂–µ —Ä–æ–∑—Ç—è–≥—É–≤–∞—Ç–∏—Å—å –Ω–∞ 10-20\xa0—Å–º —É –¥–æ–≤–∂–∏–Ω—É. –†–æ—Ç –∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –±—ñ–ª—è –æ—Å–Ω–æ–≤–∏ —Ö–æ–±–æ—Ç–∫–∞. –ó–∞–±–∞—Ä–≤–ª–µ–Ω–Ω—è —Ç—ñ–ª–∞ –º–æ–∂–µ –±—É—Ç–∏ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–∏–º\xa0‚Äî —Å–∏–Ω—î, —Å—ñ—Ä–µ, –∂–æ–≤—Ç–µ, –ø–æ–º–∞—Ä–∞–Ω—á–µ–≤–µ, —Ä–æ–∂–µ–≤–µ. –ù–∞ –ø–µ—Ä–µ–¥–Ω—å–æ–º—É –∫—ñ–Ω—Ü—ñ —Ç—ñ–ª–∞ –¥–≤—ñ —á–µ—Ä–µ–≤—Ü—ñ —â–µ—Ç–∏–Ω–∫–∏, –Ω–∞ –∑–∞–¥–Ω—å–æ–º—É –∫—ñ–Ω—Ü—ñ –≤–æ–Ω–∏ –≤—ñ–¥—Å—É—Ç–Ω—ñ.\n\n–°–ø–æ—Å—ñ–± –∂–∏—Ç—Ç—è \n–ú–µ—à–∫–∞—î —É –ø—ñ—Å–∫—É —Ç–∞ –º—É–ª—ñ –ø—Ä–∏–ø–ª–∏–≤–Ω–æ—ó –∑–æ–Ω–∏. –ñ–∏–≤–∏—Ç—å—Å—è –¥–µ—Ç—Ä–∏—Ç–æ–º —Ç–∞ –º—ñ–∫—Ä–æ–æ—Ä–≥–∞–Ω—ñ–∑–º–∞–º–∏. –ê–∫—Ç–∏–≤–Ω–∏–π –≤–Ω–æ—á—ñ.\n\n–†–æ–∑–º–Ω–æ–∂–µ–Ω–Ω—è \n–°—Ç–∞—Ç–µ–≤–∏–π –¥–∏–º–æ—Ä—Ñ—ñ–∑–º –≤—ñ–¥—Å—É—Ç–Ω—ñ–π. –ó–∞–ø–ª—ñ–¥–Ω–µ–Ω–Ω—è –∑–æ–≤–Ω—ñ—à–Ω—î. –ü–ª–∞–≤–∞—é—á–∞ –ª–∏—á–∏–Ω–∫–∞ —Ç—Ä–æ—Ö–æ—Ñ–æ—Ä–∞ –∂–∏–≤–µ –¥–µ—è–∫–∏–π —á–∞—Å —è–∫ –∑–æ–æ–ø–ª–∞–Ω–∫—Ç–æ–Ω, –ø–æ—Ç—ñ–º –æ—Å—ñ–¥–∞—î –Ω–∞ –¥–Ω–æ —ñ –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î—Ç—å—Å—è –Ω–∞ —á–µ—Ä–≤'—è–∫–∞.\n\n–ü–æ—Å–∏–ª–∞–Ω–Ω—è \n Lexikon der Biologie: Thalassema\n Saskiya Richards: A spoon worm (Thalassema thalassema)  MarLIN, The Marine Life Information Network, 2009.\n\n–ï—Ö—ñ—É—Ä–∏\n–ö—ñ–ª—å—á–∞—Å—Ç—ñ —á–µ—Ä–≤–∏ –ê—Ç–ª–∞–Ω—Ç–∏—á–Ω–æ–≥–æ –æ–∫–µ–∞–Ω—É\n–§–∞—É–Ω–∞ –°–µ—Ä–µ–¥–∑–µ–º–Ω–æ–≥–æ –º–æ—Ä—è\n–¢–≤–∞—Ä–∏–Ω–∏, –æ–ø–∏—Å–∞–Ω—ñ 1774",
  "question": "–Ø–∫–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —á–µ—Ä–µ–≤–Ω–∏—Ö —â–µ—Ç–∏–Ω–æ–∫ —Ä–æ–∑—Ç–∞—à–æ–≤–∞–Ω–∞ –Ω–∞ –ø–µ—Ä–µ–¥–Ω—ñ–π —á–∞—Å—Ç–∏–Ω—ñ —Ç—ñ–ª–∞ Thalassema thalassema?",
  "answers": {
    "answer_start": [466],
    "text": ["–¥–≤—ñ"]
    }
}
```

```json
{
  "context": "–°–µ–∑–æ–Ω 2007‚Äì08 –≤ –°–µ—Ä—ñ—ó A\xa0‚Äî —Ñ—É—Ç–±–æ–ª—å–Ω–µ –∑–º–∞–≥–∞–Ω–Ω—è —É –Ω–∞–π–≤–∏—â–æ–º—É –¥–∏–≤—ñ–∑—ñ–æ–Ω—ñ —á–µ–º–ø—ñ–æ–Ω–∞—Ç—É –Ü—Ç–∞–ª—ñ—ó, —â–æ –ø—Ä–æ—Ö–æ–¥–∏–ª–æ –º—ñ–∂ 26 —Å–µ—Ä–ø–Ω—è 2007 —Ç–∞ 18 —Ç—Ä–∞–≤–Ω—è 2008 —Ä–æ–∫—É. –°—Ç–∞–≤ 76-–º —Ç—É—Ä–Ω—ñ—Ä–æ–º –∑ –º–æ–º–µ–Ω—Ç—É –∑–∞—Å–Ω—É–≤–∞–Ω–Ω—è –°–µ—Ä—ñ—ó A. –£—á–∞—Å—Ç—å —É –∑–º–∞–≥–∞–Ω–Ω—ñ –±—Ä–∞–ª–∏ 20 –∫–æ–º–∞–Ω–¥, —É —Ç–æ–º—É —á–∏—Å–ª—ñ 3 –∫–æ–º–∞–Ω–¥–∏, —è–∫—ñ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ —Å–µ–∑–æ–Ω—É –ø—ñ–¥–≤–∏—â–∏–ª–∏—Å—è —É –∫–ª–∞—Å—ñ –∑ –°–µ—Ä—ñ—ó B. –ó–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Å–µ–∑–æ–Ω—É 17 –∫–æ–º–∞–Ω–¥ –ø—Ä–æ–¥–æ–≤–∂–∏–ª–∏ –≤–∏—Å—Ç—É–ø–∏ –≤ –µ–ª—ñ—Ç–Ω–æ–º—É –¥–∏–≤—ñ–∑—ñ–æ–Ω—ñ, –∞ —Ç—Ä–∏ –Ω–∞–π–≥—ñ—Ä—à–∏—Ö –∫–ª—É–±–∏ –≤–∏–±—É–ª–∏ –¥–æ –°–µ—Ä—ñ—ó B.\n\n–ü–µ—Ä–µ–º–æ–∂—Ü–µ–º —Ç—É—Ä–Ω—ñ—Ä—É —Å—Ç–∞–≤ –º—ñ–ª–∞–Ω—Å—å–∫–∏–π ¬´–Ü–Ω—Ç–µ—Ä–Ω–∞—Ü—ñ–æ–Ω–∞–ª–µ¬ª, —è–∫–∏–π –∑–¥–æ–±—É–≤ —Å–≤—ñ–π —Ç—Ä–µ—Ç—ñ–π –ø–æ—Å–ø—ñ–ª—å —Ç–∞ 16-–π –≤ —ñ—Å—Ç–æ—Ä—ñ—ó —á–µ–º–ø—ñ–æ–Ω—Å—å–∫–∏–π —Ç–∏—Ç—É–ª. –ú–∞–π–±—É—Ç–Ω—ñ —á–µ–º–ø—ñ–æ–Ω–∏ –∑–∞—Ö–æ–ø–∏–ª–∏ –æ–¥–Ω–æ–æ—Å—ñ–±–Ω–µ –ª—ñ–¥–µ—Ä—Å—Ç–≤–æ —É 6 —Ç—É—Ä—ñ —Ç—É—Ä–Ω—ñ—Ä—É, –ø—ñ—Å–ª—è —á–æ–≥–æ –≤–∂–µ –Ω–µ –∑–∞–ª–∏—à–∞–ª–∏ –ø–µ—Ä—à–æ–≥–æ —Ä—è–¥–∫–∞ —Ç—É—Ä–Ω—ñ—Ä–Ω–æ—ó —Ç–∞–±–ª–∏—Ü—ñ. –•–æ—á–∞ –ø–æ—Å–µ—Ä–µ–¥ –∑–º–∞–≥–∞–Ω–Ω—è –≤—ñ–¥—Ä–∏–≤ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø–µ—Ä–µ—Å–ª—ñ–¥—É–≤–∞—á–∞, ¬´–†–æ–º–∏¬ª, –≤—ñ–¥ –ª—ñ–¥–µ—Ä–∞ —Å—è–≥–∞–≤ 11 –æ—á–æ–∫, –ø–µ—Ä–µ–¥ –æ—Å—Ç–∞–Ω–Ω—ñ–º —Ç—É—Ä–æ–º –∫–æ–º–∞–Ω–¥–∏ —Ä–æ–∑–¥—ñ–ª—è–≤ –ª–∏—à–µ –æ–¥–∏–Ω –∑–∞–ª—ñ–∫–æ–≤–∏–π –ø—É–Ω–∫—Ç. ¬´–Ü–Ω—Ç–µ—Ä¬ª –∑–∞–±–µ–∑–ø–µ—á–∏–≤ –ø–µ—Ä–µ–º–æ–≥—É –≤ —Å–µ–∑–æ–Ω—ñ, –∑–¥–æ–ª–∞–≤—à–∏ –≤ —Ü—å–æ–º—É –æ—Å—Ç–∞–Ω–Ω—å–æ–º—É —Ç—É—Ä—ñ –æ–¥–Ω–æ–≥–æ –∑ –∞—É—Ç—Å–∞–π–¥–µ—Ä—ñ–≤ —Å–µ–∑–æ–Ω—É, ¬´–ü–∞—Ä–º—É¬ª, –∑ —Ä–∞—Ö—É–Ω–∫–æ–º 2:0.\n\n–ö–æ–º–∞–Ω–¥–∏ \n\n–£—á–∞—Å—Ç—å —É —Ç—É—Ä–Ω—ñ—Ä—ñ –°–µ—Ä—ñ—ó A —Å–µ–∑–æ–Ω—É 2007‚Äì08 –±—Ä–∞–ª–∏ 20 –∫–æ–º–∞–Ω–¥:\n\n–¢—É—Ä–Ω—ñ—Ä–Ω–∞ —Ç–∞–±–ª–∏—Ü—è\n\n–†–µ–∑—É–ª—å—Ç–∞—Ç–∏\n\n–ë–æ–º–±–∞—Ä–¥–∏—Ä–∏ \n–ó–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Å–µ–∑–æ–Ω—É —Ç–∞–±–ª–∏—Ü—é –Ω–∞–π–∫—Ä–∞—â–∏—Ö –±–æ–º–±–∞—Ä–¥–∏—Ä—ñ–≤ –°–µ—Ä—ñ—ó –ê –æ—á–æ–ª–∏–ª–∞ –ø–∞—Ä–∞ –Ω–∞–ø–∞–¥–Ω–∏–∫—ñ–≤ —Ç—É—Ä–∏–Ω—Å—å–∫–æ–≥–æ ¬´–Æ–≤–µ–Ω—Ç—É—Å–∞¬ª\xa0‚Äî –ê–ª–µ—Å—Å–∞–Ω–¥—Ä–æ –î–µ–ª—å –ü'—î—Ä–æ —Ç–∞ –î–∞–≤—ñ–¥ –¢—Ä–µ–∑–µ–≥–µ, —è–∫—ñ –∑–∞–±–∏–ª–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ 21 —Ç–∞ 20 –≥–æ–ª—ñ–≤ –≤ –º–∞—Ç—á–∞—Ö —Ç—É—Ä–Ω—ñ—Ä—É.\n\n–ü–æ–≤–Ω–∏–π –ø–µ—Ä–µ–ª—ñ–∫ –≥—Ä–∞–≤—Ü—ñ–≤, —â–æ –∑–∞–±–∏–ª–∏ –ø—Ä–∏–Ω–∞–π–º–Ω—ñ 10 –≥–æ–ª—ñ–≤ –≤ —Ä–∞–º–∫–∞—Ö –°–µ—Ä—ñ—ó A —Å–µ–∑–æ–Ω—É 2007‚Äî08:\n\n 21 –≥–æ–ª\n  –ê–ª–µ—Å—Å–∞–Ω–¥—Ä–æ –î–µ–ª—å –ü'—î—Ä–æ (¬´–Æ–≤–µ–Ω—Ç—É—Å¬ª)\n 20 –≥–æ–ª—ñ–≤\n  –î–∞–≤—ñ–¥ –¢—Ä–µ–∑–µ–≥–µ (¬´–Æ–≤–µ–Ω—Ç—É—Å¬ª)\n 19 –≥–æ–ª—ñ–≤\n  –ú–∞—Ä–∫–æ –ë–æ—Ä—Ä—ñ—î–ª–ª–æ (¬´–î–∂–µ–Ω–æ–∞¬ª)\n 17 –≥–æ–ª—ñ–≤\n  –ê–Ω—Ç–æ–Ω—ñ–æ –î—ñ –ù–∞—Ç–∞–ª–µ (¬´–£–¥—ñ–Ω–µ–∑–µ¬ª)\n  –ó–ª–∞—Ç–∞–Ω –Ü–±—Ä–∞–≥—ñ–º–æ–≤–∏—á (¬´–Ü–Ω—Ç–µ—Ä–Ω–∞—Ü—ñ–æ–Ω–∞–ª–µ¬ª)\n  –ê–¥—Ä—ñ–∞–Ω –ú—É—Ç—É (¬´–§—ñ–æ—Ä–µ–Ω—Ç–∏–Ω–∞¬ª)\n 15 –≥–æ–ª—ñ–≤\n  –ê–º–∞—É—Ä—ñ (¬´–ü–∞–ª–µ—Ä–º–æ¬ª)\n  –ö–∞–∫–∞ (¬´–ú—ñ–ª–∞–Ω¬ª)\n 14 –≥–æ–ª—ñ–≤\n  –ì–æ—Ä–∞–Ω –ü–∞–Ω–¥–µ–≤ (¬´–õ–∞—Ü—ñ–æ¬ª)\n  –¢–æ–º–º–∞–∑–æ –†–æ–∫–∫—ñ (¬´–õ–∞—Ü—ñ–æ¬ª)\n  –§—Ä–∞–Ω—á–µ—Å–∫–æ –¢–æ—Ç—Ç—ñ (¬´–†–æ–º–∞¬ª)\n 13 –≥–æ–ª—ñ–≤\n  –•—É–ª—ñ–æ –†—ñ–∫–∞—Ä–¥–æ –ö—Ä—É—Å (¬´–Ü–Ω—Ç–µ—Ä–Ω–∞—Ü—ñ–æ–Ω–∞–ª–µ¬ª)\n  –ú–∞—Å—Å—ñ–º–æ –ú–∞–∫–∫–∞—Ä–æ–Ω–µ (¬´–°—ñ—î–Ω–∞¬ª)\n 12 –≥–æ–ª—ñ–≤\n  –ù—ñ–∫–æ–ª–∞ –ê–º–æ—Ä—É–∑–æ (¬´–†–µ–¥–∂–∏–Ω–∞¬ª)\n  –ö–ª–∞—É–¥—ñ–æ –ë–µ–ª—É—á—á—ñ (¬´–°–∞–º–ø–¥–æ—Ä—ñ—è¬ª)\n  –ö—Ä—ñ—Å—Ç—ñ–∞–Ω–æ –î–æ–Ω—ñ (¬´–ê—Ç–∞–ª–∞–Ω—Ç–∞¬ª)\n  –§–∞–±—ñ–æ –ö–≤–∞–ª—å—è—Ä–µ–ª–ª–∞ (¬´–£–¥—ñ–Ω–µ–∑–µ¬ª)\n 11 –≥–æ–ª—ñ–≤\n  –§—ñ–ª—ñ–ø–ø–æ –Ü–Ω–¥–∑–∞–≥—ñ (¬´–ú—ñ–ª–∞–Ω¬ª)\n 10 –≥–æ–ª—ñ–≤\n  –†–æ–±–µ—Ä—Ç –ê–∫–≤–∞—Ñ—Ä–µ—Å–∫–∞ (¬´–ö–∞–ª—å—è—Ä—ñ¬ª)\n  –ê–Ω—Ç–æ–Ω—ñ–æ –ö–∞—Å—Å–∞–Ω–æ (¬´–°–∞–º–ø–¥–æ—Ä—ñ—è¬ª)\n  –§—Ä–∞–Ω—á–µ—Å–∫–æ –¢–∞–≤–∞–Ω–æ (¬´–õ—ñ–≤–æ—Ä–Ω–æ¬ª)\n\n–ê–ª—å–±–µ—Ä—Ç–æ –î–∂–∏–ª–∞—Ä–¥—ñ–Ω–æ, –î–∞–≤—ñ–¥ –¢—Ä–µ–∑–µ–≥–µ —ñ –ù—ñ–∫–æ–ª–∞ –ê–º–æ—Ä—É–∑–æ –∑–∞–±–∏–ª–∏ –ø–æ —Å—Ç–æ –º'—è—á—ñ–≤ —É –º–∞—Ç—á–∞—Ö –°–µ—Ä—ñ—ó ¬´–ê¬ª. –ü–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ñ —Å–µ–∑–æ–Ω—É, –¥–æ –¥–µ—Å—è—Ç–∫–∏ –Ω–∞–π–≤–ª—É—á–Ω—ñ—à–∏—Ö –≥–æ–ª–µ–∞–¥–æ—Ä—ñ–≤ –ª—ñ–≥–∏ –≤—Ö–æ–¥—è—Ç—å: –°—ñ–ª—å–≤—ñ–æ –ü—ñ–æ–ª–∞ (275), –ì—É–Ω–Ω–∞—Ä –ù–æ—Ä–¥–∞–ª—å (225), –î–∂—É–∑–µ–ø–ø–µ –ú–µ–∞—Ü—Ü–∞ (216), –ñ–æ–∑–µ –ê–ª—Ç–∞—Ñ—ñ–Ω—ñ (216), –†–æ–±–µ—Ä—Ç–æ –ë–∞–¥–∂–æ (205), –ö—É—Ä—Ç –•–∞–º—Ä—ñ–Ω (190), –î–∂—É–∑–µ–ø–ø–µ –°—ñ–Ω—å–π–æ—Ä—ñ (188), –ì–∞–±—Ä—ñ—î–ª—å –ë–∞—Ç—ñ—Å—Ç—É—Ç–∞ (184), –î–∂–∞–º–ø'—î—Ä–æ –ë–æ–Ω—ñ–ø–µ—Ä—Ç—ñ (178), –ê–º–µ–¥–µ–æ –ê–º–∞–¥–µ—ó (174).\n\n–ü–æ—Å–∏–ª–∞–Ω–Ω—è \n –°–µ—Ä—ñ—è A 2007‚Äì08 –Ω–∞ RSSSF  \n\n2007-2008\n2007 —É —Ñ—É—Ç–±–æ–ª—ñ\n2008 —É —Ñ—É—Ç–±–æ–ª—ñ\n2007 –≤ —ñ—Ç–∞–ª—ñ–π—Å—å–∫–æ–º—É —Å–ø–æ—Ä—Ç—ñ\n2008 –≤ —ñ—Ç–∞–ª—ñ–π—Å—å–∫–æ–º—É —Å–ø–æ—Ä—Ç—ñ", "question": "–Ø–∫–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –≥–æ–ª—ñ–≤ –±—É–ª–∞ –∑–∞–±–∏—Ç–∞ –ê–ª–µ—Å—Å–∞–Ω–¥—Ä–æ –î–µ–ª—å –ü'—î—Ä–æ –ø—Ä–æ—Ç—è–≥–æ–º —Å–µ–∑–æ–Ω—É –°–µ—Ä—ñ—ó –ê 2007‚Äì2008 —Ä–æ–∫—ñ–≤?",
  "answers": {
    "answer_start": [1353],
    "text": ['21 –≥–æ–ª']
    }
}
```

```json
{
  "context": "–¢—ñ–º –°–º–æ–ª–¥–µ—Ä—Å (,  26 —Å–µ—Ä–ø–Ω—è 1980, –ì–µ–ª)\xa0‚Äî –±–µ–ª—å–≥—ñ–π—Å—å–∫–∏–π —Ñ—É—Ç–±–æ–ª—ñ—Å—Ç, —â–æ –≥—Ä–∞–≤ –Ω–∞ –ø–æ–∑–∏—Ü—ñ—ó –ø—ñ–≤–∑–∞—Ö–∏—Å–Ω–∏–∫–∞. –ü–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ñ —ñ–≥—Ä–æ–≤–æ—ó –∫–∞—Ä'—î—Ä–∏\xa0‚Äî —Ç—Ä–µ–Ω–µ—Ä.\n\n–Ü–≥—Ä–æ–≤–∞ –∫–∞—Ä'—î—Ä–∞ \n–£ –¥–æ—Ä–æ—Å–ª–æ–º—É —Ñ—É—Ç–±–æ–ª—ñ –¥–µ–±—é—Ç—É–≤–∞–≤ 1998 —Ä–æ–∫—É –≤–∏—Å—Ç—É–ø–∞–º–∏ –∑–∞ –∫–æ–º–∞–Ω–¥—É ¬´–ë—Ä—é–≥–≥–µ¬ª, –≤ —è–∫—ñ–π –ø—Ä–æ–≤—ñ–≤ —à—ñ—Å—Ç—å —Å–µ–∑–æ–Ω—ñ–≤, –≤–∑—è–≤—à–∏ —É—á–∞—Å—Ç—å —É 63 –º–∞—Ç—á–∞—Ö —á–µ–º–ø—ñ–æ–Ω–∞—Ç—É. –ó–∞ —Ü–µ–π —á–∞—Å –≤–∏–±–æ—Ä–æ–≤ —Ç–∏—Ç—É–ª —á–µ–º–ø—ñ–æ–Ω–∞ –ë–µ–ª—å–≥—ñ—ó.\n\n–ó–≥–æ–¥–æ–º –∑ 2004 –ø–æ 2015 —Ä—ñ–∫ –≥—Ä–∞–≤ —É —Å–∫–ª–∞–¥—ñ –Ω—ñ–¥–µ—Ä–ª–∞–Ω–¥—Å—å–∫–æ–≥–æ ¬´–†–æ—Å–µ–Ω–¥–∞–ª–∞¬ª, –∞ —Ç–∞–∫–æ–∂ –Ω–∞ –±–∞—Ç—å–∫—ñ–≤—â–∏–Ω—ñ –∑–∞ ¬´–®–∞—Ä–ª–µ—Ä—É–∞¬ª, ¬´–ì–µ–Ω—Ç¬ª —Ç–∞ ¬´–°–µ—Ä–∫–ª—å¬ª.\n\n–ü—Ä–æ—Ç—è–≥–æ–º 2015‚Äî2018 —Ä–æ–∫—ñ–≤ –≥—Ä–∞–≤ –∑–∞ –Ω–∏–∂—á–æ–ª—ñ–≥–æ–≤–∏–π ¬´–ó–≤–µ–≤–µ–∑–µ–ª–µ¬ª.\n\n–ö–∞—Ä'—î—Ä–∞ —Ç—Ä–µ–Ω–µ—Ä–∞\n–ü–µ—Ä—à–∏–π –¥–æ—Å–≤—ñ–¥ —Ç—Ä–µ–Ω–µ—Ä—Å—å–∫–æ—ó —Ä–æ–±–æ—Ç–∏ –æ—Ç—Ä–∏–º–∞–≤ —â–µ –≥—Ä–∞—é—á–∏ –Ω–∞ –ø–æ–ª—ñ —è–∫ –ø–æ–º—ñ—á–Ω–∏–∫ –≥–æ–ª–æ–≤–Ω–æ–≥–æ —Ç—Ä–µ–Ω–µ—Ä–∞ ¬´–°–µ—Ä–∫–ª—è¬ª —É 2014‚Äî2015 —Ä–æ–∫–∞—Ö.\n\n–ó–≥–æ–¥–æ–º –≤—Ö–æ–¥–∏–≤ –¥–æ —Ç—Ä–µ–Ω–µ—Ä—Å—å–∫–∏—Ö —à—Ç–∞–±—ñ–≤ —é–Ω–∞—Ü—å–∫–æ—ó –∑–±—ñ—Ä–Ω–æ—ó –ë–µ–ª—å–≥—ñ—ó (U-19) —Ç–∞ –º–æ–ª–æ–¥—ñ–∂–Ω–æ—ó –∫–æ–º–∞–Ω–¥–∏ ¬´–ë—Ä—é–≥–≥–µ¬ª.\n\n2021 —Ä–æ–∫—É –æ—á–æ–ª–∏–≤ —Ñ—É—Ç–±–æ–ª—å–Ω—É –∞–∫–∞–¥–µ–º—ñ—é ¬´–ë—Ä—é–≥–≥–µ¬ª.\n\n–¢–∏—Ç—É–ª–∏ —ñ –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è\n –ß–µ–º–ø—ñ–æ–Ω –ë–µ–ª—å–≥—ñ—ó (1):\n¬´–ë—Ä—é–≥–≥–µ¬ª: 2002-2003\n –í–æ–ª–æ–¥–∞—Ä –ö—É–±–∫–∞ –ë–µ–ª—å–≥—ñ—ó (2):\n¬´–ë—Ä—é–≥–≥–µ¬ª: 2002, 2004\n –í–æ–ª–æ–¥–∞—Ä –°—É–ø–µ—Ä–∫—É–±–∫–∞ –ë–µ–ª—å–≥—ñ—ó (3):\n¬´–ë—Ä—é–≥–≥–µ¬ª: 1998, 2002, 2003\n\n–ü–æ—Å–∏–ª–∞–Ω–Ω—è \n\n–±–µ–ª—å–≥—ñ–π—Å—å–∫—ñ —Ñ—É—Ç–±–æ–ª—ñ—Å—Ç–∏\n–±–µ–ª—å–≥—ñ–π—Å—å–∫—ñ —Ñ—É—Ç–±–æ–ª—å–Ω—ñ —Ç—Ä–µ–Ω–µ—Ä–∏\n–§—É—Ç–±–æ–ª—ñ—Å—Ç–∏ ¬´–ë—Ä—é–≥–≥–µ¬ª\n–§—É—Ç–±–æ–ª—ñ—Å—Ç–∏ ¬´–†–æ—Å–µ–Ω–¥–∞–ª–∞¬ª\n–§—É—Ç–±–æ–ª—ñ—Å—Ç–∏ ¬´–®–∞—Ä–ª–µ—Ä—É–∞¬ª\n–§—É—Ç–±–æ–ª—ñ—Å—Ç–∏ ¬´–ì–µ–Ω—Ç–∞¬ª\n–§—É—Ç–±–æ–ª—ñ—Å—Ç–∏ ¬´–°–µ—Ä–∫–ª—è¬ª\n–¢—Ä–µ–Ω–µ—Ä–∏ –§–ö ¬´–°–µ—Ä–∫–ª—å¬ª\n–¢—Ä–µ–Ω–µ—Ä–∏ —é–Ω–∞—Ü—å–∫–æ—ó –∑–±—ñ—Ä–Ω–æ—ó –ë–µ–ª—å–≥—ñ—ó –∑ —Ñ—É—Ç–±–æ–ª—É\n–¢—Ä–µ–Ω–µ—Ä–∏ –§–ö ¬´–ë—Ä—é–≥–≥–µ¬ª\n–±–µ–ª—å–≥—ñ–π—Å—å–∫—ñ —Ñ—É—Ç–±–æ–ª—å–Ω—ñ –ª–µ–≥—ñ–æ–Ω–µ—Ä–∏\n–§—É—Ç–±–æ–ª—å–Ω—ñ –ª–µ–≥—ñ–æ–Ω–µ—Ä–∏ –≤ –ù—ñ–¥–µ—Ä–ª–∞–Ω–¥–∞—Ö",
  "question": "–î–µ –°–º–æ–ª–¥–µ—Ä—Å –≤–ø–µ—Ä—à–µ —Å–ø—Ä–æ–±—É–≤–∞–≤ —Å–µ–±–µ –≤ —Ä–æ–ª—ñ —Ç—Ä–µ–Ω–µ—Ä–∞?",
  "answers": {
    "answer_start": [629],
    "text": ["¬´–°–µ—Ä–∫–ª—è¬ª"]
    }
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 4
- Prefix prompt:

  ```text
  –ù–∏–∂—á–µ –Ω–∞–≤–µ–¥–µ–Ω—ñ —Ç–µ–∫—Å—Ç–∏ –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–º–∏ –ø–∏—Ç–∞–Ω–Ω—è–º–∏ —Ç–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—è–º–∏.
  ```

- Base prompt template:

  ```text
  –¢–µ–∫—Å—Ç: {text}
  –ü–∏—Ç–∞–Ω–Ω—è: {question}
  –í—ñ–¥–ø–æ–≤—ñ–¥—å –º–∞–∫—Å–∏–º—É–º 3 —Å–ª–æ–≤–∞–º–∏:
  ```

- Instruction-tuned prompt template:

  ```text
  –¢–µ–∫—Å—Ç: {text}

  –í—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω–µ –ø–∏—Ç–∞–Ω–Ω—è –ø—Ä–æ –≤–∏—â–µ–∑–∞–∑–Ω–∞—á–µ–Ω–∏–π —Ç–µ–∫—Å—Ç –º–∞–∫—Å–∏–º—É–º 3 —Å–ª–æ–≤–∞–º–∏.

  –ü–∏—Ç–∞–Ω–Ω—è: {question}
  ```

You can evaluate this dataset directly as follows:

```bash
euroeval --model <model-id> --dataset multi-wiki-qa-uk
```

## Knowledge

### MMLU-sk

This dataset is a machine translated version of the English [MMLU
dataset](https://openreview.net/forum?id=d7KBjmI3GmQ) and features questions within 57
different topics, such as elementary mathematics, US history and law. The translation to
Slovak was done by the University of Oregon as part of [this
paper](https://aclanthology.org/2023.emnlp-demo.28/), using GPT-3.5-turbo.

The original full dataset consists of 269 / 1,410 / 13,200 samples for training,
validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training,
validation and testing, respectively (so 3,328 samples used in total). These splits are
new and there can thus be some overlap between the original validation and test sets and
our validation and test sets.

Here are a few examples from the training split:

```json
{
  "text": "V ak√Ωch smeroch je pr√≠pad pre humanit√°rnu intervenciu, ako je uveden√© v tejto kapitol... mocn√Ωmi ≈°t√°tmi.\nd. V≈°etky tieto mo≈ænosti.",
  "label": "d",
}
```

```json
{
  "text": "FAKTORI√ÅLOV√ù ANOVA sa pou≈æ√≠va v pr√≠pade, ≈æe ≈°t√∫dia zah≈ï≈àa viac ako 1 VI. Ak√Ω je INTER...ƒçinok VI na rovnakej √∫rovni ako ostatn√© VI",
  "label": "a"
}
```

```json
{
  "text": "Pre ktor√∫ z t√Ωchto dvoch situ√°ci√≠ urob√≠ hlavn√° postava (ktor√° pou≈æ√≠va ja/m≈àa/m√¥j) nie...ie zl√©\nc. Nie zl√©, zl√©\nd. Nie zl√©, nie zl√©",
  "label": "d",
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 5
- Prefix prompt:

  ```text
  Nasleduj√∫ ot√°zky s viacer√Ωmi mo≈ænos≈•ami (s odpoveƒèami).
  ```

- Base prompt template:

  ```text
  Ot√°zka: {text}
  Mo≈ænosti:
  a. {option_a}
  b. {option_b}
  c. {option_c}
  d. {option_d}
  Odpoveƒè: {label}
  ```

- Instruction-tuned prompt template:

  ```text
  Ot√°zka: {text}

  Odpovedzte na nasleduj√∫cu ot√°zku pou≈æit√≠m 'a', 'b', 'c' alebo 'd', a niƒç in√©.
  ```

You can evaluate this dataset directly as follows:

```bash
euroeval --model <model-id> --dataset mmlu-sk
```

### Winogrande-sk

This dataset was published in [this paper](https://doi.org/10.48550/arXiv.2506.19468)
and is a translated and filtered version of the English [Winogrande
dataset](https://doi.org/10.1145/3474381).

The original full dataset consists of 47 / 1,210 samples for training and testing, and
we use 128 of the test samples for validation, resulting in a 47 / 128 / 1,085 split for
training, validation and testing, respectively.

Here are a few examples from the training split:

```json
{
  "text": "Nedok√°zal som ovl√°da≈• vlhkos≈• ako som ovl√°dal d√°≈æƒè, preto≈æe _ prich√°dzalo odv≈°adiaƒæ. Na koho sa vz≈•ahuje pr√°zdne miesto _?\nMo≈ænosti:\na. Mo≈ænos≈• A: vlhkos≈•\nb. Mo≈ænos≈• B: d√°≈æƒè",
  "label": "a"
}
```

```json
{
  "text": "Jessica si myslela, ≈æe Sandstorm je najlep≈°ia piese≈à, ak√° bola kedy nap√≠san√°, ale Patricia ju nen√°videla. _ si k√∫pila l√≠stok na jazzov√Ω koncert. Na koho sa vz≈•ahuje pr√°zdne miesto _?\nMo≈ænosti:\na. Mo≈ænos≈• A: Jessica\nb. Mo≈ænos≈• B: Patricia",
  "label": "b"
}
```

```json
{
  "text": "Termostat ukazoval, ≈æe dole bolo o dvadsa≈• stup≈àov chladnej≈°ie ako hore, tak≈æe Byron zostal v _ preto≈æe mu bola zima. Na koho sa vz≈•ahuje pr√°zdne miesto _?\nMo≈ænosti:\na. Mo≈ænos≈• A: dole\nb. Mo≈ænos≈• B: hore",
  "label": "b"
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 5
- Prefix prompt:

  ```text
  Nasleduj√∫ ot√°zky s viacer√Ωmi mo≈ænos≈•ami (s odpoveƒèami).
  ```

- Base prompt template:

  ```text
  Ot√°zka: {text}
  Mo≈ænosti:
  a. {option_a}
  b. {option_b}
  Odpoveƒè: {label}
  ```

- Instruction-tuned prompt template:

  ```text
  Ot√°zka: {text}
  Mo≈ænosti:
  a. {option_a}
  b. {option_b}

  Odpovedzte na nasleduj√∫cu ot√°zku pou≈æit√≠m 'a' alebo 'b', a niƒç in√©.
  ```

You can evaluate this dataset directly as follows:

```bash
euroeval --model <model-id> --dataset winogrande-sk
```

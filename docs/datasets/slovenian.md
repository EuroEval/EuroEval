# üá∏üáÆ Slovenian

This is an overview of all the datasets used in the Slovenian part of EuroEval. The
datasets are grouped by their task - see the [task overview](/tasks) for more
information about what these constitute.

## Sentiment Classification

### Sentinews

This dataset was published in
[this paper](https://link.springer.com/article/10.1007/s10579-018-9413-3) and
consists of five Slovene media resources on the web.

The original dataset contains 168,899 samples. We use 1,024 / 256 / 2,048 samples for
our training, validation and test splits, respectively.

Here are a few examples from the training split:

```json
{
    "text": "V dr≈æavo pa je vpeljal stabilnost, katero je Rusija potrebovala.",
    "label": "positive"
}
```

```json
{
    "text": "Po najbolj ƒçrnogledih napovedih bo konec leta ≈æe sto tisoƒç brezposelnih.",
    "label": "negative"
}
```

```json
{
    "text": "Zdenko Pavƒçek bo vlo≈æil zasebno to≈æbo zoper Walterja Wolfa zaradi kaznivega dejanja raz≈æalitve.",
    "label": "negative"
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 12
- Prefix prompt:

  ```text
  Spodaj so dokumenti in njihov sentiment, ki je lahko 'pozitivno', 'nevtralno' ali 'negativno'.
  ```

- Base prompt template:

  ```text
  Dokument: {text}
  Sentiment: {label}
  ```

- Instruction-tuned prompt template:

  ```text
  Dokument: {text}

  Klasificirajte sentiment v dokumentu. Odgovorite z 'pozitivno', 'nevtralno' ali 'negativno', in niƒç drugega.
  ```

- Label mapping:
  - `positive` ‚û°Ô∏è `pozitivno`
  - `neutral` ‚û°Ô∏è `nevtralno`
  - `negative` ‚û°Ô∏è `negativno`

You can evaluate this dataset directly as follows:

```bash
euroeval --model <model-id> --dataset sentinews
```

## Named Entity Recognition

### UNER-sk

This dataset was published in
[this paper](https://aclanthology.org/2024.naacl-long.243/).

The original dataset consists of 8,482 / 1,059 / 1,060 samples for the
training, validation, and test splits, respectively. We use 1,024 / 256 / 2,048
samples for our training, validation and test splits, respectively. The train and
validation splits are subsets of the original splits, while the test split is
created using additional samples from the train split.

Here are a few examples from the training split:

```json
{
  "tokens": ["Bude", "ma≈•", "n√°zov", "Shanghai", "Noon", "a", "re≈æis√©rom", "bude", "debutuj√∫ci", "Tom", "Dey", "."],
  "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-PER", "I-PER", "O"]
}
```

```json
{
  "tokens": ["Ako", "≈°es≈•roƒçn√©ho", "(", "o", "rok", "sk√¥r", ",", "ne≈æ", "bolo", "zvykom", ")", "ho", "na", "z√°klade", "zvl√°≈°tnej", "v√Ωnimky", "prijali", "medzi", "Zvedov", "a", "ako", "dev√§≈•roƒçn√Ω", "sa", "stal", "ved√∫cim", "skupiny", "."],
  "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-ORG", "O", "O", "O", "O", "O", "O", "O", "O"]
}
```

```json
{
  "tokens": ["To", "predsa", "stoj√≠", "za", "pokus", "!"],
  "labels": ["O", "O", "O", "O", "O", "O"]
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 8
- Prefix prompt:

  ```text
  Nasleduj√∫ce s√∫ vety a JSON-objekty s pomenovan√Ωmi entitami, ktor√© sa nach√°dzaj√∫ v danej vete.
  ```

- Base prompt template:

  ```text
  Veta: {text}
  Pomenovan√© entity: {label}
  ```

- Instruction-tuned prompt template:

  ```text
  Veta: {text}

  Identifikujte pomenovan√© entity vo vete. V√Ωstup by mal by≈• vo forme JSON-objektu s kƒæ√∫ƒçmi 'osoba', 'miesto', 'organiz√°cia' a 'r√¥zne'. Hodnoty by mali by≈• zoznamy pomenovan√Ωch ent√≠t danej kateg√≥rie, presne tak, ako sa vyskytuj√∫ vo vete.
  ```

- Label mapping:
  - `B-PER` ‚û°Ô∏è `osoba`
  - `I-PER` ‚û°Ô∏è `osoba`
  - `B-LOC` ‚û°Ô∏è `miesto`
  - `I-LOC` ‚û°Ô∏è `miesto`
  - `B-ORG` ‚û°Ô∏è `organiz√°cia`
  - `I-ORG` ‚û°Ô∏è `organiz√°cia`
  - `B-MISC` ‚û°Ô∏è `r√¥zne`
  - `I-MISC` ‚û°Ô∏è `r√¥zne`

You can evaluate this dataset directly as follows:

```bash
euroeval --model <model-id> --dataset uner-sk
```

## Linguistic Acceptability

### ScaLA-sl

This dataset was published in [this paper](https://aclanthology.org/2023.nodalida-1.20/)
and was automatically created from the [Slovenian Universal Dependencies
treebank](https://github.com/UniversalDependencies/UD_Slovenian-SSJ) by assuming that the
documents in the treebank are correct, and corrupting the samples to create
grammatically incorrect samples. The corruptions were done by either removing a word
from a sentence, or by swapping two neighbouring words in a sentence. To ensure that
this does indeed break the grammaticality of the sentence, a set of rules were used on
the part-of-speech tags of the words in the sentence.

The original full dataset consists of 1,024 / 256 / 2,048 samples for training,
validation and testing, respectively (so 3,328 samples used in total). These splits are
used as-is in the framework.

Here are a few examples from the training split:

```json
{
    "text": "Potem je nekdo planil na sejo in povedal, da je v Trade Centru pri≈°lo do eksplozije.",
    "label": "correct"
}
```

```json
{
    "text": "Miroslav Klun: S primerno zakonodajo lahko slovenska obrt ponudi 60.000 novih delovnih mest.",
    "label": "correct"
}
```

```json
{
    "text": "Priroƒçno za vse, ki veliko kupujete drugih v dr≈æavah.",
    "label": "incorrect"
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 12
- Prefix prompt:

  ```text
  Spodaj so povedi in ali so slovniƒçno pravilne.
  ```

- Base prompt template:

  ```text
  Poved: {text}
  Slovniƒçno pravilna: {label}
  ```

- Instruction-tuned prompt template:

  ```text
  Poved: {text}

  Ugotovite, ali je poved slovniƒçno pravilna ali ne. Odgovorite z 'da', ƒçe je poved pravilna, in 'ne', ƒçe ni pravilna. Odgovorite samo s to besedo, in niƒç drugega.
  ```

- Label mapping:
  - `correct` ‚û°Ô∏è `da`
  - `incorrect` ‚û°Ô∏è `ne`

You can evaluate this dataset directly as follows:

```bash
euroeval --model <model-id> --dataset scala-sl
```

## Reading Comprehension

### MultiWikiQA-sl

This dataset was published in [this paper](https://doi.org/10.48550/arXiv.2509.04111)
and contains Wikipedia articles with LLM-generated questions and answers in 300+
languages.

The original full dataset consists of 5,000 samples in a single split. We use a 1,024 /
256 / 2,048 split for training, validation and testing, respectively, sampled randomly.

Here are a few examples from the training split:

```json
{
    "context": "1891 (MDCCCXCI) je bilo navadno leto, ki se je po gregorijanskem koledarju zaƒçelo na ƒçetrtek, po 12 dni poƒçasnej≈°em julijanskem koledarju pa na torek.\n\nDogodki \n 15. maj ‚Äì pape≈æ Leon XIII. izda encikliko Rerum Novarum\n\nRojstva \n 22. januar - Antonio Gramsci, italijanski filozof, politik in politiƒçni teoretik (‚Ä† 1937)\n 24. januar - Abraham Samojloviƒç Bezikoviƒç, ruski matematik (‚Ä† 1970)\n 11. marec - Michael Polanyi, mad≈æarsko-britanski kemik in filozof (‚Ä† 1976)\n 24. marec - John Knittel, ≈°vicarski pisatelj (‚Ä† 1970)\n 14. april - Bhimrao Ramd≈æi Ambedkar, indijski budistiƒçni socialni reformator, pravnik in filozof (‚Ä† 1956)\n 22. april - sir Harold Jeffreys, angle≈°ki geofizik, astronom, matematik (‚Ä† 1989)\n 23. april - Sergej Sergejeviƒç Prokofjev, ruski skladatelj, pianist (‚Ä† 1953)\n 15. maj - Mihail Afanasjeviƒç Bulgakov, ruski pisatelj (‚Ä† 1940)\n 18. maj - Rudolf Carnap, nem≈°ki filozof (‚Ä† 1970)\n 16. junij - Vladimir Aleksandroviƒç Albicki, ruski astronom (‚Ä† 1952)\n 19. avgust - Milton Lasell Humason, ameri≈°ki astronom (‚Ä† 1972)\n 26. september - Hans Reichenbach, nem≈°ki filozof (‚Ä† 1953)\n 20. oktober - sir James Chadwick, angle≈°ki fizik, nobelovec 1935 (‚Ä† 1974)\n 12. november - Seth Barnes Nicholson, ameri≈°ki astronom (‚Ä† 1963)\n 15. november - Erwin Rommel, nem≈°ki feldmar≈°al in voja≈°ki strateg (‚Ä† 1944)\n 26. december - Henry Miller, ameri≈°ki pisatelj (‚Ä† 1980)\n\nSmrti \n 23. junij - Norman Robert Pogson, angle≈°ki astronom (* 1829)\n 23. junij - Wilhelm Eduard Weber nem≈°ki fizik (* 1804)\n 3. oktober - √âdouard Lucas, francoski matematik (* 1842)\n 10. november - ≈†tefan ≈Ωemliƒç, mad≈æarsko-slovenski pisatelj (* 1840)\n 20. december - George Bassett Clark, ameri≈°ki astronom, optik (* 1827)\n 29. december - Leopold Kronecker, nem≈°ki matematik, logik (* 1823)",
    "question": "Kateri je bil prvi dan leta 1891 po gregorijanskem koledarju?",
    "answers": {
        "answer_start": [82],
        "text": ["na ƒçetrtek"]
    }
}
```

```json
{
    "context": "The Night the Light Went On in Long Beach je prvi album v ≈æivo skupine Electric Light Orchestra, ki je iz≈°el leta 1974, posnet pa je bil 12. maja 1974 v Long Beach Auditoriumu na Long Beachu. Naslov albuma je sposojen od pesmi ¬ªThe Night the Lights Went Out in Georgia¬´, ki jo je posnela Vicki Lawrence.\n\nOzadje in omejena izdaja \nAlbum je bil predviden kot naslednik albuma On the Third Day, a so bili posnetki po≈°kodovani zaradi tehniƒçnih te≈æav tako na odru kot zunaj njega. Te≈æave so se zaƒçele ko se je tovornjak z opremo skupine na poti pokvaril, pred koncertom pa skupina ni imela dovolj ƒçasa za preizkus zvoka.\n\n≈†tevilna pre≈°anja albuma so bila tako slabe kvalitete, da je vodstvo skupine vlo≈æilo to≈æbo proti proizvodnem podjetju. Naslovnico albuma je oblikoval Mick Haggerty. \n\nNa koncu sta se tako ameri≈°ka kot britanska zalo≈æba odloƒçili da ne izdata albuma. Album je tako iz≈°el le v Nemƒçiji in nekaterih drugih dr≈æavah, leta 1985 pa je vseeno iz≈°el v Zdru≈æenem kraljestvu. Album ni nikoli iz≈°el v ZDA, ƒçeprav je bil tja uvo≈æen in se je dobro prodajal, je pa ≈æiva verzija ¬ª10538 Overture¬´ iz≈°la kot b-stran singla ¬ªEvil Woman¬´ z albuma Face the Music. ≈Ωiva verzija ¬ªRoll Over Beethoven¬´ je v ZDA iz≈°la kot b-stran alternativne verzije ¬ªTelephone Line¬´ v seriji reizdaj.\n\nObnova \nRemastering v 90. letih je popravil slabo kvaliteto albuma. Odkrito je bilo, da je bila originalna LP plo≈°ƒça masterizirana z uporabo slab≈°e kopije koncerta, zaradi katere je bila kvaliteta zvoka slaba. Originalni trak je bil odkrit v trezorju proizvodnje plo≈°ƒç in album je bil obnovljen v bolj≈°i kvaliteti zvoka.\n\nTo je edini ≈æivi album ELO iz ƒçasa zaƒçetkov skupine.\n\nSeznam skladb\n\nZasedba \nJeff Lynne\t‚Äì solo vokal, elektriƒçna kitara\nBev Bevan ‚Äì bobni\nRichard Tandy ‚Äì Wurlitzer, Minimoog\nMike de Albuquerque ‚Äì solo vokal, spremljevalni vokal, bas\nMik Kaminski ‚Äì violina\nHugh McDowell ‚Äì ƒçelo\nMike Edwards ‚Äì ƒçelo\n\nSklici \n\nAlbumi leta 1974\nAlbumi Electric Light Orchestra\nAlbumi v ≈æivo\nAlbumi, ki jih je produciral Jeff Lynne",
    "question": "Zaradi ƒçesa je bila slaba kakovost albuma The Night the Light Went On in Long Beach odpravljena?",
    "answers": {
        "answer_start": [1287],
        "text": ["Remastering v 90. letih"]
    }
}
```

```json
{
    "context": "Surangel S. Whipps ml., palavski poslovne≈æ in politik; * 9. avgust 1968, Baltimore, Maryland, Zdru≈æene dr≈æave Amerike.\n\nOd 21. januarja 2021 je predsednik Palava. Senator je bil od leta 2008 do 2016. Prihaja iz de≈æele Ngatpang.\n\nZgodnje ≈æivljenje in izobra≈æevanje \nWhipps se je rodil v Baltimorju v Marylandu materi samohranilki Surangel Whipps Sr., rojeni v Marylandu. Diplomiral je iz poslovne administracije in ekonomije na Univerzi Andrews in magistriral iz poslovne znanosti na kalifornijski univerzi v Los Angelesu. Poleg tega vodi verigo supermarketov Palauan. Na splo≈°nih volitvah leta 2016 v Palavu se je potegoval proti svojemu zetu, predsedniku Thomasu Remengesauju mlaj≈°emu.\n\nMandat \nWhipps je na predsedni≈°kih volitvah 2020 kandidiral za predsednika in premagal podpredsednika Raynolda Oiloucha. V intervjuju za Guardian je takratni izvoljeni predsednik Whipps ml. podal izjavo, da se bo Palav odloƒçneje upiral ukrepom kitajske vlade, vkljuƒçno z nezakonitim ribolovom in vstopom v palavske vode ter obljubil, da bo ohranil priznanje Tajvana. Poleg tega je predlagal distribucijo cepiva proti COVID-19 med Palavƒçani, s poudarkom na zdravstvenih delavcih.\n\nSklici \nWhipps, Surangel\nWhipps, Surangel",
    "question": "Proti komu je Whipps zmagal na predsedni≈°kih volitvah leta 2020?",
    "answers": {
        "answer_start": [790],
        "text": ["Raynolda Oiloucha"]
    }
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 4
- Prefix prompt:

  ```text
  Spodaj so besedila z ustreznimi vpra≈°anji in odgovori.
  ```

- Base prompt template:

  ```text
  Besedilo: {text}
  Vpra≈°anje: {question}
  Odgovor v najveƒç 3 besedah:
  ```

- Instruction-tuned prompt template:

  ```text
  Besedilo: {text}

  Odgovorite na naslednje vpra≈°anje o zgornjem besedilu v najveƒç 3 besedah.

  Vpra≈°anje: {question}
  ```

You can evaluate this dataset directly as follows:

```bash
euroeval --model <model-id> --dataset multi-wiki-qa-sl
```

## Knowledge

### MMLU-sk

This dataset is a machine translated version of the English [MMLU
dataset](https://openreview.net/forum?id=d7KBjmI3GmQ) and features questions within 57
different topics, such as elementary mathematics, US history and law. The translation to
Slovak was done by the University of Oregon as part of [this
paper](https://aclanthology.org/2023.emnlp-demo.28/), using GPT-3.5-turbo.

The original full dataset consists of 269 / 1,410 / 13,200 samples for training,
validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training,
validation and testing, respectively (so 3,328 samples used in total). These splits are
new and there can thus be some overlap between the original validation and test sets and
our validation and test sets.

Here are a few examples from the training split:

```json
{
  "text": "V ak√Ωch smeroch je pr√≠pad pre humanit√°rnu intervenciu, ako je uveden√© v tejto kapitol... mocn√Ωmi ≈°t√°tmi.\nd. V≈°etky tieto mo≈ænosti.",
  "label": "d",
}
```

```json
{
  "text": "FAKTORI√ÅLOV√ù ANOVA sa pou≈æ√≠va v pr√≠pade, ≈æe ≈°t√∫dia zah≈ï≈àa viac ako 1 VI. Ak√Ω je INTER...ƒçinok VI na rovnakej √∫rovni ako ostatn√© VI",
  "label": "a"
}
```

```json
{
  "text": "Pre ktor√∫ z t√Ωchto dvoch situ√°ci√≠ urob√≠ hlavn√° postava (ktor√° pou≈æ√≠va ja/m≈àa/m√¥j) nie...ie zl√©\nc. Nie zl√©, zl√©\nd. Nie zl√©, nie zl√©",
  "label": "d",
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 5
- Prefix prompt:

  ```text
  Nasleduj√∫ ot√°zky s viacer√Ωmi mo≈ænos≈•ami (s odpoveƒèami).
  ```

- Base prompt template:

  ```text
  Ot√°zka: {text}
  Mo≈ænosti:
  a. {option_a}
  b. {option_b}
  c. {option_c}
  d. {option_d}
  Odpoveƒè: {label}
  ```

- Instruction-tuned prompt template:

  ```text
  Ot√°zka: {text}

  Odpovedzte na nasleduj√∫cu ot√°zku pou≈æit√≠m 'a', 'b', 'c' alebo 'd', a niƒç in√©.
  ```

You can evaluate this dataset directly as follows:

```bash
euroeval --model <model-id> --dataset mmlu-sk
```

## Common-sense Reasoning

### Winogrande-sk

This dataset was published in [this paper](https://doi.org/10.48550/arXiv.2506.19468)
and is a translated and filtered version of the English [Winogrande
dataset](https://doi.org/10.1145/3474381).

The original full dataset consists of 47 / 1,210 samples for training and testing, and
we use 128 of the test samples for validation, resulting in a 47 / 128 / 1,085 split for
training, validation and testing, respectively.

Here are a few examples from the training split:

```json
{
  "text": "Nedok√°zal som ovl√°da≈• vlhkos≈• ako som ovl√°dal d√°≈æƒè, preto≈æe _ prich√°dzalo odv≈°adiaƒæ. Na koho sa vz≈•ahuje pr√°zdne miesto _?\nMo≈ænosti:\na. vlhkos≈•\nb. d√°≈æƒè",
  "label": "a"
}
```

```json
{
  "text": "Jessica si myslela, ≈æe Sandstorm je najlep≈°ia piese≈à, ak√° bola kedy nap√≠san√°, ale Patricia ju nen√°videla. _ si k√∫pila l√≠stok na jazzov√Ω koncert. Na koho sa vz≈•ahuje pr√°zdne miesto _?\nMo≈ænosti:\na. Jessica\nb. Patricia",
  "label": "b"
}
```

```json
{
  "text": "Termostat ukazoval, ≈æe dole bolo o dvadsa≈• stup≈àov chladnej≈°ie ako hore, tak≈æe Byron zostal v _ preto≈æe mu bola zima. Na koho sa vz≈•ahuje pr√°zdne miesto _?\nMo≈ænosti:\na. dole\nb. hore",
  "label": "b"
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 5
- Prefix prompt:

  ```text
  Nasleduj√∫ ot√°zky s viacer√Ωmi mo≈ænos≈•ami (s odpoveƒèami).
  ```

- Base prompt template:

  ```text
  Ot√°zka: {text}
  Mo≈ænosti:
  a. {option_a}
  b. {option_b}
  Odpoveƒè: {label}
  ```

- Instruction-tuned prompt template:

  ```text
  Ot√°zka: {text}
  Mo≈ænosti:
  a. {option_a}
  b. {option_b}

  Odpovedzte na nasleduj√∫cu ot√°zku pou≈æit√≠m 'a' alebo 'b', a niƒç in√©.
  ```

You can evaluate this dataset directly as follows:

```bash
euroeval --model <model-id> --dataset winogrande-sk
```

# üá´üá¥ Faroese

This is an overview of all the datasets used in the Faroese part of ScandEval. The
datasets are grouped by their task - see the [task overview](/tasks) for more
information about what these constitute.

## Sentiment Classification

### [dataset]

[description]

[size-info]

Here are a few examples from the training split:

```
[example-1]
```
```
[example-2]
```
```
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: XX
- Prefix prompt:
  ```
  [prefix-prompt]
  ```
- Base prompt template:
  ```
  [base-prompt]
  ```
- Instruction-tuned prompt template:
  ```
  [instruction-tuned-prompt]
  ```
- Label mapping:
    - `positive` ‚û°Ô∏è `X`
    - `neutral` ‚û°Ô∏è `X`
    - `negative` ‚û°Ô∏è `X`

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset X
```


## Named Entity Recognition

### [dataset]

[description]

[size-info]

Here are a few examples from the training split:

```
[example-1]
```
```
[example-2]
```
```
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: XX
- Prefix prompt:
  ```
  [prefix-prompt]
  ```
- Base prompt template:
  ```
  [base-prompt]
  ```
- Instruction-tuned prompt template:
  ```
  [instruction-tuned-prompt]
  ```
- Label mapping:
    - `B-PER` ‚û°Ô∏è `X`
    - `I-PER` ‚û°Ô∏è `X`
    - `B-LOC` ‚û°Ô∏è `X`
    - `I-LOC` ‚û°Ô∏è `X`
    - `B-ORG` ‚û°Ô∏è `X`
    - `I-ORG` ‚û°Ô∏è `X`
    - `B-MISC` ‚û°Ô∏è `X`
    - `I-MISC` ‚û°Ô∏è `X`

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset X
```


## Linguistic Acceptability

### [dataset]

[description]

[size-info]

Here are a few examples from the training split:

```
[example-1]
```
```
[example-2]
```
```
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: XX
- Prefix prompt:
  ```
  [prefix-prompt]
  ```
- Base prompt template:
  ```
  [base-prompt]
  ```
- Instruction-tuned prompt template:
  ```
  [instruction-tuned-prompt]
  ```
- Label mapping:
    - `correct` ‚û°Ô∏è `X`
    - `incorrect` ‚û°Ô∏è `X`

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset X
```


## Reading Comprehension

### [dataset]

[description]

[size-info]

Here are a few examples from the training split:

```
[example-1]
```
```
[example-2]
```
```
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: XX
- Prefix prompt:
  ```
  [prefix-prompt]
  ```
- Base prompt template:
  ```
  [base-prompt]
  ```
- Instruction-tuned prompt template:
  ```
  [instruction-tuned-prompt]
  ```
- Label mapping:
    - `X` ‚û°Ô∏è `Y`

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset X
```


## Knowledge

### [dataset]

[description]

[size-info]

Here are a few examples from the training split:

```
[example-1]
```
```
[example-2]
```
```
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: XX
- Prefix prompt:
  ```
  [prefix-prompt]
  ```
- Base prompt template:
  ```
  [base-prompt]
  ```
- Instruction-tuned prompt template:
  ```
  [instruction-tuned-prompt]
  ```

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset X
```


## Common-sense Reasoning

### [dataset]

[description]

[size-info]

Here are a few examples from the training split:

```
[example-1]
```
```
[example-2]
```
```
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: XX
- Prefix prompt:
  ```
  [prefix-prompt]
  ```
- Base prompt template:
  ```
  [base-prompt]
  ```
- Instruction-tuned prompt template:
  ```
  [instruction-tuned-prompt]
  ```

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset X
```


## Summarization

### [dataset]

[description]

[size-info]

Here are a few examples from the training split:

```
[example-1]
```
```
[example-2]
```
```
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: XX
- Prefix prompt:
  ```
  [prefix-prompt]
  ```
- Base prompt template:
  ```
  [base-prompt]
  ```
- Instruction-tuned prompt template:
  ```
  [instruction-tuned-prompt]
  ```

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset X
```

# üá∑üá¥ Romanian

This is an overview of all the datasets used in the Romanian part of EuroEval. The
datasets are grouped by their task - see the [task overview](/tasks) for more
information about what these constitute.

## Sentiment Classification

### HuSST

This dataset was first published in [this repository](https://github.com/katakonst/sentiment-analysis-tensorflow)
and consists of reviews from yelp and imdb.

The original dataset contains 17,941 / 11,005 samples for the training and test splits,
respectively. We use 1,024 / 256 / 2,048 samples for our training,
validation and test splits, respectively. The train and test splits are subsets of
the original splits, while the validation split is created from the training split.

Here are a few examples from the training split:

```json
{
    "text": "acest film are mari staruri in anii lor mai devreme: ingor stevens nu a fost niciodata mai frumos; yul brynner a fost un jean lafitte foarte convingator, in conflict cu pirateria sa si dorind sa pastreze neutralitatea cu statele unite. charlton heston a facut o treaba destul de buna ca andrew jackson, dar cateva momente au fost un pic stomac. este un film bun pentru elevii sa invete acea parte a istoriei noastre si arata ca toate incheierile fericite nu includ iubitorii care se intalnesc unul cu celalalt - uneori, sfarsitul fericit este acela ca ei navigheaza departe si gasesc parteneri de acelasi gen care le va intelege mai bine pe termen lung. am vazut-o in fiecare an de cel putin doua ori, timp de 16 ani; si desi nu este cel mai bun film pe care l-am vazut vreodata, il iubesc de fiecare data!",
    "label": "positive"
}
```

```json
{
    "text": "un film foarte interesant, inteligent si bine facut. liam neeson si tim roth joaca foarte bine rolurile lor. cinematografia este remarcabila. scenele de lupta sunt uimitoare. acesta este un film pe care il voi bucura de vizionarea din nou si din nou. unul dintre preferatele mele.",
    "label": "positive"
}
```

```json
{
    "text": "prea tare filmul!!!!de-abia ieri l-am vazut si mi-e placut foarte mult! merita vazut!:x",
    "label": "positive"
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 12
- Prefix prompt:

  ```text
  UrmeazƒÉ documentele »ôi sentimentul acestora, care poate fi pozitiv, neutru sau negativ.
  ```

- Base prompt template:

  ```text
  Document: {text}
  Sentiment: {label}
  ```

- Instruction-tuned prompt template:

  ```text
  Document: {text}

  Clasifica»õi sentimentul documentului. RƒÉspunde»õi cu pozitiv, neutru sau negativ, »ôi nimic altceva.
  ```

- Label mapping:
  - `positive` ‚û°Ô∏è `pozitiv`
  - `neutral` ‚û°Ô∏è `neutru`
  - `negative` ‚û°Ô∏è `negativ`

You can evaluate this dataset directly as follows:

```bash
euroeval --model <model-id> --dataset ro-sent
```

## Named Entity Recognition

### RoNEC

This dataset was published in [this paper](https://aclanthology.org/2020.lrec-1.546/).
The sentences have been extracted from a copy-right free newspaper,
covering several styles.

The original dataset consists of 9,000 / 1,330 / 2,000 samples for the
training, validation, and test splits, respectively. We use 1,024 / 256 / 2,048
samples for our training, validation and test splits, respectively. The training
and validation splits are subsets of the original splits, while the test split is
created using additional samples from the validation split.

Here are a few examples from the training split:

```json
{
    "tokens": ["√én", "secolele", "al", "XVII", "-lea", "»ôi", "al", "XVIII", "-lea", ",", "acestea", "erau", ":", "Conseil", "d'en", "haut", "(", "‚Äû", "√énaltul", "Consiliu", "‚Äù", ")", "-", "format", "din", "rege", ",", "prin»õul", "mo»ôtenitor", "(", "‚Äû", "le", "dauphin", "‚Äù", ")", ",", "cancelarul", ",", "controlorul", "general", "de", "finan»õe", "»ôi", "din", "secretarul", "de", "stat", "responsabil", "cu", "afacerile", "externe", "."],
    "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-PER", "O", "B-PER", "O", "O", "O", "O", "O", "O", "O", "O", "B-PER", "O", "B-PER", "O", "O", "O", "O", "O", "B-PER", "O", "O", "O", "O", "O", "O", "O"]
}
```

```json
{
    "tokens": ["DupƒÉ", "ce", "am", "trecut", "de", "Ob√¢r»ôia-Clo»ôani", "(", "localitate", "renumitƒÉ", "datoritƒÉ", "Pe»ôterii", "Clo»ôani", ",", "√Æn", "interiorul", "cƒÉreia", ",", "√Æn", "1961", ",", "s-", "a", "√Ænfiin»õat", "prima", "Sta»õiune", "de", "cercetƒÉri", "speologice", "din", "Rom√¢nia", ")", ",", "urcƒÉm", "la", "CumpƒÉna", "Apelor", ",", "de", "unde", "cobor√¢m", "br√¢ul", "drumului", "√Æn", "serpentine", "str√¢mte", ",", "p√¢nƒÉ", "√Æn", "Valea", "Cernei", "."],
    "labels": ["O", "O", "O", "O", "O", "B-LOC", "O", "O", "O", "O", "B-MISC", "I-MISC", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-LOC", "O", "O", "O", "O", "B-MISC", "I-MISC", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-MISC", "I-MISC", "O"]
}
```

```json
{
    "tokens": ["La", "data", "de", "26", "octombrie", "1994", ",", "»ôi-", "a", "sus»õinut", "teza", "de", "doctorat", "√Æn", "limba", "francezƒÉ", ",", "cu", "denumirea", "de", "La", "de", "l'homme", "la", "du", "Dumitru", "StƒÉniloae", "(", ")", ".", "Cartea", "a", "fost", "publicatƒÉ", "la", "Editura", "Trinitas", "din", "Ia»ôi", ",", "√Æn", "2003", ",", "cu", "prilejul", "‚Äû", "Anului", "StƒÉniloae", "‚Äù", "(", "100", "ani", "de", "la", "na»ôtere", "»ôi", "10", "de", "la", "trecerea", "sa", "la", "cele", "ve»ônice", ")", "."],
    "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-ORG", "I-ORG", "O", "B-LOC", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"]
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 8
- Prefix prompt:

  ```text
  Mai jos sunt propozi»õii »ôi dic»õionare JSON cu entitƒÉ»õile numite
  care apar √Æn propozi»õia datƒÉ.
  ```

- Base prompt template:

  ```text
  Propozi»õie: {text}
  EntitƒÉ»õi numite: {label}
  ```

- Instruction-tuned prompt template:

  ```text
  Propozi»õie: {text}

  IdentificƒÉ entitƒÉ»õile numite din propozi»õie. Ar trebui sƒÉ le enumeri
  ca un dic»õionar JSON cu cheile {labels_str}. Valorile cheilor ar
  trebui sƒÉ fie liste de entitƒÉ»õi numite de tipul respectiv, exact
  cum apar √Æn propozi»õie.
  ```

- Label mapping:
  - `B-PER` ‚û°Ô∏è `persoanƒÉ`
  - `I-PER` ‚û°Ô∏è `persoanƒÉ`
  - `B-LOC` ‚û°Ô∏è `loca»õie`
  - `I-LOC` ‚û°Ô∏è `loca»õie`
  - `B-ORG` ‚û°Ô∏è `organiza»õie`
  - `I-ORG` ‚û°Ô∏è `organiza»õie`
  - `B-MISC` ‚û°Ô∏è `diverse`
  - `I-MISC` ‚û°Ô∏è `diverse`

You can evaluate this dataset directly as follows:

```bash
euroeval --model <model-id> --dataset ronec
```

## Linguistic Acceptability

### ScaLA-ro

This dataset was published in [this paper](https://aclanthology.org/2023.nodalida-1.20/)
and was automatically created from the [Romanian Universal Dependencies
treebank](https://github.com/UniversalDependencies/UD_Romanian-RRT) by assuming that
the documents in the treebank are correct, and corrupting the samples to create
grammatically incorrect samples. The corruptions were done by either removing a word
from a sentence, or by swapping two neighbouring words in a sentence. To ensure that
this does indeed break the grammaticality of the sentence, a set of rules were used on
the part-of-speech tags of the words in the sentence.

The original full dataset consists of 1,024 / 256 / 2,048 samples for training,
validation and testing, respectively (so 3,328 samples used in total). These splits are
used as-is in the framework.

Here are a few examples from the training split:

```json
{
    "text": "Era o fantomƒÉ singuraticƒÉ, rostind un adevƒÉr pe care nimeni nu avea sƒÉ -l audƒÉ vreodatƒÉ.",
    "label": "correct"
}
```

```json
{
    "text": "Pe multe locuri avem apoi dovezi de o solicitudine deosebitƒÉ, nu numai pentru paza pƒÉdurilor, dar »ôi pentru nevoile locuitorilor sƒÉteni.",
    "label": "correct"
}
```

```json
{
    "text": "DacƒÉ experien»õa nu ne- a reu»ôit √ÆnsƒÉ, este numai numai »ôi din pricina timpului ur√¢t de afarƒÉ.",
    "label": "incorrect"
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 12
- Prefix prompt:

  ```text
  UrmƒÉtoarele sunt fraze »ôi dacƒÉ sunt gramatical corecte.
  ```

- Base prompt template:

  ```text
  Fraza: {text}
  Gramatical corect: {label}
  ```

- Instruction-tuned prompt template:

  ```text
  Fraza: {text}

  Stabili»õi dacƒÉ fraza este gramatical corectƒÉ sau nu. RƒÉspunde»õi cu 'da' dacƒÉ este corectƒÉ, »ôi cu 'nu' dacƒÉ nu este corectƒÉ. RƒÉspunde»õi doar cu acest cuv√¢nt, »ôi nimic altceva.
  ```

- Label mapping:
  - `correct` ‚û°Ô∏è `da`
  - `incorrect` ‚û°Ô∏è `nu`

You can evaluate this dataset directly as follows:

```bash
euroeval --model <model-id> --dataset scala-ro
```

## Reading Comprehension

### MultiWikiQA-ro

This dataset was published in [this paper](https://doi.org/10.48550/arXiv.2509.04111)
and contains Wikipedia articles with LLM-generated questions and answers in 300+
languages.

The original full dataset consists of 5,000 samples in a single split. We use a 1,024 /
256 / 2,048 split for training, validation and testing, respectively, sampled randomly.

Here are a few examples from the training split:

```json
{
    "context": "Cornel Braha»ô, pe numele real Ionel Vi»õu (n. 23 mai 1950, Poiana, Gala»õi ‚Äì d. 23 noiembrie 2005, BrƒÉhƒÉ»ôe»ôti, Gala»õi), a fost scriitor »ôi deputat rom√¢n √Æn legislatura 1992-1996, ales √Æn municipiul Bucure»ôti pe listele PUNR.\n\nBiografie\nA fost membru al Uniunii Scriitorilor din Rom√¢nia. \n\nA scris nouƒÉ volume de poezie, trei romane-document, o carte de reportaj, douƒÉ romane »ôi un volum de reportaj-document. \n\nActivitate politicƒÉ/Func»õii: \n membru PUNR (1990-1994, exclus), apoi Partidul Dreapta Rom√¢neascƒÉ (din 1995) si PPR;\n deputat PUNR de Bucure»ôti (27.09.1992-3.11.1996);\n vicepre»ôedinte al PUNR (3.10.1992) »ôi pre»ôedinte al filialei Bucure»ôti (1992-9.11.1994);\n purtƒÉtor de cuv√¢nt al PUNR (eliberat la 7.09.1994);\n secretar executiv al Partidului Dreapta Rom√¢neascƒÉ (1995-2000);\n vicepre»ôedinte al PPR (3.02.2000)\n\nOpera\nP√¢nƒÉ la capƒÉt »ôi mai departe (roman-document)\n53 de poeme de dragoste »ôi speran»õƒÉ\nPoezii foarte frumoase\nSf√¢r»ôit de v√¢nƒÉtoare\nPenultimele poeme de dragoste\nPoezii din capul meu\n√éntors\nDespre mor»õi numai de bine (reportaj-document)\nClasa muncitoare - clasa deschisƒÉ (roman √Æn probe)\nMocƒÉne»ôti. Oamenii dracului\nMor»õii nu mai »ôtiu drumul cƒÉtre casƒÉ (roman, Ed. MilitarƒÉ 1990)\nAnno Domini - 2004 \nLaptus Vulgata\nJurnal dactilografiat (1985-1989)\nPoezii fƒÉrƒÉ mijloace\n\nNa»ôteri √Æn 1950\nDecese √Æn 2005\nDeputa»õi rom√¢ni 1992-1996\nScriitori rom√¢ni din secolul al XX-lea\nPoliticieni rom√¢ni din secolul al XX-lea\nScriitori rom√¢ni din secolul al XXI-lea\nPoliticieni rom√¢ni din secolul al XXI-lea\nMembri ai Uniunii Scriitorilor din Rom√¢nia\nRom√¢ni cunoscu»õi sub pseudonimele folosite\nPoe»õi rom√¢ni din secolul al XX-lea\nPoe»õi rom√¢ni din secolul al XXI-lea\nMembri ai PUNR\nScriitori cunoscu»õi sub pseudonimele folosite",
    "question": "Cum se nume»ôte romanul pe care Cornel Braha»ô l-a publicat √Æn anul 1990 la Editura MilitarƒÉ?",
    "answers": {
        "answer_start": [1136],
        "text": ["Mor»õii nu mai »ôtiu drumul cƒÉtre casƒÉ"]
    }
}
```

```json
{
    "context": "Gerardus Mercator () a fost un cartograf, geograf »ôi matematician flamand de renume din Rena»ôtere. Acest nume este latinizat, un obicei pe atunci foarte rƒÉsp√¢ndit; numele sƒÉu real √Æn germanƒÉ a fost Gerhard Kremer (‚ÄûKremer‚Äù √ÆnseamnƒÉ ‚Äûnegustor‚Äù). S-a nƒÉscut la 5 martie 1512 la Rupelmonde, Flandra, »ôi a murit la 2 decembrie 1594 √Æn Duisburg, Germania. A fost considerat un  \"Ptolemeu contemporan\".\n\nMercator se considera cercetƒÉtor cosmograf care nu e nevoit sƒÉ v√¢ndƒÉ hƒÉr»õi. De la el au rƒÉmas doar 5 hƒÉr»õi, pƒÉstrate √Æn Muzeul de istorie din Duisburg. √én anul 1562 realizeazƒÉ prima hartƒÉ a Europei, care este una din hƒÉr»õile atlasului sƒÉu. Numele »ôi l-a schimbat √Æn perioada c√¢nd era la Universitatea Essen-Duisburg.\n\nRealizƒÉri \n\n 1530 devine \"Magister\" la \"Universitatea catolicƒÉ\" din Leuven\n 1537 √ÆnsƒÉrcineazƒÉ pe me»ôte»ôugarul Gaspard van der Heyden sƒÉ-i confec»õioneze globul terestru, »ôi bolta cerului\n 1537 Harta \"PƒÉm√¢ntului sf√¢nt\"\n 1538 o hartƒÉ micƒÉ de proiec»õie √Æn formƒÉ de inimƒÉ a lumii, »ôi o hartƒÉ de perete a Flandrei\n 1540 publicƒÉ cartea  Literarum latinarum, quas italicas, cursoriasque vocant, scribendarum ratio, (pe lemn)\n 1541 √Æ»ôi continuƒÉ cercetƒÉrile de proiec»õie a globului pe o hartƒÉ (plan), are probleme  cu biserica catolicƒÉ (acuzat de erezie)\n 1551 realizeazƒÉ un nou glob pƒÉm√¢ntesc »ôi unul al boltei cere»ôti\n 1552 urmƒÉrit de inchizi»õie se refugiazƒÉ cu toatƒÉ familia la Duisburg, principatul J√ºlich-Kleve-Berg, prin»õul  Wilhelm der Reiche fiind sub influen»õa humanistului Erasmus von Rotterdam\n 1554 Realizarea lui cea mai valoroasƒÉ este \"Proiec»õia Mercator\", o proiec»õie a globului terestru pe un plan (hartƒÉ). AceastƒÉ proiec»õie redƒÉ fidel unghiurile, fiind prin aceasta de importan»õƒÉ majorƒÉ pentru naviga»õia pe PƒÉm√¢nt.\n 1559 - 1562 predƒÉ matematicƒÉ »ôi cosmologie la Gimnaziul din Duisburg\n 1563 este numit de  Wilhelm der Reiche cartograf princiar\n 1562 Sub √ÆndrumƒÉrile lui Johannes Corputius, √Æntocme»ôte o hartƒÉ exactƒÉ a Duisburgului\n 1594 moare ca un om respectat »ôi bogat, fiind √Ængropat √Æn cimitirul bisericii \"Salvator\" din Duisburg.\n\nNote\n\nBibliografie\n\nLegƒÉturi\xa0externe\n\n Cartographic images of maps and globes \nMercator\'s maps at the Eran Laor Cartographic Collection, the National Library of Israel\n\nNa»ôteri √Æn 1512\nDecese √Æn 1594\nExploratori belgieni\nCartografi flamanzi\nPerioada Marilor descoperiri\nIstoria naviga»õiei\nEponime ale craterelor de pe LunƒÉ\nEponime ale asteroizilor",
    "question": "√én ce an s-a nƒÉscut Gerardus Mercator?",
    "answers": {
        "answer_start": [259],
        "text": ["5 martie 1512"]
    }
}
```

```json
{
    "context": "Un cod de aeroport ICAO sau un identificator de loca»õie ICAO este un cod alfanumeric, format din patru litere, care desemneazƒÉ fiecare din aeroporturile din lume.  Aceste coduri au fost definite de International Civil Aviation Organization »ôi au fost publicate √Æn documentul ICAO 7910: Location Indicators ().\n\nCodurile ICAO sunt folosite √Æn controlul traficului aerian »ôi √Æn operƒÉrile liniilor aeriene cum ar fi planificarea zborurilor.  Ele nu sunt acela»ôi lucru cu codurile IATA, √Ænt√¢lnite de publicul obi»ônuit »ôi folosite de cƒÉtre companiile aeriene √Æn orarele zborurilor, rezervƒÉri »ôi opera»õiile legate de bagaje.  Codurile ICAO sunt folosite de asemenea pentru identificarea altor loca»õii precum sta»õii meteo, sta»õii interna»õionale de servicii ale zborurilor sau centre de control al zonelor, fie cƒÉ acestea sunt amplasate sau nu √Æn aeroporturi.\n\nSpre deosebire de codurile IATA, codurile ICAO au o structurƒÉ regionalƒÉ la bazƒÉ, astfel √Ænc√¢t ele nu sunt duplicate ci identifica un singur aeroport.  √én general, prima literƒÉ alocatƒÉ dupƒÉ continent »ôi reprezintƒÉ o »õarƒÉ sau un grup de »õƒÉri de pe acel continent.  A doua literƒÉ √Æn general reprezintƒÉ o »õarƒÉ din acea regiune, iar celelate douƒÉ litere rƒÉmase sunt folosite la identificarea fiecƒÉrui aeroport.  Excep»õiile de la aceastƒÉ regulƒÉ sunt »õƒÉrile foarte √Æntinse care au coduri de »õarƒÉ formate dintr-o singurƒÉ literƒÉ, iar celelalte trei litere care rƒÉm√¢n desemneazƒÉ aeroportul.\n\n√én zona √ÆntinsƒÉ formatƒÉ de Statele Unite »ôi Canada, celor mai multor aeroporturi li se asociazƒÉ codurile de trei litere IATA, care sunt acelea»ôi cu codurile lor ICAO, √ÆnsƒÉ fƒÉrƒÉ litera K sau C de la √Ænceput, d.e., YYC »ôi CYYC (Calgary International Airport, Calgary, Alberta), IAD »ôi KIAD (Dulles International Airport, Chantilly, Virginia).  Aceste coduri nu trebuie confundate cu semnalele de apel pentru radio sau pentru televiziune, chiar dacƒÉ ambele »õƒÉri folosesc semnale de apel de formate din patru litere care √Æncep cu aceste litere.\n\nTotu»ôi, fiindcƒÉ Alaska, Hawaii »ôi alte teritorii din Statele Unite au propriile prefixe ICAO formate din douƒÉ litere, situa»õia pentru ele este similarƒÉ altor »õƒÉri mici, iar codurile ICAO ale aeroporturilor lor sunt √Æn general diferite de identificatoarele FAA/IATA formate din trei litere.  De exemplu, Hilo International Airport (PHTO comparativ cu ITO) »ôi Juneau International Airport (PAJN comparativ cu JNU).\n\nZZZZ este un cod special care se folose»ôte atunci c√¢nd nu existƒÉ nici un cod ICAO pentru aeroport, »ôi este folosit de obicei √Æn planurile de zbor.\n\nAeroportul Interna»õional Henri CoandƒÉ din Otopeni are codul LROP, iar Aeroportul Interna»õional Aurel Vlaicu de la BƒÉneasa are codul LRBS  .\n\nPrefixuri\n\nVezi »ôi\nListƒÉ de aeroporturi dupƒÉ codul ICAO\nListƒÉ de aeroporturi dupƒÉ codul IATA\nAeroport\n\nLegƒÉturi externe\nInternational Civil Aviation Organization (official site)\nICAO On-line Publications Purchasing  (official site)\nICAO 7910 - Location Indicators (online version provided by EUROCONTROL)\nCatalogue of ICAO Airfields \nICAO airport code prefixes \n\nCoduri\nAeroporturi",
    "question": "Care este codul ICAO pentru un aeroport?",
    "answers": {
        "answer_start": [66],
        "text": ["un cod alfanumeric, format din patru litere, care desemneazƒÉ fiecare din aeroporturile din lume"]
    }
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 4
- Prefix prompt:

  ```text
  IatƒÉ texte cu √ÆntrebƒÉri »ôi rƒÉspunsuri √Ænso»õite.
  ```

- Base prompt template:

  ```text
  Text: {text}
  √éntrebare: {question}
  RƒÉspuns de maxim 3 cuvinte:
  ```

- Instruction-tuned prompt template:

  ```text
  Text: {text}

  RƒÉspunde la urmƒÉtoarea √Æntrebare referitoare la textul de mai sus folosind maxim 3 cuvinte.

  √éntrebare: {question}
  ```

You can evaluate this dataset directly as follows:

```bash
euroeval --model <model-id> --dataset multi-wiki-qa-ro
```

## Knowledge

### MMLU-hu

This dataset is a machine translated version of the English [MMLU
dataset](https://openreview.net/forum?id=d7KBjmI3GmQ) and features questions within 57
different topics, such as elementary mathematics, US history and law. The translation to
Hungarian was done by the University of Oregon as part of [this
paper](https://aclanthology.org/2023.emnlp-demo.28/), using GPT-3.5-turbo.

The original full dataset consists of 278 / 1,408 / 13,024 samples for training,
validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training,
validation and testing, respectively (so 3,328 samples used in total). These splits are
new and there can thus be some overlap between the original validation and test sets and
our validation and test sets.

Here are a few examples from the training split:

```json
{
    "text": "Ha a College Board az egyik √©vben elhanyagolta volna az agykutat√°ssal kapcsolatos k√©rd√©sek felt√©tel√©t az AP pszichol√≥giai vizsg√°n, a teszt hi√°nyozni foghat.\nV√°laszlehet≈ës√©gek:\na. konstruktum validit√°st.\nb. predikt√≠v validit√°st.\nc. egyidej≈± validit√°st.\nd. tartalmi validit√°st.",
    "label": "d"
}
```

```json
{
    "text": "Ha $\\log_{b}343=-\\frac{3}{2}$, mennyi az $b$ √©rt√©ke?\nV√°laszlehet≈ës√©gek:\na. 3\nb. \\frac{1}{49}\nc. \\frac{1}{7}\nd. 7",
    "label": "b"
}
```

```json
{
    "text": "Egy gyalog, akinek lakhelye az A √°llamban van, az B √°llamban keresztezte az utat, amikor egy k√ºlf√∂ldi √°llampolg√°r √°ltal vezetett aut√≥ elg√°zolta. Mindk√©t f√©l s√©r√ºl√©seket szenvedett. A gyalog $100,000 k√°rt√©r√≠t√©si √∂sszeget k√©r≈ë k√°rt√©r√≠t√©si pert ind√≠tott a vezet≈ëvel szemben az B √°llam sz√∂vets√©gi ker√ºleti b√≠r√≥s√°g√°ban. A vezet≈ë √∫gy v√©li, hogy a gyalog illeg√°lisan keresztezte az utat, √©s ez√©rt ≈ë a felel≈ës az √ºtk√∂z√©s√©rt. Az √ºgyv√©d tan√°csad√°st k√©r a vezet≈ët≈ël arra vonatkoz√≥an, hogy hogyan kell a legjobban reag√°lni a keresetre. Felt√©telezz√ºk, hogy B √°llam egy olyan hozz√°j√°rul√≥ hanyags√°g √°llam, amely szerint mindk√©t f√©l r√©szben felel≈ës az eset√©rt. Hogyan tan√°csolja az √ºgyv√©d a vezet≈ënek, hogy reag√°ljon erre?\nV√°laszlehet≈ës√©gek:\na. V√°laszk√©nt adjon be egy beadv√°nyt, amelyben az hozz√°j√°rul√≥ hanyags√°g pozit√≠v v√©delm√©t √©s a gondatlans√°g elleni ellenk√©relmet emeli, a vezet≈ë s√©r√ºl√©seinek k√°rt√©r√≠t√©si √∂sszeg√©t k√©rve.\nb. V√°laszk√©nt adjon be egy beadv√°nyt, amelyben az hozz√°j√°rul√≥ hanyags√°g pozit√≠v v√©delm√©t √©s az anyagi bizony√≠t√©k alapj√°n t√∂rt√©n≈ë √≠t√©let k√©relm√©vel v√©dekezik.\nc. K√©rje az √ºgy elutas√≠t√°s√°t a szem√©lyi hat√°sk√∂r hi√°nya miatt, mert az aut√≥ vezet≈ëje nem B √°llam √°llampolg√°ra.\nd. K√©rje az √ºgy elutas√≠t√°s√°t az √ºgy t√°rgyi hat√°sk√∂r√©nek hi√°nya miatt, mert az aut√≥ vezet≈ëje nem amerikai √°llampolg√°r.",
    "label": "a"
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 5
- Prefix prompt:

  ```text
  Az al√°bbiakban t√∂bb v√°laszt√°si lehet≈ës√©get tartalmaz√≥ k√©rd√©sek tal√°lhat√≥k (v√°laszokkal egy√ºtt).
  ```

- Base prompt template:

  ```text
  K√©rd√©s: {text}
  V√°laszlehet≈ës√©gek:
  a. {option_a}
  b. {option_b}
  c. {option_c}
  d. {option_d}
  V√°lasz: {label}
  ```

- Instruction-tuned prompt template:

  ```text
  K√©rd√©s: {text}

  V√°laszoljon a fenti k√©rd√©sre az el√©rhet≈ë lehet≈ës√©gek k√∂z√ºl 'a', 'b', 'c' vagy 'd' haszn√°lat√°val, √©s semmi m√°ssal.
  ```

You can evaluate this dataset directly as follows:

```bash
euroeval --model <model-id> --dataset mmlu-hu
```

## Common-sense Reasoning

### Winogrande-hu

This dataset was published in [this paper](https://doi.org/10.48550/arXiv.2506.19468)
and is a translated and filtered version of the English [Winogrande
dataset](https://doi.org/10.1145/3474381).

The original full dataset consists of 47 / 1,210 samples for training and testing, and
we use 128 of the test samples for validation, resulting in a 47 / 128 / 1,085 split for
training, validation and testing, respectively.

Here are a few examples from the training split:

```json
{
    "text": "Nem tudtam ir√°ny√≠tani a nedvess√©get √∫gy, mint az es≈ët, mert a _ mindenhol bej√∂tt. Mire utal a hi√°nyz√≥ _?\nV√°laszlehet≈ës√©gek:\na. nedvess√©g\nb. es≈ë",
    "label": "a"
}
```

```json
{
    "text": "Jessica √∫gy gondolta, hogy a Sandstorm a valaha √≠rt legjobb dal, de Patricia ut√°lta. _ jegyet vett a jazz koncertre. Mire utal a hi√°nyz√≥ _?\nV√°laszlehet≈ës√©gek:\na. Jessica\nb. Patricia",
    "label": "b"
}
```

```json
{
    "text": "A termoszt√°t azt mutatta, hogy lent h√∫sz fokkal h≈±v√∂sebb volt, mint fent, √≠gy Byron a _ maradt, mert f√°zott. Mire utal a hi√°nyz√≥ _?\nV√°laszlehet≈ës√©gek:\na. lent\nb. fent",
    "label": "b"
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 5
- Prefix prompt:

  ```text
  Az al√°bbiakban t√∂bb v√°laszt√°si lehet≈ës√©get tartalmaz√≥ k√©rd√©sek tal√°lhat√≥k (v√°laszokkal egy√ºtt).
  ```

- Base prompt template:

  ```text
  K√©rd√©s: {text}
  Lehet≈ës√©gek:
  a. {option_a}
  b. {option_b}
  V√°lasz: {label}
  ```

- Instruction-tuned prompt template:

  ```text
  K√©rd√©s: {text}
  Lehet≈ës√©gek:
  a. {option_a}
  b. {option_b}

  V√°laszoljon a fenti k√©rd√©sre az el√©rhet≈ë lehet≈ës√©gek k√∂z√ºl 'a' vagy 'b' haszn√°lat√°val, √©s semmi m√°ssal.
  ```

You can evaluate this dataset directly as follows:

```bash
euroeval --model <model-id> --dataset winogrande-hu
```

## Summarisation

### HunSum

[The dataset](https://huggingface.co/datasets/ariel-ml/hun-sum-chatml-5k) consists of samples
from Hungarian news articles, with the summaries given by the lead paragraphs.

The original full dataset consists of 5,000 / 200 / 200 samples for training,
validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training,
validation and testing, respectively.

Here are a few examples from the training split:

```json
{
    "text": "M√°sf√©l √©ven bel√ºl rend lehet Sz√≠ri√°ban\n\nA sz√≠riai korm√°ny √©s az ellenz√©ki csoportok k√©pvisel≈ëi m√©g id√©n t√°rgyal√°sokat kezden√©nek, f√©l √©ven bel√ºl √°tmeneti korm√°ny alakulna, m√°sf√©l √©ven bel√ºl pedig v√°laszt√°sokat tartan√°nak a tervek szerint ‚Äì k√∂z√∂lte Frank-Walter Steinmeier n√©met k√ºl√ºgyminiszter.\n\nJohn Kerry amerikai k√ºl√ºgyminiszter szerint ahhoz, hogy mindezt el√©rj√©k, t≈±zsz√ºnetet kell hirdetni a korm√°ny √©s a l√°zad√≥ csoportok k√∂z√∂tt. Az ENSZ Biztons√°gi Tan√°cs√°nak √∂t √°lland√≥ tagja megegyezett, hogy hat√°rozatot fogad el err≈ël. Kerry szerint a legfontosabb, hogy ne a m√©rs√©kelt ellenz√©kkel szembeni harc, hanem az Iszl√°m √Ållam (I√Å) √©s az an-Nuszra Front ellen k√ºzdelem folytat√≥djon.\n\nAz amerikai k√ºl√ºgyminiszter elmondta: az Egyes√ºlt √Ållamok √©s Oroszorsz√°g k√∂z√∂tt v√©lem√©nyk√ºl√∂nbs√©gek vannak, azonban folytatni kell a k√∂z√∂s munk√°t, ahogy ezt az Ir√°nnal folytatott t√°rgyal√°sok kapcs√°n tett√©k kor√°bban, √©s hozz√°tette: a t√°rgyal√≥partnerek mindannyian Sz√≠ria stabilit√°s√°t tartj√°k szem el≈ëtt.\n\nSzergej Lavrov orosz k√ºl√ºgyminiszter a sajt√≥t√°j√©koztat√≥n kijelentette: csak a sz√≠riai emberek d√∂nthetnek orsz√°guk √©s eln√∂k√ºk sors√°r√≥l. Lavrov szerint a val√≥di ellens√©g azonban nem Aszad, hanem az I√Å. Elmondta azt is, hogy a t√°rgyal√°son r√©szt vev≈ë orsz√°gok sz√°mba vett√©k a terrorcsoportokat, ezen list√°k √∂sszehangol√°s√°t Jord√°nia v√©gzi majd, √©s az ENSZ Biztons√°gi Tan√°csa szavazni fog r√≥la.\n\nA b√©kefolyamatot Staffan de Mistura, az ENSZ sz√≠riai k√ºl√∂nmegb√≠zottja vezeti √©s szervezi majd ‚Äì mondta Frank-Walter Steinmeier a 17 orsz√°g magas rang√∫ k√©pvisel≈ëinek r√©szv√©tel√©vel zajl√≥ tan√°cskoz√°s ut√°n.",
    "target_text": "Swaney Elizabeth tr√ºkk√∂k n√©lk√ºl mutatta be a gyakorlatait, pedig ennek a sport√°gnak pont az lenne a l√©nyege."
}
```

```json
{
    "text": "Hoffmann R√≥zsa a CEU-r√≥l: eddig is j√°rtak magyar fiatalok b√©csi egyetemekre\n\nAz ATV Egyenes besz√©d c√≠m≈± m≈±sor√°nak vend√©ge volt h√©tf≈ë este Hoffmann R√≥zsa. A volt k√∂znevel√©s√©rt felel≈ës √°llamtitk√°rt a CEU-r√≥l is k√©rdezt√©k, ezzel kapcsolatban a politikus azt mondta, szerinte nem a korm√°ny ≈±zte el az egyetemet, hanem az int√©zm√©ny d√∂nt√∂tt √∫gy, hogy az amerikai diplom√°t ad√≥ k√©pz√©seiket kiviszik az orsz√°gb√≥l.\n\nAmikor a m≈±sorvezet≈ë megk√©rdezte Hoffmannt√≥l, hogy j√≥l van-e ez √≠gy, Hoffmann azt v√°laszolta:\n\n  Nem tudom, hogy j√≥l van, vagy nincs j√≥l, de B√©cs nincs a vil√°g v√©g√©n.\n\nA politikus hozz√°tette, nincs ebben semmi k√ºl√∂n√∂s, hiszen eddig is j√°rtak magyar fiatalok b√©csi egyetemekre, ing√°zni is sokan ing√°ztak eddig. Hoffmann azt mondta, "emberileg" meg√©rti a CEU vezet≈ës√©g√©nek elkesered√©s√©t, de szerinte ez egy t√∫lpolitiz√°lt √ºgy.\n\nH√©tf≈ën eld≈ëlt, hogy a CEU B√©csbe k√∂lt√∂zteti el amerikai diplom√°t ad√≥ k√©pz√©seit, miut√°n az elm√∫lt 20 h√≥napban mindent megtettek az√©rt, hogy megfeleljenek a t√∂rv√©nyeknek, a magyar hat√≥s√°gok viszont annak ellen√©re sem √≠rt√°k al√° a m≈±k√∂d√©shez sz√ºks√©ges √°llamk√∂zi meg√°llapod√°st, hogy a CEU az amerikai hat√≥s√°gok √°ltal j√≥v√°hagyott fels≈ëoktat√°si k√©pz√©st ind√≠tott az Egyes√ºlt √Ållamokban.\n\nAz egyetem ugyanakkor k√∂zlem√©nye szerint meg≈ërzi magyar egyetemi akkredit√°ci√≥j√°t, √©s arra t√∂rekszik, hogy a j√∂v≈ëben is folytasson tan√≠t√°si √©s kutat√°si tev√©kenys√©get Budapesten.",
    "target_text": "A volt k√∂znevel√©si √°llamtitk√°r \"emberileg\" meg√©rti az egyetem vezet≈ëinek elkeseredetts√©g√©t."
}
```

```json
{
    "text": "P√∂r√∂g a turizmus Budapesten: elk√©peszt≈ëen er≈ës volt az okt√≥ber\n\nUgyanakkor k√©rd√©sesnek nevezik, hogy a kiugr√≥ n√∂veked√©s tart√≥snak bizonyul-e november-decemberben is, √©s ami tal√°n m√©g enn√©l is fontosabb: a k√ºsz√∂b√∂n √°ll√≥ - imm√°r 2020. janu√°r 31-i hat√°rid≈ëvel √©les√≠tett - Brexit, √©s annak gazdas√°gi k√∂vetkezm√©nyei milyen hat√°st id√©znek el≈ë a k√∂vetkez≈ë h√≥napok, √©vek budapesti vend√©gforgalm√°ban √©s a kiutaz√°si trendekben.\nA f≈ëv√°rosi kereskedelmi sz√°ll√°shelyek √°rbev√©tele megk√∂zel√≠tette a 25 milli√°rd forintot. Hossz√∫ id≈ë √≥ta el≈ësz√∂r nem csup√°n a sz√°ll√°sd√≠j-bev√©telek emelkedtek sz√°mottev≈ëen, hanem a vend√©gforgalom is - jegyezt√©k meg.\nBudapesten a vend√©g√©rkez√©sek 5,5 sz√°zal√©kkal n≈ëttek a vend√©g√©jszak√°k pedig 6,3 sz√°zal√©kkal.\nAz elemz√©s szerint ezen bel√ºl a h√∫z√≥er≈ë a k√ºlf√∂ldi vend√©gforgalom volt: okt√≥berben 372 068 vend√©g √©rkezett √©s 862 427 vend√©g√©jszak√°t t√∂lt√∂tt el, el≈ëbbi 8,3 sz√°zal√©kos, ut√≥bbi 9,6 sz√°zal√©kos n√∂veked√©st mutat az el≈ëz≈ë √©v tizedik h√≥napj√°val √∂sszehasonl√≠tva. Mindek√∂zben a belf√∂ldr≈ël √©rkez≈ë vend√©gforgalom tov√°bb cs√∂kkent.\nA Sz√©chenyi Pihen≈ë K√°rtya k√∂lt√©si √©rt√©ke okt√≥berben 69,4 milli√≥ forintot √©rt el Budapest kereskedelmi sz√°ll√°shelyein, ez az els≈ë 10 havi - budapesti - SZ√âP K√°rtya-bev√©tel 10 sz√°zal√©ka. A janu√°r √≥ta Budapesten keletkezett, nagys√°grendileg 700 milli√≥ forintos SZ√âP K√°rtya-√°rbev√©tel 55 sz√°zal√©kos n√∂veked√©s a tavalyi √©v azonos id≈ëszak√°ban el√©rt 450 milli√≥ forint k√∂zeli √°rbev√©telhez k√©pest.\nA k√ºld≈ëorsz√°gok k√∂z√∂tt p√©ld√°ul kiemelt√©k, hogy impoz√°ns n√∂veked√©si √ºtemet mutat a francia, az izraeli, az orosz √©s a brit k√ºld≈ëpiac, az ut√≥bbi h√≥napokban pedig felz√°rk√≥zott a TOP10-be Lengyelorsz√°g is.", "target_text": "Az idei okt√≥ber volt a 2019-es √©v legdinamikusabban n√∂vekv≈ë h√≥napja a vend√©g√©rkez√©seket √©s a vend√©g√©jszak√°kat tekintve Budapesten - h√≠vta fel a figyelmet a Budapesti Fesztiv√°l- √©s Turisztikai K√∂zpont (BFTK) elemz√©s√©ben."
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 1
- Prefix prompt:

  ```text
  Az al√°bbi sz√∂vegek tartalmazz√°k az eredeti cikkeket √©s azok √∂sszefoglal√≥it.
  ```

- Base prompt template:

  ```text
  Sz√∂veg: {text}
  √ñsszefoglal√≥: {target_text}
  ```

- Instruction-tuned prompt template:

  ```text
  Sz√∂veg: {text}

  Adjon egy r√∂vid √∂sszefoglal√≥t a fenti sz√∂vegr≈ël.
  ```

You can evaluate this dataset directly as follows:

```bash
euroeval --model <model-id> --dataset hunsum
```

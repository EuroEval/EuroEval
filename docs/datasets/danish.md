# üá©üá∞ Danish

This is an overview of all the datasets used in the Danish part of ScandEval. The
datasets are grouped by their task - see the [task overview](/tasks) for more
information about what these constitute.


## Sentiment Classification

### Angry Tweeets

This dataset was published in [this
paper](https://aclanthology.org/2021.nodalida-main.53/) and was a crowd-sourcing effort
to annotate sentiment of Danish tweets. The original full dataset consists of 3,458
samples, and we are using a split of 1,024 / 256 / 2,048 samples for training,
validation and testing, respectively (so 3,328 samples used in total). All the samples
in the original test set are included in our test set, but our test set is furthermore
using a subset of the original training set as test samples as well. The original
dataset did not have a validation split, so we have created one by sampling from the
training set.

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 12
- Prefix prompt:
  ```
  F√∏lgende er tweets og deres sentiment, som kan v√¶re 'positiv', 'neutral'
  eller 'negativ'.
  ```
- Base prompt template:
  ```
  Tweet: {text}\nSentiment: {label}
  ```
- Instruction-tuned prompt template:
  ```
  Tweet: {text}\n\nKlassificer sentimentet i tweetet. Svar kun med 'positiv',
  'neutral' eller 'negativ'.
  ```
- Label mapping:
    - `positive` ‚û°Ô∏è `positiv`
    - `neutral` ‚û°Ô∏è `neutral`
    - `negative` ‚û°Ô∏è `negativ`


## Named Entity Recognition

### DANSK

This dataset was published in [this
paper](https://doi.org/10.3384/nejlt.2000-1533.2024.5249) and is a manually annotated
subset of [Danish Gigaword](https://aclanthology.org/2021.nodalida-main.46/) with the 18
different named entities, following the OntoNotes 5.0 scheme. It was annotated by 10
different annotators.

The original full dataset consists of 15,062 samples, and we are using a split of 1,024
/ 256 / 1,024 samples for training, validation and testing, respectively (so 2,304
samples used in total). All samples in the validation and test sets of our version also
belong to the original validation and test set, respectively.

We have furthermore converted the OntoNotes 5.0 labelling scheme to the CoNLL-2003
labelling scheme, which is more common in the NER literature. The mapping is as follows:

- `PERSON` ‚û°Ô∏è `PER`
- `LOCATION` ‚û°Ô∏è `LOC`
- `FACILITY` ‚û°Ô∏è `LOC`
- `GPE` ‚û°Ô∏è `LOC`
- `ORGANIZATION` ‚û°Ô∏è `PER`
- `EVENT` ‚û°Ô∏è `MISC`
- `LANGUAGE` ‚û°Ô∏è `MISC`
- `PRODUCT` ‚û°Ô∏è `MISC`
- `WORK OF ART` ‚û°Ô∏è `MISC`
- `NORP` ‚û°Ô∏è `MISC`
- `CARDINAL` ‚û°Ô∏è `O`
- `DATE` ‚û°Ô∏è `O`
- `LAW` ‚û°Ô∏è `O`
- `MONEY` ‚û°Ô∏è `O`
- `ORDINAL` ‚û°Ô∏è `O`
- `PERCENT` ‚û°Ô∏è `O`
- `QUANTITY` ‚û°Ô∏è `O`
- `TIME` ‚û°Ô∏è `O`

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 8
- Prefix prompt:
  ```
  F√∏lgende er s√¶tninger og JSON-ordb√∏ger med de navngivne enheder, som
  forekommer i den givne s√¶tning.
  ```
- Base prompt template:
  ```
  S√¶tning: {text}\nNavngivne enheder: {label}
  ```
- Instruction-tuned prompt template:
  ```
  S√¶tning: {text}\n\nIdentific√©r de navngivne enheder i s√¶tningen. Du skal
  outputte dette som en JSON-ordbog med n√∏glerne 'person', 'sted',
  'organisation' og 'diverse'. V√¶rdierne skal v√¶re lister over de navngivne
  enheder af den type, pr√¶cis som de forekommer i s√¶tningen.
  ```
- Label mapping:
    - `B-PER` ‚û°Ô∏è `person`
    - `I-PER` ‚û°Ô∏è `person`
    - `B-LOC` ‚û°Ô∏è `sted`
    - `I-LOC` ‚û°Ô∏è `sted`
    - `B-ORG` ‚û°Ô∏è `organisation`
    - `I-ORG` ‚û°Ô∏è `organisation`
    - `B-MISC` ‚û°Ô∏è `diverse`
    - `I-MISC` ‚û°Ô∏è `diverse`


### Unofficial: DaNE

Missing.


## Linguistic Acceptability

### ScaLA-da

This dataset was published in [this paper](https://aclanthology.org/2023.nodalida-1.20/)
and was automatically created from the [Danish Universal Dependencies
treebank](https://github.com/UniversalDependencies/UD_Danish-DDT/tree/master) by
assuming that the documents in the treebank are correct, and corrupting the samples to
create grammatically incorrect samples. The corruptions were done by either removing a
word from a sentence, or by swapping two neighbouring words in a sentence. To ensure
that this does indeed break the grammaticality of the sentence, a set of rules were used
on the part-of-speech tags of the words in the sentence.

The original full dataset consists of 1,024 / 256 / 2,048 samples for training,
validation and testing, respectively (so 3,328 samples used in total). These splits are
used as-is in the framework.

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 12
- Prefix prompt:
  ```
  F√∏lgende er s√¶tninger og om de er grammatisk korrekte.
  ```
- Base prompt template:
  ```
  S√¶tning: {text}\nGrammatisk korrekt: {label}
  ```
- Instruction-tuned prompt template:
  ```
  S√¶tning: {text}\n\nBestem om s√¶tningen er grammatisk korrekt eller ej. Svar med 'ja',
  hvis s√¶tningen er korrekt, og 'nej', hvis den ikke er.
  ```
- Label mapping:
    - `correct` ‚û°Ô∏è `ja`
    - `incorrect` ‚û°Ô∏è `nej`


## Reading Comprehension

### ScandiQA-da

Missing.


## Knowledge

### Danske Talem√•der

Missing.

### Danish Citizen Tests

Missing.

### Unofficial: MMLU-da

Missing.

### Unofficial: ARC-da

Missing.


## Common-sense Reasoning

### HellaSwag-da

Missing.


## Summarization

### Nordjylland News

Missing.

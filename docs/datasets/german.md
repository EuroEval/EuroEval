# üá©üá™ German

This is an overview of all the datasets used in the German part of ScandEval. The
datasets are grouped by their task - see the [task overview](/tasks) for more
information about what these constitute.


## Sentiment Classification

### SB10k

[description]

[size-info]

Here are a few examples from the training split:

```json
[example-1]
```
```json
[example-2]
```
```json
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 12
- Prefix prompt:
  ```
  Im Folgenden sind Tweets und ihre Stimmung aufgef√ºhrt, die 'positiv', 'neutral' oder 'negativ' sein kann.
  ```
- Base prompt template:
  ```
  Tweet: {text}
  Stimmungslage: {label}
  ```
- Instruction-tuned prompt template:
  ```
  Tweet: {text}

  Klassifizieren Sie die Stimmung im Tweet. Antworten Sie mit 'positiv', 'neutral' oder 'negativ'.
  ```
- Label mapping:
    - `positive` ‚û°Ô∏è `positiv`
    - `neutral` ‚û°Ô∏è `neutral`
    - `negative` ‚û°Ô∏è `negativ`

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset sb10k
```


## Named Entity Recognition

### GermEval

[description]

[size-info]

Here are a few examples from the training split:

```json
[example-1]
```
```json
[example-2]
```
```json
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 8
- Prefix prompt:
  ```
  Es folgen S√§tze und JSON-W√∂rterb√ºcher mit den benannten Entit√§ten, die in der angegebenen Phrase vorkommen.
  ```
- Base prompt template:
  ```
  Satz: {text}
  Benannte Entit√§ten: {label}
  ```
- Instruction-tuned prompt template:
  ```
  Satz: {text}

  Identifizieren Sie die benannten Entit√§ten im Satz. Sie sollten dies als JSON-W√∂rterbuch mit den Schl√ºsseln 'person', 'ort', 'organisation' und 'verschiedenes' ausgeben. Die Werte sollten Listen der benannten Entit√§ten dieses Typs sein, genau wie sie im Satz erscheinen.
  ```
- Label mapping:
    - `B-PER` ‚û°Ô∏è `person`
    - `I-PER` ‚û°Ô∏è `person`
    - `B-LOC` ‚û°Ô∏è `ort`
    - `I-LOC` ‚û°Ô∏è `ort`
    - `B-ORG` ‚û°Ô∏è `organisation`
    - `I-ORG` ‚û°Ô∏è `organisation`
    - `B-MISC` ‚û°Ô∏è `verschiedenes`
    - `I-MISC` ‚û°Ô∏è `verschiedenes`

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset germeval
```


## Linguistic Acceptability

### ScaLA-de

[description]

[size-info]

Here are a few examples from the training split:

```json
[example-1]
```
```json
[example-2]
```
```json
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 12
- Prefix prompt:
  ```
  Die folgenden S√§tze und ob sie grammatikalisch korrekt sind.
  ```
- Base prompt template:
  ```
  Satz: {text}
  Grammatikalisch richtig: {label}
  ```
- Instruction-tuned prompt template:
  ```
  Satz: {text}

  Bestimmen Sie, ob der Satz grammatikalisch korrekt ist oder nicht. Antworten Sie mit 'ja', wenn der Satz korrekt ist und 'nein', wenn er es nicht ist.
  ```
- Label mapping:
    - `correct` ‚û°Ô∏è `ja`
    - `incorrect` ‚û°Ô∏è `nein`

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset scala-de
```


## Reading Comprehension

### GermanQuAD

[description]

[size-info]

Here are a few examples from the training split:

```json
[example-1]
```
```json
[example-2]
```
```json
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 4
- Prefix prompt:
  ```
  Im Folgenden finden Sie Texte mit den dazugeh√∂rigen Fragen und Antworten.
  ```
- Base prompt template:
  ```
  Text: {text}
  Fragen: {question}
  Fragen Antwort in maximal 3 W√∂rtern: {label}
  ```
- Instruction-tuned prompt template:
  ```
  Text: {text}

  Beantworten Sie die folgende Frage zum obigen Text in h√∂chstens 3 W√∂rtern.

  Frage: {question}
  ```

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset germanquad
```


## Knowledge

### MMLU-de

[description]

[size-info]

Here are a few examples from the training split:

```json
[example-1]
```
```json
[example-2]
```
```json
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 5
- Prefix prompt:
  ```
  Die folgenden Fragen sind Multiple-Choice-Fragen (mit Antworten).
  ```
- Base prompt template:
  ```
  Frage: {text}
  Antwort: {label}
  ```
- Instruction-tuned prompt template:
  ```
  Frage: {text}
  Antwortm√∂glichkeiten:
  a. {option_a}
  b. {option_b}
  c. {option_c}
  d. {option_d}

  Beantworten Sie die obige Frage mit 'a', 'b', 'c' oder 'd'.
  ```

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset mmlu-de
```


### Unofficial: ARC-de

[description]

[size-info]

Here are a few examples from the training split:

```json
[example-1]
```
```json
[example-2]
```
```json
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 5
- Prefix prompt:
  ```
  Die folgenden Fragen sind Multiple-Choice-Fragen (mit Antworten).
  ```
- Base prompt template:
  ```
  Frage: {text}
  Antwort: {label}
  ```
- Instruction-tuned prompt template:
  ```
  Frage: {text}
  Antwortm√∂glichkeiten:
  a. {option_a}
  b. {option_b}
  c. {option_c}
  d. {option_d}

  Beantworten Sie die obige Frage mit 'a', 'b', 'c' oder 'd'.
  ```

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset arc-de
```


## Common-sense Reasoning

### HellaSwag-de

[description]

[size-info]

Here are a few examples from the training split:

```json
[example-1]
```
```json
[example-2]
```
```json
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 5
- Prefix prompt:
  ```
  Die folgenden Fragen sind Multiple-Choice-Fragen (mit Antworten).
  ```
- Base prompt template:
  ```
  Frage: {text}
  Antwort: {label}
  ```
- Instruction-tuned prompt template:
  ```
  Frage: {text}
  Antwortm√∂glichkeiten:
  a. {option_a}
  b. {option_b}
  c. {option_c}
  d. {option_d}

  Beantworten Sie die obige Frage mit 'a', 'b', 'c' oder 'd'.
  ```

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset hellaswag-de
```


## Summarization

### MLSum

[description]

[size-info]

Here are a few examples from the training split:

```json
[example-1]
```
```json
[example-2]
```
```json
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 1
- Prefix prompt:
  ```
  Im Folgenden finden Sie Nachrichtenartikel mit den dazugeh√∂rigen Zusammenfassungen.
  ```
- Base prompt template:
  ```
  Nachrichtenartikel: {text}
  Zusammenfassung: {target_text}
  ```
- Instruction-tuned prompt template:
  ```
  Nachrichtenartikel: {text}

  Schreiben Sie eine Zusammenfassung des obigen Artikels.
  ```

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset mlsum
```
